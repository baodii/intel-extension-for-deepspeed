commit cbfad658e599831ce1470f8f3c2bcf774d6120d4
Author: baodii <di.bao@intel.com>
Date:   Mon Dec 18 01:39:08 2023 -0800

    update idex dpct OpBuilders

diff --git a/intel_extension_for_deepspeed/op_builder/__init__.py b/intel_extension_for_deepspeed/op_builder/__init__.py
index 18e4cff..067e668 100755
--- a/intel_extension_for_deepspeed/op_builder/__init__.py
+++ b/intel_extension_for_deepspeed/op_builder/__init__.py
@@ -2,7 +2,6 @@ from .builder import OpBuilder
 from .cpu_adam import CPUAdamBuilder
 from .cpu_adagrad import CPUAdagradBuilder
 from .fused_adam import FusedAdamBuilder
-from .transformer import TransformerBuilder
 from .transformer_inference import InferenceBuilder
 from .quantizer import QuantizerBuilder
 from .utils import UtilsBuilder
diff --git a/intel_extension_for_deepspeed/op_builder/async_io.py b/intel_extension_for_deepspeed/op_builder/async_io.py
index 075d775..9b8a643 100644
--- a/intel_extension_for_deepspeed/op_builder/async_io.py
+++ b/intel_extension_for_deepspeed/op_builder/async_io.py
@@ -5,10 +5,11 @@
 
 import distutils.spawn
 import subprocess
+import torch
 
-from .builder import SYCLOpBuilder, sycl_kernel_path, sycl_kernel_include
+from deepspeed.ops.op_builder.builder import OpBuilder, TORCH_MAJOR, TORCH_MINOR
 
-class AsyncIOBuilder(SYCLOpBuilder):
+class AsyncIOBuilder(OpBuilder):
     BUILD_VAR = "DS_BUILD_AIO"
     NAME = "async_io"
 
@@ -24,7 +25,7 @@ class AsyncIOBuilder(SYCLOpBuilder):
             'csrc/aio/py_lib/deepspeed_py_aio.cpp', 'csrc/aio/py_lib/deepspeed_py_aio_handle.cpp',
             'csrc/aio/py_lib/deepspeed_aio_thread.cpp', 'csrc/aio/common/deepspeed_aio_utils.cpp',
             'csrc/aio/common/deepspeed_aio_common.cpp', 'csrc/aio/common/deepspeed_aio_types.cpp',
-            'csrc/aio/py_lib/deepspeed_pin_tensor.cpp',
+            'csrc/aio/py_lib/deepspeed_pin_tensor.cpp'
         ]
 
     def include_paths(self):
@@ -34,11 +35,16 @@ class AsyncIOBuilder(SYCLOpBuilder):
         # -O0 for improved debugging, since performance is bound by I/O
         CPU_ARCH = self.cpu_arch()
         SIMD_WIDTH = self.simd_width()
+        TORCH_MAJOR, TORCH_MINOR = map(int, torch.__version__.split('.')[0:2])
+        if TORCH_MAJOR >= 2 and TORCH_MINOR >= 1:
+            CPP_STD = '-std=c++17'
+        else:
+            CPP_STD = '-std=c++14'
         return [
             '-g',
             '-Wall',
             '-O0',
-            '-std=c++20',
+            CPP_STD,
             '-shared',
             '-fPIC',
             '-Wno-reorder',
@@ -49,7 +55,7 @@ class AsyncIOBuilder(SYCLOpBuilder):
         ]
 
     def extra_ldflags(self):
-        return []
+        return ['-laio']
 
     def check_for_libaio_pkg(self):
         libs = dict(
@@ -78,7 +84,7 @@ class AsyncIOBuilder(SYCLOpBuilder):
         # which is a function provided by libaio that is used in the async_io op.
         # If needed, one can define -I and -L entries in CFLAGS and LDFLAGS
         # respectively to specify the directories for libaio.h and libaio.so.
-        aio_compatible = self.has_function('io_submit', ('aio', ))
+        aio_compatible = self.has_function('io_pgetevents', ('aio', ))
         if verbose and not aio_compatible:
             self.warning(f"{self.NAME} requires the dev libaio .so object and headers but these were not found.")
 
diff --git a/intel_extension_for_deepspeed/op_builder/cpu_adagrad.py b/intel_extension_for_deepspeed/op_builder/cpu_adagrad.py
index 796fb93..f653c18 100644
--- a/intel_extension_for_deepspeed/op_builder/cpu_adagrad.py
+++ b/intel_extension_for_deepspeed/op_builder/cpu_adagrad.py
@@ -16,13 +16,11 @@ class CPUAdagradBuilder(SYCLOpBuilder):
 
     def sources(self):
         return [
-            sycl_kernel_path('csrc/adagrad/cpu_adagrad.dp.cpp'),
-            sycl_kernel_path('csrc/adam/custom_sycl_kernel.dp.cpp'),
+            sycl_kernel_path('csrc/adagrad/cpu_adagrad.cpp'),
+            sycl_kernel_path('csrc/common/custom_cuda_kernel.dp.cpp'),
         ]
 
     def include_paths(self):
         return [
             sycl_kernel_include('csrc/includes'),
-            sycl_kernel_include('csrc/adagrad'),
-            'csrc/includes'
         ]
diff --git a/intel_extension_for_deepspeed/op_builder/cpu_adam.py b/intel_extension_for_deepspeed/op_builder/cpu_adam.py
index f0bd5ca..d67292f 100644
--- a/intel_extension_for_deepspeed/op_builder/cpu_adam.py
+++ b/intel_extension_for_deepspeed/op_builder/cpu_adam.py
@@ -16,8 +16,9 @@ class CPUAdamBuilder(SYCLOpBuilder):
 
     def sources(self):
         return [
-            sycl_kernel_path('csrc/adam/cpu_adam.dp.cpp'),
-            sycl_kernel_path('csrc/adam/custom_sycl_kernel.dp.cpp'),
+            sycl_kernel_path('csrc/adam/cpu_adam.cpp'),
+            sycl_kernel_path('csrc/adam/cpu_adam_impl.cpp'),
+            sycl_kernel_path('csrc/common/custom_cuda_kernel.dp.cpp'),
         ]
 
     def libraries_args(self):
@@ -27,5 +28,4 @@ class CPUAdamBuilder(SYCLOpBuilder):
     def include_paths(self):
         return [
             sycl_kernel_include('csrc/includes'),
-            sycl_kernel_include('csrc/adam'), 'csrc/includes'
         ]
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/adagrad/cpu_adagrad.dp.cpp b/intel_extension_for_deepspeed/op_builder/csrc/adagrad/cpu_adagrad.cpp
similarity index 69%
rename from intel_extension_for_deepspeed/op_builder/csrc/adagrad/cpu_adagrad.dp.cpp
rename to intel_extension_for_deepspeed/op_builder/csrc/adagrad/cpu_adagrad.cpp
index a872c3e..ce4775b 100644
--- a/intel_extension_for_deepspeed/op_builder/csrc/adagrad/cpu_adagrad.dp.cpp
+++ b/intel_extension_for_deepspeed/op_builder/csrc/adagrad/cpu_adagrad.cpp
@@ -1,49 +1,57 @@
-#if __has_include(<sycl/sycl.hpp>)
-#include <sycl/sycl.hpp>
-#elif __has_include(<CL/sycl.hpp>)
-#include <CL/sycl.hpp>
-#else
-#error "Unsupported compiler"
-#endif
-#include "cpu_adagrad.hpp"
-#include <math.h>
-#include <omp.h>
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+#include "cpu_adagrad.h"
 #include <torch/extension.h>
 #include <iostream>
 #include <memory>
 #include <type_traits>
 #include <unordered_map>
-#include <oneapi/mkl.hpp>
-#include <oneapi/mkl/rng/device.hpp>
-#include "custom_sycl_layers.hpp"
+#include <cmath>
 
+#if defined(__ENABLE_CUDA__)
+#include <cuda_runtime_api.h>
+#include "cublas_v2.h"
+#include "cuda.h"
+#include "curand.h"
+#include "custom_cuda_layers.h"
+#endif
 
 static std::unordered_map<int, std::shared_ptr<void>> s_optimizers;
 
+// C++ interface
 
 void Adagrad_Optimizer::Step_1(float* _params,
                                float* grads,
                                float* _exp_avg_sq,
                                size_t _param_size,
-                               sycl::half* dev_params,
+                               ds_half_precision_t* dev_params,
                                bool half_precision)
 {
     size_t rounded_size = 0;
-
-    if(_param_size > rounded_size) {
-        sycl::half* grads_cast_h;
-        sycl::half* params_cast_h;
+#if defined(__AVX512__) or defined(__AVX256__)
+    Step_AVX<1>(
+        &rounded_size, _params, grads, _exp_avg_sq, _param_size, dev_params, half_precision);
+#endif
+    if (_param_size > rounded_size) {
+        float step_size = -1 * _alpha;
+        ds_half_precision_t* grads_cast_h;
+        ds_half_precision_t* params_cast_h;
         if (half_precision) {
-            grads_cast_h = reinterpret_cast<sycl::half*>(grads);
-            params_cast_h = reinterpret_cast<sycl::half*>(_params);
+            grads_cast_h = reinterpret_cast<ds_half_precision_t*>(grads);
+            params_cast_h = reinterpret_cast<ds_half_precision_t*>(_params);
         }
-
-        float step_size = -1 * _alpha;
         for (size_t t = rounded_size; t < _param_size; t += TILE) {
             size_t copy_size = TILE;
             if ((t + TILE) > _param_size) copy_size = _param_size - t;
             size_t offset = copy_size + t;
-            if ((t / TILE) >= 2) { _streams[_buf_index]->wait(); }
+#if defined(__ENABLE_CUDA__)
+            if ((t / TILE) >= 2) { cudaStreamSynchronize(_streams[_buf_index]); }
+#elif defined(__ENABLE_CANN__)
+            if ((t / TILE) >= 2) { aclrtSynchronizeStream(_streams[_buf_index].stream()); }
+#endif
 #pragma omp parallel for
             for (size_t k = t; k < offset; k++) {
                 float grad = half_precision ? (float)grads_cast_h[k] : grads[k];
@@ -58,21 +66,35 @@ void Adagrad_Optimizer::Step_1(float* _params,
                 grad += _eps;
                 grad = momentum / grad;
                 param = grad * step_size + param;
+#if defined(__ENABLE_CUDA__) or defined(__ENABLE_CANN__)
                 if (dev_params) _doubled_buffer[_buf_index][k - t] = param;
-
+#endif
                 if (half_precision)
-                    params_cast_h[k] = (sycl::half)param;
+                    params_cast_h[k] = (ds_half_precision_t)param;
                 else
                     _params[k] = param;
                 // STORE UPDATE TERM TO GRAD'S MEMORY
                 grads[k] = grad * step_size;
                 _exp_avg_sq[k] = variance;
             }
+#if defined(__ENABLE_CUDA__)
             if (dev_params) {
                 launch_param_update(
                     _doubled_buffer[_buf_index], dev_params + t, (copy_size), _streams[_buf_index]);
                 _buf_index = !_buf_index;
             }
+#elif defined(__ENABLE_CANN__)
+            if (dev_params) {
+                size_t memcpy_size = copy_size * sizeof(_doubled_buffer[_buf_index][0]);
+                aclrtMemcpy(dev_params + t,
+                            memcpy_size,
+                            _doubled_buffer[_buf_index],
+                            memcpy_size,
+                            aclrtMemcpyKind::ACL_MEMCPY_HOST_TO_DEVICE);
+
+                _buf_index = !_buf_index;
+            }
+#endif
         }
     }
 }
@@ -81,12 +103,15 @@ void Adagrad_Optimizer::Step_4(float* _params,
                                float* grads,
                                float* _exp_avg_sq,
                                size_t _param_size,
-                               sycl::half* dev_params,
+                               ds_half_precision_t* dev_params,
                                bool half_precision)
 {
     size_t rounded_size = 0;
-
-     if (_param_size > rounded_size)
+#if defined(__AVX512__) or defined(__AVX256__)
+    Step_AVX<4>(
+        &rounded_size, _params, grads, _exp_avg_sq, _param_size, dev_params, half_precision);
+#endif
+    if (_param_size > rounded_size)
         Step_1((_params + rounded_size),
                (grads + rounded_size),
                (_exp_avg_sq + rounded_size),
@@ -95,7 +120,6 @@ void Adagrad_Optimizer::Step_4(float* _params,
                half_precision);
 }
 
-
 int create_adagrad_optimizer(int optimizer_id,
                              float alpha = 1e-2,
                              float eps = 1e-8,
@@ -108,25 +132,38 @@ int create_adagrad_optimizer(int optimizer_id,
 
     if (should_log) {
         std::string avx_type = "";
-    return 0;
-            printf("Adagrad Optimizer #%d is created with %s arithmetic capability.\n",
+#if defined(__AVX512__)
+        avx_type = "AVX512";
+#else
+#if defined(__AVX256__)
+        avx_type = "AVX2";
+#else
+        avx_type = "scalar";
+#endif
+#endif
+
+        printf("Adagrad Optimizer #%d is created with %s arithmetic capability.\n",
                optimizer_id,
                avx_type.c_str());
         printf("Config: alpha=%f, weight_decay=%f\n", alpha, weight_decay);
     }
+
     return 0;
 }
 
-
 void Adagrad_Optimizer::Step_8(float* _params,
                                float* grads,
                                float* _exp_avg_sq,
                                size_t _param_size,
-                               sycl::half* dev_params,
+                               ds_half_precision_t* dev_params,
                                bool half_precision)
 {
     size_t rounded_size = 0;
-        if (_param_size > rounded_size)
+#if defined(__AVX512__) or defined(__AVX256__)
+    Step_AVX<8>(
+        &rounded_size, _params, grads, _exp_avg_sq, _param_size, dev_params, half_precision);
+#endif
+    if (_param_size > rounded_size)
         Step_4((_params + rounded_size),
                (grads + rounded_size),
                (_exp_avg_sq + rounded_size),
@@ -134,6 +171,7 @@ void Adagrad_Optimizer::Step_8(float* _params,
                (dev_params != nullptr ? (dev_params + rounded_size) : dev_params),
                half_precision);
 }
+
 int ds_adagrad_step(int optimizer_id,
                     size_t step,
                     float lr,
@@ -155,13 +193,14 @@ int ds_adagrad_step(int optimizer_id,
         std::static_pointer_cast<Adagrad_Optimizer>(s_optimizers[optimizer_id]);
     opt->IncrementStep(step);
     opt->update_state(lr, epsilon, weight_decay);
-    opt->Step_8(params_ptr, grads_ptr, exp_avg_sq_ptr, params_c.size(0));
+    opt->Step_8(params_ptr, grads_ptr, exp_avg_sq_ptr, params_c.numel());
 
+#if defined(__ENABLE_CUDA__) or defined(__ENABLE_CANN__)
     opt->SynchronizeStreams();
+#endif
     return 0;
 }
 
-
 int ds_adagrad_step_plus_copy(int optimizer_id,
                               size_t step,
                               float lr,
@@ -172,6 +211,7 @@ int ds_adagrad_step_plus_copy(int optimizer_id,
                               torch::Tensor& exp_avg_sq,
                               torch::Tensor& gpu_params)
 {
+#if defined(__ENABLE_CUDA__) or defined(__ENABLE_CANN__)
     auto params_c = params.contiguous();
     auto gpu_params_c = gpu_params.contiguous();
     auto exp_avg_sq_c = exp_avg_sq.contiguous();
@@ -179,7 +219,7 @@ int ds_adagrad_step_plus_copy(int optimizer_id,
 
     float* params_ptr = (float*)params_c.data_ptr();
     float* grads_ptr = (float*)grads_c.data_ptr();
-    sycl::half* gpu_params_ptr = (sycl::half*)gpu_params_c.data_ptr();
+    ds_half_precision_t* gpu_params_ptr = (ds_half_precision_t*)gpu_params_c.data_ptr();
     float* exp_avg_sq_ptr = (float*)exp_avg_sq_c.data_ptr();
 
     std::shared_ptr<Adagrad_Optimizer> opt =
@@ -189,21 +229,24 @@ int ds_adagrad_step_plus_copy(int optimizer_id,
     opt->Step_8(params_ptr,
                 grads_ptr,
                 exp_avg_sq_ptr,
-                params_c.size(0),
-                gpu_params_ptr
-                );
+                params_c.numel(),
+                gpu_params_ptr,
+                (params.options().dtype() == at::kHalf));
 
     opt->SynchronizeStreams();
+#else
+    assert(false);
+#endif
     return 0;
 }
 
-
 int destroy_adagrad_optimizer(int optimizer_id)
 {
     s_optimizers.erase(optimizer_id);
 
     return 0;
 }
+
 PYBIND11_MODULE(TORCH_EXTENSION_NAME, m)
 {
     m.def("adagrad_update", &ds_adagrad_step, "DeepSpeed CPU Adagrad update (C++)");
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/adam/cpu_adam.cpp b/intel_extension_for_deepspeed/op_builder/csrc/adam/cpu_adam.cpp
new file mode 100644
index 0000000..9680982
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/adam/cpu_adam.cpp
@@ -0,0 +1,16 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+#include "cpu_adam.h"
+
+PYBIND11_MODULE(TORCH_EXTENSION_NAME, m)
+{
+    m.def("adam_update", &ds_adam_step, "DeepSpeed CPU Adam update (C++)");
+    m.def("adam_update_copy",
+          &ds_adam_step_plus_copy,
+          "DeepSpeed CPU Adam update and param copy (C++)");
+    m.def("create_adam", &create_adam_optimizer, "DeepSpeed CPU Adam (C++)");
+    m.def("destroy_adam", &destroy_adam_optimizer, "DeepSpeed CPU Adam destroy (C++)");
+}
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/adam/cpu_adam.dp.cpp b/intel_extension_for_deepspeed/op_builder/csrc/adam/cpu_adam.dp.cpp
deleted file mode 100644
index 5d8a72b..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/adam/cpu_adam.dp.cpp
+++ /dev/null
@@ -1,713 +0,0 @@
-#include "cpu_adam.hpp"
-#include <math.h>
-#include <omp.h>
-#include <torch/extension.h>
-#if __has_include(<sycl/sycl.hpp>)
-#include <sycl/sycl.hpp>
-#elif __has_include(<CL/sycl.hpp>)
-#include <CL/sycl.hpp>
-#else
-#error "Unsupported compiler"
-#endif
-#include <iostream>
-#include <memory>
-#include <oneapi/mkl.hpp>
-#include <oneapi/mkl/rng/device.hpp>
-#include <type_traits>
-#include <unordered_map>
-#include "custom_sycl_layers.hpp"
-
-static std::unordered_map<int, std::shared_ptr<void>> s_optimizers;
-
-#define ROUND_DOWN(size, step) ((size) & ~((step)-1))
-
-// C++ interface
-
-void Adam_Optimizer::Step(float* _params,
-                          float* grads,
-                          float* _exp_avg,
-                          float* _exp_avg_sq,
-                          size_t _param_size,
-                          sycl::half* dev_params,
-                          bool half_precision)
-{
-    sycl::half* grads_cast_h;
-    sycl::half* params_cast_h;
-    if (half_precision) {
-        grads_cast_h = reinterpret_cast<sycl::half*>(grads);
-        params_cast_h = reinterpret_cast<sycl::half*>(_params);
-    }
-
-    float betta1_minus1 = 1 - _betta1;
-    float betta2_minus1 = 1 - _betta2;
-
-    float step_size = -1 * _alpha / _bias_correction1;
-    float w_decay = -1 * _alpha * _weight_decay;
-    size_t rounded_size = 0;
-
-#if defined(__AVX512__) or defined(__AVX256__)
-
-    AVX_Data betta1_4;
-    betta1_4.data = SIMD_SET(_betta1);
-    AVX_Data betta2_4;
-    betta2_4.data = SIMD_SET(_betta2);
-
-    AVX_Data betta1_minus1_4;
-    betta1_minus1_4.data = SIMD_SET(betta1_minus1);
-    AVX_Data betta2_minus1_4;
-    betta2_minus1_4.data = SIMD_SET(betta2_minus1);
-
-    AVX_Data bias2_sqrt;
-    bias2_sqrt.data = SIMD_SET(_bias_correction2);
-
-    AVX_Data eps_4;
-    eps_4.data = SIMD_SET(_eps);
-
-    AVX_Data step_size_4;
-    step_size_4.data = SIMD_SET(step_size);
-
-    AVX_Data weight_decay4;
-    if (_weight_decay > 0)
-        weight_decay4.data = (_adamw_mode ? SIMD_SET(w_decay) : SIMD_SET(_weight_decay));
-    rounded_size = ROUND_DOWN(_param_size, SIMD_WIDTH);
-
-    for (size_t t = 0; t < rounded_size; t += TILE) {
-        size_t copy_size = TILE;
-        if ((t + TILE) > rounded_size) copy_size = rounded_size - t;
-        size_t offset = copy_size + t;
-        if ((t / TILE) >= 2) { _streams[_buf_index]->wait(); }
-
-#pragma omp parallel for
-        for (size_t i = t; i < offset; i += SIMD_WIDTH) {
-            AVX_Data grad_4;
-            grad_4.data = SIMD_LOAD(grads + i);
-
-            AVX_Data momentum_4;
-            momentum_4.data = SIMD_LOAD(_exp_avg + i);
-            AVX_Data variance_4;
-            variance_4.data = SIMD_LOAD(_exp_avg_sq + i);
-
-            AVX_Data param_4;
-            param_4.data = SIMD_LOAD(_params + i);
-
-            if (_weight_decay > 0 && !_adamw_mode) {
-                grad_4.data = SIMD_FMA(param_4.data, weight_decay4.data, grad_4.data);
-            }
-            momentum_4.data = SIMD_MUL(momentum_4.data, betta1_4.data);
-            momentum_4.data = SIMD_FMA(grad_4.data, betta1_minus1_4.data, momentum_4.data);
-
-            variance_4.data = SIMD_MUL(variance_4.data, betta2_4.data);
-            grad_4.data = SIMD_MUL(grad_4.data, grad_4.data);
-            variance_4.data = SIMD_FMA(grad_4.data, betta2_minus1_4.data, variance_4.data);
-
-            grad_4.data = SIMD_SQRT(variance_4.data);
-            grad_4.data = SIMD_FMA(grad_4.data, bias2_sqrt.data, eps_4.data);
-            grad_4.data = SIMD_DIV(momentum_4.data, grad_4.data);
-            if (_weight_decay > 0 && _adamw_mode) {
-                param_4.data = SIMD_FMA(param_4.data, weight_decay4.data, param_4.data);
-            }
-            param_4.data = SIMD_FMA(grad_4.data, step_size_4.data, param_4.data);
-
-            SIMD_STORE(_params + i, param_4.data);
-
-            if (dev_params) SIMD_STORE(_doubled_buffer[_buf_index] + (i - t), param_4.data);
-
-            SIMD_STORE(_exp_avg + i, momentum_4.data);
-            SIMD_STORE(_exp_avg_sq + i, variance_4.data);
-        }
-        if (dev_params) {
-            launch_param_update(
-                _doubled_buffer[_buf_index], dev_params + t, copy_size, _streams[_buf_index]);
-            _buf_index = !_buf_index;
-        }
-    }
-
-#endif
-
-    if (_param_size > rounded_size) {
-        for (size_t t = rounded_size; t < _param_size; t += TILE) {
-            size_t copy_size = TILE;
-            if ((t + TILE) > _param_size) copy_size = _param_size - t;
-            size_t offset = copy_size + t;
-            if ((t / TILE) >= 2) { _streams[_buf_index]->wait(); }
-#pragma omp parallel for
-            for (size_t k = t; k < offset; k++) {
-                float grad = half_precision ? (float)grads_cast_h[k] : grads[k];
-                float param = half_precision ? (float)params_cast_h[k] : _params[k];
-                float momentum = _exp_avg[k];
-                float variance = _exp_avg_sq[k];
-                if (_weight_decay > 0 && !_adamw_mode) { grad = param * _weight_decay + grad; }
-                momentum = momentum * _betta1;
-                momentum = grad * betta1_minus1 + momentum;
-
-                variance = variance * _betta2;
-                grad = grad * grad;
-                variance = grad * betta2_minus1 + variance;
-
-                grad = sqrt(variance);
-                grad = grad * _bias_correction2 + _eps;
-                grad = momentum / grad;
-                if (_weight_decay > 0 && _adamw_mode) { param += w_decay * param; }
-                param = grad * step_size + param;
-                if (dev_params) _doubled_buffer[_buf_index][k - t] = param;
-
-                if (half_precision)
-                    params_cast_h[k] = (sycl::half)param;
-                else
-                    _params[k] = param;
-
-                // _params[k] = param;
-                _exp_avg[k] = momentum;
-                _exp_avg_sq[k] = variance;
-            }
-            if (dev_params) {
-                launch_param_update(
-                    _doubled_buffer[_buf_index], dev_params + t, (copy_size), _streams[_buf_index]);
-                _buf_index = !_buf_index;
-            }
-        }
-    }
-}
-
-void Adam_Optimizer::Step_4(float* _params,
-                            float* grads,
-                            float* _exp_avg,
-                            float* _exp_avg_sq,
-                            size_t _param_size,
-                            sycl::half* dev_params,
-                            bool half_precision)
-{
-    size_t rounded_size = 0;
-
-#if defined(__AVX512__) or defined(__AVX256__)
-
-    AVX_Data betta1_4;
-    betta1_4.data = SIMD_SET(_betta1);
-    AVX_Data betta2_4;
-    betta2_4.data = SIMD_SET(_betta2);
-
-    float betta1_minus1 = 1 - _betta1;
-    float betta2_minus1 = 1 - _betta2;
-    AVX_Data betta1_minus1_4;
-    betta1_minus1_4.data = SIMD_SET(betta1_minus1);
-    AVX_Data betta2_minus1_4;
-    betta2_minus1_4.data = SIMD_SET(betta2_minus1);
-
-    AVX_Data bias2_sqrt;
-    bias2_sqrt.data = SIMD_SET(_bias_correction2);
-
-    AVX_Data eps_4;
-    eps_4.data = SIMD_SET(_eps);
-
-    float step_size = -1 * _alpha / _bias_correction1;
-    AVX_Data step_size_4;
-    step_size_4.data = SIMD_SET(step_size);
-
-    float w_decay = -1 * _alpha * _weight_decay;
-    AVX_Data weight_decay4;
-    if (_weight_decay > 0)
-        weight_decay4.data = (_adamw_mode ? SIMD_SET(w_decay) : SIMD_SET(_weight_decay));
-    rounded_size = ROUND_DOWN(_param_size, (SIMD_WIDTH << 2));
-
-    for (size_t t = 0; t < rounded_size; t += TILE) {
-        size_t copy_size = TILE;
-        if ((t + TILE) > rounded_size) copy_size = rounded_size - t;
-        size_t offset = copy_size + t;
-        if ((t / TILE) >= 2) { _streams[_buf_index]->wait(); }
-#pragma omp parallel for
-        for (size_t i = t; i < offset; i += (SIMD_WIDTH << 2)) {
-            AVX_Data grad_4[4];
-            grad_4[0].data = SIMD_LOAD(grads + i);
-            grad_4[1].data = SIMD_LOAD(grads + i + SIMD_WIDTH);
-            grad_4[2].data = SIMD_LOAD(grads + i + (SIMD_WIDTH << 1));
-            grad_4[3].data = SIMD_LOAD(grads + i + SIMD_WIDTH * 3);
-
-            AVX_Data momentum_4[4];
-            momentum_4[0].data = SIMD_LOAD(_exp_avg + i);
-            momentum_4[1].data = SIMD_LOAD(_exp_avg + i + SIMD_WIDTH);
-            momentum_4[2].data = SIMD_LOAD(_exp_avg + i + (SIMD_WIDTH << 1));
-            momentum_4[3].data = SIMD_LOAD(_exp_avg + i + SIMD_WIDTH * 3);
-
-            AVX_Data variance_4[4];
-            variance_4[0].data = SIMD_LOAD(_exp_avg_sq + i);
-            variance_4[1].data = SIMD_LOAD(_exp_avg_sq + i + SIMD_WIDTH);
-            variance_4[2].data = SIMD_LOAD(_exp_avg_sq + i + (SIMD_WIDTH << 1));
-            variance_4[3].data = SIMD_LOAD(_exp_avg_sq + i + SIMD_WIDTH * 3);
-
-            AVX_Data param_4[4];
-            param_4[0].data = SIMD_LOAD(_params + i);
-            param_4[1].data = SIMD_LOAD(_params + i + SIMD_WIDTH);
-            param_4[2].data = SIMD_LOAD(_params + i + (SIMD_WIDTH << 1));
-            param_4[3].data = SIMD_LOAD(_params + i + SIMD_WIDTH * 3);
-
-            if (_weight_decay > 0 && !_adamw_mode) {
-                grad_4[0].data = SIMD_FMA(param_4[0].data, weight_decay4.data, grad_4[0].data);
-                grad_4[1].data = SIMD_FMA(param_4[1].data, weight_decay4.data, grad_4[1].data);
-                grad_4[2].data = SIMD_FMA(param_4[2].data, weight_decay4.data, grad_4[2].data);
-                grad_4[3].data = SIMD_FMA(param_4[3].data, weight_decay4.data, grad_4[3].data);
-            }
-
-            momentum_4[0].data = SIMD_MUL(momentum_4[0].data, betta1_4.data);
-            momentum_4[0].data = SIMD_FMA(grad_4[0].data, betta1_minus1_4.data, momentum_4[0].data);
-            momentum_4[1].data = SIMD_MUL(momentum_4[1].data, betta1_4.data);
-            momentum_4[1].data = SIMD_FMA(grad_4[1].data, betta1_minus1_4.data, momentum_4[1].data);
-            momentum_4[2].data = SIMD_MUL(momentum_4[2].data, betta1_4.data);
-            momentum_4[2].data = SIMD_FMA(grad_4[2].data, betta1_minus1_4.data, momentum_4[2].data);
-            momentum_4[3].data = SIMD_MUL(momentum_4[3].data, betta1_4.data);
-            momentum_4[3].data = SIMD_FMA(grad_4[3].data, betta1_minus1_4.data, momentum_4[3].data);
-
-            variance_4[0].data = SIMD_MUL(variance_4[0].data, betta2_4.data);
-            variance_4[1].data = SIMD_MUL(variance_4[1].data, betta2_4.data);
-            variance_4[2].data = SIMD_MUL(variance_4[2].data, betta2_4.data);
-            variance_4[3].data = SIMD_MUL(variance_4[3].data, betta2_4.data);
-            grad_4[0].data = SIMD_MUL(grad_4[0].data, grad_4[0].data);
-            grad_4[1].data = SIMD_MUL(grad_4[1].data, grad_4[1].data);
-            grad_4[2].data = SIMD_MUL(grad_4[2].data, grad_4[2].data);
-            grad_4[3].data = SIMD_MUL(grad_4[3].data, grad_4[3].data);
-            variance_4[0].data = SIMD_FMA(grad_4[0].data, betta2_minus1_4.data, variance_4[0].data);
-            variance_4[1].data = SIMD_FMA(grad_4[1].data, betta2_minus1_4.data, variance_4[1].data);
-            variance_4[2].data = SIMD_FMA(grad_4[2].data, betta2_minus1_4.data, variance_4[2].data);
-            variance_4[3].data = SIMD_FMA(grad_4[3].data, betta2_minus1_4.data, variance_4[3].data);
-
-            grad_4[0].data = SIMD_SQRT(variance_4[0].data);
-            grad_4[1].data = SIMD_SQRT(variance_4[1].data);
-            grad_4[2].data = SIMD_SQRT(variance_4[2].data);
-            grad_4[3].data = SIMD_SQRT(variance_4[3].data);
-
-            grad_4[0].data = SIMD_FMA(grad_4[0].data, bias2_sqrt.data, eps_4.data);
-            grad_4[1].data = SIMD_FMA(grad_4[1].data, bias2_sqrt.data, eps_4.data);
-            grad_4[2].data = SIMD_FMA(grad_4[2].data, bias2_sqrt.data, eps_4.data);
-            grad_4[3].data = SIMD_FMA(grad_4[3].data, bias2_sqrt.data, eps_4.data);
-            grad_4[0].data = SIMD_DIV(momentum_4[0].data, grad_4[0].data);
-            grad_4[1].data = SIMD_DIV(momentum_4[1].data, grad_4[1].data);
-            grad_4[2].data = SIMD_DIV(momentum_4[2].data, grad_4[2].data);
-            grad_4[3].data = SIMD_DIV(momentum_4[3].data, grad_4[3].data);
-
-            if (_weight_decay > 0 && _adamw_mode) {
-                param_4[0].data = SIMD_FMA(param_4[0].data, weight_decay4.data, param_4[0].data);
-                param_4[1].data = SIMD_FMA(param_4[1].data, weight_decay4.data, param_4[1].data);
-                param_4[2].data = SIMD_FMA(param_4[2].data, weight_decay4.data, param_4[2].data);
-                param_4[3].data = SIMD_FMA(param_4[3].data, weight_decay4.data, param_4[3].data);
-            }
-
-            param_4[0].data = SIMD_FMA(grad_4[0].data, step_size_4.data, param_4[0].data);
-            param_4[1].data = SIMD_FMA(grad_4[1].data, step_size_4.data, param_4[1].data);
-            param_4[2].data = SIMD_FMA(grad_4[2].data, step_size_4.data, param_4[2].data);
-            param_4[3].data = SIMD_FMA(grad_4[3].data, step_size_4.data, param_4[3].data);
-
-            SIMD_STORE(_params + i, param_4[0].data);
-            SIMD_STORE(_params + i + SIMD_WIDTH, param_4[1].data);
-            SIMD_STORE(_params + i + (SIMD_WIDTH << 1), param_4[2].data);
-            SIMD_STORE(_params + i + SIMD_WIDTH * 3, param_4[3].data);
-
-            if (dev_params) {
-                SIMD_STORE(_doubled_buffer[_buf_index] + (i - t), param_4[0].data);
-                SIMD_STORE(_doubled_buffer[_buf_index] + (i - t) + SIMD_WIDTH, param_4[1].data);
-                SIMD_STORE(_doubled_buffer[_buf_index] + (i - t) + (SIMD_WIDTH << 1),
-                           param_4[2].data);
-                SIMD_STORE(_doubled_buffer[_buf_index] + (i - t) + SIMD_WIDTH * 3, param_4[3].data);
-            }
-
-            SIMD_STORE(_exp_avg + i, momentum_4[0].data);
-            SIMD_STORE(_exp_avg + i + SIMD_WIDTH, momentum_4[1].data);
-            SIMD_STORE(_exp_avg + i + (SIMD_WIDTH << 1), momentum_4[2].data);
-            SIMD_STORE(_exp_avg + i + SIMD_WIDTH * 3, momentum_4[3].data);
-
-            SIMD_STORE(_exp_avg_sq + i, variance_4[0].data);
-            SIMD_STORE(_exp_avg_sq + i + SIMD_WIDTH, variance_4[1].data);
-            SIMD_STORE(_exp_avg_sq + i + (SIMD_WIDTH << 1), variance_4[2].data);
-            SIMD_STORE(_exp_avg_sq + i + SIMD_WIDTH * 3, variance_4[3].data);
-        }
-
-        if (dev_params) {
-            launch_param_update(
-                _doubled_buffer[_buf_index], dev_params + t, copy_size, _streams[_buf_index]);
-            _buf_index = !_buf_index;
-        }
-    }
-#endif
-    if (_param_size > rounded_size)
-        Step((_params + rounded_size),
-             (grads + rounded_size),
-             (_exp_avg + rounded_size),
-             (_exp_avg_sq + rounded_size),
-             (_param_size - rounded_size),
-             (dev_params != nullptr ? (dev_params + rounded_size) : dev_params),
-             half_precision);
-}
-
-int create_adam_optimizer(int optimizer_id,
-                          float alpha = 1e-3,
-                          float betta1 = 0.9,
-                          float betta2 = 0.999,
-                          float eps = 1e-8,
-                          float weight_decay = 0,
-                          bool adamw_mode = true,
-                          bool should_log = false)
-{
-    auto opt =
-        std::make_shared<Adam_Optimizer>(alpha, betta1, betta2, eps, weight_decay, adamw_mode);
-
-    s_optimizers[optimizer_id] = opt;
-
-    if (should_log) {
-        std::string avx_type = "";
-#if defined(__AVX512__)
-        avx_type = "AVX512";
-#else
-#if defined(__AVX256__)
-        avx_type = "AVX2";
-#else
-        avx_type = "scalar";
-#endif
-#endif
-
-        printf("Adam Optimizer #%d is created with %s arithmetic capability.\n",
-               optimizer_id,
-               avx_type.c_str());
-        printf("Config: alpha=%f, betas=(%f, %f), weight_decay=%f, adam_w=%d\n",
-               alpha,
-               betta1,
-               betta2,
-               weight_decay,
-               (int)adamw_mode);
-    }
-
-    return 0;
-}
-
-void Adam_Optimizer::Step_8(float* _params,
-                            float* grads,
-                            float* _exp_avg,
-                            float* _exp_avg_sq,
-                            size_t _param_size,
-                            sycl::half* dev_params,
-                            bool half_precision)
-{
-    size_t rounded_size = 0;
-
-#if defined(__AVX512__) or defined(__AVX256__)
-
-    AVX_Data betta1_4;
-    betta1_4.data = SIMD_SET(_betta1);
-    AVX_Data betta2_4;
-    betta2_4.data = SIMD_SET(_betta2);
-
-    float betta1_minus1 = 1 - _betta1;
-    float betta2_minus1 = 1 - _betta2;
-    AVX_Data betta1_minus1_4;
-    betta1_minus1_4.data = SIMD_SET(betta1_minus1);
-    AVX_Data betta2_minus1_4;
-    betta2_minus1_4.data = SIMD_SET(betta2_minus1);
-
-    AVX_Data bias2_sqrt;
-    bias2_sqrt.data = SIMD_SET(_bias_correction2);
-
-    AVX_Data eps_4;
-    eps_4.data = SIMD_SET(_eps);
-
-    float step_size = -1 * _alpha / _bias_correction1;
-    AVX_Data step_size_4;
-    step_size_4.data = SIMD_SET(step_size);
-
-    float w_decay = -1 * _alpha * _weight_decay;
-    AVX_Data weight_decay4;
-    if (_weight_decay > 0)
-        weight_decay4.data = (_adamw_mode ? SIMD_SET(w_decay) : SIMD_SET(_weight_decay));
-    rounded_size = ROUND_DOWN(_param_size, (SIMD_WIDTH << 3));
-
-    for (size_t t = 0; t < rounded_size; t += TILE) {
-        size_t copy_size = TILE;
-        if ((t + TILE) > rounded_size) copy_size = rounded_size - t;
-        size_t offset = copy_size + t;
-        if ((t / TILE) >= 2) { _streams[_buf_index]->wait(); }
-#pragma omp parallel for
-        for (size_t i = t; i < offset; i += (SIMD_WIDTH << 3)) {
-            AVX_Data grad_4[8];
-            grad_4[0].data = SIMD_LOAD(grads + i);
-            grad_4[1].data = SIMD_LOAD(grads + i + SIMD_WIDTH);
-            grad_4[2].data = SIMD_LOAD(grads + i + (SIMD_WIDTH << 1));
-            grad_4[3].data = SIMD_LOAD(grads + i + SIMD_WIDTH * 3);
-            grad_4[4].data = SIMD_LOAD(grads + i + (SIMD_WIDTH << 2));
-            grad_4[5].data = SIMD_LOAD(grads + i + SIMD_WIDTH * 5);
-            grad_4[6].data = SIMD_LOAD(grads + i + SIMD_WIDTH * 6);
-            grad_4[7].data = SIMD_LOAD(grads + i + SIMD_WIDTH * 7);
-
-            AVX_Data momentum_4[8];
-            momentum_4[0].data = SIMD_LOAD(_exp_avg + i);
-            momentum_4[1].data = SIMD_LOAD(_exp_avg + i + SIMD_WIDTH);
-            momentum_4[2].data = SIMD_LOAD(_exp_avg + i + (SIMD_WIDTH << 1));
-            momentum_4[3].data = SIMD_LOAD(_exp_avg + i + SIMD_WIDTH * 3);
-            momentum_4[4].data = SIMD_LOAD(_exp_avg + i + (SIMD_WIDTH << 2));
-            momentum_4[5].data = SIMD_LOAD(_exp_avg + i + SIMD_WIDTH * 5);
-            momentum_4[6].data = SIMD_LOAD(_exp_avg + i + SIMD_WIDTH * 6);
-            momentum_4[7].data = SIMD_LOAD(_exp_avg + i + SIMD_WIDTH * 7);
-
-            AVX_Data variance_4[8];
-            variance_4[0].data = SIMD_LOAD(_exp_avg_sq + i);
-            variance_4[1].data = SIMD_LOAD(_exp_avg_sq + i + SIMD_WIDTH);
-            variance_4[2].data = SIMD_LOAD(_exp_avg_sq + i + (SIMD_WIDTH << 1));
-            variance_4[3].data = SIMD_LOAD(_exp_avg_sq + i + SIMD_WIDTH * 3);
-            variance_4[4].data = SIMD_LOAD(_exp_avg_sq + i + (SIMD_WIDTH << 2));
-            variance_4[5].data = SIMD_LOAD(_exp_avg_sq + i + SIMD_WIDTH * 5);
-            variance_4[6].data = SIMD_LOAD(_exp_avg_sq + i + SIMD_WIDTH * 6);
-            variance_4[7].data = SIMD_LOAD(_exp_avg_sq + i + SIMD_WIDTH * 7);
-
-            AVX_Data param_4[8];
-            param_4[0].data = SIMD_LOAD(_params + i);
-            param_4[1].data = SIMD_LOAD(_params + i + SIMD_WIDTH);
-            param_4[2].data = SIMD_LOAD(_params + i + (SIMD_WIDTH << 1));
-            param_4[3].data = SIMD_LOAD(_params + i + SIMD_WIDTH * 3);
-            param_4[4].data = SIMD_LOAD(_params + i + (SIMD_WIDTH << 2));
-            param_4[5].data = SIMD_LOAD(_params + i + SIMD_WIDTH * 5);
-            param_4[6].data = SIMD_LOAD(_params + i + SIMD_WIDTH * 6);
-            param_4[7].data = SIMD_LOAD(_params + i + SIMD_WIDTH * 7);
-
-            if (_weight_decay > 0 && !_adamw_mode) {
-                grad_4[0].data = SIMD_FMA(param_4[0].data, weight_decay4.data, grad_4[0].data);
-                grad_4[1].data = SIMD_FMA(param_4[1].data, weight_decay4.data, grad_4[1].data);
-                grad_4[2].data = SIMD_FMA(param_4[2].data, weight_decay4.data, grad_4[2].data);
-                grad_4[3].data = SIMD_FMA(param_4[3].data, weight_decay4.data, grad_4[3].data);
-                grad_4[4].data = SIMD_FMA(param_4[4].data, weight_decay4.data, grad_4[4].data);
-                grad_4[5].data = SIMD_FMA(param_4[5].data, weight_decay4.data, grad_4[5].data);
-                grad_4[6].data = SIMD_FMA(param_4[6].data, weight_decay4.data, grad_4[6].data);
-                grad_4[7].data = SIMD_FMA(param_4[7].data, weight_decay4.data, grad_4[7].data);
-            }
-
-            momentum_4[0].data = SIMD_MUL(momentum_4[0].data, betta1_4.data);
-            momentum_4[0].data = SIMD_FMA(grad_4[0].data, betta1_minus1_4.data, momentum_4[0].data);
-            momentum_4[1].data = SIMD_MUL(momentum_4[1].data, betta1_4.data);
-            momentum_4[1].data = SIMD_FMA(grad_4[1].data, betta1_minus1_4.data, momentum_4[1].data);
-            momentum_4[2].data = SIMD_MUL(momentum_4[2].data, betta1_4.data);
-            momentum_4[2].data = SIMD_FMA(grad_4[2].data, betta1_minus1_4.data, momentum_4[2].data);
-            momentum_4[3].data = SIMD_MUL(momentum_4[3].data, betta1_4.data);
-            momentum_4[3].data = SIMD_FMA(grad_4[3].data, betta1_minus1_4.data, momentum_4[3].data);
-            momentum_4[4].data = SIMD_MUL(momentum_4[4].data, betta1_4.data);
-            momentum_4[4].data = SIMD_FMA(grad_4[4].data, betta1_minus1_4.data, momentum_4[4].data);
-            momentum_4[5].data = SIMD_MUL(momentum_4[5].data, betta1_4.data);
-            momentum_4[5].data = SIMD_FMA(grad_4[5].data, betta1_minus1_4.data, momentum_4[5].data);
-            momentum_4[6].data = SIMD_MUL(momentum_4[6].data, betta1_4.data);
-            momentum_4[6].data = SIMD_FMA(grad_4[6].data, betta1_minus1_4.data, momentum_4[6].data);
-            momentum_4[7].data = SIMD_MUL(momentum_4[7].data, betta1_4.data);
-            momentum_4[7].data = SIMD_FMA(grad_4[7].data, betta1_minus1_4.data, momentum_4[7].data);
-
-            variance_4[0].data = SIMD_MUL(variance_4[0].data, betta2_4.data);
-            variance_4[1].data = SIMD_MUL(variance_4[1].data, betta2_4.data);
-            variance_4[2].data = SIMD_MUL(variance_4[2].data, betta2_4.data);
-            variance_4[3].data = SIMD_MUL(variance_4[3].data, betta2_4.data);
-            variance_4[4].data = SIMD_MUL(variance_4[4].data, betta2_4.data);
-            variance_4[5].data = SIMD_MUL(variance_4[5].data, betta2_4.data);
-            variance_4[6].data = SIMD_MUL(variance_4[6].data, betta2_4.data);
-            variance_4[7].data = SIMD_MUL(variance_4[7].data, betta2_4.data);
-            grad_4[0].data = SIMD_MUL(grad_4[0].data, grad_4[0].data);
-            grad_4[1].data = SIMD_MUL(grad_4[1].data, grad_4[1].data);
-            grad_4[2].data = SIMD_MUL(grad_4[2].data, grad_4[2].data);
-            grad_4[3].data = SIMD_MUL(grad_4[3].data, grad_4[3].data);
-            grad_4[4].data = SIMD_MUL(grad_4[4].data, grad_4[4].data);
-            grad_4[5].data = SIMD_MUL(grad_4[5].data, grad_4[5].data);
-            grad_4[6].data = SIMD_MUL(grad_4[6].data, grad_4[6].data);
-            grad_4[7].data = SIMD_MUL(grad_4[7].data, grad_4[7].data);
-            variance_4[0].data = SIMD_FMA(grad_4[0].data, betta2_minus1_4.data, variance_4[0].data);
-            variance_4[1].data = SIMD_FMA(grad_4[1].data, betta2_minus1_4.data, variance_4[1].data);
-            variance_4[2].data = SIMD_FMA(grad_4[2].data, betta2_minus1_4.data, variance_4[2].data);
-            variance_4[3].data = SIMD_FMA(grad_4[3].data, betta2_minus1_4.data, variance_4[3].data);
-            variance_4[4].data = SIMD_FMA(grad_4[4].data, betta2_minus1_4.data, variance_4[4].data);
-            variance_4[5].data = SIMD_FMA(grad_4[5].data, betta2_minus1_4.data, variance_4[5].data);
-            variance_4[6].data = SIMD_FMA(grad_4[6].data, betta2_minus1_4.data, variance_4[6].data);
-            variance_4[7].data = SIMD_FMA(grad_4[7].data, betta2_minus1_4.data, variance_4[7].data);
-
-            grad_4[0].data = SIMD_SQRT(variance_4[0].data);
-            grad_4[1].data = SIMD_SQRT(variance_4[1].data);
-            grad_4[2].data = SIMD_SQRT(variance_4[2].data);
-            grad_4[3].data = SIMD_SQRT(variance_4[3].data);
-            grad_4[4].data = SIMD_SQRT(variance_4[4].data);
-            grad_4[5].data = SIMD_SQRT(variance_4[5].data);
-            grad_4[6].data = SIMD_SQRT(variance_4[6].data);
-            grad_4[7].data = SIMD_SQRT(variance_4[7].data);
-
-            grad_4[0].data = SIMD_FMA(grad_4[0].data, bias2_sqrt.data, eps_4.data);
-            grad_4[1].data = SIMD_FMA(grad_4[1].data, bias2_sqrt.data, eps_4.data);
-            grad_4[2].data = SIMD_FMA(grad_4[2].data, bias2_sqrt.data, eps_4.data);
-            grad_4[3].data = SIMD_FMA(grad_4[3].data, bias2_sqrt.data, eps_4.data);
-            grad_4[4].data = SIMD_FMA(grad_4[4].data, bias2_sqrt.data, eps_4.data);
-            grad_4[5].data = SIMD_FMA(grad_4[5].data, bias2_sqrt.data, eps_4.data);
-            grad_4[6].data = SIMD_FMA(grad_4[6].data, bias2_sqrt.data, eps_4.data);
-            grad_4[7].data = SIMD_FMA(grad_4[7].data, bias2_sqrt.data, eps_4.data);
-            grad_4[0].data = SIMD_DIV(momentum_4[0].data, grad_4[0].data);
-            grad_4[1].data = SIMD_DIV(momentum_4[1].data, grad_4[1].data);
-            grad_4[2].data = SIMD_DIV(momentum_4[2].data, grad_4[2].data);
-            grad_4[3].data = SIMD_DIV(momentum_4[3].data, grad_4[3].data);
-            grad_4[4].data = SIMD_DIV(momentum_4[4].data, grad_4[4].data);
-            grad_4[5].data = SIMD_DIV(momentum_4[5].data, grad_4[5].data);
-            grad_4[6].data = SIMD_DIV(momentum_4[6].data, grad_4[6].data);
-            grad_4[7].data = SIMD_DIV(momentum_4[7].data, grad_4[7].data);
-
-            if (_weight_decay > 0 && _adamw_mode) {
-                param_4[0].data = SIMD_FMA(param_4[0].data, weight_decay4.data, param_4[0].data);
-                param_4[1].data = SIMD_FMA(param_4[1].data, weight_decay4.data, param_4[1].data);
-                param_4[2].data = SIMD_FMA(param_4[2].data, weight_decay4.data, param_4[2].data);
-                param_4[3].data = SIMD_FMA(param_4[3].data, weight_decay4.data, param_4[3].data);
-                param_4[4].data = SIMD_FMA(param_4[4].data, weight_decay4.data, param_4[4].data);
-                param_4[5].data = SIMD_FMA(param_4[5].data, weight_decay4.data, param_4[5].data);
-                param_4[6].data = SIMD_FMA(param_4[6].data, weight_decay4.data, param_4[6].data);
-                param_4[7].data = SIMD_FMA(param_4[7].data, weight_decay4.data, param_4[7].data);
-            }
-
-            param_4[0].data = SIMD_FMA(grad_4[0].data, step_size_4.data, param_4[0].data);
-            param_4[1].data = SIMD_FMA(grad_4[1].data, step_size_4.data, param_4[1].data);
-            param_4[2].data = SIMD_FMA(grad_4[2].data, step_size_4.data, param_4[2].data);
-            param_4[3].data = SIMD_FMA(grad_4[3].data, step_size_4.data, param_4[3].data);
-            param_4[4].data = SIMD_FMA(grad_4[4].data, step_size_4.data, param_4[4].data);
-            param_4[5].data = SIMD_FMA(grad_4[5].data, step_size_4.data, param_4[5].data);
-            param_4[6].data = SIMD_FMA(grad_4[6].data, step_size_4.data, param_4[6].data);
-            param_4[7].data = SIMD_FMA(grad_4[7].data, step_size_4.data, param_4[7].data);
-
-            SIMD_STORE(_params + i, param_4[0].data);
-            SIMD_STORE(_params + i + SIMD_WIDTH, param_4[1].data);
-            SIMD_STORE(_params + i + (SIMD_WIDTH << 1), param_4[2].data);
-            SIMD_STORE(_params + i + SIMD_WIDTH * 3, param_4[3].data);
-            SIMD_STORE(_params + i + (SIMD_WIDTH << 2), param_4[4].data);
-            SIMD_STORE(_params + i + SIMD_WIDTH * 5, param_4[5].data);
-            SIMD_STORE(_params + i + SIMD_WIDTH * 6, param_4[6].data);
-            SIMD_STORE(_params + i + SIMD_WIDTH * 7, param_4[7].data);
-
-            if (dev_params) {
-                SIMD_STORE(_doubled_buffer[_buf_index] + (i - t), param_4[0].data);
-                SIMD_STORE(_doubled_buffer[_buf_index] + (i - t) + SIMD_WIDTH, param_4[1].data);
-                SIMD_STORE(_doubled_buffer[_buf_index] + (i - t) + (SIMD_WIDTH << 1),
-                           param_4[2].data);
-                SIMD_STORE(_doubled_buffer[_buf_index] + (i - t) + SIMD_WIDTH * 3, param_4[3].data);
-                SIMD_STORE(_doubled_buffer[_buf_index] + (i - t) + (SIMD_WIDTH << 2),
-                           param_4[4].data);
-                SIMD_STORE(_doubled_buffer[_buf_index] + (i - t) + SIMD_WIDTH * 5, param_4[5].data);
-                SIMD_STORE(_doubled_buffer[_buf_index] + (i - t) + SIMD_WIDTH * 6, param_4[6].data);
-                SIMD_STORE(_doubled_buffer[_buf_index] + (i - t) + SIMD_WIDTH * 7, param_4[7].data);
-            }
-
-            SIMD_STORE(_exp_avg + i, momentum_4[0].data);
-            SIMD_STORE(_exp_avg + i + SIMD_WIDTH, momentum_4[1].data);
-            SIMD_STORE(_exp_avg + i + (SIMD_WIDTH << 1), momentum_4[2].data);
-            SIMD_STORE(_exp_avg + i + SIMD_WIDTH * 3, momentum_4[3].data);
-            SIMD_STORE(_exp_avg + i + (SIMD_WIDTH << 2), momentum_4[4].data);
-            SIMD_STORE(_exp_avg + i + SIMD_WIDTH * 5, momentum_4[5].data);
-            SIMD_STORE(_exp_avg + i + SIMD_WIDTH * 6, momentum_4[6].data);
-            SIMD_STORE(_exp_avg + i + SIMD_WIDTH * 7, momentum_4[7].data);
-
-            SIMD_STORE(_exp_avg_sq + i, variance_4[0].data);
-            SIMD_STORE(_exp_avg_sq + i + SIMD_WIDTH, variance_4[1].data);
-            SIMD_STORE(_exp_avg_sq + i + (SIMD_WIDTH << 1), variance_4[2].data);
-            SIMD_STORE(_exp_avg_sq + i + SIMD_WIDTH * 3, variance_4[3].data);
-            SIMD_STORE(_exp_avg_sq + i + (SIMD_WIDTH << 2), variance_4[4].data);
-            SIMD_STORE(_exp_avg_sq + i + SIMD_WIDTH * 5, variance_4[5].data);
-            SIMD_STORE(_exp_avg_sq + i + SIMD_WIDTH * 6, variance_4[6].data);
-            SIMD_STORE(_exp_avg_sq + i + SIMD_WIDTH * 7, variance_4[7].data);
-        }
-        if (dev_params) {
-            launch_param_update(
-                _doubled_buffer[_buf_index], dev_params + t, copy_size, _streams[_buf_index]);
-            _buf_index = !_buf_index;
-        }
-    }
-#endif
-    if (_param_size > rounded_size)
-        Step_4((_params + rounded_size),
-               (grads + rounded_size),
-               (_exp_avg + rounded_size),
-               (_exp_avg_sq + rounded_size),
-               (_param_size - rounded_size),
-               (dev_params != nullptr ? (dev_params + rounded_size) : dev_params),
-               half_precision);
-}
-
-int ds_adam_step(int optimizer_id,
-                 size_t step,
-                 float lr,
-                 float beta1,
-                 float beta2,
-                 float epsilon,
-                 float weight_decay,
-                 bool bias_correction,
-                 torch::Tensor& params,
-                 torch::Tensor& grads,
-                 torch::Tensor& exp_avg,
-                 torch::Tensor& exp_avg_sq)
-{
-    auto params_c = params.contiguous();
-    auto grads_c = grads.contiguous();
-    auto exp_avg_c = exp_avg.contiguous();
-    auto exp_avg_sq_c = exp_avg_sq.contiguous();
-
-    float* params_ptr = (float*)params_c.data_ptr();
-    float* grads_ptr = (float*)grads_c.data_ptr();
-    float* exp_avg_ptr = (float*)exp_avg_c.data_ptr();
-    float* exp_avg_sq_ptr = (float*)exp_avg_sq_c.data_ptr();
-
-    std::shared_ptr<Adam_Optimizer> opt =
-        std::static_pointer_cast<Adam_Optimizer>(s_optimizers[optimizer_id]);
-    opt->IncrementStep(step, beta1, beta2);
-    opt->update_state(lr, epsilon, weight_decay, bias_correction);
-    opt->Step_8(params_ptr,
-                grads_ptr,
-                exp_avg_ptr,
-                exp_avg_sq_ptr,
-                params_c.size(0),
-                nullptr,
-                (params.options().dtype() == at::kHalf));
-
-    opt->SynchronizeStreams();
-    return 0;
-}
-
-int ds_adam_step_plus_copy(int optimizer_id,
-                           size_t step,
-                           float lr,
-                           float beta1,
-                           float beta2,
-                           float epsilon,
-                           float weight_decay,
-                           bool bias_correction,
-                           torch::Tensor& params,
-                           torch::Tensor& grads,
-                           torch::Tensor& exp_avg,
-                           torch::Tensor& exp_avg_sq,
-                           torch::Tensor& gpu_params)
-{
-    auto params_c = params.contiguous();
-    auto gpu_params_c = gpu_params.contiguous();
-    auto exp_avg_c = exp_avg.contiguous();
-    auto exp_avg_sq_c = exp_avg_sq.contiguous();
-    auto grads_c = grads.contiguous();
-
-    float* params_ptr = (float*)params_c.data_ptr();
-    float* grads_ptr = (float*)grads_c.data_ptr();
-    sycl::half* gpu_params_ptr = (sycl::half*)gpu_params_c.data_ptr();
-    float* exp_avg_ptr = (float*)exp_avg_c.data_ptr();
-    float* exp_avg_sq_ptr = (float*)exp_avg_sq_c.data_ptr();
-
-    std::shared_ptr<Adam_Optimizer> opt =
-        std::static_pointer_cast<Adam_Optimizer>(s_optimizers[optimizer_id]);
-    opt->IncrementStep(step, beta1, beta2);
-    opt->update_state(lr, epsilon, weight_decay, bias_correction);
-    opt->Step_8(
-        params_ptr, grads_ptr, exp_avg_ptr, exp_avg_sq_ptr, params_c.size(0), gpu_params_ptr);
-
-    opt->SynchronizeStreams();
-    return 0;
-}
-
-int destroy_adam_optimizer(int optimizer_id)
-{
-    s_optimizers.erase(optimizer_id);
-
-    return 0;
-}
-
-PYBIND11_MODULE(TORCH_EXTENSION_NAME, m)
-{
-    m.def("adam_update", &ds_adam_step, "DeepSpeed CPU Adam update (C++)");
-    m.def("adam_update_copy",
-          &ds_adam_step_plus_copy,
-          "DeepSpeed CPU Adam update and param copy (C++)");
-    m.def("create_adam", &create_adam_optimizer, "DeepSpeed CPU Adam (C++)");
-    m.def("destroy_adam", &destroy_adam_optimizer, "DeepSpeed CPU Adam destroy (C++)");
-}
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/adam/cpu_adam_impl.cpp b/intel_extension_for_deepspeed/op_builder/csrc/adam/cpu_adam_impl.cpp
new file mode 100644
index 0000000..f4c5fdd
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/adam/cpu_adam_impl.cpp
@@ -0,0 +1,313 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+#include <torch/extension.h>
+#include <cassert>
+#include <iostream>
+#include <memory>
+#include <type_traits>
+#include <unordered_map>
+#include "cpu_adam.h"
+#include <cmath>
+
+#if defined(__ENABLE_CUDA__)
+#include <cuda_runtime_api.h>
+#include "cublas_v2.h"
+#include "cuda.h"
+#include "curand.h"
+#include "custom_cuda_layers.h"
+#endif
+
+static std::unordered_map<int, std::shared_ptr<void>> s_optimizers;
+
+// C++ interface
+
+void Adam_Optimizer::Step_1(float* _params,
+                            float* grads,
+                            float* _exp_avg,
+                            float* _exp_avg_sq,
+                            size_t _param_size,
+                            ds_half_precision_t* dev_params,
+                            bool half_precision)
+{
+    size_t rounded_size = 0;
+#if defined(__AVX512__) or defined(__AVX256__)
+    Step_AVX<1>(&rounded_size,
+                _params,
+                grads,
+                _exp_avg,
+                _exp_avg_sq,
+                _param_size,
+                dev_params,
+                half_precision);
+#endif
+    if (_param_size > rounded_size) {
+        float betta1_minus1 = 1 - _betta1;
+        float betta2_minus1 = 1 - _betta2;
+
+        float step_size = -1 * _alpha / _bias_correction1;
+        float w_decay = -1 * _alpha * _weight_decay;
+        ds_half_precision_t* grads_cast_h;
+        ds_half_precision_t* params_cast_h;
+        if (half_precision) {
+            grads_cast_h = reinterpret_cast<ds_half_precision_t*>(grads);
+            params_cast_h = reinterpret_cast<ds_half_precision_t*>(_params);
+        }
+
+        for (size_t t = rounded_size; t < _param_size; t += TILE) {
+            size_t copy_size = TILE;
+            if ((t + TILE) > _param_size) copy_size = _param_size - t;
+            size_t offset = copy_size + t;
+#if defined(__ENABLE_CUDA__)
+            if ((t / TILE) >= 2) { cudaStreamSynchronize(_streams[_buf_index]); }
+#elif defined(__ENABLE_CANN__)
+            if ((t / TILE) >= 2) { aclrtSynchronizeStream(_streams[_buf_index].stream()); }
+#endif
+#pragma omp parallel for
+            for (size_t k = t; k < offset; k++) {
+                float grad = half_precision ? (float)grads_cast_h[k] : grads[k];
+                float param = half_precision ? (float)params_cast_h[k] : _params[k];
+                float momentum = _exp_avg[k];
+                float variance = _exp_avg_sq[k];
+                if (_weight_decay > 0 && !_adamw_mode) { grad = param * _weight_decay + grad; }
+                momentum = momentum * _betta1;
+                momentum = grad * betta1_minus1 + momentum;
+
+                variance = variance * _betta2;
+                grad = grad * grad;
+                variance = grad * betta2_minus1 + variance;
+
+                grad = sqrt(variance);
+                grad = grad * _bias_correction2 + _eps;
+                grad = momentum / grad;
+                if (_weight_decay > 0 && _adamw_mode) { param += w_decay * param; }
+                param = grad * step_size + param;
+#if defined(__ENABLE_CUDA__) or defined(__ENABLE_CANN__)
+                if (dev_params) _doubled_buffer[_buf_index][k - t] = param;
+#endif
+                if (half_precision)
+                    params_cast_h[k] = (ds_half_precision_t)param;
+                else
+                    _params[k] = param;
+                _exp_avg[k] = momentum;
+                _exp_avg_sq[k] = variance;
+            }
+#if defined(__ENABLE_CUDA__)
+            if (dev_params) {
+                launch_param_update(
+                    _doubled_buffer[_buf_index], dev_params + t, (copy_size), _streams[_buf_index]);
+
+                _buf_index = !_buf_index;
+            }
+#elif defined(__ENABLE_CANN__)
+            if (dev_params) {
+                size_t memcpy_size = copy_size * sizeof(_doubled_buffer[_buf_index][0]);
+                aclrtMemcpy(dev_params + t,
+                            memcpy_size,
+                            _doubled_buffer[_buf_index],
+                            memcpy_size,
+                            aclrtMemcpyKind::ACL_MEMCPY_HOST_TO_DEVICE);
+
+                _buf_index = !_buf_index;
+            }
+#endif
+        }
+    }
+}
+
+void Adam_Optimizer::Step_4(float* _params,
+                            float* grads,
+                            float* _exp_avg,
+                            float* _exp_avg_sq,
+                            size_t _param_size,
+                            ds_half_precision_t* dev_params,
+                            bool half_precision)
+{
+    size_t rounded_size = 0;
+#if defined(__AVX512__) or defined(__AVX256__)
+    Step_AVX<4>(&rounded_size,
+                _params,
+                grads,
+                _exp_avg,
+                _exp_avg_sq,
+                _param_size,
+                dev_params,
+                half_precision);
+#endif
+    if (_param_size > rounded_size)
+        Step_1((_params + rounded_size),
+               (grads + rounded_size),
+               (_exp_avg + rounded_size),
+               (_exp_avg_sq + rounded_size),
+               (_param_size - rounded_size),
+               (dev_params != nullptr ? (dev_params + rounded_size) : dev_params),
+               half_precision);
+}
+
+int create_adam_optimizer(int optimizer_id,
+                          float alpha,
+                          float betta1,
+                          float betta2,
+                          float eps,
+                          float weight_decay,
+                          bool adamw_mode,
+                          bool should_log)
+{
+    auto opt =
+        std::make_shared<Adam_Optimizer>(alpha, betta1, betta2, eps, weight_decay, adamw_mode);
+
+    s_optimizers[optimizer_id] = opt;
+
+    if (should_log) {
+        std::string avx_type = "";
+#if defined(__AVX512__)
+        avx_type = "AVX512";
+#else
+#if defined(__AVX256__)
+        avx_type = "AVX2";
+#else
+        avx_type = "scalar";
+#endif
+#endif
+
+        printf("Adam Optimizer #%d is created with %s arithmetic capability.\n",
+               optimizer_id,
+               avx_type.c_str());
+        printf("Config: alpha=%f, betas=(%f, %f), weight_decay=%f, adam_w=%d\n",
+               alpha,
+               betta1,
+               betta2,
+               weight_decay,
+               (int)adamw_mode);
+    }
+
+    return 0;
+}
+
+void Adam_Optimizer::Step_8(float* _params,
+                            float* grads,
+                            float* _exp_avg,
+                            float* _exp_avg_sq,
+                            size_t _param_size,
+                            ds_half_precision_t* dev_params,
+                            bool half_precision)
+{
+    size_t rounded_size = 0;
+#if defined(__AVX512__) or defined(__AVX256__)
+    Step_AVX<8>(&rounded_size,
+                _params,
+                grads,
+                _exp_avg,
+                _exp_avg_sq,
+                _param_size,
+                dev_params,
+                half_precision);
+#endif
+    if (_param_size > rounded_size)
+        Step_4((_params + rounded_size),
+               (grads + rounded_size),
+               (_exp_avg + rounded_size),
+               (_exp_avg_sq + rounded_size),
+               (_param_size - rounded_size),
+               (dev_params != nullptr ? (dev_params + rounded_size) : dev_params),
+               half_precision);
+}
+
+int ds_adam_step(int optimizer_id,
+                 size_t step,
+                 float lr,
+                 float beta1,
+                 float beta2,
+                 float epsilon,
+                 float weight_decay,
+                 bool bias_correction,
+                 torch::Tensor& params,
+                 torch::Tensor& grads,
+                 torch::Tensor& exp_avg,
+                 torch::Tensor& exp_avg_sq)
+{
+    auto params_c = params.contiguous();
+    auto grads_c = grads.contiguous();
+    auto exp_avg_c = exp_avg.contiguous();
+    auto exp_avg_sq_c = exp_avg_sq.contiguous();
+
+    // assert(params.options().dtype() == grads.options().dtype());
+
+    float* params_ptr = (float*)params_c.data_ptr();
+    float* grads_ptr = (float*)grads_c.data_ptr();
+    float* exp_avg_ptr = (float*)exp_avg_c.data_ptr();
+    float* exp_avg_sq_ptr = (float*)exp_avg_sq_c.data_ptr();
+
+    std::shared_ptr<Adam_Optimizer> opt =
+        std::static_pointer_cast<Adam_Optimizer>(s_optimizers[optimizer_id]);
+    opt->IncrementStep(step, beta1, beta2);
+    opt->update_state(lr, epsilon, weight_decay, bias_correction);
+
+    opt->Step_8(params_ptr,
+                grads_ptr,
+                exp_avg_ptr,
+                exp_avg_sq_ptr,
+                params_c.numel(),
+                nullptr,
+                (params.options().dtype() == at::kHalf));
+
+#if defined(__ENABLE_CUDA__) or defined(__ENABLE_CANN__)
+    opt->SynchronizeStreams();
+#endif
+    return 0;
+}
+
+int ds_adam_step_plus_copy(int optimizer_id,
+                           size_t step,
+                           float lr,
+                           float beta1,
+                           float beta2,
+                           float epsilon,
+                           float weight_decay,
+                           bool bias_correction,
+                           torch::Tensor& params,
+                           torch::Tensor& grads,
+                           torch::Tensor& exp_avg,
+                           torch::Tensor& exp_avg_sq,
+                           torch::Tensor& device_params)
+{
+#if defined(__ENABLE_CUDA__) or defined(__ENABLE_CANN__)
+    auto params_c = params.contiguous();
+    auto device_params_c = device_params.contiguous();
+    auto exp_avg_c = exp_avg.contiguous();
+    auto exp_avg_sq_c = exp_avg_sq.contiguous();
+    auto grads_c = grads.contiguous();
+
+    float* params_ptr = (float*)params_c.data_ptr();
+    float* grads_ptr = (float*)grads_c.data_ptr();
+    ds_half_precision_t* device_params_ptr = (ds_half_precision_t*)device_params_c.data_ptr();
+    float* exp_avg_ptr = (float*)exp_avg_c.data_ptr();
+    float* exp_avg_sq_ptr = (float*)exp_avg_sq_c.data_ptr();
+
+    std::shared_ptr<Adam_Optimizer> opt =
+        std::static_pointer_cast<Adam_Optimizer>(s_optimizers[optimizer_id]);
+    opt->IncrementStep(step, beta1, beta2);
+    opt->update_state(lr, epsilon, weight_decay, bias_correction);
+    opt->Step_8(params_ptr,
+                grads_ptr,
+                exp_avg_ptr,
+                exp_avg_sq_ptr,
+                params_c.numel(),
+                device_params_ptr,
+                (params.options().dtype() == at::kHalf));
+
+    opt->SynchronizeStreams();
+#else
+    assert(false);
+#endif
+    return 0;
+}
+
+int destroy_adam_optimizer(int optimizer_id)
+{
+    s_optimizers.erase(optimizer_id);
+
+    return 0;
+}
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/adam/custom_sycl_kernel.dp.cpp b/intel_extension_for_deepspeed/op_builder/csrc/adam/custom_sycl_kernel.dp.cpp
deleted file mode 100644
index 2c073ee..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/adam/custom_sycl_kernel.dp.cpp
+++ /dev/null
@@ -1,32 +0,0 @@
-#if __has_include(<sycl/sycl.hpp>)
-#include <sycl/sycl.hpp>
-#elif __has_include(<CL/sycl.hpp>)
-#include <CL/sycl.hpp>
-#else
-#error "Unsupported compiler"
-#endif
-#include "custom_sycl_layers.hpp"
-
-void param_update_kernel(const float* input,
-                         sycl::half* output,
-                         int size,
-                         sycl::nd_item<3> item_ct1)
-{
-    int id = item_ct1.get_group(2) * item_ct1.get_local_range(2) + item_ct1.get_local_id(2);
-
-    if (id < size) { output[id] = (sycl::half)input[id]; }
-}
-
-void launch_param_update(const float* input, sycl::half* output, int size, sycl::queue* stream)
-{
-    int threads = 1024;
-
-    sycl::range<3> grid_dim(1, 1, (size - 1) / threads + 1);
-    sycl::range<3> block_dim(1, 1, threads);
-
-    stream->submit([&](sycl::handler& cgh) {
-        cgh.parallel_for(
-            sycl::nd_range<3>(grid_dim * block_dim, block_dim),
-            [=](sycl::nd_item<3> item_ct1) { param_update_kernel(input, output, size, item_ct1); });
-    });
-}
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/adam/fused_adam_frontend.cpp b/intel_extension_for_deepspeed/op_builder/csrc/adam/fused_adam_frontend.cpp
index 37bab4a..13b3902 100644
--- a/intel_extension_for_deepspeed/op_builder/csrc/adam/fused_adam_frontend.cpp
+++ b/intel_extension_for_deepspeed/op_builder/csrc/adam/fused_adam_frontend.cpp
@@ -1,6 +1,11 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
 #include <torch/extension.h>
 
-void multi_tensor_adam_sycl(int chunk_size,
+void multi_tensor_adam_cuda(int chunk_size,
                             at::Tensor noop_flag,
                             std::vector<std::vector<at::Tensor>> tensor_lists,
                             const float lr,
@@ -15,6 +20,6 @@ void multi_tensor_adam_sycl(int chunk_size,
 PYBIND11_MODULE(TORCH_EXTENSION_NAME, m)
 {
     m.def("multi_tensor_adam",
-          &multi_tensor_adam_sycl,
+          &multi_tensor_adam_cuda,
           "Compute and apply gradient update to parameters for Adam optimizer");
 }
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/adam/multi_tensor_adam.dp.cpp b/intel_extension_for_deepspeed/op_builder/csrc/adam/multi_tensor_adam.dp.cpp
index 2e1e674..ab6ccd7 100644
--- a/intel_extension_for_deepspeed/op_builder/csrc/adam/multi_tensor_adam.dp.cpp
+++ b/intel_extension_for_deepspeed/op_builder/csrc/adam/multi_tensor_adam.dp.cpp
@@ -1,18 +1,27 @@
-/* Copyright 2020 The Microsoft DeepSpeed Team
-   Copyright NVIDIA/apex
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+/*
+Copyright NVIDIA/apex
+This file is adapted from fused adam in NVIDIA/apex, commit a109f85
 */
 
-#include <assert.h>
-#if __has_include(<sycl/sycl.hpp>)
 #include <sycl/sycl.hpp>
-#elif __has_include(<CL/sycl.hpp>)
-#include <CL/sycl.hpp>
-#else
-#error "Unsupported compiler"
-#endif
+#include <dpct/dpct.hpp>
+#include <ATen/ATen.h>
+#include <ATen/AccumulateType.h>
+// #include <ATen/cuda/CUDAContext.h>
+// #include <ATen/cuda/Exceptions.h>
+// Another possibility:
+// #include <torch/all.h>
+
+#include <assert.h>
 
 #include "multi_tensor_apply.dp.hpp"
-#include "type_shim.hpp"
+#include "type_shim.h"
+#include <cmath>
 
 #define BLOCK_SIZE 512
 #define ILP 4
@@ -25,164 +34,109 @@ typedef enum {
 using MATH_T = float;
 
 template <typename T>
-void AdamFunctor(sycl::nd_item<1> item_ct1,
-                 int chunk_size,
-                 int* noop_gmem,
-                 const int tensor_loc,
-                 const int chunk_idx,
-                 int n,
-                 T* g,
-                 T* p,
-                 T* m,
-                 T* v,
-                 const float beta1,
-                 const float beta2,
-                 const float beta1_correction,
-                 const float beta2_correction,
-                 const float epsilon,
-                 const float lr,
-                 const int mode,
-                 const float decay)
-{
-    g += chunk_idx * chunk_size;
-
-    p += chunk_idx * chunk_size;
-
-    m += chunk_idx * chunk_size;
-
-    v += chunk_idx * chunk_size;
-
-    n -= chunk_idx * chunk_size;
-
-    // see note in multi_tensor_scale_kernel.cu
-    for (int i_start = 0; i_start < n && i_start < chunk_size;
-         i_start += item_ct1.get_local_range(0) * ILP) {
-        MATH_T r_g[ILP];
-        MATH_T r_p[ILP];
-        MATH_T r_m[ILP];
-        MATH_T r_v[ILP];
+struct AdamFunctor {
+    /*
+    DPCT1110:4: The total declared local variable size in device function operator() exceeds 128
+    bytes and may cause high register pressure. Consult with your hardware vendor to find the total
+    register size available and adjust the code, or use smaller sub-group size to avoid high
+    register pressure.
+    */
+    __dpct_inline__ void operator()(int chunk_size,
+                                    volatile int* noop_gmem,
+                                    TensorListMetadata<4>& tl,
+                                    const float beta1,
+                                    const float beta2,
+                                    const float beta1_correction,
+                                    const float beta2_correction,
+                                    const float epsilon,
+                                    const float lr,
+                                    adamMode_t mode,
+                                    const float decay)
+    {
+        // I'd like this kernel to propagate infs/nans.
+        // if(*noop_gmem == 1)
+        //   return;
+
+        auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+        int tensor_loc = tl.block_to_tensor[item_ct1.get_group(2)];
+
+        // potentially use to pass in list of scalar
+        // int tensor_num = tl.start_tensor_this_launch + tensor_loc;
+
+        int chunk_idx = tl.block_to_chunk[item_ct1.get_group(2)];
+        int n = tl.sizes[tensor_loc];
+
+        T* g = (T*)tl.addresses[0][tensor_loc];
+        g += chunk_idx * chunk_size;
+
+        T* p = (T*)tl.addresses[1][tensor_loc];
+        p += chunk_idx * chunk_size;
+
+        T* m = (T*)tl.addresses[2][tensor_loc];
+        m += chunk_idx * chunk_size;
+
+        T* v = (T*)tl.addresses[3][tensor_loc];
+        v += chunk_idx * chunk_size;
+
+        n -= chunk_idx * chunk_size;
+
+        // see note in multi_tensor_scale_kernel.cu
+        for (int i_start = 0; i_start < n && i_start < chunk_size;
+             i_start += item_ct1.get_local_range(2) * ILP) {
+            MATH_T r_g[ILP];
+            MATH_T r_p[ILP];
+            MATH_T r_m[ILP];
+            MATH_T r_v[ILP];
 #pragma unroll
-        for (int ii = 0; ii < ILP; ii++) {
-            int i = i_start + item_ct1.get_local_id(0) + ii * item_ct1.get_local_range(0);
-            if (i < n && i < chunk_size) {
-                r_g[ii] = g[i];
-                r_p[ii] = p[i];
-                r_m[ii] = m[i];
-                r_v[ii] = v[i];
-            } else {
-                r_g[ii] = MATH_T(0);
-                r_p[ii] = MATH_T(0);
-                r_m[ii] = MATH_T(0);
-                r_v[ii] = MATH_T(0);
+            for (int ii = 0; ii < ILP; ii++) {
+                int i = i_start + item_ct1.get_local_id(2) + ii * item_ct1.get_local_range(2);
+                if (i < n && i < chunk_size) {
+                    r_g[ii] = g[i];
+                    r_p[ii] = p[i];
+                    r_m[ii] = m[i];
+                    r_v[ii] = v[i];
+                } else {
+                    r_g[ii] = MATH_T(0);
+                    r_p[ii] = MATH_T(0);
+                    r_m[ii] = MATH_T(0);
+                    r_v[ii] = MATH_T(0);
+                }
             }
-        }
-
 #pragma unroll
-        for (int ii = 0; ii < ILP; ii++) {
-            if (mode == ADAM_MODE_0) {  // L2
-                r_g[ii] = r_g[ii] + (decay * r_p[ii]);
-                r_m[ii] = beta1 * r_m[ii] + (1 - beta1) * r_g[ii];
-                r_v[ii] = beta2 * r_v[ii] + (1 - beta2) * r_g[ii] * r_g[ii];
-                MATH_T next_m_unbiased = r_m[ii] / beta1_correction;
-                MATH_T next_v_unbiased = r_v[ii] / beta2_correction;
-                MATH_T denom = sycl::sqrt((float)next_v_unbiased) + epsilon;
-                MATH_T update = next_m_unbiased / denom;
-                r_p[ii] = r_p[ii] - (lr * update);
-            } else {  // weight decay
-                r_m[ii] = beta1 * r_m[ii] + (1 - beta1) * r_g[ii];
-                r_v[ii] = beta2 * r_v[ii] + (1 - beta2) * r_g[ii] * r_g[ii];
-                MATH_T next_m_unbiased = r_m[ii] / beta1_correction;
-                MATH_T next_v_unbiased = r_v[ii] / beta2_correction;
-                MATH_T denom = sycl::sqrt((float)next_v_unbiased) + epsilon;
-                MATH_T update = (next_m_unbiased / denom) + (decay * r_p[ii]);
-                r_p[ii] = r_p[ii] - (lr * update);
+            for (int ii = 0; ii < ILP; ii++) {
+                if (mode == ADAM_MODE_0) {  // L2
+                    r_g[ii] = r_g[ii] + (decay * r_p[ii]);
+                    r_m[ii] = beta1 * r_m[ii] + (1 - beta1) * r_g[ii];
+                    r_v[ii] = beta2 * r_v[ii] + (1 - beta2) * r_g[ii] * r_g[ii];
+                    MATH_T next_m_unbiased = r_m[ii] / beta1_correction;
+                    MATH_T next_v_unbiased = r_v[ii] / beta2_correction;
+                    MATH_T denom = sycl::sqrt(next_v_unbiased) + epsilon;
+                    MATH_T update = next_m_unbiased / denom;
+                    r_p[ii] = r_p[ii] - (lr * update);
+                } else {  // weight decay
+                    r_m[ii] = beta1 * r_m[ii] + (1 - beta1) * r_g[ii];
+                    r_v[ii] = beta2 * r_v[ii] + (1 - beta2) * r_g[ii] * r_g[ii];
+                    MATH_T next_m_unbiased = r_m[ii] / beta1_correction;
+                    MATH_T next_v_unbiased = r_v[ii] / beta2_correction;
+                    MATH_T denom = sycl::sqrt(next_v_unbiased) + epsilon;
+                    MATH_T update = (next_m_unbiased / denom) + (decay * r_p[ii]);
+                    r_p[ii] = r_p[ii] - (lr * update);
+                }
             }
-        }
 #pragma unroll
-        for (int ii = 0; ii < ILP; ii++) {
-            int i = i_start + item_ct1.get_local_id(0) + ii * item_ct1.get_local_range(0);
-            if (i < n && i < chunk_size) {
-                p[i] = r_p[ii];
-                m[i] = r_m[ii];
-                v[i] = r_v[ii];
+            for (int ii = 0; ii < ILP; ii++) {
+                int i = i_start + item_ct1.get_local_id(2) + ii * item_ct1.get_local_range(2);
+                if (i < n && i < chunk_size) {
+                    p[i] = r_p[ii];
+                    m[i] = r_m[ii];
+                    v[i] = r_v[ii];
+                }
             }
         }
     }
-}
-
-void test_queue_with_accessor(void)
-{
-    printf("Test queue with accessor\n");
-    auto type_ = c10::DeviceType::XPU;
-    c10::impl::VirtualGuardImpl impl(type_);
-    auto device_ = c10::Device(type_);
-    c10::Stream dpcpp_stream = impl.getStream(device_);
-    sycl::queue* stream = &(xpu::get_queue_from_stream(dpcpp_stream));
-    sycl::default_selector d_selector;
-    static auto exception_handler = [](sycl::exception_list e_list) {
-        for (std::exception_ptr const& e : e_list) {
-            try {
-                std::rethrow_exception(e);
-            } catch (std::exception const& e) {
-                std::cout << "Failure" << std::endl;
-                std::terminate();
-            }
-        }
-    };
-    sycl::queue dq(d_selector,
-                   exception_handler,
-                   {sycl::property::queue::in_order(), sycl::property::queue::enable_profiling()});
-    struct {
-        unsigned char block_to_tensor[320];
-        int block_to_chunk[320];
-        void* addresses[4][36];
-        int sizes[36];
-    } tll;
-    sycl::buffer<unsigned char, 1> block_to_tensor_buf(&(tll.block_to_tensor[0]), {320});
-    sycl::buffer<int, 1> block_to_chunk_buf(&(tll.block_to_chunk[0]), {320});
-    sycl::buffer<void*, 2> addresses_buf(&(tll.addresses[0][0]), {4, 36});
-    sycl::buffer<int, 1> sizes_buf(&(tll.sizes[0]), {36});
-    printf("submit dq without accessor ");
-    dq.submit([&](sycl::handler& cgh) {
-        cgh.parallel_for(sycl::nd_range<1>(320 * 512, 512), [=](sycl::nd_item<1> item_ct1) {});
-    });
-    dq.wait();
-    printf("done\n");
-    printf("submit dq with accessor ");
-    dq.submit([&](sycl::handler& cgh) {
-        sycl::accessor tl_block_to_tensor(block_to_tensor_buf, cgh, sycl::read_only);
-        sycl::accessor tl_block_to_chunk(block_to_chunk_buf, cgh, sycl::read_only);
-        sycl::accessor tl_addresses(addresses_buf, cgh, sycl::read_only);
-        sycl::accessor tl_sizes(sizes_buf, cgh, sycl::read_only);
-        cgh.parallel_for(sycl::nd_range<1>(320 * 512, 512), [=](sycl::nd_item<1> item_ct1) {});
-    });
-    dq.wait();
-    printf("done\n");
-    printf("submit xpu::stream without accessor ");
-    stream->submit([&](sycl::handler& cgh) {
-        cgh.parallel_for(sycl::nd_range<1>(320 * 512, 512), [=](sycl::nd_item<1> item_ct1) {});
-    });
-    stream->wait();
-    printf("done\n");
-    printf("submit xpu::stream with accessor ");
-    stream->submit([&](sycl::handler& cgh) {
-        sycl::accessor tl_block_to_tensor(block_to_tensor_buf, cgh, sycl::read_only);
-        sycl::accessor tl_block_to_chunk(block_to_chunk_buf, cgh, sycl::read_only);
-        sycl::accessor tl_addresses(addresses_buf, cgh, sycl::read_only);
-        sycl::accessor tl_sizes(sizes_buf, cgh, sycl::read_only);
-        cgh.parallel_for(sycl::nd_range<1>(320 * 512, 512), [=](sycl::nd_item<1> item_ct1) {});
-    });
-    stream->wait();
-    printf("done\n");
-}
-
-void multi_tensor_test(void)
-{
-    printf("inside multi_tensor_test\n");
-    test_queue_with_accessor();
-}
+};
 
-void multi_tensor_adam_sycl(int chunk_size,
+void multi_tensor_adam_cuda(int chunk_size,
                             at::Tensor noop_flag,
                             std::vector<std::vector<at::Tensor>> tensor_lists,
                             const float lr,
@@ -202,20 +156,28 @@ void multi_tensor_adam_sycl(int chunk_size,
         bias_correction1 = 1 - std::pow(beta1, step);
         bias_correction2 = 1 - std::pow(beta2, step);
     }
-    // Assume single type across p,g,m1,m2
+
+    // Assume single type across p,g,m1,m2 now
     DISPATCH_DOUBLE_FLOAT_AND_HALF(tensor_lists[0][0].scalar_type(),
                                    0,
                                    "adam",
-                                   multi_tensor_apply<4, scalar_t_0>(BLOCK_SIZE,
-                                                                     chunk_size,
-                                                                     noop_flag,
-                                                                     tensor_lists,
-                                                                     beta1,
-                                                                     beta2,
-                                                                     bias_correction1,
-                                                                     bias_correction2,
-                                                                     epsilon,
-                                                                     lr,
-                                                                     mode,
-                                                                     weight_decay))
+                                   multi_tensor_apply<4>(BLOCK_SIZE,
+                                                         chunk_size,
+                                                         noop_flag,
+                                                         tensor_lists,
+                                                         AdamFunctor<scalar_t_0>(),
+                                                         beta1,
+                                                         beta2,
+                                                         bias_correction1,
+                                                         bias_correction2,
+                                                         epsilon,
+                                                         lr,
+                                                         (adamMode_t)mode,
+                                                         weight_decay);)
+
+    /*
+    DPCT1010:9: SYCL uses exceptions to report errors and does not use the error codes. The call was
+    replaced with 0. You need to rewrite this code.
+    */
+    // AT_CUDA_CHECK(0);
 }
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/adam/multi_tensor_apply.dp.hpp b/intel_extension_for_deepspeed/op_builder/csrc/adam/multi_tensor_apply.dp.hpp
new file mode 100644
index 0000000..8fc7747
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/adam/multi_tensor_apply.dp.hpp
@@ -0,0 +1,233 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+/*
+Copyright NVIDIA/apex
+This file is adapted from fused adam in NVIDIA/apex, commit a109f85
+*/
+
+#include <ATen/ATen.h>
+#include <ATen/AccumulateType.h>
+#include <dpct/dpct.hpp>
+#include <sycl/sycl.hpp>
+// #include <ATen/cuda/CUDAContext.h>
+// #include <ATen/cuda/Exceptions.h>
+#include <ipex.h>
+#include "compat.h"
+
+#include <assert.h>
+#include <tuple>
+#include <utility>
+
+namespace at {
+namespace cuda {
+dpct::queue_ptr getCurrentCUDAStream()
+{
+    auto device_type = c10::DeviceType::XPU;
+    c10::impl::VirtualGuardImpl impl(device_type);
+    c10::Stream c10_stream = impl.getStream(c10::Device(device_type));
+    auto& queue = xpu::get_queue_from_stream(c10_stream);
+    return &queue;
+}
+
+dpct::queue_ptr getStreamFromPool(bool)
+{
+    // not implemented
+    return nullptr;
+}
+}  // namespace cuda
+}  // namespace at
+// #include <iostream>
+
+// This header is the one-stop shop for all your multi-tensor apply needs.
+
+// TODO:  Kernel arg size limit may be <4KB for some other cards (ie Jetson)
+constexpr int depth_to_max_tensors[5] = {110, 64, 48, 36, 30};
+constexpr int depth_to_max_blocks[5] = {320, 320, 320, 320, 320};
+
+template <int n>
+struct TensorListMetadata {
+    void* addresses[n][depth_to_max_tensors[n - 1]];
+    int sizes[depth_to_max_tensors[n - 1]];
+    unsigned char block_to_tensor[depth_to_max_blocks[n - 1]];
+    int block_to_chunk[depth_to_max_blocks[n - 1]];  // I fear this needs to be a full int.
+    int start_tensor_this_launch;
+};
+
+template <typename T, typename U, typename... ArgTypes>
+class multi_tensor_apply_kernel {
+public:
+    multi_tensor_apply_kernel(int chunk_size,
+                              volatile int* noop_flag,
+                              T tl,
+                              U callable,
+                              ArgTypes... args)
+        : chunk_size(chunk_size), noop_flag(noop_flag), tl(tl), callable(callable), args(args...)
+    {
+    }
+
+    // This should be identical to original __global__ function
+    static void inline __global__function(int chunk_size,
+                                          volatile int* noop_flag,
+                                          T tl,
+                                          U callable,
+                                          ArgTypes... args)
+    {
+        callable(chunk_size, noop_flag, tl, args...);
+    }
+
+    // If global function template contains parameter pack,
+    // we only deal with parameter pack at the end of template parameter list
+    template <typename Tuple, std::size_t... I>
+    static void inline __tuple_expand_driver(int chunk_size,
+                                             volatile int* noop_flag,
+                                             T tl,
+                                             U callable,
+                                             Tuple args,
+                                             std::index_sequence<I...>)
+    {
+        __global__function(chunk_size, noop_flag, tl, callable, std::get<I>(args)...);
+    }
+
+    //
+    // Because __global__ function can't really use any reference types, we can sure that args
+    // are all good behaviors
+    //
+    void operator()(sycl::nd_item<3>) const
+    {
+        __tuple_expand_driver(chunk_size,
+                              noop_flag,
+                              tl,
+                              callable,
+                              args,
+                              std::make_index_sequence<sizeof...(ArgTypes)>());
+    }
+
+private:
+    int chunk_size;
+    volatile int* noop_flag;
+    T tl;
+    U callable;
+    std::tuple<ArgTypes...> args;
+};
+
+template <int depth, typename T, typename... ArgTypes>
+void multi_tensor_apply(int block_size,
+                        int chunk_size,
+                        const at::Tensor& noop_flag,
+                        const std::vector<std::vector<at::Tensor>>& tensor_lists,
+                        T callable,
+                        ArgTypes... args)
+{
+    TORCH_CHECK(tensor_lists.size() == depth, "tensor_lists.size() != depth");
+    int len0 = tensor_lists[0].size();
+    TORCH_CHECK(len0 > 0, "tensor_lists[0].size() is not > 0");
+    auto ref_device = tensor_lists[0][0].device();
+    TORCH_CHECK(ref_device.type() == at::kXPU, "expected input to be on cuda");
+    for (int l = 0; l < tensor_lists.size(); l++)  // No range-based for because I need indices
+    {
+        TORCH_CHECK(tensor_lists[l].size() == len0, "Size mismatch among tensor lists");
+        for (int t = 0; t < tensor_lists[l].size(); t++) {
+            // TODO:  Print which tensor fails.
+            bool contiguous_memory = tensor_lists[l][t].is_contiguous();
+#ifdef VERSION_GE_1_5
+            contiguous_memory = (contiguous_memory ||
+                                 tensor_lists[l][t].is_contiguous(at::MemoryFormat::ChannelsLast));
+#endif
+            TORCH_CHECK(contiguous_memory, "A tensor was not contiguous.");
+            TORCH_CHECK(tensor_lists[l][t].device() == ref_device,
+                        "A tensor was not on the same device as the first tensor");
+            TORCH_CHECK(tensor_lists[l][t].numel() == tensor_lists[0][t].numel(), "Size mismatch");
+        }
+    }
+
+    int ntensors = tensor_lists[0].size();
+
+    TensorListMetadata<depth> tl;
+
+    /* const at::cuda::OptionalCUDAGuard device_guard(device_of(tensor_lists[0][0])); */
+    auto stream = at::cuda::getCurrentCUDAStream();
+
+    tl.start_tensor_this_launch = 0;
+    int loc_block_info = 0;
+    int loc_tensor_info = 0;
+    for (int t = 0; t < ntensors; t++) {
+        tl.sizes[loc_tensor_info] = tensor_lists[0][t].numel();
+        for (int d = 0; d < depth; d++)
+            tl.addresses[d][loc_tensor_info] = tensor_lists[d][t].data_ptr();
+        loc_tensor_info++;
+
+        int chunks_this_tensor = (tensor_lists[0][t].numel() + chunk_size - 1) / chunk_size;
+
+        for (int chunk = 0; chunk < chunks_this_tensor; chunk++) {
+            // std::cout << chunks_this_tensor << std::endl;
+            tl.block_to_tensor[loc_block_info] = loc_tensor_info - 1;
+            tl.block_to_chunk[loc_block_info] = chunk;
+            loc_block_info++;
+
+            bool tensors_full = (loc_tensor_info == depth_to_max_tensors[depth - 1] &&
+                                 chunk == chunks_this_tensor - 1);
+            bool blocks_full = (loc_block_info == depth_to_max_blocks[depth - 1]);
+            bool last_chunk = (t == ntensors - 1 && chunk == chunks_this_tensor - 1);
+            if (tensors_full || blocks_full || last_chunk) {
+                // using accscalar_t = acc_type<scalar_t, true>;
+                /*
+                DPCT1049:0: The work-group size passed to the SYCL kernel may exceed the limit. To
+                get the device limit, query info::device::max_work_group_size. Adjust the work-group
+                size if needed.
+                */
+                /* multi_tensor_apply_kernel<TensorListMetadata<depth>, T, ArgTypes...>
+                 * fn(chunk_size, noop_flag.DATA_PTR<int>(), tl, callable, args...); */
+                if constexpr (sizeof(multi_tensor_apply_kernel(
+                                  chunk_size, noop_flag.DATA_PTR<int>(), tl, callable, args...)) <
+                              2048) {
+                    ((sycl::queue*)(stream))
+                        ->parallel_for(
+                            sycl::nd_range<3>(sycl::range<3>(1, 1, loc_block_info) *
+                                                  sycl::range<3>(1, 1, block_size),
+                                              sycl::range<3>(1, 1, block_size)),
+                            multi_tensor_apply_kernel(
+                                chunk_size, noop_flag.DATA_PTR<int>(), tl, callable, args...));
+                } else {
+                    auto capture = multi_tensor_apply_kernel(
+                        chunk_size, noop_flag.DATA_PTR<int>(), tl, callable, args...);
+                    sycl::buffer params(const_cast<const decltype(capture)*>(&capture),
+                                        sycl::range<1>(1));
+                    stream->submit([&](sycl::handler& cgh) {
+                        auto device_params =
+                            params.template get_access<sycl::access_mode::read,
+                                                       sycl::target::constant_buffer>(cgh);
+                        cgh.parallel_for(sycl::nd_range<3>(sycl::range<3>(1, 1, loc_block_info) *
+                                                               sycl::range<3>(1, 1, block_size),
+                                                           sycl::range<3>(1, 1, block_size)),
+                                         [=](sycl::nd_item<3> item) { device_params[0](item); });
+                    });
+                }
+                /*
+                DPCT1010:5: SYCL uses exceptions to report errors and does not use the error codes.
+                The call was replaced with 0. You need to rewrite this code.
+                */
+                0;
+
+                // Reset.  The control flow possibilities here make my brain hurt.
+                loc_block_info = 0;
+                if (chunk == chunks_this_tensor - 1) {
+                    // std::cout << "Hit case 1 " << cond1 << " " << cond2 << " " << cond3 <<
+                    // std::endl;
+                    loc_tensor_info = 0;
+                    tl.start_tensor_this_launch = t + 1;
+                } else {
+                    // std::cout << "Hit case 2 " << cond1 << " " << cond2 << " " << cond3 <<
+                    // std::endl;
+                    tl.sizes[0] = tl.sizes[loc_tensor_info - 1];
+                    for (int d = 0; d < depth; d++)
+                        tl.addresses[d][0] = tl.addresses[d][loc_tensor_info - 1];
+                    loc_tensor_info = 1;
+                    tl.start_tensor_this_launch = t;
+                }
+            }
+        }
+    }
+}
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/common/custom_cuda_kernel.dp.cpp b/intel_extension_for_deepspeed/op_builder/csrc/common/custom_cuda_kernel.dp.cpp
new file mode 100644
index 0000000..d4b045f
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/common/custom_cuda_kernel.dp.cpp
@@ -0,0 +1,71 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include "custom_cuda_layers.h"
+
+void param_update_kernel(const float* input, sycl::half* output, int size)
+{
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    int id = item_ct1.get_group(2) * item_ct1.get_local_range(2) + item_ct1.get_local_id(2);
+
+    if (id < size) { output[id] = (sycl::half)input[id]; }
+}
+
+void launch_param_update(const float* input, sycl::half* output, int size, dpct::queue_ptr stream)
+{
+    int threads = 1024;
+
+    sycl::range<3> grid_dim(1, 1, (size - 1) / threads + 1);
+    sycl::range<3> block_dim(1, 1, threads);
+
+    /*
+    DPCT1049:0: The work-group size passed to the SYCL kernel may exceed the limit. To get the
+    device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
+    */
+    {
+        dpct::has_capability_or_fail(stream->get_device(), {sycl::aspect::fp16});
+        stream->parallel_for(sycl::nd_range<3>(grid_dim * block_dim, block_dim),
+                             [=](sycl::nd_item<3> item_ct1) {
+                                 param_update_kernel(input, output, size);
+                             });
+    }
+}
+
+void param_update_kernel_half(const float* input, sycl::half* output, int size)
+{
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    int id = item_ct1.get_group(2) * item_ct1.get_local_range(2) + item_ct1.get_local_id(2);
+    sycl::half2* output_cast = reinterpret_cast<sycl::half2*>(output);
+    if (id < size) {
+        float input_f = input[id];
+        sycl::half2* input_h = reinterpret_cast<sycl::half2*>(&input_f);
+        output_cast[id] = *input_h;
+    }
+}
+
+void launch_param_update_half(const float* input,
+                              sycl::half* output,
+                              int size,
+                              dpct::queue_ptr stream)
+{
+    int threads = 1024;
+    size /= 2;
+    sycl::range<3> grid_dim(1, 1, (size - 1) / threads + 1);
+    sycl::range<3> block_dim(1, 1, threads);
+
+    /*
+    DPCT1049:1: The work-group size passed to the SYCL kernel may exceed the limit. To get the
+    device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
+    */
+    {
+        dpct::has_capability_or_fail(stream->get_device(), {sycl::aspect::fp16});
+        stream->parallel_for(sycl::nd_range<3>(grid_dim * block_dim, block_dim),
+                             [=](sycl::nd_item<3> item_ct1) {
+                                 param_update_kernel_half(input, output, size);
+                             });
+    }
+}
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/StopWatch.h b/intel_extension_for_deepspeed/op_builder/csrc/includes/StopWatch.h
new file mode 100644
index 0000000..42e4eb8
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/includes/StopWatch.h
@@ -0,0 +1,103 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+#pragma once
+#ifdef _WIN32
+#include <windows.h>
+#else
+#include <time.h>
+#endif
+
+#ifdef _WIN32
+
+class Stopwatch {
+private:
+    double m_total_time;
+    LARGE_INTEGER m_start_time;
+
+public:
+    Stopwatch() { m_total_time = 0.0; }
+
+    ~Stopwatch() {}
+
+    void Reset() { m_total_time = 0.0; }
+
+    void Start() { QueryPerformanceCounter(&m_start_time); }
+
+    void Restart()
+    {
+        m_total_time = 0.0;
+        QueryPerformanceCounter(&m_start_time);
+    }
+
+    void Stop()
+    {
+        LARGE_INTEGER frequency;
+        LARGE_INTEGER stop_time;
+        QueryPerformanceFrequency(&frequency);
+        QueryPerformanceCounter(&stop_time);
+        m_total_time +=
+            ((double)(stop_time.QuadPart - m_start_time.QuadPart) / (double)frequency.QuadPart);
+    }
+
+    double GetTimeInSeconds() { return m_total_time; }
+};
+
+#else
+
+class Stopwatch {
+private:
+    double m_total_time;
+    struct timespec m_start_time;
+    bool m_is_started;
+
+public:
+    Stopwatch()
+    {
+        m_total_time = 0.0;
+        m_is_started = false;
+    }
+
+    ~Stopwatch() {}
+
+    void Reset() { m_total_time = 0.0; }
+
+    void Start()
+    {
+        clock_gettime(CLOCK_MONOTONIC, &m_start_time);
+        m_is_started = true;
+    }
+
+    void Restart()
+    {
+        m_total_time = 0.0;
+        clock_gettime(CLOCK_MONOTONIC, &m_start_time);
+        m_is_started = true;
+    }
+
+    void Stop()
+    {
+        if (m_is_started) {
+            m_is_started = false;
+
+            struct timespec end_time;
+            clock_gettime(CLOCK_MONOTONIC, &end_time);
+
+            m_total_time += (double)(end_time.tv_sec - m_start_time.tv_sec) +
+                            (double)(end_time.tv_nsec - m_start_time.tv_nsec) / 1e9;
+        }
+    }
+
+    double GetTimeInSeconds()
+    {
+        if (m_is_started) {
+            Stop();
+            Start();
+        }
+        return m_total_time;
+    }
+};
+
+#endif
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/Timer.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/Timer.hpp
deleted file mode 100644
index c0dac6c..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/includes/Timer.hpp
+++ /dev/null
@@ -1,47 +0,0 @@
-
-#ifndef __TIMER_H__
-#define __TIMER_H__
-
-#if __has_include(<sycl/sycl.hpp>)
-#include <sycl/sycl.hpp>
-#elif __has_include(<CL/sycl.hpp>)
-#include <CL/sycl.hpp>
-#else
-#error "Unsupported compiler"
-#endif
-#include <chrono>
-
-class GPUTimer {
-    sycl::event start, stop;
-    std::chrono::time_point<std::chrono::steady_clock> start_ct1;
-    std::chrono::time_point<std::chrono::steady_clock> stop_ct1;
-
-public:
-    GPUTimer() {}
-    ~GPUTimer() {}
-
-    inline void Record() { start_ct1 = std::chrono::steady_clock::now(); }
-    inline void Elapsed(float& time_elapsed)
-    {
-        stop_ct1 = std::chrono::steady_clock::now();
-        stop.wait_and_throw();
-        time_elapsed = std::chrono::duration<float, std::milli>(stop_ct1 - start_ct1).count();
-    }
-};
-
-class CPUTimer {
-    std::chrono::high_resolution_clock::time_point start;
-
-public:
-    CPUTimer() : start(std::chrono::high_resolution_clock::now()) {}
-    inline void Reset() { start = std::chrono::high_resolution_clock::now(); }
-    inline float Elapsed()
-    {
-        auto temp = start;
-        start = std::chrono::high_resolution_clock::now();
-        return (float)(std::chrono::duration_cast<std::chrono::microseconds>(start - temp).count() /
-                       1e3);
-    }
-};
-
-#endif
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/common.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/common.hpp
deleted file mode 100644
index 953bd3c..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/includes/common.hpp
+++ /dev/null
@@ -1,30 +0,0 @@
-#pragma once
-#include <torch/extension.h>
-
-#define CHECK_XPU(x) AT_ASSERTM(x.is_xpu(), #x " must be a XPU tensor")
-#define CHECK_CONTIGUOUS(x) AT_ASSERTM(x.is_contiguous(), #x " must be contiguous")
-#define CHECK_INPUT(x) \
-    CHECK_XPU(x);      \
-    CHECK_CONTIGUOUS(x)
-
-template <typename T>
-inline void print_nan(sycl::queue* stream, int bsz, const T* buf, char* name)
-{
-    T temp_tensor[10000];
-    bool has_nan = false;
-    stream->wait();
-    stream->memcpy(temp_tensor, buf, bsz * sizeof(T));
-    stream->wait();
-    for (int i = 0; i < bsz; i++) {
-        if (isnan(float(temp_tensor[i]))) { has_nan = true; }
-    }
-    printf("%s[%d](%p)%s --> ", name, bsz, buf, has_nan ? "has_nan" : "");
-    for (int i = 0; i < bsz; i++) {
-        if (isnan(float(temp_tensor[i]))) {
-            printf("%d:nan ", i);
-        } else {
-            printf("%d:%f, ", i, float(temp_tensor[i]));
-        }
-    }
-    printf("\n");
-}
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/compat.h b/intel_extension_for_deepspeed/op_builder/csrc/includes/compat.h
new file mode 100644
index 0000000..6d54446
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/includes/compat.h
@@ -0,0 +1,19 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+/*
+Copyright NVIDIA/apex
+This file is adapted from fused adam in NVIDIA/apex, commit a109f85
+*/
+
+#ifndef TORCH_CHECK
+#define TORCH_CHECK AT_CHECK
+#endif
+
+#ifdef VERSION_GE_1_3
+#define DATA_PTR data_ptr
+#else
+#define DATA_PTR data
+#endif
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/context.h b/intel_extension_for_deepspeed/op_builder/csrc/includes/context.h
new file mode 100644
index 0000000..80e7cd3
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/includes/context.h
@@ -0,0 +1,215 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+#pragma once
+
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+/* #include <ATen/cuda/CUDAContext.h> */
+#include <cassert>
+#include <iostream>
+#include <vector>
+#include <dpct/blas_utils.hpp>
+
+#include <dpct/rng_utils.hpp>
+
+#include "gemm_test.h"
+
+#include <ipex.h>
+
+#ifndef SYCL_CUDA_STREAM
+#define SYCL_CUDA_STREAM
+namespace quant {
+namespace at {
+  namespace cuda {
+    dpct::queue_ptr getCurrentCUDAStream() {
+      auto device_type = c10::DeviceType::XPU;
+      c10::impl::VirtualGuardImpl impl(device_type);
+      c10::Stream c10_stream = impl.getStream(c10::Device(device_type));
+      auto& queue = xpu::get_queue_from_stream(c10_stream);
+      return &queue;
+    }
+
+    dpct::queue_ptr getStreamFromPool(bool) {
+      // not implemented
+      return nullptr;
+    }
+    
+    dpct::queue_ptr getStreamFromPool() {
+      // not implemented
+      return nullptr;
+    }
+  }
+}
+}
+#endif
+
+#define WARP_SIZE 32
+
+#define CUDA_CHECK(callstr)                                                                    \
+    {                                                                                          \
+        cudaError_t error_code = callstr;                                                      \
+        if (error_code != cudaSuccess) {                                                       \
+            std::cerr << "CUDA error " << error_code << " at " << __FILE__ << ":" << __LINE__; \
+            assert(0);                                                                         \
+        }                                                                                      \
+    }
+
+#define CUDA_1D_KERNEL_LOOP(i, n) \
+    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < (n); i += blockDim.x * gridDim.x)
+
+#define CUDA_2D_KERNEL_LOOP(i, n, j, m)                                                          \
+    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < (n); i += blockDim.x * gridDim.x) \
+        for (size_t j = blockIdx.y * blockDim.y + threadIdx.y; j < (m); j += blockDim.y * gridDim.y)
+
+#define DS_CUDA_NUM_THREADS 512
+#define DS_MAXIMUM_NUM_BLOCKS 262144
+
+inline int DS_GET_BLOCKS(const int N)
+{
+    return (std::max)(
+        (std::min)((N + DS_CUDA_NUM_THREADS - 1) / DS_CUDA_NUM_THREADS, DS_MAXIMUM_NUM_BLOCKS),
+        // Use at least 1 block, since CUDA does not allow empty block
+        1);
+}
+
+class TrainingContext {
+public:
+    TrainingContext() try : _workspace(nullptr), _seed(42), _curr_offset(0) {
+        _gen = dpct::rng::create_host_rng(dpct::rng::random_engine_type::mcg59);
+        _gen->set_seed(123);
+        int stat = DPCT_CHECK_ERROR(_cublasHandle = &dpct::get_in_order_queue());
+        if (stat != 0) {
+            // It would be nice to use cublasGetStatusName and
+            // cublasGetStatusString, but they were only added in CUDA 11.4.2.
+            auto message = std::string("Failed to create cublas handle: cublasStatus_t was ") +
+                           std::to_string(stat);
+            std::cerr << message << std::endl;
+            throw std::runtime_error(message);
+        }
+    }
+    catch (sycl::exception const& exc) {
+      std::cerr << exc.what() << "Exception caught at file:" << __FILE__ << ", line:" << __LINE__
+                << std::endl;
+      std::exit(1);
+    }
+
+    virtual ~TrainingContext()
+    {
+        _cublasHandle = nullptr;
+        sycl::free(_workspace, dpct::get_in_order_queue());
+    }
+
+    static TrainingContext& Instance()
+    {
+        static TrainingContext _ctx;
+        return _ctx;
+    }
+
+    void SetWorkSpace(void* workspace)
+    {
+        if (!workspace) { throw std::runtime_error("Workspace is null."); }
+        _workspace = workspace;
+    }
+
+    void* GetWorkSpace() { return _workspace; }
+
+    dpct::rng::host_rng_ptr& GetRandGenerator() { return _gen; }
+
+    dpct::queue_ptr GetCurrentStream()
+    {
+        // get current pytorch stream.
+        dpct::queue_ptr stream = quant::at::cuda::getCurrentCUDAStream();
+        return stream;
+    }
+
+    dpct::queue_ptr GetNewStream() { return quant::at::cuda::getStreamFromPool(); }
+
+    dpct::queue_ptr GetCublasHandle() { return _cublasHandle; }
+
+    std::pair<uint64_t, uint64_t> IncrementOffset(uint64_t offset_inc)
+    {
+        uint64_t offset = _curr_offset;
+        _curr_offset += offset_inc;
+        return std::pair<uint64_t, uint64_t>(_seed, offset);
+    }
+
+    void SetSeed(uint64_t new_seed) { _seed = new_seed; }
+
+    void TestGemmFP16(bool test_gemm, int batch_size, int seq_len, int head_num, int size_per_head)
+    {
+        // avoid rerun.
+        if (_gemm_algos.size() > 0) return;
+
+        if (test_gemm) {
+            dpct::queue_ptr handle = GetCublasHandle();
+
+            std::unique_ptr<GemmTest<sycl::half>> test_qkv_fw(
+                new GemmTest<sycl::half>(batch_size * seq_len,      // M
+                                     head_num * size_per_head,  // N
+                                     head_num * size_per_head,  // K
+                                     oneapi::mkl::transpose::trans,
+                                     oneapi::mkl::transpose::nontrans,
+                                     handle));
+
+            std::unique_ptr<GemmTest<sycl::half>> test_inter(
+                new GemmTest<sycl::half>(batch_size * seq_len,          // M
+                                     4 * head_num * size_per_head,  // N
+                                     head_num * size_per_head,      // K
+                                     oneapi::mkl::transpose::trans,
+                                     oneapi::mkl::transpose::nontrans,
+                                     handle));
+
+            std::unique_ptr<GemmTest<sycl::half>> test_output(
+                new GemmTest<sycl::half>(batch_size * seq_len,          // M
+                                     head_num * size_per_head,      // N
+                                     4 * head_num * size_per_head,  // K
+                                     oneapi::mkl::transpose::trans,
+                                     oneapi::mkl::transpose::nontrans,
+                                     handle));
+
+            std::unique_ptr<StridedGemmTest<sycl::half>> test_attn_scores(
+                new StridedGemmTest<sycl::half>(batch_size * head_num,  // batch
+                                            seq_len,                // M
+                                            seq_len,                // N
+                                            size_per_head,          // K
+                                            oneapi::mkl::transpose::trans,
+                                            oneapi::mkl::transpose::nontrans,
+                                            handle));
+
+            std::unique_ptr<StridedGemmTest<sycl::half>> test_attn_context(
+                new StridedGemmTest<sycl::half>(batch_size * head_num,  // batch
+                                            size_per_head,          // M
+                                            seq_len,                // N
+                                            seq_len,                // K
+                                            oneapi::mkl::transpose::nontrans,
+                                            oneapi::mkl::transpose::nontrans,
+                                            handle));
+
+            _gemm_algos.push_back(test_qkv_fw->TestAlgo(100));
+            _gemm_algos.push_back(test_inter->TestAlgo(100));
+            _gemm_algos.push_back(test_output->TestAlgo(100));
+            _gemm_algos.push_back(test_attn_scores->TestAlgo(100));
+            _gemm_algos.push_back(test_attn_context->TestAlgo(100));
+        } else {
+            // Use default algo.
+            _gemm_algos.push_back(std::array<int, 3>({99, 99, 99}));
+            _gemm_algos.push_back(std::array<int, 3>({99, 99, 99}));
+            _gemm_algos.push_back(std::array<int, 3>({99, 99, 99}));
+            _gemm_algos.push_back(std::array<int, 3>({99, 99, 99}));
+            _gemm_algos.push_back(std::array<int, 3>({99, 99, 99}));
+        }
+    }
+
+    const std::vector<std::array<int, 3>>& GetGemmAlgos() const { return _gemm_algos; }
+
+private:
+    dpct::rng::host_rng_ptr _gen;
+    dpct::queue_ptr _cublasHandle;
+    void* _workspace;
+    uint64_t _seed;
+    uint64_t _curr_offset;
+    std::vector<std::array<int, 3>> _gemm_algos;
+};
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/context.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/context.hpp
deleted file mode 100644
index 4e90bd5..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/includes/context.hpp
+++ /dev/null
@@ -1,152 +0,0 @@
-#pragma once
-
-#include <ATen/record_function.h>
-#include <c10/core/Stream.h>
-#include <ipex.h>
-#include <torch/extension.h>
-#include <torch/library.h>
-#if __has_include(<sycl/sycl.hpp>)
-#include <sycl/sycl.hpp>
-#elif __has_include(<CL/sycl.hpp>)
-#include <CL/sycl.hpp>
-#else
-#error "Unsupported compiler"
-#endif
-#include <cassert>
-#include <ext/oneapi/bfloat16.hpp>
-#include <iostream>
-#include <oneapi/mkl.hpp>
-#include <oneapi/mkl/rng/device.hpp>
-#include <vector>
-using bf16 = sycl::ext::oneapi::bfloat16;
-
-#define WARP_SIZE 32
-#define ONEMKL_OP_T oneapi::mkl::transpose::trans
-#define ONEMKL_OP_N oneapi::mkl::transpose::nontrans
-
-#define DPCPP_1D_KERNEL_LOOP(i, n) \
-    for (size_t(i) = item_ct1.get_global_id(2); (i) < (n); (i) += item_ct1.get_global_range(2))
-
-#define DPCPP_2D_KERNEL_LOOP(i, n, j, m)                                                       \
-    for (size_t i = item_ct1.get_global_id(2); (i) < (n); (i) += item_ct1.get_global_range(2)) \
-        for (size_t j = item_ct1.get_global_id(1); (j) < (m); (j) += item_ct1.get_global_range(1))
-
-#define DS_CUDA_NUM_THREADS 512
-#define DS_MAXIMUM_NUM_BLOCKS 262144
-
-inline int DS_GET_BLOCKS(const int N)
-{
-    return (std::max)(
-        (std::min)((N + DS_CUDA_NUM_THREADS - 1) / DS_CUDA_NUM_THREADS, DS_MAXIMUM_NUM_BLOCKS),
-        // Use at least 1 block, since CUDA does not allow empty block
-        1);
-}
-
-class SyclContext {
-public:
-    SyclContext()
-    try : _workspace(nullptr), _seed(42), _curr_offset(0) {
-        auto type_ = c10::DeviceType::XPU;
-        c10::impl::VirtualGuardImpl impl(type_);
-        auto device_ = c10::Device(type_);
-        c10::Stream dpcpp_stream = impl.getStream(device_);
-        _gen = new oneapi::mkl::rng::philox4x32x10(xpu::get_queue_from_stream(dpcpp_stream), 123);
-        if ((_onemklQ = &xpu::get_queue_from_stream(dpcpp_stream), 0) != 0) {
-            auto message = std::string("Fail to create onemkl queue.");
-            std::cerr << message << std::endl;
-            throw std::runtime_error(message);
-        }
-    } catch (sycl::exception const& exc) {
-        std::cerr << exc.what() << "Exception caught at file:" << __FILE__ << ", line:" << __LINE__
-                  << std::endl;
-        std::exit(1);
-    }
-
-    virtual ~SyclContext()
-    {
-        _onemklQ = nullptr;
-        free(_gen);
-
-        auto type_ = c10::DeviceType::XPU;
-        c10::impl::VirtualGuardImpl impl(type_);
-        auto device_ = c10::Device(type_);
-        c10::Stream dpcpp_stream = impl.getStream(device_);
-        sycl::free(_workspace, xpu::get_queue_from_stream(dpcpp_stream));
-    }
-
-    static SyclContext& Instance()
-    {
-        static SyclContext _ctx;
-        return _ctx;
-    }
-
-    void SetWorkSpace(void* workspace)
-    {
-        if (!workspace) { throw std::runtime_error("Workspace is null."); }
-        _workspace = workspace;
-    }
-
-    void* GetWorkSpace() { return _workspace; }
-
-    sycl::queue* GetCurrentStream()
-    {
-        // get current pytorch stream.
-        // return &xpu::dpcpp::getCurrentDPCPPStream().dpcpp_queue();
-
-        auto type_ = c10::DeviceType::XPU;
-        c10::impl::VirtualGuardImpl impl(type_);
-        auto device_ = c10::Device(type_);
-        c10::Stream dpcpp_stream = impl.getStream(device_);
-        return &xpu::get_queue_from_stream(dpcpp_stream);
-    }
-
-    sycl::queue* GetNewStream()
-    {
-        auto type_ = c10::DeviceType::XPU;
-        c10::impl::VirtualGuardImpl impl(type_);
-        auto device_ = c10::Device(type_);
-        c10::Stream dpcpp_stream = impl.getStream(device_);
-        c10::Stream stream = impl.getStreamFromGlobalPool(device_, /*isHighPriority=*/false);
-
-        return &xpu::get_queue_from_stream(dpcpp_stream);
-    }
-
-    sycl::queue* GetOneMKLQ() { return _onemklQ; }
-
-    std::pair<uint64_t, uint64_t> IncrementOffset(uint64_t offset_inc)
-    {
-        uint64_t offset = _curr_offset;
-        _curr_offset += offset_inc;
-        // set _GPT_DEBUG_ and fix seed to avoid randomness
-#ifdef _GPT_DEBUG_
-        return std::pair<uint64_t, uint64_t>(_seed, 0);
-#else
-        return std::pair<uint64_t, uint64_t>(_seed, offset);
-#endif
-    }
-
-    void SetSeed(uint64_t new_seed) { _seed = new_seed; }
-
-    void TestGemmFP16(bool test_gemm, int batch_size, int seq_len, int head_num, int size_per_head)
-    {
-        // avoid rerun.
-        if (_gemm_algos.size() > 0) return;
-
-        // Use default algo.
-        _gemm_algos.push_back(std::array<int, 3>({99, 99, 99}));
-        _gemm_algos.push_back(std::array<int, 3>({99, 99, 99}));
-        _gemm_algos.push_back(std::array<int, 3>({99, 99, 99}));
-        _gemm_algos.push_back(std::array<int, 3>({99, 99, 99}));
-        _gemm_algos.push_back(std::array<int, 3>({99, 99, 99}));
-    }
-
-    const std::vector<std::array<int, 3>>& GetGemmAlgos() const { return _gemm_algos; }
-
-private:
-    oneapi::mkl::rng::philox4x32x10* _gen;
-    sycl::queue* _onemklQ;
-    void* _workspace;
-    uint64_t _seed;
-    uint64_t _curr_offset;
-    std::vector<std::array<int, 3>> _gemm_algos;
-};
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/conversion_utils.h b/intel_extension_for_deepspeed/op_builder/csrc/includes/conversion_utils.h
new file mode 100644
index 0000000..7b7adda
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/includes/conversion_utils.h
@@ -0,0 +1,645 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+#pragma once
+
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include "ds_kernel_utils.h"
+
+#include <stdint.h>
+
+#ifdef BF16_AVAILABLE
+#endif
+
+namespace conversion {
+
+// Basic primitive for constructing conversions
+template <typename TO, typename FROM>
+DS_D_INLINE TO to(FROM val)
+{
+    return to(val);
+}
+
+// Specializations
+
+/********************* Identity Conversions *********************/
+/*
+Identity conversions are useful in templated functions where we might have
+a fixed destination type. For example, I might have a kernel that accepts
+sycl::half, __nv_bfloat16, and float but always want to do the core computation
+at floating point:
+
+T mem_value = input[idx];
+float compute_value = conversion::to<float, T>(mem_value);
+
+In practice, we should be able to elide the second template parameter:
+float compute_val = conversion::to<float>(mem_value);
+
+In this case, we need an implementation to handle the T = float case
+
+NOTE: The type inferencing system appears to be unable to handle inferring the first
+template parameter, even in the trivial case.
+*/
+
+// Floating point types
+template <>
+DS_D_INLINE double to(double val)
+{
+    return val;
+}
+template <>
+DS_D_INLINE float to(float val)
+{
+    return val;
+}
+template <>
+DS_D_INLINE sycl::half to(sycl::half val)
+{
+    return val;
+}
+#ifdef BF16_AVAILABLE
+template <>
+DS_D_INLINE sycl::ext::oneapi::bfloat16 to(sycl::ext::oneapi::bfloat16 val)
+{
+    return val;
+}
+#endif
+
+// Integer types
+template <>
+DS_D_INLINE int8_t to(int8_t val)
+{
+    return val;
+}
+template <>
+DS_D_INLINE uint8_t to(uint8_t val)
+{
+    return val;
+}
+template <>
+DS_D_INLINE int16_t to(int16_t val)
+{
+    return val;
+}
+template <>
+DS_D_INLINE uint16_t to(uint16_t val)
+{
+    return val;
+}
+template <>
+DS_D_INLINE int32_t to(int32_t val)
+{
+    return val;
+}
+template <>
+DS_D_INLINE uint32_t to(uint32_t val)
+{
+    return val;
+}
+template <>
+DS_D_INLINE int64_t to(int64_t val)
+{
+    return val;
+}
+template <>
+DS_D_INLINE uint64_t to(uint64_t val)
+{
+    return val;
+}
+
+// TODO: evaluate if we want bools
+
+/*********************  To Double Conversions *********************/
+
+// * to double variants
+
+// Would normally like to not use C cast, but this is an important enough conversion
+// to keep
+template <>
+DS_D_INLINE double to(float val)
+{
+#ifdef PTX_AVAILABLE
+    double ret_val;
+    /*
+    DPCT1053:0: Migration of device assembly code is not supported.
+    */
+    asm("ctv.rn.f64.f32 %0, %1;\n" : "=d"(ret_val) : "f"(val));
+    return ret_val;
+#else
+    return double(val);
+#endif
+}
+// Note: there is a CVT instruction for sycl::half -> double, but there's no inline interface
+// for passing a single half value
+template <>
+DS_D_INLINE double to(sycl::half val)
+{
+    return to<double>(
+        sycl::vec<sycl::half, 1>{val}.convert<float, sycl::rounding_mode::automatic>()[0]);
+}
+template <>
+DS_D_INLINE double to(int64_t val)
+{
+    return sycl::vec<long long, 1>{val}.convert<double, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE double to(int32_t val)
+{
+    return sycl::vec<int, 1>{val}.convert<double, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE double to(int16_t val)
+{
+    return sycl::vec<int, 1>{val}.convert<double, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE double to(int8_t val)
+{
+    return sycl::vec<int, 1>{val}.convert<double, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE double to(uint64_t val)
+{
+    return sycl::vec<unsigned long long, 1>{val}.convert<double, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE double to(uint32_t val)
+{
+    return sycl::vec<unsigned int, 1>{val}.convert<double, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE double to(uint16_t val)
+{
+    return sycl::vec<unsigned int, 1>{val}.convert<double, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE double to(uint8_t val)
+{
+    return sycl::vec<unsigned int, 1>{val}.convert<double, sycl::rounding_mode::rte>()[0];
+}
+
+// Same applies here
+#ifdef BF16_AVAILABLE
+template <>
+DS_D_INLINE double to(sycl::ext::oneapi::bfloat16 val)
+{
+    return to<double>(static_cast<float>(val));
+}
+#endif
+
+/*********************  To Float Conversions *********************/
+
+template <>
+DS_D_INLINE float to(double val)
+{
+    return sycl::vec<double, 1>{val}.convert<float, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE float to(sycl::half val)
+{
+    return sycl::vec<sycl::half, 1>{val}.convert<float, sycl::rounding_mode::automatic>()[0];
+}
+template <>
+DS_D_INLINE float to(int64_t val)
+{
+    return sycl::vec<long long, 1>{val}.convert<float, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE float to(int32_t val)
+{
+    return sycl::vec<int, 1>{val}.convert<float, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE float to(int16_t val)
+{
+    return sycl::vec<int, 1>{val}.convert<float, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE float to(int8_t val)
+{
+    return sycl::vec<int, 1>{val}.convert<float, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE float to(uint64_t val)
+{
+    return sycl::vec<unsigned long long, 1>{val}.convert<float, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE float to(uint32_t val)
+{
+    return sycl::vec<unsigned int, 1>{val}.convert<float, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE float to(uint16_t val)
+{
+    return sycl::vec<unsigned int, 1>{val}.convert<float, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE float to(uint8_t val)
+{
+    return sycl::vec<unsigned int, 1>{val}.convert<float, sycl::rounding_mode::rte>()[0];
+}
+
+#ifdef BF16_AVAILABLE
+template <>
+DS_D_INLINE float to(sycl::ext::oneapi::bfloat16 val)
+{
+    return static_cast<float>(val);
+}
+#endif
+
+/*********************  To Float2 Conversions *********************/
+template <>
+DS_D_INLINE sycl::float2 to(sycl::half2 val)
+{
+    return val.convert<float, sycl::rounding_mode::automatic>();
+}
+
+#ifdef BF16_AVAILABLE
+template <>
+DS_D_INLINE sycl::float2 to(sycl::marray<sycl::ext::oneapi::bfloat16, 2> val)
+{
+    return sycl::float2(val[0], val[1]);
+}
+#endif
+
+/*********************  To Half Conversions *********************/
+template <>
+DS_D_INLINE sycl::half to(double val)
+{
+#ifdef __HIP_PLATFORM_AMD__
+    float val_f = __double2float_rn(val);
+    return __float2half(val_f);
+#else
+    return sycl::half(val);
+#endif
+}
+template <>
+DS_D_INLINE sycl::half to(float val)
+{
+    return sycl::vec<float, 1>{val}.convert<sycl::half, sycl::rounding_mode::automatic>()[0];
+}
+template <>
+DS_D_INLINE sycl::half to(int64_t val)
+{
+    return sycl::vec<long long, 1>{val}.convert<sycl::half, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE sycl::half to(int32_t val)
+{
+    return sycl::vec<int, 1>{val}.convert<sycl::half, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE sycl::half to(int16_t val)
+{
+    return sycl::vec<short, 1>{val}.convert<sycl::half, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE sycl::half to(int8_t val)
+{
+    return sycl::vec<int, 1>{val}.convert<sycl::half, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE sycl::half to(uint64_t val)
+{
+    return sycl::vec<unsigned long long, 1>{val}.convert<sycl::half, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE sycl::half to(uint32_t val)
+{
+    return sycl::vec<unsigned int, 1>{val}.convert<sycl::half, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE sycl::half to(uint16_t val)
+{
+    return sycl::vec<unsigned short, 1>{val}.convert<sycl::half, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE sycl::half to(uint8_t val)
+{
+    return sycl::vec<unsigned int, 1>{val}.convert<sycl::half, sycl::rounding_mode::rte>()[0];
+}
+
+#ifdef BF16_AVAILABLE
+// No direct conversion
+template <>
+DS_D_INLINE sycl::half to(sycl::ext::oneapi::bfloat16 val)
+{
+    return to<sycl::half>(to<float>(val));
+}
+#endif
+
+/*********************  To Half2 Conversions *********************/
+template <>
+DS_D_INLINE sycl::half2 to(sycl::float2 val)
+{
+    return val.convert<sycl::half, sycl::rounding_mode::rte>();
+}
+template <>
+DS_D_INLINE sycl::half2 to(float val)
+{
+    return sycl::float2{val, val}.convert<sycl::half, sycl::rounding_mode::rte>();
+}
+
+#ifdef BF16_AVAILABLE
+// No direct conversion
+template <>
+DS_D_INLINE sycl::half2 to(sycl::marray<sycl::ext::oneapi::bfloat16, 2> val)
+{
+    return to<sycl::half2>(to<sycl::float2>(val));
+}
+#endif
+
+/*********************  To BF16 Conversions *********************/
+#ifdef BF16_AVAILABLE
+template <>
+DS_D_INLINE sycl::ext::oneapi::bfloat16 to(double val)
+{
+    return sycl::ext::oneapi::bfloat16(val);
+}
+template <>
+DS_D_INLINE sycl::ext::oneapi::bfloat16 to(float val)
+{
+    return sycl::ext::oneapi::bfloat16(val);
+}
+template <>
+DS_D_INLINE sycl::ext::oneapi::bfloat16 to(int64_t val)
+{
+    return sycl::ext::oneapi::bfloat16(val);
+}
+template <>
+DS_D_INLINE sycl::ext::oneapi::bfloat16 to(int32_t val)
+{
+    return sycl::ext::oneapi::bfloat16(val);
+}
+template <>
+DS_D_INLINE sycl::ext::oneapi::bfloat16 to(int16_t val)
+{
+    return sycl::ext::oneapi::bfloat16(val);
+}
+template <>
+DS_D_INLINE sycl::ext::oneapi::bfloat16 to(int8_t val)
+{
+    return sycl::ext::oneapi::bfloat16(val);
+}
+template <>
+DS_D_INLINE sycl::ext::oneapi::bfloat16 to(uint64_t val)
+{
+    return sycl::ext::oneapi::bfloat16(val);
+}
+template <>
+DS_D_INLINE sycl::ext::oneapi::bfloat16 to(uint32_t val)
+{
+    return sycl::ext::oneapi::bfloat16(val);
+}
+template <>
+DS_D_INLINE sycl::ext::oneapi::bfloat16 to(uint16_t val)
+{
+    return sycl::ext::oneapi::bfloat16(val);
+}
+template <>
+DS_D_INLINE sycl::ext::oneapi::bfloat16 to(uint8_t val)
+{
+    return sycl::ext::oneapi::bfloat16(val);
+}
+#endif
+
+/*********************  To BF162 Conversions *********************/
+#ifdef BF16_AVAILABLE
+template <>
+DS_D_INLINE sycl::marray<sycl::ext::oneapi::bfloat16, 2> to(sycl::float2 val)
+{
+    return sycl::marray<sycl::ext::oneapi::bfloat16, 2>(val[0], val[1]);
+}
+template <>
+DS_D_INLINE sycl::marray<sycl::ext::oneapi::bfloat16, 2> to(float val)
+{
+    return sycl::marray<sycl::ext::oneapi::bfloat16, 2>(val, val);
+}
+template <>
+DS_D_INLINE sycl::marray<sycl::ext::oneapi::bfloat16, 2> to(sycl::half2 val)
+{
+    return to<sycl::marray<sycl::ext::oneapi::bfloat16, 2>>(to<sycl::float2>(val));
+}
+#endif
+
+/*********************  To INT64_T Conversions *********************/
+template <>
+DS_D_INLINE int64_t to(double val)
+{
+    return sycl::vec<double, 1>{val}.convert<long long, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE int64_t to(float val)
+{
+    return sycl::vec<float, 1>{val}.convert<long long, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE int64_t to(sycl::half val)
+{
+    return sycl::vec<sycl::half, 1>{val}.convert<long long, sycl::rounding_mode::rte>()[0];
+}
+// No direct support for integer casts at the C++ level and I don't feel they're so important
+// to demand an PTX at this time
+
+#ifdef BF16_AVAILABLE
+template <>
+DS_D_INLINE int64_t to(sycl::ext::oneapi::bfloat16 val)
+{
+    return sycl::vec<float, 1>(val).convert<long long, sycl::rounding_mode::rte>()[0];
+}
+#endif
+
+/*********************  To INT32_T Conversions *********************/
+template <>
+DS_D_INLINE int32_t to(double val)
+{
+    return sycl::vec<double, 1>{val}.convert<int, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE int32_t to(float val)
+{
+    return sycl::vec<float, 1>{val}.convert<int, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE int32_t to(sycl::half val)
+{
+    return sycl::vec<sycl::half, 1>{val}.convert<int, sycl::rounding_mode::rte>()[0];
+}
+// No direct support for integer casts at the C++ level and I don't feel they're so important
+// to demand an PTX at this time
+
+#ifdef BF16_AVAILABLE
+template <>
+DS_D_INLINE int32_t to(sycl::ext::oneapi::bfloat16 val)
+{
+    return sycl::vec<float, 1>(val).convert<int, sycl::rounding_mode::rte>()[0];
+}
+#endif
+
+/*********************  To INT16_T Conversions *********************/
+template <>
+DS_D_INLINE int16_t to(double val)
+{
+    return sycl::vec<double, 1>{val}.convert<int, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE int16_t to(float val)
+{
+    return sycl::vec<float, 1>{val}.convert<int, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE int16_t to(sycl::half val)
+{
+    return sycl::vec<sycl::half, 1>{val}.convert<int, sycl::rounding_mode::rte>()[0];
+}
+// No direct support for integer casts at the C++ level and I don't feel they're so important
+// to demand an PTX at this time
+
+#ifdef BF16_AVAILABLE
+template <>
+DS_D_INLINE int16_t to(sycl::ext::oneapi::bfloat16 val)
+{
+    return sycl::vec<float, 1>(val).convert<int, sycl::rounding_mode::rte>()[0];
+}
+#endif
+
+/*********************  To INT8_T Conversions *********************/
+template <>
+DS_D_INLINE int8_t to(double val)
+{
+    return sycl::vec<double, 1>{val}.convert<int, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE int8_t to(float val)
+{
+    return sycl::vec<float, 1>{val}.convert<int, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE int8_t to(sycl::half val)
+{
+    return sycl::vec<sycl::half, 1>{val}.convert<int, sycl::rounding_mode::rte>()[0];
+}
+// No direct support for integer casts at the C++ level and I don't feel they're so important
+// to demand an PTX at this time
+
+#ifdef BF16_AVAILABLE
+template <>
+DS_D_INLINE int8_t to(sycl::ext::oneapi::bfloat16 val)
+{
+    return sycl::vec<float, 1>(val).convert<int, sycl::rounding_mode::rte>()[0];
+}
+#endif
+
+/*********************  To UINT64_T Conversions *********************/
+template <>
+DS_D_INLINE uint64_t to(double val)
+{
+    return sycl::vec<double, 1>{val}.convert<unsigned long long, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE uint64_t to(float val)
+{
+    return sycl::vec<float, 1>{val}.convert<unsigned long long, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE uint64_t to(sycl::half val)
+{
+    return sycl::vec<sycl::half, 1>{val}.convert<unsigned long long, sycl::rounding_mode::rte>()[0];
+}
+// No direct support for integer casts at the C++ level and I don't feel they're so important
+// to demand an PTX at this time
+
+#ifdef BF16_AVAILABLE
+template <>
+DS_D_INLINE uint64_t to(sycl::ext::oneapi::bfloat16 val)
+{
+    return sycl::vec<float, 1>(val).convert<unsigned long long, sycl::rounding_mode::rte>()[0];
+}
+#endif
+
+/*********************  To UINT32_T Conversions *********************/
+template <>
+DS_D_INLINE uint32_t to(double val)
+{
+    return sycl::vec<double, 1>{val}.convert<unsigned int, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE uint32_t to(float val)
+{
+    return sycl::vec<float, 1>{val}.convert<unsigned int, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE uint32_t to(sycl::half val)
+{
+    return sycl::vec<sycl::half, 1>{val}.convert<unsigned int, sycl::rounding_mode::rte>()[0];
+}
+// No direct support for integer casts at the C++ level and I don't feel they're so important
+// to demand an PTX at this time
+
+#ifdef BF16_AVAILABLE
+template <>
+DS_D_INLINE uint32_t to(sycl::ext::oneapi::bfloat16 val)
+{
+    return sycl::vec<float, 1>(val).convert<unsigned, sycl::rounding_mode::rte>()[0];
+}
+#endif
+
+/*********************  To UINT16_T Conversions *********************/
+template <>
+DS_D_INLINE uint16_t to(double val)
+{
+    return sycl::vec<double, 1>{val}.convert<unsigned int, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE uint16_t to(float val)
+{
+    return sycl::vec<float, 1>{val}.convert<unsigned int, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE uint16_t to(sycl::half val)
+{
+    return sycl::vec<sycl::half, 1>{val}.convert<unsigned int, sycl::rounding_mode::rte>()[0];
+}
+// No direct support for integer casts at the C++ level and I don't feel they're so important
+// to demand an PTX at this time
+
+#ifdef BF16_AVAILABLE
+template <>
+DS_D_INLINE uint16_t to(sycl::ext::oneapi::bfloat16 val)
+{
+    return sycl::vec<float, 1>(val).convert<unsigned, sycl::rounding_mode::rte>()[0];
+}
+#endif
+
+/*********************  To UINT8_T Conversions *********************/
+template <>
+DS_D_INLINE uint8_t to(double val)
+{
+    return sycl::vec<double, 1>{val}.convert<unsigned int, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE uint8_t to(float val)
+{
+    return sycl::vec<float, 1>{val}.convert<unsigned int, sycl::rounding_mode::rte>()[0];
+}
+template <>
+DS_D_INLINE uint8_t to(sycl::half val)
+{
+    return sycl::vec<sycl::half, 1>{val}.convert<unsigned int, sycl::rounding_mode::rte>()[0];
+}
+// No direct support for integer casts at the C++ level and I don't feel they're so important
+// to demand an PTX at this time
+
+#ifdef BF16_AVAILABLE
+template <>
+DS_D_INLINE uint8_t to(sycl::ext::oneapi::bfloat16 val)
+{
+    return sycl::vec<float, 1>(val).convert<unsigned, sycl::rounding_mode::rte>()[0];
+}
+#endif
+
+}  // namespace conversion
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/cpu_adagrad.h b/intel_extension_for_deepspeed/op_builder/csrc/includes/cpu_adagrad.h
new file mode 100644
index 0000000..1064a7b
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/includes/cpu_adagrad.h
@@ -0,0 +1,209 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+#pragma once
+
+#define NOMINMAX  // Windows idiosyncrasy
+                  // https://stackoverflow.com/questions/4913922/possible-problems-with-nominmax-on-visual-c
+
+#include <stdio.h>
+#include <cassert>
+#include "simd.h"
+
+#if defined(__ENABLE_CUDA__)
+#include <cuda_fp16.h>
+#include <cuda_runtime_api.h>
+#include "cuda.h"
+#include "custom_cuda_layers.h"
+typedef sycl::half ds_half_precision_t;
+#elif defined(__ENABLE_CANN__)
+#include "acl/acl.h"
+#include "torch_npu/csrc/core/npu/NPUStream.h"
+typedef c10::Half ds_half_precision_t;
+#else
+typedef unsigned short ds_half_precision_t;
+#endif
+
+#define STEP(SPAN)                                             \
+    void Step_##SPAN(float* _params,                           \
+                     float* grads,                             \
+                     float* _exp_avg_sq,                       \
+                     size_t _param_size,                       \
+                     ds_half_precision_t* dev_param = nullptr, \
+                     bool half_precision = false);
+
+class Adagrad_Optimizer {
+public:
+    Adagrad_Optimizer(float alpha = 1e-2, float eps = 1e-8, float weight_decay = 0)
+        : _alpha(alpha), _eps(eps), _weight_decay(weight_decay)
+    {
+#if defined(__ENABLE_CUDA__)
+        cudaMallocHost((void**)_doubled_buffer, TILE * sizeof(float));
+        cudaMallocHost((void**)(_doubled_buffer + 1), TILE * sizeof(float));
+
+        _streams[0] = TrainingContext::Instance().GetCurrentStream();
+        _streams[1] = TrainingContext::Instance().GetNewStream();
+        _buf_index = false;
+#elif defined(__ENABLE_CANN__)
+        aclrtMallocHost((void**)_doubled_buffer, TILE * sizeof(float));
+        aclrtMallocHost((void**)(_doubled_buffer + 1), TILE * sizeof(float));
+
+        _buf_index = false;
+#endif
+    }
+    ~Adagrad_Optimizer()
+    {
+#if defined(__ENABLE_CUDA__)
+        cudaFreeHost(_doubled_buffer[0]);
+        cudaFreeHost(_doubled_buffer[1]);
+#elif defined(__ENABLE_CANN__)
+        aclrtFreeHost(_doubled_buffer[0]);
+        aclrtFreeHost(_doubled_buffer[1]);
+#endif
+    }
+#if defined(__AVX512__) or defined(__AVX256__)
+    template <int span>
+    void Step_AVX(size_t* rounded_size,
+                  float* _params,
+                  float* grads,
+                  float* _exp_avg_sq,
+                  size_t param_size,
+                  ds_half_precision_t* dev_param = nullptr,
+                  bool half_precision = false);
+#endif
+    STEP(1)
+    STEP(4)
+    STEP(8)
+#if defined(__ENABLE_CUDA__)
+    inline void SynchronizeStreams()
+    {
+        for (int i = 0; i < 2; i++) cudaStreamSynchronize(_streams[i]);
+    }
+#elif defined(__ENABLE_CANN__)
+    inline void SynchronizeStreams()
+    {
+        for (int i = 0; i < 2; i++) aclrtSynchronizeStream(_streams[i].stream());
+    }
+#endif
+    inline void IncrementStep(size_t step)
+    {
+        _step++;
+        if (_step != step) { _step = step; }
+    }
+    inline void update_state(float lr, float epsilon, float weight_decay)
+    {
+        _alpha = lr;
+        _eps = epsilon;
+        _weight_decay = weight_decay;
+    }
+
+private:
+    float _alpha;
+    float _eps;
+    float _weight_decay;
+
+    float _betta1_t;
+    float _betta2_t;
+    size_t _step;
+
+#if defined(__ENABLE_CUDA__)
+    bool _buf_index;
+    float* _doubled_buffer[2];
+    cudaStream_t _streams[2];
+#elif defined(__ENABLE_CANN__)
+    float* _doubled_buffer[2];
+    c10_npu::NPUStream _streams[2] = {c10_npu::getCurrentNPUStream(),
+                                      c10_npu::getNPUStreamFromPool()};
+    bool _buf_index;
+#endif
+};
+
+#if defined(__AVX512__) or defined(__AVX256__)
+template <int span>
+void Adagrad_Optimizer::Step_AVX(size_t* rounded_size,
+                                 float* _params,
+                                 float* grads,
+                                 float* _exp_avg_sq,
+                                 size_t _param_size,
+                                 ds_half_precision_t* dev_params,
+                                 bool half_precision)
+{
+    size_t new_rounded_size = 0;
+    AVX_Data eps_4;
+    eps_4.data = SIMD_SET(_eps);
+
+    float step_size = -1 * _alpha;
+    AVX_Data step_size_4;
+    step_size_4.data = SIMD_SET(step_size);
+
+    AVX_Data weight_decay4;
+    if (_weight_decay > 0) weight_decay4.data = SIMD_SET(_weight_decay);
+    new_rounded_size = ROUND_DOWN(_param_size, SIMD_WIDTH * span);
+    for (size_t t = 0; t < new_rounded_size; t += TILE) {
+        size_t copy_size = TILE;
+        if ((t + TILE) > new_rounded_size) copy_size = new_rounded_size - t;
+        size_t offset = copy_size + t;
+#if defined(__ENABLE_CUDA__)
+        if ((t / TILE) >= 2) { cudaStreamSynchronize(_streams[_buf_index]); }
+#elif defined(__ENABLE_CANN__)
+        if ((t / TILE) >= 2) { aclrtSynchronizeStream(_streams[_buf_index].stream()); }
+#endif
+#pragma omp parallel for
+        for (size_t i = t; i < offset; i += SIMD_WIDTH * span) {
+            AVX_Data grad_4[span];
+            simd_load<span>(grad_4, grads + i, half_precision);
+
+            AVX_Data momentum_4[span];
+            simd_load<span>(momentum_4, grads + i, false);
+
+            AVX_Data variance_4[span];
+            simd_load<span>(variance_4, _exp_avg_sq + i, false);
+
+            AVX_Data param_4[span];
+            simd_load<span>(param_4, _params + i, half_precision);
+
+            if (_weight_decay > 0) { simd_fma<span>(grad_4, param_4, weight_decay4, grad_4); }
+
+            simd_fma<span>(variance_4, grad_4, grad_4, variance_4);
+            simd_sqrt<span>(grad_4, variance_4);
+            simd_add<span>(grad_4, grad_4, eps_4);
+            simd_div<span>(grad_4, momentum_4, grad_4);
+            simd_fma<span>(param_4, grad_4, step_size_4, param_4);
+
+            simd_store<span>(_params + i, param_4, half_precision);
+#if defined(__ENABLE_CUDA__) or defined(__ENABLE_CANN__)
+            if (dev_params) {
+                simd_store<span>(_doubled_buffer[_buf_index] + (i - t), param_4, half_precision);
+            }
+#endif
+            simd_store<span>(_exp_avg_sq + i, variance_4, false);
+        }
+#if defined(__ENABLE_CUDA__)
+        if (dev_params) {
+            if (half_precision)
+                launch_param_update_half(
+                    _doubled_buffer[_buf_index], dev_params + t, copy_size, _streams[_buf_index]);
+            else
+                launch_param_update(
+                    _doubled_buffer[_buf_index], dev_params + t, copy_size, _streams[_buf_index]);
+
+            _buf_index = !_buf_index;
+        }
+#elif defined(__ENABLE_CANN__)
+        if (dev_params) {
+            size_t memcpy_size = copy_size * sizeof(_doubled_buffer[_buf_index][0]);
+            if (half_precision) memoryCopySize /= 2;
+            aclrtMemcpy(dev_params + t,
+                        memcpy_size,
+                        _doubled_buffer[_buf_index],
+                        memcpy_size,
+                        aclrtMemcpyKind::ACL_MEMCPY_HOST_TO_DEVICE);
+
+            _buf_index = !_buf_index;
+#endif
+    }
+    *rounded_size = new_rounded_size;
+}
+#endif
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/cpu_adagrad.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/cpu_adagrad.hpp
deleted file mode 100644
index 09dcf21..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/includes/cpu_adagrad.hpp
+++ /dev/null
@@ -1,84 +0,0 @@
-#pragma once
-#if (__x86_64__ || __i386__)
-#if __has_include(<sycl/sycl.hpp>)
-#include <sycl/sycl.hpp>
-#elif __has_include(<CL/sycl.hpp>)
-#include <CL/sycl.hpp>
-#else
-#error "Unsupported compiler"
-#endif
-#include <cpuid.h>
-#include <x86intrin.h>
-#endif
-
-#include <stdio.h>
-#include <cassert>
-#include "context.hpp"
-#include <oneapi/mkl.hpp>
-
-#include <oneapi/mkl.hpp>
-#include <oneapi/mkl/rng/device.hpp>
-
-#include <cmath>
-#define STEP(SPAN)                                \
-    void Step_##SPAN(float* _params,              \
-                     float* grads,                \
-                     float* _exp_avg_sq,          \
-                     size_t _param_size,          \
-                     sycl::half* dev_param = nullptr, \
-                     bool half_precision = false);
-
-#define TILE (128 * 1024 * 1024)
-
-
-class Adagrad_Optimizer {
-public:
-    Adagrad_Optimizer(float alpha = 1e-2, float eps = 1e-8, float weight_decay = 0)
-        : _alpha(alpha), _eps(eps), _weight_decay(weight_decay), _buf_index(false)
-    {
-        _streams[0] = ::SyclContext::Instance().GetCurrentStream();
-        _streams[1] = ::SyclContext::Instance().GetNewStream();
-        sycl::queue& q_ct1 = *_streams[0];
-
-        *_doubled_buffer = sycl::malloc_host<float>(TILE, q_ct1);
-        *(_doubled_buffer + 1) = sycl::malloc_host<float>(TILE, q_ct1);
-    }
-    ~Adagrad_Optimizer()
-    {
-            sycl::queue& q_ct1 = *_streams[0];
-            sycl::free(_doubled_buffer[0], q_ct1);
-            sycl::free(_doubled_buffer[1], q_ct1);
-    }
-
-    STEP(1)
-    STEP(4)
-    STEP(8)
-    inline void SynchronizeStreams()
-    {
-        for (int i = 0; i < 2; i++) _streams[i]->wait();
-    }
-    inline void IncrementStep(size_t step)
-    {
-        _step++;
-        if (_step != step) { _step = step; }
-    }
-    inline void update_state(float lr, float epsilon, float weight_decay)
-    {
-        _alpha = lr;
-        _eps = epsilon;
-        _weight_decay = weight_decay;
-    }
-private:
-    float _alpha;
-    float _eps;
-    float _weight_decay;
-
-    float _betta1_t;
-    float _betta2_t;
-    size_t _step;
-
-    float* _doubled_buffer[2];
-    bool _buf_index;
-
-    sycl::queue* _streams[2];
-};
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/cpu_adam.h b/intel_extension_for_deepspeed/op_builder/csrc/includes/cpu_adam.h
new file mode 100644
index 0000000..0d4a45a
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/includes/cpu_adam.h
@@ -0,0 +1,327 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+#pragma once
+
+#define NOMINMAX  // Windows idiosyncrasy
+                  // https://stackoverflow.com/questions/4913922/possible-problems-with-nominmax-on-visual-c
+
+#include <stdio.h>
+#include <torch/extension.h>
+#include <cassert>
+#include "simd.h"
+
+#if defined(__ENABLE_CUDA__)
+#include <cuda_fp16.h>
+#include <cuda_runtime_api.h>
+#include "cuda.h"
+#include "custom_cuda_layers.h"
+typedef sycl::half ds_half_precision_t;
+#elif defined(__ENABLE_CANN__)
+#include "acl/acl.h"
+#include "torch_npu/csrc/core/npu/NPUStream.h"
+typedef c10::Half ds_half_precision_t;
+#else
+#include <cmath>
+typedef unsigned short ds_half_precision_t;
+#endif
+
+#define STEP(SPAN)                                             \
+    void Step_##SPAN(float* _params,                           \
+                     float* grads,                             \
+                     float* _exp_avg,                          \
+                     float* _exp_avg_sq,                       \
+                     size_t _param_size,                       \
+                     ds_half_precision_t* dev_param = nullptr, \
+                     bool half_precision = false);
+
+class Adam_Optimizer {
+public:
+    Adam_Optimizer(float alpha = 1e-3,
+                   float betta1 = 0.9,
+                   float betta2 = 0.999,
+                   float eps = 1e-8,
+                   float weight_decay = 0,
+                   bool adamw_mode = true)
+        : _alpha(alpha),
+          _betta1(betta1),
+          _betta2(betta2),
+          _eps(eps),
+          _weight_decay(weight_decay),
+          _betta1_t(1.0),
+          _betta2_t(1.0),
+          _step(0),
+          _adamw_mode(adamw_mode)
+    {
+#if defined(__ENABLE_CUDA__)
+        cudaMallocHost((void**)_doubled_buffer, TILE * sizeof(float));
+        cudaMallocHost((void**)(_doubled_buffer + 1), TILE * sizeof(float));
+
+        _streams[0] = TrainingContext::Instance().GetCurrentStream();
+        _streams[1] = TrainingContext::Instance().GetNewStream();
+        _buf_index = false;
+#elif defined(__ENABLE_CANN__)
+        aclrtMallocHost((void**)_doubled_buffer, TILE * sizeof(float));
+        aclrtMallocHost((void**)(_doubled_buffer + 1), TILE * sizeof(float));
+
+        _buf_index = false;
+#endif
+    }
+    ~Adam_Optimizer()
+    {
+#if defined(__ENABLE_CUDA__)
+        cudaFreeHost(_doubled_buffer[0]);
+        cudaFreeHost(_doubled_buffer[1]);
+#elif defined(__ENABLE_CANN__)
+        aclrtFreeHost(_doubled_buffer[0]);
+        aclrtFreeHost(_doubled_buffer[1]);
+#endif
+    }
+
+#if defined(__AVX512__) or defined(__AVX256__)
+    template <int span>
+    void Step_AVX(size_t* rounded_size,
+                  float* _params,
+                  float* grads,
+                  float* _exp_avg,
+                  float* _exp_avg_sq,
+                  size_t param_size,
+                  ds_half_precision_t* dev_param = nullptr,
+                  bool half_precision = false);
+#endif
+    STEP(1)
+    STEP(4)
+    STEP(8)
+#if defined(__ENABLE_CUDA__)
+    inline void SynchronizeStreams()
+    {
+        for (int i = 0; i < 2; i++) cudaStreamSynchronize(_streams[i]);
+    }
+#elif defined(__ENABLE_CANN__)
+    inline void SynchronizeStreams()
+    {
+        for (int i = 0; i < 2; i++) aclrtSynchronizeStream(_streams[i].stream());
+    }
+#endif
+    inline void IncrementStep(size_t step, float beta1, float beta2)
+    {
+        if (beta1 != _betta1 || beta2 != _betta2) {
+            _step = step;
+            _betta1 = beta1;
+            _betta2 = beta2;
+            _betta1_t = std::pow(_betta1, step);
+            _betta2_t = std::pow(_betta2, step);
+        } else {
+            _step++;
+            if (_step != step) {
+                _betta1_t = std::pow(_betta1, step);
+                _betta2_t = std::pow(_betta2, step);
+                _step = step;
+            } else {
+                _betta1_t *= _betta1;
+                _betta2_t *= _betta2;
+            }
+        }
+    }
+    inline void update_state(float lr, float epsilon, float weight_decay, bool bias_correction)
+    {
+        _alpha = lr;
+        _eps = epsilon;
+        _weight_decay = weight_decay;
+
+        _bias_correction1 = 1.0f;
+        _bias_correction2 = 1.0f;
+        if (bias_correction == 1) {
+            _bias_correction1 = 1 - _betta1_t;
+            _bias_correction2 = 1 / sqrt(1 - _betta2_t);
+        }
+    }
+
+private:
+    float _alpha;
+    float _betta1;
+    float _betta2;
+    float _eps;
+    float _weight_decay;
+
+    float _betta1_t;
+    float _betta2_t;
+    size_t _step;
+
+    float _bias_correction1;
+    float _bias_correction2;
+
+    bool _adamw_mode;
+
+#if defined(__ENABLE_CUDA__)
+    float* _doubled_buffer[2];
+    cudaStream_t _streams[2];
+    bool _buf_index;
+#elif defined(__ENABLE_CANN__)
+    float* _doubled_buffer[2];
+    c10_npu::NPUStream _streams[2] = {c10_npu::getCurrentNPUStream(),
+                                      c10_npu::getNPUStreamFromPool()};
+    bool _buf_index;
+#endif
+};
+
+#if defined(__AVX512__) or defined(__AVX256__)
+template <int span>
+void Adam_Optimizer::Step_AVX(size_t* rounded_size,
+                              float* _params,
+                              float* grads,
+                              float* _exp_avg,
+                              float* _exp_avg_sq,
+                              size_t _param_size,
+                              ds_half_precision_t* dev_params,
+                              bool half_precision)
+{
+    size_t new_rounded_size = 0;
+    int rshft = half_precision ? 1 : 0;
+
+    AVX_Data betta1_4;
+    betta1_4.data = SIMD_SET(_betta1);
+    AVX_Data betta2_4;
+    betta2_4.data = SIMD_SET(_betta2);
+
+    float betta1_minus1 = 1 - _betta1;
+    float betta2_minus1 = 1 - _betta2;
+    AVX_Data betta1_minus1_4;
+    betta1_minus1_4.data = SIMD_SET(betta1_minus1);
+    AVX_Data betta2_minus1_4;
+    betta2_minus1_4.data = SIMD_SET(betta2_minus1);
+
+    AVX_Data bias2_sqrt;
+    bias2_sqrt.data = SIMD_SET(_bias_correction2);
+
+    AVX_Data eps_4;
+    eps_4.data = SIMD_SET(_eps);
+
+    float step_size = -1 * _alpha / _bias_correction1;
+    AVX_Data step_size_4;
+    step_size_4.data = SIMD_SET(step_size);
+
+    float w_decay = -1 * _alpha * _weight_decay;
+    AVX_Data weight_decay4;
+    if (_weight_decay > 0)
+        weight_decay4.data = (_adamw_mode ? SIMD_SET(w_decay) : SIMD_SET(_weight_decay));
+    new_rounded_size = ROUND_DOWN(_param_size, SIMD_WIDTH * span);
+    for (size_t t = 0; t < new_rounded_size; t += TILE) {
+        size_t copy_size = TILE;
+        if ((t + TILE) > new_rounded_size) copy_size = new_rounded_size - t;
+        size_t offset = copy_size + t;
+#if defined(__ENABLE_CUDA__)
+        if ((t / TILE) >= 2) { cudaStreamSynchronize(_streams[_buf_index]); }
+#elif defined(__ENABLE_CANN__)
+        if ((t / TILE) >= 2) { aclrtSynchronizeStream((_streams[_buf_index].stream());
+        }
+#endif
+#pragma omp parallel for
+        for (size_t i = t; i < offset; i += SIMD_WIDTH * span) {
+            AVX_Data grad_4[span];
+            simd_load<span>(grad_4, grads + (i >> rshft), half_precision);
+
+            AVX_Data momentum_4[span];
+            simd_load<span>(momentum_4, _exp_avg + i, false);
+
+            AVX_Data variance_4[span];
+            simd_load<span>(variance_4, _exp_avg_sq + i, false);
+
+            AVX_Data param_4[span];
+            simd_load<span>(param_4, _params + (i >> rshft), half_precision);
+
+            if (_weight_decay > 0 && !_adamw_mode) {
+                simd_fma<span>(grad_4, param_4, weight_decay4, grad_4);
+            }
+
+            simd_mul<span>(momentum_4, momentum_4, betta1_4);
+            simd_fma<span>(momentum_4, grad_4, betta1_minus1_4, momentum_4);
+            simd_mul<span>(variance_4, variance_4, betta2_4);
+            simd_mul<span>(grad_4, grad_4, grad_4);
+            simd_fma<span>(variance_4, grad_4, betta2_minus1_4, variance_4);
+            simd_sqrt<span>(grad_4, variance_4);
+            simd_fma<span>(grad_4, grad_4, bias2_sqrt, eps_4);
+            simd_div<span>(grad_4, momentum_4, grad_4);
+
+            if (_weight_decay > 0 && _adamw_mode) {
+                simd_fma<span>(param_4, param_4, weight_decay4, param_4);
+            }
+
+            simd_fma<span>(param_4, grad_4, step_size_4, param_4);
+
+            simd_store<span>(_params + (i >> rshft), param_4, half_precision);
+#if defined(__ENABLE_CUDA__) or defined(__ENABLE_CANN__)
+            if (dev_params) {
+                simd_store<span>(_doubled_buffer[_buf_index] + (i - t), param_4, half_precision);
+            }
+#endif
+            simd_store<span>(_exp_avg + i, momentum_4, false);
+            simd_store<span>(_exp_avg_sq + i, variance_4, false);
+        }
+#if defined(__ENABLE_CUDA__)
+        if (dev_params) {
+            if (half_precision)
+                launch_param_update_half(
+                    _doubled_buffer[_buf_index], dev_params + t, copy_size, _streams[_buf_index]);
+            else
+                launch_param_update(
+                    _doubled_buffer[_buf_index], dev_params + t, copy_size, _streams[_buf_index]);
+
+            _buf_index = !_buf_index;
+        }
+#elif defined(__ENABLE_CANN__)
+        if (dev_params) {
+            size_t memcpy_size = copy_size * sizeof(_doubled_buffer[_buf_index][0]);
+            if (half_precision) memoryCopySize /= 2;
+            aclrtMemcpy(dev_params + t,
+                        memcpy_size,
+                        _doubled_buffer[_buf_index],
+                        memcpy_size,
+                        aclrtMemcpyKind::ACL_MEMCPY_HOST_TO_DEVICE);
+
+            _buf_index = !_buf_index;
+#endif
+    }
+    *rounded_size = new_rounded_size;
+}
+#endif
+
+int create_adam_optimizer(int optimizer_id,
+                          float alpha = 1e-3,
+                          float betta1 = 0.9,
+                          float betta2 = 0.999,
+                          float eps = 1e-8,
+                          float weight_decay = 0,
+                          bool adamw_mode = true,
+                          bool should_log = false);
+
+int ds_adam_step(int optimizer_id,
+                 size_t step,
+                 float lr,
+                 float beta1,
+                 float beta2,
+                 float epsilon,
+                 float weight_decay,
+                 bool bias_correction,
+                 torch::Tensor& params,
+                 torch::Tensor& grads,
+                 torch::Tensor& exp_avg,
+                 torch::Tensor& exp_avg_sq);
+
+int ds_adam_step_plus_copy(int optimizer_id,
+                           size_t step,
+                           float lr,
+                           float beta1,
+                           float beta2,
+                           float epsilon,
+                           float weight_decay,
+                           bool bias_correction,
+                           torch::Tensor& params,
+                           torch::Tensor& grads,
+                           torch::Tensor& exp_avg,
+                           torch::Tensor& exp_avg_sq,
+                           torch::Tensor& gpu_params);
+
+int destroy_adam_optimizer(int optimizer_id);
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/cpu_adam.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/cpu_adam.hpp
deleted file mode 100644
index 715c161..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/includes/cpu_adam.hpp
+++ /dev/null
@@ -1,170 +0,0 @@
-#pragma once
-
-#if (__x86_64__ || __i386__)
-#include <cpuid.h>
-#include <x86intrin.h>
-#if __has_include(<sycl/sycl.hpp>)
-#include <sycl/sycl.hpp>
-#elif __has_include(<CL/sycl.hpp>)
-#include <CL/sycl.hpp>
-#else
-#error "Unsupported compiler"
-#endif
-#endif
-
-#include <stdio.h>
-#include <cassert>
-#include <oneapi/mkl.hpp>
-#include "context.hpp"
-
-#include <oneapi/mkl.hpp>
-#include <oneapi/mkl/rng/device.hpp>
-
-#include <cmath>
-
-#define TILE (128 * 1024 * 1024)
-
-#if defined(__AVX512__)
-#define SIMD_STORE(a, d) _mm512_storeu_ps(a, d)
-#define SIMD_LOAD(x) _mm512_loadu_ps(x)
-#define SIMD_SET(x) _mm512_set1_ps(x)
-#define SIMD_MUL(x, y) _mm512_mul_ps(x, y)
-#define SIMD_FMA(x, y, c) _mm512_fmadd_ps(x, y, c)
-#define SIMD_SQRT(x) _mm512_sqrt_ps(x)
-#define SIMD_DIV(x, y) _mm512_div_ps(x, y)
-#define SIMD_WIDTH 16
-#else
-#if defined(__AVX256__)
-#define SIMD_STORE(a, d) _mm256_storeu_ps(a, d)
-#define SIMD_LOAD(x) _mm256_loadu_ps(x)
-#define SIMD_SET(x) _mm256_set1_ps(x)
-#define SIMD_MUL(x, y) _mm256_mul_ps(x, y)
-#define SIMD_FMA(x, y, c) _mm256_fmadd_ps(x, y, c)
-#define SIMD_SQRT(x) _mm256_sqrt_ps(x)
-#define SIMD_DIV(x, y) _mm256_div_ps(x, y)
-#define SIMD_WIDTH 8
-#endif
-#endif
-
-class Adam_Optimizer {
-public:
-    Adam_Optimizer(float alpha = 1e-3,
-                   float betta1 = 0.9,
-                   float betta2 = 0.999,
-                   float eps = 1e-8,
-                   float weight_decay = 0,
-                   bool adamw_mode = true)
-        : _alpha(alpha),
-          _betta1(betta1),
-          _betta2(betta2),
-          _eps(eps),
-          _weight_decay(weight_decay),
-          _betta1_t(1.0),
-          _betta2_t(1.0),
-          _step(0),
-          _buf_index(false),
-          _adamw_mode(adamw_mode)
-    {
-        _streams[0] = ::SyclContext::Instance().GetCurrentStream();
-        _streams[1] = ::SyclContext::Instance().GetNewStream();
-        sycl::queue& q_ct1 = *_streams[0];
-
-        *_doubled_buffer = sycl::malloc_host<float>(TILE, q_ct1);
-        *(_doubled_buffer + 1) = sycl::malloc_host<float>(TILE, q_ct1);
-    }
-    ~Adam_Optimizer()
-    {
-        sycl::queue& q_ct1 = *_streams[0];
-        sycl::free(_doubled_buffer[0], q_ct1);
-        sycl::free(_doubled_buffer[1], q_ct1);
-    }
-    void Step(float* _params,
-              float* grads,
-              float* _exp_avg,
-              float* _exp_avg_sq,
-              size_t param_size,
-              sycl::half* dev_param = nullptr,
-              bool half_precision = false);
-    void Step_4(float* _params,
-                float* grads,
-                float* _exp_avg,
-                float* _exp_avg_sa,
-                size_t param_size,
-                sycl::half* dev_param = nullptr,
-                bool half_precision = false);
-    void Step_8(float* _params,
-                float* grads,
-                float* _exp_avg,
-                float* _exp_avg_sq,
-                size_t _param_size,
-                sycl::half* dev_params = nullptr,
-                bool half_precision = false);
-    inline void SynchronizeStreams()
-    {
-        for (int i = 0; i < 2; i++) _streams[i]->wait();
-    }
-    inline void IncrementStep(size_t step, float beta1, float beta2)
-    {
-        if (beta1 != _betta1 || beta2 != _betta2) {
-            _step = step;
-            _betta1 = beta1;
-            _betta2 = beta2;
-            _betta1_t = std::pow(_betta1, step);
-            _betta2_t = std::pow(_betta2, step);
-        } else {
-            _step++;
-            if (_step != step) {
-                _betta1_t = std::pow(_betta1, step);
-                _betta2_t = std::pow(_betta2, step);
-                _step = step;
-            } else {
-                _betta1_t *= _betta1;
-                _betta2_t *= _betta2;
-            }
-        }
-    }
-    inline void update_state(float lr, float epsilon, float weight_decay, bool bias_correction)
-    {
-        _alpha = lr;
-        _eps = epsilon;
-        _weight_decay = weight_decay;
-
-        _bias_correction1 = 1.0f;
-        _bias_correction2 = 1.0f;
-        if (bias_correction == 1) {
-            _bias_correction1 = 1 - _betta1_t;
-            _bias_correction2 = 1 / sqrt(1 - _betta2_t);
-        }
-    }
-
-private:
-#if defined(__AVX512__) or defined(__AVX256__)
-    union AVX_Data {
-#if defined(__AVX512__)
-        __m512 data;
-#else
-        __m256 data;
-#endif
-        // float data_f[16];
-    };
-#endif
-
-    float _alpha;
-    float _betta1;
-    float _betta2;
-    float _eps;
-    float _weight_decay;
-
-    float _betta1_t;
-    float _betta2_t;
-    size_t _step;
-
-    float _bias_correction1;
-    float _bias_correction2;
-
-    float* _doubled_buffer[2];
-    bool _buf_index;
-    bool _adamw_mode;
-
-    sycl::queue* _streams[2];
-};
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/onemkl_wrappers.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/cublas_wrappers.h
similarity index 56%
rename from intel_extension_for_deepspeed/op_builder/csrc/includes/onemkl_wrappers.hpp
rename to intel_extension_for_deepspeed/op_builder/csrc/includes/cublas_wrappers.h
index e69abee..7291bcf 100644
--- a/intel_extension_for_deepspeed/op_builder/csrc/includes/onemkl_wrappers.hpp
+++ b/intel_extension_for_deepspeed/op_builder/csrc/includes/cublas_wrappers.h
@@ -1,47 +1,62 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
 #pragma once
 
-#include <assert.h>
-#if __has_include(<sycl/sycl.hpp>)
 #include <sycl/sycl.hpp>
-#elif __has_include(<CL/sycl.hpp>)
-#include <CL/sycl.hpp>
-#else
-#error "Unsupported compiler"
-#endif
-#include <oneapi/mkl.hpp>
+#include <dpct/dpct.hpp>
+#include <assert.h>
+#include <dpct/blas_utils.hpp>
 
+#ifndef __HIP_PLATFORM_AMD__
+#endif
+#ifdef __HIP_PLATFORM_AMD__
+#include <rocblas/rocblas.h>
+#endif
 #include <stdio.h>
 
-int onemkl_gemm_ex(sycl::queue* handle,
+int cublas_gemm_ex(dpct::queue_ptr handle,
                    oneapi::mkl::transpose transa,
                    oneapi::mkl::transpose transb,
                    int m,
                    int n,
                    int k,
-                   const float alpha,
-                   const float beta,
+                   const float* alpha,
+                   const float* beta,
                    const float* A,
                    const float* B,
-                   float* C);
+                   float* C,
+#ifdef __HIP_PLATFORM_AMD__
+                   rocblas_gemm_algo algo = rocblas_gemm_algo_standard);
+#else
+                   int algo = -1);
+#endif
 
-int onemkl_gemm_ex(sycl::queue* handle,
+int cublas_gemm_ex(dpct::queue_ptr handle,
                    oneapi::mkl::transpose transa,
                    oneapi::mkl::transpose transb,
                    int m,
                    int n,
                    int k,
-                   const sycl::half alpha,
-                   const sycl::half beta,
+                   const float* alpha,
+                   const float* beta,
                    const sycl::half* A,
                    const sycl::half* B,
-                   sycl::half* C);
+                   sycl::half* C,
+#ifdef __HIP_PLATFORM_AMD__
+                   rocblas_gemm_algo algo = rocblas_gemm_algo_standard);
+#else
+                   int algo = 99);
+#endif
 
-int onemkl_strided_batched_gemm(sycl::queue* handle,
+int cublas_strided_batched_gemm(dpct::queue_ptr handle,
                                 int m,
                                 int n,
                                 int k,
-                                const float alpha,
-                                const float beta,
+                                const float* alpha,
+                                const float* beta,
                                 const float* A,
                                 const float* B,
                                 float* C,
@@ -51,14 +66,18 @@ int onemkl_strided_batched_gemm(sycl::queue* handle,
                                 int stride_B,
                                 int stride_C,
                                 int batch,
+#ifdef __HIP_PLATFORM_AMD__
+                                rocblas_gemm_algo algo = rocblas_gemm_algo_standard);
+#else
                                 int algo = -1);
+#endif
 
-int onemkl_strided_batched_gemm(sycl::queue* handle,
+int cublas_strided_batched_gemm(dpct::queue_ptr handle,
                                 int m,
                                 int n,
                                 int k,
-                                const sycl::half alpha,
-                                const sycl::half beta,
+                                const float* alpha,
+                                const float* beta,
                                 const sycl::half* A,
                                 const sycl::half* B,
                                 sycl::half* C,
@@ -68,4 +87,8 @@ int onemkl_strided_batched_gemm(sycl::queue* handle,
                                 int stride_B,
                                 int stride_C,
                                 int batch,
+#ifdef __HIP_PLATFORM_AMD__
+                                rocblas_gemm_algo algo = rocblas_gemm_algo_standard);
+#else
                                 int algo = 99);
+#endif
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/custom_sycl_layers.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/custom_cuda_layers.h
similarity index 64%
rename from intel_extension_for_deepspeed/op_builder/csrc/includes/custom_sycl_layers.hpp
rename to intel_extension_for_deepspeed/op_builder/csrc/includes/custom_cuda_layers.h
index c5a1a89..5e6d0e6 100644
--- a/intel_extension_for_deepspeed/op_builder/csrc/includes/custom_sycl_layers.hpp
+++ b/intel_extension_for_deepspeed/op_builder/csrc/includes/custom_cuda_layers.h
@@ -1,30 +1,38 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
 #pragma once
 
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include "ds_kernel_utils.h"
+#include <dpct/rng_utils.hpp>
+
 #include <stdio.h>
 #include <stdlib.h>
-#if __has_include(<sycl/sycl.hpp>)
-#include <sycl/sycl.hpp>
-#elif __has_include(<CL/sycl.hpp>)
-#include <CL/sycl.hpp>
-#else
-#error "Unsupported compiler"
-#endif
-#include <oneapi/mkl.hpp>
-#include <oneapi/mkl/rng/device.hpp>
-
-#include "context.hpp"
-#include "onednn_wrappers.hpp"
-#include "onemkl_wrappers.hpp"
+
+#include "context.h"
+/* #include "cublas_wrappers.h" */
+
+#define CUDA_CHECK(callstr)                                                                    \
+    {                                                                                          \
+        cudaError_t error_code = callstr;                                                      \
+        if (error_code != cudaSuccess) {                                                       \
+            std::cerr << "CUDA error " << error_code << " at " << __FILE__ << ":" << __LINE__; \
+            assert(0);                                                                         \
+        }                                                                                      \
+    }
 
 #define MAX_THREADS 1024
 #define THREADS 256
 
 #define MAX_THREAD_STRIDE 32
-#define TILE_DIM 16
+#define TILE_DIM 32
 
-// Maximum sequence-length support based on the number of threads (2048) allowed
-// in each block and this MAX is 8K For higher sequence length we need to use
-// higher Max, like for 64K : 32
+// Maximum sequence-length support based on the number of threads (2048) allowed in each block and
+// this MAX is 8K For higher sequence length we need to use higher Max, like for 64K : 32
 #define MAX_THREAD_ITERATIONS 8  // Maximum 8K
 #define MAX_WARP_NUM 32
 
@@ -32,30 +40,8 @@
 
 #define MAX_REG 256
 
-template <typename T>
-void launch_qunatize_kernel(T* vals,
-                            int total_count,
-                            int group_num,
-                            int num_bits,
-                            sycl::queue* stream);
-template <typename T>
-void launch_sr_qunatize_kernel(T* vals,
-                               int total_count,
-                               int group_num,
-                               int num_bits,
-                               sycl::queue* stream);
-template <typename T>
-void launch_qunatize_kernel_asym(T* vals,
-                                 int total_count,
-                                 int group_num,
-                                 int num_bits,
-                                 sycl::queue* stream);
-template <typename T>
-void launch_sr_qunatize_kernel_asym(T* vals,
-                                    int total_count,
-                                    int group_num,
-                                    int num_bits,
-                                    sycl::queue* stream);
+#define WARP_SIZE_BITS 5
+
 // Fused bias add with gelu activation
 template <typename T>
 void launch_bias_gelu(const T* input,
@@ -63,14 +49,14 @@ void launch_bias_gelu(const T* input,
                       T* output,
                       int intermediate_size,
                       int batch_size,
-                      sycl::queue* stream);
+                      dpct::queue_ptr stream);
 
 template <typename T>
 void launch_gelu(const T* input,
                  T* output,
                  int intermediate_size,
                  int batch_size,
-                 sycl::queue* stream);
+                 dpct::queue_ptr stream);
 
 template <typename T>
 void launch_d_gelu(T* d_output,
@@ -78,7 +64,7 @@ void launch_d_gelu(T* d_output,
                    const T* bias,
                    int intermediate_size,
                    int batch_size,
-                   sycl::queue* stream);
+                   dpct::queue_ptr stream);
 
 // Custom fused bias add with layer normalization
 template <typename T>
@@ -89,7 +75,7 @@ void launch_bias_residual_layer_norm(T* vals,
                                      float epsilon,
                                      int batch_size,
                                      int hidden_dim,
-                                     sycl::queue* stream,
+                                     dpct::queue_ptr stream,
                                      bool preLayerNorm,
                                      bool training,
                                      T* vars,
@@ -103,7 +89,7 @@ void launch_bias_residual_layer_norm(T* vals,
                                      float epsilon,
                                      int batch_size,
                                      int hidden_dim,
-                                     sycl::queue* stream,
+                                     dpct::queue_ptr stream,
                                      bool preLayerNorm,
                                      bool training,
                                      T* vars);
@@ -120,7 +106,7 @@ void launch_layerNorm_backward_fused_add(const T* out_grad1,
                                          T* inp_grad,
                                          int batch_size,
                                          int hidden_dim,
-                                         sycl::queue* stream[2]);
+                                         dpct::queue_ptr stream[2]);
 template <typename T>
 void launch_layerNorm_backward_fused_add(const T* out_grad1,
                                          const T* out_grad2,
@@ -132,7 +118,7 @@ void launch_layerNorm_backward_fused_add(const T* out_grad1,
                                          T* inp_grad,
                                          int batch_size,
                                          int hidden_dim,
-                                         sycl::queue* stream[2],
+                                         dpct::queue_ptr stream[2],
                                          bool invertible = false,
                                          const T* betta = nullptr);
 
@@ -147,7 +133,7 @@ void launch_layerNorm_backward(const T* out_grad,
                                T* inp_grad,
                                int batch_size,
                                int hidden_dim,
-                               sycl::queue* stream[2]);
+                               dpct::queue_ptr stream[2]);
 
 template <typename T>
 void launch_layerNorm_backward(const T* out_grad,
@@ -159,7 +145,7 @@ void launch_layerNorm_backward(const T* out_grad,
                                T* inp_grad,
                                int batch_size,
                                int hidden_dim,
-                               sycl::queue* stream[2],
+                               dpct::queue_ptr stream[2],
                                bool invertible = false,
                                const T* betta = nullptr);
 
@@ -176,10 +162,10 @@ void launch_layerNorm_backward_nreversible(const T* out_grad,
                                            T* inp_grad,
                                            int batch_size,
                                            int hidden_dim,
-                                           sycl::queue* stream[2]);
+                                           dpct::queue_ptr stream[2]);
 
 template <typename T>
-void Transpose(const T* inp_mat, T* out_mat, int rows, int cols, sycl::queue* stream);
+void Transpose(const T* inp_mat, T* out_mat, int rows, int cols, dpct::queue_ptr stream);
 
 template <typename T>
 void launch_attn_softmax_backward(T* out_grad,
@@ -187,7 +173,7 @@ void launch_attn_softmax_backward(T* out_grad,
                                   int batch_size,
                                   int heads,
                                   int seq_length,
-                                  sycl::queue* stream);
+                                  dpct::queue_ptr stream);
 
 template <typename T>
 void launch_attn_softmax_backward_v2(T* out_grad,
@@ -195,7 +181,7 @@ void launch_attn_softmax_backward_v2(T* out_grad,
                                      int batch_size,
                                      int heads,
                                      int seq_length,
-                                     sycl::queue* stream);
+                                     dpct::queue_ptr stream);
 
 // Custom softmax with scaling and attention mask addition
 template <typename T>
@@ -204,7 +190,7 @@ void launch_attn_softmax(T* vals,
                          int batch_size,
                          int heads,
                          int sequence_length,
-                         sycl::queue* stream);
+                         dpct::queue_ptr stream);
 
 template <typename T>
 void launch_transform_0213(T* output,
@@ -213,7 +199,7 @@ void launch_transform_0213(T* output,
                            int seq_length,
                            int hidden_dim,
                            int heads,
-                           sycl::queue* stream);
+                           dpct::queue_ptr stream);
 
 // Custom bias add
 template <typename T>
@@ -224,7 +210,7 @@ void launch_bias_add_transform_0213(T* outputs,
                                     int seq_length,
                                     int hidden_dim,
                                     int heads,
-                                    sycl::queue* stream,
+                                    dpct::queue_ptr stream,
                                     int trans_count);
 
 // 4D transform [0, 1, 2, 3] -> [0, 2, 1, 3]
@@ -235,7 +221,7 @@ void launch_transform4d_0213(T* out,
                              int heads,
                              int seq_length,
                              int hidden_dim,
-                             sycl::queue* stream,
+                             dpct::queue_ptr stream,
                              int trans_count);
 
 template <typename T>
@@ -245,7 +231,7 @@ void launch_dropout(T* vals,
                     int batch,
                     int dim,
                     float ratio,
-                    sycl::queue* stream);
+                    dpct::queue_ptr stream);
 
 template <typename T>
 void launch_dropout(T* vals_out,
@@ -254,7 +240,7 @@ void launch_dropout(T* vals_out,
                     int total_count,
                     int dim,
                     float ratio,
-                    sycl::queue* stream,
+                    dpct::queue_ptr stream,
                     bool bwd = false);
 
 template <typename T>
@@ -266,10 +252,14 @@ void launch_dropout(T* out,
                     int batch,
                     int dim,
                     float ratio,
-                    sycl::queue* stream);
+                    dpct::queue_ptr stream);
 
 template <typename T>
-void launch_dropout_grad(T* vals, uint8_t* mask, int total_count, float ratio, sycl::queue* stream);
+void launch_dropout_grad(T* vals,
+                         uint8_t* mask,
+                         int total_count,
+                         float ratio,
+                         dpct::queue_ptr stream);
 
 template <typename T>
 void launch_dropout_grad(T* vals_out,
@@ -277,13 +267,68 @@ void launch_dropout_grad(T* vals_out,
                          uint8_t* mask,
                          int total_count,
                          float ratio,
-                         sycl::queue* stream);
+                         dpct::queue_ptr stream);
 
 template <typename T>
 void launch_fuse_transpose_bias_kernel(const T* inp,
                                        T* out,
                                        int rows,
                                        int cols,
-                                       sycl::queue* stream);
+                                       dpct::queue_ptr stream);
+
+void launch_param_update(const float* input, sycl::half* output, int size, dpct::queue_ptr stream);
+void launch_param_update_half(const float* input,
+                              sycl::half* output,
+                              int size,
+                              dpct::queue_ptr stream);
+
+void launch_token_sort(int32_t* indices,
+                       int layers,
+                       int batch_size,
+                       int reserved_size,
+                       int original_tokens,
+                       dpct::queue_ptr stream);
+
+template <typename T>
+void launch_gather_tokens(T* retained_tokens,
+                          T* activations,
+                          int32_t* gather_indices,
+                          int32_t batch_size,
+                          int32_t sampled_tokens,
+                          int32_t channels,
+                          int32_t read_batch_stride,
+                          int32_t read_seq_stride,
+                          int32_t write_batch_stride,
+                          int32_t write_seq_stride,
+                          dpct::queue_ptr stream);
 
-void launch_param_update(const float* input, sycl::half* output, int size, sycl::queue* stream);
+template <typename T>
+void launch_scatter_tokens(T* all_activations,
+                           T* layer_activations,
+                           int32_t* gather_indices,
+                           int32_t batch_size,
+                           int32_t sampled_tokens,
+                           int32_t channels,
+                           int32_t read_batch_stride,
+                           int32_t read_seq_stride,
+                           int32_t write_batch_stride,
+                           int32_t write_seq_stride,
+                           dpct::queue_ptr stream);
+
+template <typename T>
+void launch_slice_gpt_mask(T* output_mask,
+                           const T* input_mask,
+                           int batch_size,
+                           int truncated_seq_len,
+                           int orig_seq_len,
+                           dpct::queue_ptr stream);
+
+template <typename T>
+void launch_slice_bert_mask(T* output_mask,
+                            const T* input_mask,
+                            const int32_t* retained_indices,
+                            int32_t layers,
+                            int32_t batch_size,
+                            int32_t truncated_seq_len,
+                            int32_t orig_seq_len,
+                            dpct::queue_ptr stream);
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/dequantization_utils.h b/intel_extension_for_deepspeed/op_builder/csrc/includes/dequantization_utils.h
new file mode 100644
index 0000000..3c54962
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/includes/dequantization_utils.h
@@ -0,0 +1,186 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include "conversion_utils.h"
+#include "ds_kernel_utils.h"
+#include "quantization.h"
+#include "quantization_utils.h"
+
+#pragma once
+
+namespace dequantize {
+using Type = quantize::Type;
+
+template <Type qType, int numBits>
+using Params = quantize::Params<qType, numBits>;
+
+constexpr int granularity = quantize::granularity;
+using PackedInt4 = quantize::PackedInt4;
+
+constexpr int h_per_chunk = granularity / sizeof(sycl::half);
+constexpr int h2_per_chunk = granularity / sizeof(sycl::half2);
+
+/*
+Device function that reads quantized data from global memory, dequantizes
+it, and stores it to global memory.
+Template Arguments :
+    numBits - Number of bits in quantized element.      int: 4, 8
+    qType - Type of quantization to perform.            Type::Symmetric or Type::Asymmetric
+    unroll - Number of load steps to internally unroll  int
+    threads - Number of threads to perform dequant      int
+Function arguments:
+    global_output - sycl::half pointer in global memory
+    data - Quantized data in global memory
+    global_params - Quantization parameters in global memory
+    elems_per_group - Number of elements in each quantization group
+    total_elems - Tensor size (note, does not need to be multiple of elems_per_group)
+*/
+template <int numBits, Type qType, int unroll, int threads>
+DS_D_INLINE void to_global(sycl::half* global_output,
+                           const int8_t* data,
+                           const float* global_params,
+                           const int elems_per_group,
+                           const int total_elems);
+
+/*
+Device function that quantizes 16 bytes of sycl::half type input data.
+Template Arguments :
+    numBits -   Number of bits in quantized element.    int : 8 or 4
+    qType   - Type of quantization to perform.          Type::Symmetric or Type::Asymmetric
+Function Arguments :
+    local_output -  Local array to store dequantized data       sycl::half* or sycl::half2*
+    data         -  Pointer to quantized input data.            int8_t*
+    Params       -  Parameters for quantization.                Params<qType, numBits>
+*/
+template <int numBits, Type qType>
+DS_D_INLINE void chunk(sycl::half2* local_output,
+                       const int8_t* data,
+                       Params<qType, numBits> q_params);
+
+template <typename T, int numBits, Type qType>
+DS_D_INLINE void chunk(T* local_output, const int8_t* data, Params<qType, numBits> q_params);
+
+/**************** Implementations ******************/
+
+template <typename T, int numBits, Type qType>
+DS_D_INLINE void chunk(T* local_output, const int8_t* data, Params<qType, numBits> q_params)
+{
+    constexpr int32_t num_elems_packed = 8 / numBits;
+    constexpr int32_t iters = h_per_chunk / num_elems_packed;
+
+#pragma unroll
+    for (int i = 0; i < iters; i++) {
+        if constexpr (num_elems_packed == 1) {
+            local_output[i] = q_params.template dequantize<T>(data[i]);
+        } else {
+            auto accessible_data = *(PackedInt4*)(&data[i]);
+            local_output[2 * i] = q_params.template dequantize<T>(accessible_data.low);
+            local_output[2 * i + 1] = q_params.template dequantize<T>(accessible_data.high);
+        }
+    }
+}
+
+template <int numBits, Type qType>
+DS_D_INLINE void chunk(sycl::half2* local_output,
+                       const int8_t* data,
+                       Params<qType, numBits> q_params)
+{
+    sycl::half* local_output_cast = reinterpret_cast<sycl::half*>(local_output);
+    chunk<sycl::half, numBits>(local_output_cast, data, q_params);
+}
+
+template <typename T, int numBits, Type qType, int unroll, int threads>
+/*
+DPCT1110:46: The total declared local variable size in device function _to_global exceeds 128 bytes
+and may cause high register pressure. Consult with your hardware vendor to find the total register
+size available and adjust the code, or use smaller sub-group size to avoid high register pressure.
+*/
+DS_D_INLINE void _to_global(T* global_output,
+                            const int8_t* data,
+                            const float* global_params,
+                            const int elems_per_group,
+                            const int total_elems)
+{
+    sycl::group<3> tb = sycl::ext::oneapi::experimental::this_group<3>();
+    sycl::sub_group warp = sycl::ext::oneapi::experimental::this_sub_group();
+
+    // Load constants
+    // TODO(cmikeh2): Refactor into functions?
+    constexpr int load_granularity = (granularity / (sizeof(T))) / (numBits == 8 ? 1 : 2);
+    constexpr int load_step_stride = load_granularity * threads;
+    constexpr int load_block_stride = load_step_stride * unroll;
+
+    // Store constants
+    constexpr int T_per_chunk = granularity / sizeof(T);
+    constexpr int store_step_stride = T_per_chunk * threads;
+    constexpr int store_block_stride = store_step_stride * unroll;
+
+    // Load offsets
+    const int load_block_offset = tb.get_group_id()[2] * load_block_stride;
+    // Note: we can use `load_granularity` since the dtype is `int8_t`.
+    const int load_thread_offset = tb.get_local_id()[2] * load_granularity;
+    const int8_t* load_base = data + load_block_offset + load_thread_offset;
+
+    // Store offsets
+    const int store_block_offset = tb.get_group_id()[2] * store_block_stride;
+    const int store_thread_offset = tb.get_local_id()[2] * T_per_chunk;
+    const int elem_id_base = store_block_offset + store_thread_offset;
+
+    int8_t local_load_buffer[load_granularity * unroll];
+    T local_dequant_buffer[T_per_chunk * unroll];
+
+    /*
+    Note: Splitting this loop in half gave about 3-5% performance increase for reasons that aren't
+    totally clear to me, so this is a deliberately weird code structure.
+    */
+#pragma unroll
+    for (int i = 0; i < unroll; i++) {
+        const int elem_id_iter = elem_id_base + i * store_step_stride;
+
+        if (elem_id_iter < total_elems) {
+            mem_access::load_global<load_granularity>(local_load_buffer + i * load_granularity,
+                                                      load_base + i * load_step_stride);
+        }
+    }
+
+#pragma unroll
+    for (int i = 0; i < unroll; i++) {
+        const int elem_id_iter = elem_id_base + i * store_step_stride;
+        if (elem_id_iter < total_elems) {
+            // TODO(cmikeh2): Can we amortize this division? Perform once on the first iteration and
+            // use indexing math to do division free interpolation of the successive groups?
+            const int group_index = elem_id_iter / elems_per_group;
+            Params<qType, numBits> q_params(global_params, group_index);
+
+            chunk<T, numBits, qType>(local_dequant_buffer + i * T_per_chunk,
+                                     local_load_buffer + i * load_granularity,
+                                     q_params);
+            mem_access::store_global<granularity>(global_output + elem_id_iter,
+                                                  local_dequant_buffer + i * T_per_chunk);
+        }
+    }
+}
+
+template <typename T, int numBits, Type qType, int unroll, int threads>
+DS_D_INLINE void to_global(T* global_output,
+                           const int8_t* data,
+                           const float* global_params,
+                           const int elems_per_group,
+                           const int total_elems)
+{
+    if constexpr (numBits == 4 || numBits == 8) {
+        _to_global<T, numBits, qType, unroll, threads>(
+            global_output, data, global_params, elems_per_group, total_elems);
+    } else if constexpr (numBits == 3) {
+        // TODO(cmikeh2): Need this implementation
+        assert(false);
+    } else {
+        assert(false);
+    }
+}
+
+}  // namespace dequantize
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/atomic.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/atomic.hpp
new file mode 100644
index 0000000..4b516f5
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/atomic.hpp
@@ -0,0 +1,842 @@
+//==---- atomic.hpp -------------------------------*- C++ -*----------------==//
+//
+// Copyright (C) Intel Corporation
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+// See https://llvm.org/LICENSE.txt for license information.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef __DPCT_ATOMIC_HPP__
+#define __DPCT_ATOMIC_HPP__
+
+#include <sycl/sycl.hpp>
+
+namespace dpct {
+
+/// Atomically add the value operand to the value at the addr and assign the
+/// result to the value at addr.
+/// \param [in, out] addr The pointer to the data.
+/// \param operand The value to add to the value at \p addr.
+/// \param memoryOrder The memory ordering used.
+/// \returns The value at the \p addr before the call.
+template <typename T,
+          sycl::access::address_space addressSpace =
+              sycl::access::address_space::global_space,
+          sycl::memory_order memoryOrder = sycl::memory_order::relaxed,
+          sycl::memory_scope memoryScope = sycl::memory_scope::device>
+inline T atomic_fetch_add(T *addr, T operand) {
+  auto atm =
+      sycl::atomic_ref<T, memoryOrder, memoryScope, addressSpace>(addr[0]);
+  return atm.fetch_add(operand);
+}
+
+template <sycl::access::address_space addressSpace =
+              sycl::access::address_space::global_space,
+          sycl::memory_order memoryOrder = sycl::memory_order::relaxed,
+          sycl::memory_scope memoryScope = sycl::memory_scope::device,
+          typename T1, typename T2>
+inline T1 atomic_fetch_add(T1 *addr, T2 operand) {
+  auto atm =
+      sycl::atomic_ref<T1, memoryOrder, memoryScope, addressSpace>(addr[0]);
+  return atm.fetch_add(operand);
+}
+
+/// Atomically add the value operand to the value at the addr and assign the
+/// result to the value at addr.
+/// \param [in, out] addr The pointer to the data.
+/// \param operand The value to add to the value at \p addr.
+/// \param memoryOrder The memory ordering used.
+/// \returns The value at the \p addr before the call.
+template <typename T, sycl::access::address_space addressSpace =
+                          sycl::access::address_space::global_space>
+inline T atomic_fetch_add(T *addr, T operand,
+                          sycl::memory_order memoryOrder) {
+  switch (memoryOrder) {
+  case sycl::memory_order::relaxed:
+    return atomic_fetch_add<T, addressSpace, sycl::memory_order::relaxed,
+                            sycl::memory_scope::device>(addr, operand);
+  case sycl::memory_order::acq_rel:
+    return atomic_fetch_add<T, addressSpace, sycl::memory_order::acq_rel,
+                            sycl::memory_scope::device>(addr, operand);
+  case sycl::memory_order::seq_cst:
+    return atomic_fetch_add<T, addressSpace, sycl::memory_order::seq_cst,
+                            sycl::memory_scope::device>(addr, operand);
+  default:
+    assert(false && "Invalid memory_order for atomics. Valid memory_order for "
+                    "atomics are: sycl::memory_order::relaxed, "
+                    "sycl::memory_order::acq_rel, sycl::memory_order::seq_cst!");
+  }
+}
+
+template <sycl::access::address_space addressSpace =
+              sycl::access::address_space::global_space,
+          typename T1, typename T2>
+inline T1 atomic_fetch_add(T1 *addr, T2 operand,
+                           sycl::memory_order memoryOrder) {
+  atomic_fetch_add<T1, addressSpace>(addr, operand, memoryOrder);
+}
+
+/// Atomically subtract the value operand from the value at the addr and assign
+/// the result to the value at addr.
+/// \param [in, out] addr The pointer to the data.
+/// \param operand The value to subtract from the value at \p addr
+/// \param memoryOrder The memory ordering used.
+/// \returns The value at the \p addr before the call.
+template <typename T,
+          sycl::access::address_space addressSpace =
+              sycl::access::address_space::global_space,
+          sycl::memory_order memoryOrder = sycl::memory_order::relaxed,
+          sycl::memory_scope memoryScope = sycl::memory_scope::device>
+inline T atomic_fetch_sub(T *addr, T operand) {
+  auto atm =
+      sycl::atomic_ref<T, memoryOrder, memoryScope, addressSpace>(addr[0]);
+  return atm.fetch_sub(operand);
+}
+
+template <sycl::access::address_space addressSpace =
+              sycl::access::address_space::global_space,
+          sycl::memory_order memoryOrder = sycl::memory_order::relaxed,
+          sycl::memory_scope memoryScope = sycl::memory_scope::device,
+          typename T1, typename T2>
+inline T1 atomic_fetch_sub(T1 *addr, T2 operand) {
+  auto atm =
+      sycl::atomic_ref<T1, memoryOrder, memoryScope, addressSpace>(addr[0]);
+  return atm.fetch_sub(operand);
+}
+
+/// Atomically subtract the value operand from the value at the addr and assign
+/// the result to the value at addr.
+/// \param [in, out] addr The pointer to the data.
+/// \param operand The value to subtract from the value at \p addr
+/// \param memoryOrder The memory ordering used.
+/// \returns The value at the \p addr before the call.
+template <typename T, sycl::access::address_space addressSpace =
+                          sycl::access::address_space::global_space>
+inline T atomic_fetch_sub(T *addr, T operand,
+                          sycl::memory_order memoryOrder) {
+  switch (memoryOrder) {
+  case sycl::memory_order::relaxed:
+    return atomic_fetch_sub<T, addressSpace, sycl::memory_order::relaxed,
+                            sycl::memory_scope::device>(addr, operand);
+  case sycl::memory_order::acq_rel:
+    return atomic_fetch_sub<T, addressSpace, sycl::memory_order::acq_rel,
+                            sycl::memory_scope::device>(addr, operand);
+  case sycl::memory_order::seq_cst:
+    return atomic_fetch_sub<T, addressSpace, sycl::memory_order::seq_cst,
+                            sycl::memory_scope::device>(addr, operand);
+  default:
+    assert(false && "Invalid memory_order for atomics. Valid memory_order for "
+                    "atomics are: sycl::memory_order::relaxed, "
+                    "sycl::memory_order::acq_rel, sycl::memory_order::seq_cst!");
+  }
+}
+
+template <sycl::access::address_space addressSpace =
+              sycl::access::address_space::global_space,
+          typename T1, typename T2>
+inline T1 atomic_fetch_sub(T1 *addr, T2 operand,
+                           sycl::memory_order memoryOrder) {
+  atomic_fetch_sub<T1, addressSpace>(addr, operand, memoryOrder);
+}
+
+/// Atomically perform a bitwise AND between the value operand and the value at the addr
+/// and assign the result to the value at addr.
+/// \param [in, out] addr The pointer to the data.
+/// \param operand The value to use in bitwise AND operation with the value at the \p addr.
+/// \param memoryOrder The memory ordering used.
+/// \returns The value at the \p addr before the call.
+template <typename T,
+          sycl::access::address_space addressSpace =
+              sycl::access::address_space::global_space,
+          sycl::memory_order memoryOrder = sycl::memory_order::relaxed,
+          sycl::memory_scope memoryScope = sycl::memory_scope::device>
+inline T atomic_fetch_and(T *addr, T operand) {
+  auto atm =
+      sycl::atomic_ref<T, memoryOrder, memoryScope, addressSpace>(addr[0]);
+  return atm.fetch_and(operand);
+}
+
+template <sycl::access::address_space addressSpace =
+              sycl::access::address_space::global_space,
+          sycl::memory_order memoryOrder = sycl::memory_order::relaxed,
+          sycl::memory_scope memoryScope = sycl::memory_scope::device,
+          typename T1, typename T2>
+inline T1 atomic_fetch_and(T1 *addr, T2 operand) {
+  auto atm =
+      sycl::atomic_ref<T1, memoryOrder, memoryScope, addressSpace>(addr[0]);
+  return atm.fetch_and(operand);
+}
+
+/// Atomically perform a bitwise AND between the value operand and the value at the addr
+/// and assign the result to the value at addr.
+/// \param [in, out] addr The pointer to the data.
+/// \param operand The value to use in bitwise AND operation with the value at the \p addr.
+/// \param memoryOrder The memory ordering used.
+/// \returns The value at the \p addr before the call.
+template <typename T, sycl::access::address_space addressSpace =
+                          sycl::access::address_space::global_space>
+inline T atomic_fetch_and(T *addr, T operand,
+                          sycl::memory_order memoryOrder) {
+  switch (memoryOrder) {
+  case sycl::memory_order::relaxed:
+    return atomic_fetch_and<T, addressSpace, sycl::memory_order::relaxed,
+                            sycl::memory_scope::device>(addr, operand);
+  case sycl::memory_order::acq_rel:
+    return atomic_fetch_and<T, addressSpace, sycl::memory_order::acq_rel,
+                            sycl::memory_scope::device>(addr, operand);
+  case sycl::memory_order::seq_cst:
+    return atomic_fetch_and<T, addressSpace, sycl::memory_order::seq_cst,
+                            sycl::memory_scope::device>(addr, operand);
+  default:
+    assert(false && "Invalid memory_order for atomics. Valid memory_order for "
+                    "atomics are: sycl::memory_order::relaxed, "
+                    "sycl::memory_order::acq_rel, sycl::memory_order::seq_cst!");
+  }
+}
+
+template <sycl::access::address_space addressSpace =
+              sycl::access::address_space::global_space,
+          typename T1, typename T2>
+inline T1 atomic_fetch_and(T1 *addr, T2 operand,
+                           sycl::memory_order memoryOrder) {
+  atomic_fetch_and<T1, addressSpace>(addr, operand, memoryOrder);
+}
+
+/// Atomically or the value at the addr with the value operand, and assign
+/// the result to the value at addr.
+/// \param [in, out] addr The pointer to the data.
+/// \param operand The value to use in bitwise OR operation with the value at the \p addr.
+/// \param memoryOrder The memory ordering used.
+/// \returns The value at the \p addr before the call.
+template <typename T,
+          sycl::access::address_space addressSpace =
+              sycl::access::address_space::global_space,
+          sycl::memory_order memoryOrder = sycl::memory_order::relaxed,
+          sycl::memory_scope memoryScope = sycl::memory_scope::device>
+inline T atomic_fetch_or(T *addr, T operand) {
+  auto atm =
+      sycl::atomic_ref<T, memoryOrder, memoryScope, addressSpace>(addr[0]);
+  return atm.fetch_or(operand);
+}
+
+template <sycl::access::address_space addressSpace =
+              sycl::access::address_space::global_space,
+          sycl::memory_order memoryOrder = sycl::memory_order::relaxed,
+          sycl::memory_scope memoryScope = sycl::memory_scope::device,
+          typename T1, typename T2>
+inline T1 atomic_fetch_or(T1 *addr, T2 operand) {
+  auto atm =
+      sycl::atomic_ref<T1, memoryOrder, memoryScope, addressSpace>(addr[0]);
+  return atm.fetch_or(operand);
+}
+
+/// Atomically or the value at the addr with the value operand, and assign
+/// the result to the value at addr.
+/// \param [in, out] addr The pointer to the data.
+/// \param operand The value to use in bitwise OR operation with the value at the \p addr.
+/// \param memoryOrder The memory ordering used.
+/// \returns The value at the \p addr before the call.
+template <typename T, sycl::access::address_space addressSpace =
+                          sycl::access::address_space::global_space>
+inline T atomic_fetch_or(T *addr, T operand,
+                         sycl::memory_order memoryOrder) {
+  switch (memoryOrder) {
+  case sycl::memory_order::relaxed:
+    return atomic_fetch_or<T, addressSpace, sycl::memory_order::relaxed,
+                           sycl::memory_scope::device>(addr, operand);
+  case sycl::memory_order::acq_rel:
+    return atomic_fetch_or<T, addressSpace, sycl::memory_order::acq_rel,
+                           sycl::memory_scope::device>(addr, operand);
+  case sycl::memory_order::seq_cst:
+    return atomic_fetch_or<T, addressSpace, sycl::memory_order::seq_cst,
+                           sycl::memory_scope::device>(addr, operand);
+  default:
+    assert(false && "Invalid memory_order for atomics. Valid memory_order for "
+                    "atomics are: sycl::memory_order::relaxed, "
+                    "sycl::memory_order::acq_rel, sycl::memory_order::seq_cst!");
+  }
+}
+
+template <sycl::access::address_space addressSpace =
+              sycl::access::address_space::global_space,
+          typename T1, typename T2>
+inline T1 atomic_fetch_or(T1 *addr, T2 operand,
+                           sycl::memory_order memoryOrder) {
+  atomic_fetch_or<T1, addressSpace>(addr, operand, memoryOrder);
+}
+
+/// Atomically xor the value at the addr with the value operand, and assign
+/// the result to the value at addr.
+/// \param [in, out] addr The pointer to the data.
+/// \param operand The value to use in bitwise XOR operation with the value at the \p addr.
+/// \param memoryOrder The memory ordering used.
+/// \returns The value at the \p addr before the call.
+template <typename T,
+          sycl::access::address_space addressSpace =
+              sycl::access::address_space::global_space,
+          sycl::memory_order memoryOrder = sycl::memory_order::relaxed,
+          sycl::memory_scope memoryScope = sycl::memory_scope::device>
+inline T atomic_fetch_xor(T *addr, T operand) {
+  auto atm =
+      sycl::atomic_ref<T, memoryOrder, memoryScope, addressSpace>(addr[0]);
+  return atm.fetch_xor(operand);
+}
+
+template <sycl::access::address_space addressSpace =
+              sycl::access::address_space::global_space,
+          sycl::memory_order memoryOrder = sycl::memory_order::relaxed,
+          sycl::memory_scope memoryScope = sycl::memory_scope::device,
+          typename T1, typename T2>
+inline T1 atomic_fetch_xor(T1 *addr, T2 operand) {
+  auto atm =
+      sycl::atomic_ref<T1, memoryOrder, memoryScope, addressSpace>(addr[0]);
+  return atm.fetch_xor(operand);
+}
+
+/// Atomically xor the value at the addr with the value operand, and assign
+/// the result to the value at addr.
+/// \param [in, out] addr The pointer to the data.
+/// \param operand The value to use in bitwise XOR operation with the value at the \p addr.
+/// \param memoryOrder The memory ordering used.
+/// \returns The value at the \p addr before the call.
+template <typename T, sycl::access::address_space addressSpace =
+                          sycl::access::address_space::global_space>
+inline T atomic_fetch_xor(T *addr, T operand,
+                          sycl::memory_order memoryOrder) {
+  switch (memoryOrder) {
+  case sycl::memory_order::relaxed:
+    return atomic_fetch_xor<T, addressSpace, sycl::memory_order::relaxed,
+                            sycl::memory_scope::device>(addr, operand);
+  case sycl::memory_order::acq_rel:
+    return atomic_fetch_xor<T, addressSpace, sycl::memory_order::acq_rel,
+                            sycl::memory_scope::device>(addr, operand);
+  case sycl::memory_order::seq_cst:
+    return atomic_fetch_xor<T, addressSpace, sycl::memory_order::seq_cst,
+                            sycl::memory_scope::device>(addr, operand);
+  default:
+    assert(false && "Invalid memory_order for atomics. Valid memory_order for "
+                    "atomics are: sycl::memory_order::relaxed, "
+                    "sycl::memory_order::acq_rel, sycl::memory_order::seq_cst!");
+  }
+}
+
+template <sycl::access::address_space addressSpace =
+              sycl::access::address_space::global_space,
+          typename T1, typename T2>
+inline T1 atomic_fetch_xor(T1 *addr, T2 operand,
+                           sycl::memory_order memoryOrder) {
+  atomic_fetch_xor<T1, addressSpace>(addr, operand, memoryOrder);
+}
+
+/// Atomically calculate the minimum of the value at addr and the value operand
+/// and assign the result to the value at addr.
+/// \param [in, out] addr The pointer to the data.
+/// \param operand.
+/// \param memoryOrder The memory ordering used.
+/// \returns The value at the \p addr before the call.
+template <typename T,
+          sycl::access::address_space addressSpace =
+              sycl::access::address_space::global_space,
+          sycl::memory_order memoryOrder = sycl::memory_order::relaxed,
+          sycl::memory_scope memoryScope = sycl::memory_scope::device>
+inline T atomic_fetch_min(T *addr, T operand) {
+  auto atm =
+      sycl::atomic_ref<T, memoryOrder, memoryScope, addressSpace>(addr[0]);
+  return atm.fetch_min(operand);
+}
+
+template <sycl::access::address_space addressSpace =
+              sycl::access::address_space::global_space,
+          sycl::memory_order memoryOrder = sycl::memory_order::relaxed,
+          sycl::memory_scope memoryScope = sycl::memory_scope::device,
+          typename T1, typename T2>
+inline T1 atomic_fetch_min(T1 *addr, T2 operand) {
+  auto atm =
+      sycl::atomic_ref<T1, memoryOrder, memoryScope, addressSpace>(addr[0]);
+  return atm.fetch_min(operand);
+}
+
+/// Atomically calculate the minimum of the value at addr and the value operand
+/// and assign the result to the value at addr.
+/// \param [in, out] addr The pointer to the data.
+/// \param operand.
+/// \param memoryOrder The memory ordering used.
+/// \returns The value at the \p addr before the call.
+template <typename T, sycl::access::address_space addressSpace =
+                          sycl::access::address_space::global_space>
+inline T atomic_fetch_min(T *addr, T operand,
+                          sycl::memory_order memoryOrder) {
+  switch (memoryOrder) {
+  case sycl::memory_order::relaxed:
+    return atomic_fetch_min<T, addressSpace, sycl::memory_order::relaxed,
+                            sycl::memory_scope::device>(addr, operand);
+  case sycl::memory_order::acq_rel:
+    return atomic_fetch_min<T, addressSpace, sycl::memory_order::acq_rel,
+                            sycl::memory_scope::device>(addr, operand);
+  case sycl::memory_order::seq_cst:
+    return atomic_fetch_min<T, addressSpace, sycl::memory_order::seq_cst,
+                            sycl::memory_scope::device>(addr, operand);
+  default:
+    assert(false && "Invalid memory_order for atomics. Valid memory_order for "
+                    "atomics are: sycl::memory_order::relaxed, "
+                    "sycl::memory_order::acq_rel, sycl::memory_order::seq_cst!");
+  }
+}
+
+template <sycl::access::address_space addressSpace =
+              sycl::access::address_space::global_space,
+          typename T1, typename T2>
+inline T1 atomic_fetch_min(T1 *addr, T2 operand,
+                           sycl::memory_order memoryOrder) {
+  atomic_fetch_min<T1, addressSpace>(addr, operand, memoryOrder);
+}
+
+/// Atomically calculate the maximum of the value at addr and the value operand
+/// and assign the result to the value at addr.
+/// \param [in, out] addr The pointer to the data.
+/// \param operand.
+/// \param memoryOrder The memory ordering used.
+/// \returns The value at the \p addr before the call.
+template <typename T,
+          sycl::access::address_space addressSpace =
+              sycl::access::address_space::global_space,
+          sycl::memory_order memoryOrder = sycl::memory_order::relaxed,
+          sycl::memory_scope memoryScope = sycl::memory_scope::device>
+inline T atomic_fetch_max(T *addr, T operand) {
+  auto atm =
+      sycl::atomic_ref<T, memoryOrder, memoryScope, addressSpace>(addr[0]);
+  return atm.fetch_max(operand);
+}
+
+template <sycl::access::address_space addressSpace =
+              sycl::access::address_space::global_space,
+          sycl::memory_order memoryOrder = sycl::memory_order::relaxed,
+          sycl::memory_scope memoryScope = sycl::memory_scope::device,
+          typename T1, typename T2>
+inline T1 atomic_fetch_max(T1 *addr, T2 operand) {
+  auto atm =
+      sycl::atomic_ref<T1, memoryOrder, memoryScope, addressSpace>(addr[0]);
+  return atm.fetch_max(operand);
+}
+
+/// Atomically calculate the maximum of the value at addr and the value operand
+/// and assign the result to the value at addr.
+/// \param [in, out] addr The pointer to the data.
+/// \param operand.
+/// \param memoryOrder The memory ordering used.
+/// \returns The value at the \p addr before the call.
+template <typename T, sycl::access::address_space addressSpace =
+                          sycl::access::address_space::global_space>
+inline T atomic_fetch_max(T *addr, T operand,
+                          sycl::memory_order memoryOrder) {
+  switch (memoryOrder) {
+  case sycl::memory_order::relaxed:
+    return atomic_fetch_max<T, addressSpace, sycl::memory_order::relaxed,
+                            sycl::memory_scope::device>(addr, operand);
+  case sycl::memory_order::acq_rel:
+    return atomic_fetch_max<T, addressSpace, sycl::memory_order::acq_rel,
+                            sycl::memory_scope::device>(addr, operand);
+  case sycl::memory_order::seq_cst:
+    return atomic_fetch_max<T, addressSpace, sycl::memory_order::seq_cst,
+                            sycl::memory_scope::device>(addr, operand);
+  default:
+    assert(false && "Invalid memory_order for atomics. Valid memory_order for "
+                    "atomics are: sycl::memory_order::relaxed, "
+                    "sycl::memory_order::acq_rel, sycl::memory_order::seq_cst!");
+  }
+}
+
+template <sycl::access::address_space addressSpace =
+              sycl::access::address_space::global_space,
+          typename T1, typename T2>
+inline T1 atomic_fetch_max(T1 *addr, T2 operand,
+                           sycl::memory_order memoryOrder) {
+  atomic_fetch_max<T1, addressSpace>(addr, operand, memoryOrder);
+}
+
+/// Atomically set \p operand to the value stored in \p addr, if old value stored in
+/// \p addr is equal to zero or greater than \p operand, else decrease the value stored
+/// in \p addr.
+/// \param [in, out] addr The pointer to the data.
+/// \param operand The threshold value.
+/// \param memoryOrder The memory ordering used.
+/// \returns The old value stored in \p addr.
+template <sycl::access::address_space addressSpace = sycl::access::address_space::global_space,
+          sycl::memory_order memoryOrder = sycl::memory_order::relaxed,
+          sycl::memory_scope memoryScope = sycl::memory_scope::device>
+inline unsigned int atomic_fetch_compare_dec(unsigned int *addr,
+                                             unsigned int operand) {
+  auto atm = sycl::atomic_ref<unsigned int, memoryOrder, memoryScope,
+                                  addressSpace>(addr[0]);
+  unsigned int old;
+
+	while (true) {
+	  old = atm.load();
+	  if (old == 0 || old > operand) {
+		  if (atm.compare_exchange_strong(old, operand))
+        break;
+	  } else if (atm.compare_exchange_strong(old, old - 1))
+      break;
+	}
+
+  return old;
+}
+
+/// Atomically increment the value stored in \p addr if old value stored in \p
+/// addr is less than \p operand, else set 0 to the value stored in \p addr.
+/// \param [in, out] addr The pointer to the data.
+/// \param operand The threshold value.
+/// \param memoryOrder The memory ordering used.
+/// \returns The old value stored in \p addr.
+template <sycl::access::address_space addressSpace =
+              sycl::access::address_space::global_space,
+          sycl::memory_order memoryOrder = sycl::memory_order::relaxed,
+          sycl::memory_scope memoryScope = sycl::memory_scope::device>
+inline unsigned int atomic_fetch_compare_inc(unsigned int *addr,
+                                             unsigned int operand) {
+  auto atm = sycl::atomic_ref<unsigned int, memoryOrder, memoryScope,
+                                  addressSpace>(addr[0]);
+  unsigned int old;
+  while (true) {
+    old = atm.load();
+    if (old >= operand) {
+      if (atm.compare_exchange_strong(old, 0))
+        break;
+    } else if (atm.compare_exchange_strong(old, old + 1))
+      break;
+  }
+  return old;
+}
+
+/// Atomically increment the value stored in \p addr if old value stored in \p
+/// addr is less than \p operand, else set 0 to the value stored in \p addr.
+/// \param [in, out] addr The pointer to the data.
+/// \param operand The threshold value.
+/// \param memoryOrder The memory ordering used.
+/// \returns The old value stored in \p addr.
+template <sycl::access::address_space addressSpace =
+              sycl::access::address_space::global_space>
+inline unsigned int
+atomic_fetch_compare_inc(unsigned int *addr, unsigned int operand,
+                         sycl::memory_order memoryOrder) {
+  switch (memoryOrder) {
+  case sycl::memory_order::relaxed:
+    return atomic_fetch_compare_inc<addressSpace, sycl::memory_order::relaxed,
+                                    sycl::memory_scope::device>(addr,
+                                                                   operand);
+  case sycl::memory_order::acq_rel:
+    return atomic_fetch_compare_inc<addressSpace, sycl::memory_order::acq_rel,
+                                    sycl::memory_scope::device>(addr,
+                                                                   operand);
+  case sycl::memory_order::seq_cst:
+    return atomic_fetch_compare_inc<addressSpace, sycl::memory_order::seq_cst,
+                                    sycl::memory_scope::device>(addr,
+                                                                   operand);
+  default:
+    assert(false && "Invalid memory_order for atomics. Valid memory_order for "
+                    "atomics are: sycl::memory_order::relaxed, "
+                    "sycl::memory_order::acq_rel, sycl::memory_order::seq_cst!");
+  }
+}
+
+/// Atomically exchange the value at the address addr with the value operand.
+/// \param [in, out] addr The pointer to the data.
+/// \param operand The value to be exchanged with the value pointed by \p addr.
+/// \param memoryOrder The memory ordering used.
+/// \returns The value at the \p addr before the call.
+template <typename T,
+          sycl::access::address_space addressSpace =
+              sycl::access::address_space::global_space,
+          sycl::memory_order memoryOrder = sycl::memory_order::relaxed,
+          sycl::memory_scope memoryScope = sycl::memory_scope::device>
+inline T atomic_exchange(T *addr, T operand) {
+  auto atm =
+      sycl::atomic_ref<T, memoryOrder, memoryScope, addressSpace>(addr[0]);
+  return atm.exchange(operand);
+}
+
+template <sycl::access::address_space addressSpace =
+              sycl::access::address_space::global_space,
+          sycl::memory_order memoryOrder = sycl::memory_order::relaxed,
+          sycl::memory_scope memoryScope = sycl::memory_scope::device,
+          typename T1, typename T2>
+inline T1 atomic_exchange(T1 *addr, T2 operand) {
+  auto atm =
+      sycl::atomic_ref<T1, memoryOrder, memoryScope, addressSpace>(addr[0]);
+  return atm.exchange(operand);
+}
+
+/// Atomically exchange the value at the address addr with the value operand.
+/// \param [in, out] addr The pointer to the data.
+/// \param operand The value to be exchanged with the value pointed by \p addr.
+/// \param memoryOrder The memory ordering used.
+/// \returns The value at the \p addr before the call.
+template <typename T, sycl::access::address_space addressSpace =
+                          sycl::access::address_space::global_space>
+inline T atomic_exchange(T *addr, T operand,
+                         sycl::memory_order memoryOrder) {
+  switch (memoryOrder) {
+  case sycl::memory_order::relaxed:
+    return atomic_exchange<T, addressSpace, sycl::memory_order::relaxed,
+                           sycl::memory_scope::device>(addr, operand);
+  case sycl::memory_order::acq_rel:
+    return atomic_exchange<T, addressSpace, sycl::memory_order::acq_rel,
+                           sycl::memory_scope::device>(addr, operand);
+  case sycl::memory_order::seq_cst:
+    return atomic_exchange<T, addressSpace, sycl::memory_order::seq_cst,
+                           sycl::memory_scope::device>(addr, operand);
+  default:
+    assert(false && "Invalid memory_order for atomics. Valid memory_order for "
+                    "atomics are: sycl::memory_order::relaxed, "
+                    "sycl::memory_order::acq_rel, sycl::memory_order::seq_cst!");
+  }
+}
+
+template <sycl::access::address_space addressSpace =
+              sycl::access::address_space::global_space,
+          typename T1, typename T2>
+inline T1 atomic_exchange(T1 *addr, T2 operand,
+                           sycl::memory_order memoryOrder) {
+  atomic_exchange<T1, addressSpace>(addr, operand, memoryOrder);
+}
+
+/// Atomically compare the value at \p addr to the value expected and exchange
+/// with the value desired if the value at \p addr is equal to the value expected.
+/// Returns the value at the \p addr before the call.
+/// \param [in, out] addr Multi_ptr.
+/// \param expected The value to compare against the value at \p addr.
+/// \param desired The value to assign to \p addr if the value at \p addr is expected.
+/// \param success The memory ordering used when comparison succeeds.
+/// \param fail The memory ordering used when comparison fails.
+/// \returns The value at the \p addr before the call.
+template <typename T,
+          sycl::access::address_space addressSpace =
+              sycl::access::address_space::global_space,
+          sycl::memory_order memoryOrder = sycl::memory_order::relaxed,
+          sycl::memory_scope memoryScope = sycl::memory_scope::device>
+T atomic_compare_exchange_strong(
+    sycl::multi_ptr<T, addressSpace> addr, T expected, T desired,
+    sycl::memory_order success = sycl::memory_order::relaxed,
+    sycl::memory_order fail = sycl::memory_order::relaxed) {
+  auto atm = sycl::atomic_ref<T, memoryOrder, memoryScope, addressSpace>(*addr);
+
+  atm.compare_exchange_strong(expected, desired, success, fail);
+  return expected;
+}
+
+template <sycl::access::address_space addressSpace =
+              sycl::access::address_space::global_space,
+          sycl::memory_order memoryOrder = sycl::memory_order::relaxed,
+          sycl::memory_scope memoryScope = sycl::memory_scope::device,
+          typename T1, typename T2, typename T3>
+T1 atomic_compare_exchange_strong(
+    sycl::multi_ptr<T1, addressSpace> addr, T2 expected, T3 desired,
+    sycl::memory_order success = sycl::memory_order::relaxed,
+    sycl::memory_order fail = sycl::memory_order::relaxed) {
+  auto atm =
+      sycl::atomic_ref<T1, memoryOrder, memoryScope, addressSpace>(*addr);
+  T1 expected_value = expected;
+  atm.compare_exchange_strong(expected_value, desired, success, fail);
+  return expected_value;
+}
+
+/// Atomically compare the value at \p addr to the value expected and exchange
+/// with the value desired if the value at \p addr is equal to the value expected.
+/// Returns the value at the \p addr before the call.
+/// \param [in] addr The pointer to the data.
+/// \param expected The value to compare against the value at \p addr.
+/// \param desired The value to assign to \p addr if the value at \p addr is expected.
+/// \param success The memory ordering used when comparison succeeds.
+/// \param fail The memory ordering used when comparison fails.
+/// \returns The value at the \p addr before the call.
+template <typename T,
+          sycl::access::address_space addressSpace =
+              sycl::access::address_space::global_space,
+          sycl::memory_order memoryOrder = sycl::memory_order::relaxed,
+          sycl::memory_scope memoryScope = sycl::memory_scope::device>
+T atomic_compare_exchange_strong(
+    T *addr, T expected, T desired,
+    sycl::memory_order success = sycl::memory_order::relaxed,
+    sycl::memory_order fail = sycl::memory_order::relaxed) {
+  auto atm =
+      sycl::atomic_ref<T, memoryOrder, memoryScope, addressSpace>(addr[0]);
+  atm.compare_exchange_strong(expected, desired, success, fail);
+  return expected;
+}
+
+template <sycl::access::address_space addressSpace =
+              sycl::access::address_space::global_space,
+          sycl::memory_order memoryOrder = sycl::memory_order::relaxed,
+          sycl::memory_scope memoryScope = sycl::memory_scope::device,
+          typename T1, typename T2, typename T3>
+T1 atomic_compare_exchange_strong(
+    T1 *addr, T2 expected, T3 desired,
+    sycl::memory_order success = sycl::memory_order::relaxed,
+    sycl::memory_order fail = sycl::memory_order::relaxed) {
+  T1 expected_value = expected;
+  auto atm =
+      sycl::atomic_ref<T1, memoryOrder, memoryScope, addressSpace>(addr[0]);
+  atm.compare_exchange_strong(expected_value, desired, success, fail);
+  return expected_value;
+}
+
+/// Atomic extension to implement standard APIs in std::atomic
+namespace detail{
+template <typename T> struct IsValidAtomicType {
+  static constexpr bool value =
+      (std::is_same<T, int>::value || std::is_same<T, unsigned int>::value ||
+       std::is_same<T, long>::value || std::is_same<T, unsigned long>::value ||
+       std::is_same<T, long long>::value ||
+       std::is_same<T, unsigned long long>::value ||
+       std::is_same<T, float>::value || std::is_same<T, double>::value ||
+       std::is_pointer<T>::value);
+};
+} // namespace detail
+
+template <typename T,
+          sycl::memory_scope DefaultScope = sycl::memory_scope::system,
+          sycl::memory_order DefaultOrder = sycl::memory_order::seq_cst,
+          sycl::access::address_space Space =
+              sycl::access::address_space::generic_space>
+class atomic{
+  static_assert(
+    detail::IsValidAtomicType<T>::value,
+    "Invalid atomic type.  Valid types are int, unsigned int, long, "
+      "unsigned long, long long, unsigned long long, float, double "
+      "and pointer types");
+  T __d;
+
+public:
+  /// default memory synchronization order
+  static constexpr sycl::memory_order default_read_order =
+      sycl::atomic_ref<T, DefaultOrder, DefaultScope, Space>::default_read_order;
+  static constexpr sycl::memory_order default_write_order =
+      sycl::atomic_ref<T, DefaultOrder, DefaultScope, Space>::default_write_order;
+  static constexpr sycl::memory_scope default_scope = DefaultScope;
+  static constexpr sycl::memory_order default_read_modify_write_order =
+      DefaultOrder;
+  
+
+  /// Default constructor.
+  constexpr atomic() noexcept = default;
+  /// Constructor with initialize value.
+  constexpr atomic(T d) noexcept : __d(d){};
+
+  /// atomically replaces the value of the referenced object with a non-atomic argument
+  /// \param operand The value to replace the pointed value.
+  /// \param memoryOrder The memory ordering used.
+  /// \param memoryScope The memory scope used.
+  void store(T operand, sycl::memory_order memoryOrder = default_write_order,
+             sycl::memory_scope memoryScope = default_scope) noexcept {
+    sycl::atomic_ref<T, DefaultOrder, DefaultScope, Space> atm(__d);
+    atm.store(operand, memoryOrder, memoryScope);
+  }
+
+  /// atomically obtains the value of the referenced object
+  /// \param memoryOrder The memory ordering used.
+  /// \param memoryScope The memory scope used.
+  /// \returns The value of the referenced object
+  T load(sycl::memory_order memoryOrder = default_read_order,
+         sycl::memory_scope memoryScope = default_scope) const noexcept {
+    sycl::atomic_ref<T, DefaultOrder, DefaultScope, Space> atm(
+      const_cast<T &>(__d));
+    return atm.load(memoryOrder, memoryScope);
+  }
+
+  /// atomically replaces the value of the referenced object and obtains the value held previously
+  /// \param operand The value to replace the pointed value.
+  /// \param memoryOrder The memory ordering used.
+  /// \param memoryScope The memory scope used.
+  /// \returns The value of the referenced object before the call.
+  T exchange(T operand,
+             sycl::memory_order memoryOrder = default_read_modify_write_order,
+             sycl::memory_scope memoryScope = default_scope) noexcept {
+
+    sycl::atomic_ref<T, DefaultOrder, DefaultScope, Space> atm(__d);
+    return atm.exchange(operand, memoryOrder, memoryScope);
+  }
+
+  /// atomically compares the value of the referenced object with non-atomic argument 
+  /// and performs atomic exchange if equal or atomic load if not
+  /// \param expected The value expected to be found in the object referenced by the atomic_ref object
+  /// \param desired  The value to store in the referenced object if it is as expected
+  /// \param success The memory models for the read-modify-write
+  /// \param failure The memory models for load operations
+  /// \param memoryScope The memory scope used.
+  /// \returns true if the referenced object was successfully changed, false otherwise.
+  bool compare_exchange_weak(
+      T &expected, T desired,
+      sycl::memory_order success, sycl::memory_order failure,
+      sycl::memory_scope memoryScope = default_scope) noexcept {
+    sycl::atomic_ref<T, DefaultOrder, DefaultScope, Space> atm(__d);
+    return atm.compare_exchange_weak(expected, desired, success, failure, memoryScope);
+  }
+  /// \param expected The value expected to be found in the object referenced by the atomic_ref object
+  /// \param desired  The value to store in the referenced object if it is as expected
+  /// \param memoryOrder 	The memory synchronization ordering for operations
+  /// \param memoryScope The memory scope used.
+  /// \returns true if the referenced object was successfully changed, false otherwise.
+  bool compare_exchange_weak(T &expected, T desired,
+                  sycl::memory_order memoryOrder = default_read_modify_write_order,
+                  sycl::memory_scope memoryScope = default_scope) noexcept {
+    sycl::atomic_ref<T, DefaultOrder, DefaultScope, Space> atm(__d);
+    return atm.compare_exchange_weak(expected, desired, memoryOrder, memoryScope);
+  }
+
+  /// atomically compares the value of the referenced object with non-atomic argument 
+  /// and performs atomic exchange if equal or atomic load if not
+  /// \param expected The value expected to be found in the object referenced by the atomic_ref object
+  /// \param desired  The value to store in the referenced object if it is as expected
+  /// \param success The memory models for the read-modify-write
+  /// \param failure The memory models for load operations
+  /// \param memoryScope The memory scope used.
+  /// \returns true if the referenced object was successfully changed, false otherwise.
+  bool compare_exchange_strong(
+      T &expected, T desired,
+      sycl::memory_order success, sycl::memory_order failure,
+      sycl::memory_scope memoryScope = default_scope) noexcept {
+
+    sycl::atomic_ref<T, DefaultOrder, DefaultScope, Space> atm(__d);
+    return atm.compare_exchange_strong(expected, desired, success, failure, memoryScope);
+  }
+  /// \param expected The value expected to be found in the object referenced by the atomic_ref object
+  /// \param desired  The value to store in the referenced object if it is as expected
+  /// \param memoryOrder 	The memory synchronization ordering for operations
+  /// \param memoryScope The memory scope used.
+  /// \returns true if the referenced object was successfully changed, false otherwise.
+  bool compare_exchange_strong(T &expected, T desired,
+                    sycl::memory_order memoryOrder = default_read_modify_write_order,
+                    sycl::memory_scope memoryScope = default_scope) noexcept {
+    sycl::atomic_ref<T, DefaultOrder, DefaultScope, Space> atm(__d);
+    return atm.compare_exchange_strong(expected, desired, memoryOrder, memoryScope);
+  }
+
+  /// atomically adds the argument to the value stored in the atomic object and obtains the value held previously
+  /// \param operand 	The other argument of arithmetic addition
+  /// \param memoryOrder The memory ordering used.
+  /// \param memoryScope The memory scope used.
+  /// \returns The value of the referenced object before the call.
+  T fetch_add(T operand,
+              sycl::memory_order memoryOrder = default_read_modify_write_order,
+              sycl::memory_scope  memoryScope = default_scope) noexcept {
+
+    sycl::atomic_ref<T, DefaultOrder, DefaultScope, Space> atm(__d);
+    return atm.fetch_add(operand, memoryOrder,  memoryScope);
+  }
+
+  /// atomically subtracts the argument from the value stored in the atomic object and obtains the value held previously
+  /// \param operand 	The other argument of arithmetic subtraction
+  /// \param memoryOrder The memory ordering used.
+  /// \param memoryScope The memory scope used.
+  /// \returns The value of the referenced object before the call.
+  T fetch_sub(T operand,
+              sycl::memory_order memoryOrder = default_read_modify_write_order,
+              sycl::memory_scope memoryScope = default_scope) noexcept {
+
+    sycl::atomic_ref<T, DefaultOrder, DefaultScope, Space> atm(__d);
+    return atm.fetch_sub(operand, memoryOrder, memoryScope);
+  }
+};
+
+} // namespace dpct
+#endif // __DPCT_ATOMIC_HPP__
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/blas_utils.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/blas_utils.hpp
new file mode 100644
index 0000000..df222c5
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/blas_utils.hpp
@@ -0,0 +1,1792 @@
+//==---- blas_utils.hpp----------------------------*- C++ -*----------------==//
+//
+// Copyright (C) Intel Corporation
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+// See https://llvm.org/LICENSE.txt for license information.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef __DPCT_BLAS_UTILS_HPP__
+#define __DPCT_BLAS_UTILS_HPP__
+
+#include "memory.hpp"
+#include "util.hpp"
+#include "lib_common_utils.hpp"
+#include <sycl/sycl.hpp>
+#include <oneapi/mkl.hpp>
+#include <utility>
+#include <vector>
+#include <thread>
+
+namespace dpct {
+
+/// Get the value of \p s.
+/// Copy the data to host synchronously, then return the data.
+/// \param [in] p The pointer points the data.
+/// \param [in] q The queue where the memory copy should be executed.
+template <typename T>
+inline auto get_value(const T *s, sycl::queue &q) {
+  return detail::get_value(s, q);
+}
+
+namespace detail {
+inline void mem_free(sycl::queue *exec_queue,
+                     std::vector<void *> pointers_array, sycl::event e) {
+  e.wait();
+  for (auto p : pointers_array)
+    sycl::free(p, *exec_queue);
+}
+
+inline int stride_for(int num_elems, int mem_align_in_elems) {
+  return ((num_elems - 1) / mem_align_in_elems + 1) * mem_align_in_elems;
+}
+
+#ifndef DPCT_USM_LEVEL_NONE
+template<typename T>
+class working_memory {
+  T *_input_ptr;
+  T *_temp_ptr;
+  bool _is_sycl_malloced = false;
+  bool _is_scalar_value = false;
+  sycl::queue _q;
+  sycl::event _e;
+
+public:
+  working_memory(size_t size, sycl::queue q) : _q(q) {
+    _is_scalar_value = false;
+    _temp_ptr = (T *)sycl::malloc_device(size, q);
+  }
+  working_memory(T *result_ptr, sycl::queue q) : _input_ptr(result_ptr), _q(q) {
+    _is_scalar_value = true;
+    _is_sycl_malloced = sycl::get_pointer_type(_input_ptr, _q.get_context()) !=
+                        sycl::usm::alloc::unknown;
+    if (!_is_sycl_malloced)
+      _temp_ptr = sycl::malloc_shared<T>(1, _q);
+  }
+  auto get_ptr() {
+    if (_is_scalar_value && _is_sycl_malloced)
+      return _input_ptr;
+    return _temp_ptr;
+  }
+  void set_event(sycl::event e) { _e = e; }
+  ~working_memory() {
+    if (_is_scalar_value) {
+      if (!_is_sycl_malloced) {
+        _q.memcpy(_input_ptr, _temp_ptr, sizeof(T)).wait();
+        sycl::free(_temp_ptr, _q);
+      }
+    } else {
+      std::vector<void *> ptrs{_temp_ptr};
+      dpct::async_dpct_free(ptrs, {_e});
+    }
+  }
+};
+#endif
+
+template <typename Tx, typename Tr>
+inline void nrm2_impl(sycl::queue &q, int n, const void *x, int incx,
+                         void *result) {
+#ifndef __INTEL_MKL__
+  throw std::runtime_error("The oneAPI Math Kernel Library (oneMKL) Interfaces "
+                           "Project does not support this API.");
+#else
+#ifdef DPCT_USM_LEVEL_NONE
+  auto x_buffer = dpct::get_buffer<Tx>(x);
+  auto r_buffer =
+      sycl::buffer<Tr, 1>(reinterpret_cast<Tr *>(result), sycl::range<1>(1));
+  if (dpct::is_device_ptr(result))
+    r_buffer = dpct::get_buffer<Tr>(result);
+  oneapi::mkl::blas::column_major::nrm2(q, n, x_buffer, incx, r_buffer);
+#else
+  working_memory<Tr> res_mem(reinterpret_cast<Tr *>(result), q);
+  oneapi::mkl::blas::column_major::nrm2(q, n, reinterpret_cast<const Tx *>(x),
+                                        incx, res_mem.get_ptr());
+#endif
+#endif
+}
+
+template <bool is_conjugate, class Txy, class Tr>
+inline void dotuc_impl(sycl::queue &q, int n, const Txy *x, int incx,
+                          const Txy *y, int incy, Tr *result) {
+#ifndef __INTEL_MKL__
+  throw std::runtime_error("The oneAPI Math Kernel Library (oneMKL) Interfaces "
+                           "Project does not support this API.");
+#else
+#ifdef DPCT_USM_LEVEL_NONE
+  auto x_buffer = dpct::get_buffer<Txy>(x);
+  auto y_buffer = dpct::get_buffer<Txy>(y);
+  auto r_buffer = sycl::buffer<Tr, 1>((Tr *)result, sycl::range<1>(1));
+  if (dpct::is_device_ptr(result))
+    r_buffer = dpct::get_buffer<Tr>(result);
+  if constexpr (std::is_same_v<Txy, std::complex<float>> ||
+                std::is_same_v<Txy, std::complex<double>>) {
+    if constexpr (is_conjugate)
+      oneapi::mkl::blas::column_major::dotc(q, n, x_buffer, incx, y_buffer,
+                                            incy, r_buffer);
+    else
+      oneapi::mkl::blas::column_major::dotu(q, n, x_buffer, incx, y_buffer,
+                                            incy, r_buffer);
+  } else
+    oneapi::mkl::blas::column_major::dot(q, n, x_buffer, incx, y_buffer, incy,
+                                         r_buffer);
+#else
+  working_memory<Tr> res_mem(result, q);
+  if constexpr (std::is_same_v<Txy, std::complex<float>> ||
+                std::is_same_v<Txy, std::complex<double>>) {
+    if constexpr (is_conjugate)
+      oneapi::mkl::blas::column_major::dotc(q, n, x, incx, y, incy, res_mem.get_ptr());
+    else
+      oneapi::mkl::blas::column_major::dotu(q, n, x, incx, y, incy, res_mem.get_ptr());
+  } else
+    oneapi::mkl::blas::column_major::dot(q, n, x, incx, y, incy, res_mem.get_ptr());
+#endif
+#endif
+}
+
+template <bool is_conjugate>
+inline void dotuc(sycl::queue &q, int n, const void *x,
+                     library_data_t x_type, int incx, const void *y,
+                     library_data_t y_type, int incy, void *result,
+                     library_data_t result_type) {
+  std::uint64_t key = detail::get_type_combination_id(x_type, y_type, result_type);
+  switch (key) {
+  case detail::get_type_combination_id(library_data_t::real_float, library_data_t::real_float,
+                       library_data_t::real_float): {
+    detail::dotuc_impl<is_conjugate>(
+        q, n, reinterpret_cast<const float *>(x), incx,
+        reinterpret_cast<const float *>(y), incy,
+        reinterpret_cast<float *>(result));
+    break;
+  }
+  case detail::get_type_combination_id(library_data_t::real_double, library_data_t::real_double,
+                       library_data_t::real_double): {
+    detail::dotuc_impl<is_conjugate>(
+        q, n, reinterpret_cast<const double *>(x), incx,
+        reinterpret_cast<const double *>(y), incy,
+        reinterpret_cast<double *>(result));
+    break;
+  }
+  case detail::get_type_combination_id(library_data_t::complex_float,
+                       library_data_t::complex_float,
+                       library_data_t::complex_float): {
+    detail::dotuc_impl<is_conjugate>(
+        q, n, reinterpret_cast<const std::complex<float> *>(x), incx,
+        reinterpret_cast<const std::complex<float> *>(y), incy,
+        reinterpret_cast<std::complex<float> *>(result));
+    break;
+  }
+  case detail::get_type_combination_id(library_data_t::complex_double,
+                       library_data_t::complex_double,
+                       library_data_t::complex_double): {
+    detail::dotuc_impl<is_conjugate>(
+        q, n, reinterpret_cast<const std::complex<double> *>(x), incx,
+        reinterpret_cast<const std::complex<double> *>(y), incy,
+        reinterpret_cast<std::complex<double> *>(result));
+    break;
+  }
+  case detail::get_type_combination_id(library_data_t::real_half, library_data_t::real_half,
+                       library_data_t::real_half): {
+    detail::dotuc_impl<is_conjugate>(
+        q, n, reinterpret_cast<const sycl::half *>(x), incx,
+        reinterpret_cast<const sycl::half *>(y), incy,
+        reinterpret_cast<sycl::half *>(result));
+    break;
+  }
+  default:
+    throw std::runtime_error("the combination of data type is unsupported");
+  }
+}
+
+template <class Tx, class Te>
+inline void scal_impl(sycl::queue &q, int n, const void *alpha, void *x,
+                         int incx) {
+#ifndef __INTEL_MKL__
+  throw std::runtime_error("The oneAPI Math Kernel Library (oneMKL) Interfaces "
+                           "Project does not support this API.");
+#else
+  Te alpha_val = dpct::get_value(reinterpret_cast<const Te *>(alpha), q);
+  auto data_x = get_memory<Tx>(x);
+  oneapi::mkl::blas::column_major::scal(q, n, alpha_val,
+                                        data_x, incx);
+#endif
+}
+
+template <class Txy, class Te>
+inline void axpy_impl(sycl::queue &q, int n, const void *alpha, const void *x,
+                        int incx, void *y, int incy) {
+#ifndef __INTEL_MKL__
+  throw std::runtime_error("The oneAPI Math Kernel Library (oneMKL) Interfaces "
+                           "Project does not support this API.");
+#else
+  Te alpha_val = dpct::get_value(reinterpret_cast<const Te *>(alpha), q);
+  auto data_x = get_memory<const Txy>(x);
+  auto data_y = get_memory<Txy>(y);
+  oneapi::mkl::blas::column_major::axpy(q, n, alpha_val,
+                                        data_x, incx,
+                                        data_y, incy);
+#endif
+}
+
+template <class Txy, class Tc, class Ts>
+inline void rot_impl(sycl::queue &q, int n, void *x, int incx, void *y,
+                        int incy, const void *c, const void *s) {
+#ifndef __INTEL_MKL__
+  throw std::runtime_error("The oneAPI Math Kernel Library (oneMKL) Interfaces "
+                           "Project does not support this API.");
+#else
+  Tc c_value = dpct::get_value(reinterpret_cast<const Tc *>(c), q);
+  Ts s_value = dpct::get_value(reinterpret_cast<const Ts *>(s), q);
+  auto data_x = get_memory<Txy>(x);
+  auto data_y = get_memory<Txy>(y);
+  oneapi::mkl::blas::column_major::rot(q, n, data_x, incx,
+                                       data_y, incy, c_value,
+                                       s_value);
+#endif
+}
+
+template <class Ta, class Tb, class Tc, class Ts>
+inline void gemm_impl(sycl::queue &q, oneapi::mkl::transpose a_trans,
+                         oneapi::mkl::transpose b_trans, int m, int n, int k,
+                         const void *alpha, const void *a, int lda, const void *b,
+                         int ldb, const void *beta, void *c, int ldc) {
+#ifndef __INTEL_MKL__
+  throw std::runtime_error("The oneAPI Math Kernel Library (oneMKL) Interfaces "
+                           "Project does not support this API.");
+#else
+  Ts alpha_value = dpct::get_value(reinterpret_cast<const Ts *>(alpha), q);
+  Ts beta_value = dpct::get_value(reinterpret_cast<const Ts *>(beta), q);
+  auto data_a = get_memory<const Ta>(a);
+  auto data_b = get_memory<const Tb>(b);
+  auto data_c = get_memory<Tc>(c);
+  oneapi::mkl::blas::column_major::gemm(
+      q, a_trans, b_trans, m, n, k, alpha_value, data_a, lda,
+      data_b, ldb, beta_value, data_c, ldc);
+#endif
+}
+
+template <class Ta, class Tb, class Tc, class Ts>
+inline void gemm_batch_impl(sycl::queue &q, oneapi::mkl::transpose a_trans,
+                            oneapi::mkl::transpose b_trans, int m, int n, int k,
+                            const void *alpha, const void **a, int lda,
+                            const void **b, int ldb, const void *beta, void **c,
+                            int ldc, int batch_size) {
+  struct matrix_info_t {
+    oneapi::mkl::transpose transpose_info[2];
+    Ts value_info[2];
+    std::int64_t size_info[3];
+    std::int64_t ld_info[3];
+    std::int64_t groupsize_info;
+  };
+
+  Ts alpha_value = dpct::get_value(reinterpret_cast<const Ts *>(alpha), q);
+  Ts beta_value = dpct::get_value(reinterpret_cast<const Ts *>(beta), q);
+
+  matrix_info_t *matrix_info =
+      (matrix_info_t *)std::malloc(sizeof(matrix_info_t));
+  matrix_info->transpose_info[0] = a_trans;
+  matrix_info->transpose_info[1] = b_trans;
+  matrix_info->value_info[0] = alpha_value;
+  matrix_info->value_info[1] = beta_value;
+  matrix_info->size_info[0] = m;
+  matrix_info->size_info[1] = n;
+  matrix_info->size_info[2] = k;
+  matrix_info->ld_info[0] = lda;
+  matrix_info->ld_info[1] = ldb;
+  matrix_info->ld_info[2] = ldc;
+  matrix_info->groupsize_info = batch_size;
+
+  sycl::event e = oneapi::mkl::blas::column_major::gemm_batch(
+      q, matrix_info->transpose_info, matrix_info->transpose_info + 1,
+      matrix_info->size_info, matrix_info->size_info + 1,
+      matrix_info->size_info + 2, matrix_info->value_info,
+      reinterpret_cast<const Ta **>(a), matrix_info->ld_info,
+      reinterpret_cast<const Tb **>(b), matrix_info->ld_info + 1,
+      matrix_info->value_info + 1, reinterpret_cast<Tc **>(c),
+      matrix_info->ld_info + 2, 1, &(matrix_info->groupsize_info));
+
+  q.submit([&](sycl::handler &cgh) {
+    cgh.depends_on(e);
+    cgh.host_task([=] { std::free(matrix_info); });
+  });
+}
+
+template <class Ta, class Tb, class Tc, class Ts>
+inline void
+gemm_batch_impl(sycl::queue &q, oneapi::mkl::transpose a_trans,
+                    oneapi::mkl::transpose b_trans, int m, int n,
+                    int k, const void *alpha, const void *a, int lda,
+                    long long int stride_a, const void *b, int ldb,
+                    long long int stride_b, const void *beta, void *c,
+                    int ldc, long long int stride_c, int batch_size) {
+  Ts alpha_value = dpct::get_value(reinterpret_cast<const Ts *>(alpha), q);
+  Ts beta_value = dpct::get_value(reinterpret_cast<const Ts *>(beta), q);
+  auto data_a = get_memory<const Ta>(a);
+  auto data_b = get_memory<const Tb>(b);
+  auto data_c = get_memory<Tc>(c);
+  oneapi::mkl::blas::column_major::gemm_batch(
+      q, a_trans, b_trans, m, n, k, alpha_value, data_a, lda,
+      stride_a, data_b, ldb, stride_b, beta_value,
+      data_c, ldc, stride_c, batch_size);
+}
+
+template <bool is_hermitian, class T, class Tbeta>
+inline void rk_impl(sycl::queue &q, oneapi::mkl::uplo uplo,
+                          oneapi::mkl::transpose trans, int n, int k,
+                          const T *alpha, const T *a, int lda, const T *b,
+                          int ldb, const Tbeta *beta, T *c, int ldc) {
+  // For symmetric matrix, this function performs: C = alpha*OP(A)*(OP(B))^T + beta*C
+  // For Hermitian matrix, this function performs: C = alpha*OP(A)*(OP(B))^H + beta*C
+  // The gemmt() function performs: C = alpha*OPA(A)*OPB(B) + beta*C
+  // So the OPB need be updated before we call gemmt().
+  using Ty = typename dpct::DataType<T>::T2;
+  using Ts = typename dpct::DataType<Tbeta>::T2;
+  Ty alpha_value = dpct::get_value(reinterpret_cast<const Ty *>(alpha), q);
+  Ts beta_value = dpct::get_value(reinterpret_cast<const Ts *>(beta), q);
+  oneapi::mkl::transpose trans_A = trans, trans_B = trans;
+  int origin_b_rows = trans == oneapi::mkl::transpose::nontrans ? n : k;
+  int origin_b_cols = trans == oneapi::mkl::transpose::nontrans ? k : n;
+
+  if ((is_hermitian && trans == oneapi::mkl::transpose::trans) ||
+      (!is_hermitian && !std::is_floating_point_v<Ty> && trans == oneapi::mkl::transpose::conjtrans)) {
+    // In this case, OPB need be a conjugate operation,
+    // but only notrans, conjtrans and trans are available.
+    // So we need do a conjtrans operation first, then do a trans operation.
+    trans_B = oneapi::mkl::transpose::trans;
+    auto data_a = get_memory<const Ty>(a);
+    auto data_c = get_memory<Ty>(c);
+#ifdef DPCT_USM_LEVEL_NONE
+    auto new_B_buffer = sycl::buffer<Ty, 1>(sycl::range<1>(origin_b_rows * origin_b_cols));
+    auto from_buffer = dpct::get_buffer<Ty>(b);
+    oneapi::mkl::blas::column_major::omatcopy_batch(
+          q, oneapi::mkl::transpose::conjtrans, origin_b_rows, origin_b_cols,
+          Ts(1.0), from_buffer, ldb, origin_b_rows * ldb, new_B_buffer,
+          origin_b_cols, origin_b_rows * origin_b_cols, 1);
+    oneapi::mkl::blas::column_major::gemmt(
+        q, uplo, trans_A, trans_B, n, k, alpha_value,
+        data_a, lda, new_B_buffer, origin_b_cols, beta_value, data_c, ldc);
+#else
+    working_memory<T> new_B(origin_b_rows * origin_b_cols * sizeof(T), q);
+    oneapi::mkl::blas::column_major::omatcopy_batch(
+        q, oneapi::mkl::transpose::conjtrans, origin_b_rows, origin_b_cols,
+        Ts(1.0), reinterpret_cast<const Ty *>(b), ldb, origin_b_rows * ldb,
+        reinterpret_cast<Ty *>(new_B.get_ptr()), origin_b_cols,
+        origin_b_rows * origin_b_cols, 1);
+    sycl::event e = oneapi::mkl::blas::column_major::gemmt(
+        q, uplo, trans_A, trans_B, n, k, alpha_value,
+        data_a, lda, reinterpret_cast<Ty *>(new_B.get_ptr()), origin_b_cols,
+        beta_value, data_c, ldc);
+    new_B.set_event(e);
+#endif
+  } else {
+    if constexpr (is_hermitian) {
+      trans_B = trans == oneapi::mkl::transpose::nontrans
+                  ? oneapi::mkl::transpose::conjtrans
+                  : oneapi::mkl::transpose::nontrans;
+    } else {
+      trans_B = trans == oneapi::mkl::transpose::nontrans
+                  ? oneapi::mkl::transpose::trans
+                  : oneapi::mkl::transpose::nontrans;
+    }
+    auto data_a = get_memory<const Ty>(a);
+    auto data_b = get_memory<const Ty>(b);
+    auto data_c = get_memory<Ty>(c);
+    oneapi::mkl::blas::column_major::gemmt(
+        q, uplo, trans_A, trans_B, n, k, alpha_value,
+        data_a, lda, data_b, ldb, beta_value, data_c, ldc);
+  }
+}
+
+template <class Ta, class Tb, class Ts>
+inline void
+trsm_batch_impl(sycl::queue &q, oneapi::mkl::side left_right,
+                oneapi::mkl::uplo upper_lower, oneapi::mkl::transpose trans,
+                oneapi::mkl::diag unit_diag, int m, int n, const void *alpha,
+                const void **a, int lda, void **b, int ldb, int batch_size) {
+  struct matrix_info_t {
+    matrix_info_t(oneapi::mkl::side side_info, oneapi::mkl::uplo uplo_info,
+                  oneapi::mkl::transpose transpose_info,
+                  oneapi::mkl::diag diag_info, Ts value_info, std::int64_t m,
+                  std::int64_t n, std::int64_t lda, std::int64_t ldb,
+                  std::int64_t groupsize_info)
+        : side_info(side_info), uplo_info(uplo_info),
+          transpose_info(transpose_info), diag_info(diag_info),
+          value_info(value_info), groupsize_info(groupsize_info) {
+      size_info[0] = m;
+      size_info[1] = n;
+      ld_info[0] = lda;
+      ld_info[1] = ldb;
+    }
+    oneapi::mkl::side side_info;
+    oneapi::mkl::uplo uplo_info;
+    oneapi::mkl::transpose transpose_info;
+    oneapi::mkl::diag diag_info;
+    Ts value_info;
+    std::int64_t size_info[2];
+    std::int64_t ld_info[2];
+    std::int64_t groupsize_info;
+  };
+
+  Ts alpha_value = dpct::get_value(reinterpret_cast<const Ts *>(alpha), q);
+
+  matrix_info_t *matrix_info =
+      new matrix_info_t(left_right, upper_lower, trans, unit_diag, alpha_value,
+                        m, n, lda, ldb, batch_size);
+
+  sycl::event e = oneapi::mkl::blas::column_major::trsm_batch(
+      q, &(matrix_info->side_info), &(matrix_info->uplo_info),
+      &(matrix_info->transpose_info), &(matrix_info->diag_info),
+      matrix_info->size_info, matrix_info->size_info + 1,
+      &(matrix_info->value_info), reinterpret_cast<const Ta **>(a),
+      matrix_info->ld_info, reinterpret_cast<Tb **>(b),
+      matrix_info->ld_info + 1, 1, &(matrix_info->groupsize_info));
+
+  q.submit([&](sycl::handler &cgh) {
+    cgh.depends_on(e);
+    cgh.host_task([=] { delete matrix_info; });
+  });
+}
+
+template <typename T>
+inline void getrfnp_batch_wrapper(sycl::queue &exec_queue, int n, T *a[],
+                                  int lda, int *info, int batch_size) {
+#ifndef __INTEL_MKL__
+  throw std::runtime_error("The oneAPI Math Kernel Library (oneMKL) Interfaces "
+                           "Project does not support this API.");
+#else
+  using Ty = typename DataType<T>::T2;
+  // Set the info array value to 0
+  detail::dpct_memset<unsigned char>(exec_queue, info, 0, sizeof(int) * batch_size);
+  std::int64_t stride_a = n * lda;
+  std::int64_t scratchpad_size =
+      oneapi::mkl::lapack::getrfnp_batch_scratchpad_size<Ty>(
+          exec_queue, n, n, lda, stride_a, batch_size);
+
+  Ty *a_strided_mem =
+      (Ty *)dpct::dpct_malloc(stride_a * batch_size * sizeof(Ty), exec_queue);
+  T **host_a = (T **)std::malloc(batch_size * sizeof(T *));
+  dpct::dpct_memcpy(host_a, a, batch_size * sizeof(T *));
+  for (std::int64_t i = 0; i < batch_size; ++i)
+    dpct::dpct_memcpy(a_strided_mem + i * stride_a, host_a[i],
+                      n * lda * sizeof(T));
+
+#ifdef DPCT_USM_LEVEL_NONE
+  {
+    sycl::buffer<Ty, 1> scratchpad{sycl::range<1>(scratchpad_size)};
+    auto a_buffer = get_buffer<Ty>(a_strided_mem);
+    oneapi::mkl::lapack::getrfnp_batch(exec_queue, n, n, a_buffer, lda,
+                                       stride_a, batch_size, scratchpad,
+                                       scratchpad_size);
+  }
+  std::vector<sycl::event> events;
+  for (std::int64_t i = 0; i < batch_size; ++i)
+    events.push_back(detail::dpct_memcpy(exec_queue, host_a[i],
+                                         a_strided_mem + i * stride_a,
+                                         n * lda * sizeof(T), automatic));
+#else
+  Ty *scratchpad = sycl::malloc_device<Ty>(scratchpad_size, exec_queue);
+  sycl::event e = oneapi::mkl::lapack::getrfnp_batch(
+      exec_queue, n, n, a_strided_mem, lda, stride_a, batch_size, scratchpad,
+      scratchpad_size);
+  std::vector<sycl::event> events;
+  for (std::int64_t i = 0; i < batch_size; ++i)
+    events.push_back(detail::dpct_memcpy(exec_queue, host_a[i],
+                                         a_strided_mem + i * stride_a,
+                                         n * lda * sizeof(T), automatic, {e}));
+
+  std::vector<void *> ptrs{scratchpad, a_strided_mem};
+  dpct::async_dpct_free(ptrs, events, exec_queue);
+#endif
+
+  exec_queue.submit([&](sycl::handler &cgh) {
+    cgh.depends_on(events);
+    cgh.host_task([=] { std::free(host_a); });
+  });
+#endif
+}
+
+} // namespace detail
+
+inline oneapi::mkl::transpose get_transpose(int t) {
+  if (t == 0) {
+    return oneapi::mkl::transpose::nontrans;
+  } else if (t == 1) {
+    return oneapi::mkl::transpose::trans;
+  } else {
+    return oneapi::mkl::transpose::conjtrans;
+  }
+}
+
+/// Computes the LU factorizations of a batch of general matrices.
+/// \param [in] exec_queue The queue where the routine should be executed.
+/// \param [in] n The order of the matrices.
+/// \param [in, out] a Array of pointers to matrices. These matrices will be
+/// overwritten by lower triangulars with unit diagonal elements and upper
+/// triangulars.
+/// \param [in] lda The leading dimension of the matrices.
+/// \param [out] ipiv An array stores the pivot indices. If \p ipiv is nullptr,
+/// non-pivoting LU factorization is computed.
+/// \param [out] info An array stores the error information.
+/// \param [in] batch_size The size of the batch.
+template <typename T>
+inline void getrf_batch_wrapper(sycl::queue &exec_queue, int n, T *a[],
+                                int lda, int *ipiv, int *info, int batch_size) {
+  if (ipiv == nullptr) {
+    detail::getrfnp_batch_wrapper(exec_queue, n, a, lda, info, batch_size);
+    return;
+  }
+  using Ty = typename DataType<T>::T2;
+  // Set the info array value to 0
+  detail::dpct_memset<unsigned char>(exec_queue, info, 0, sizeof(int) * batch_size);
+#ifdef DPCT_USM_LEVEL_NONE
+  std::int64_t stride_a = n * lda;
+  std::int64_t stride_ipiv = n;
+  std::int64_t scratchpad_size = oneapi::mkl::lapack::getrf_batch_scratchpad_size<Ty>(
+      exec_queue, n, n, lda, stride_a, stride_ipiv, batch_size);
+
+  T *a_buffer_ptr;
+  a_buffer_ptr = (T *)dpct_malloc(stride_a * batch_size * sizeof(T));
+
+  T **host_a = (T **)std::malloc(batch_size * sizeof(T *));
+  dpct_memcpy(host_a, a, batch_size * sizeof(T *));
+  for (std::int64_t i = 0; i < batch_size; ++i)
+    dpct_memcpy(a_buffer_ptr + i * stride_a, host_a[i], n * lda * sizeof(T));
+
+  {
+    sycl::buffer<std::int64_t, 1> ipiv_buf(
+        sycl::range<1>(batch_size * stride_ipiv));
+    sycl::buffer<Ty, 1> scratchpad{sycl::range<1>(scratchpad_size)};
+    auto a_buffer = get_buffer<Ty>(a_buffer_ptr);
+    oneapi::mkl::lapack::getrf_batch(exec_queue, n, n, a_buffer, lda, stride_a,
+                             ipiv_buf, stride_ipiv, batch_size, scratchpad,
+                             scratchpad_size);
+
+    auto to_buffer = get_buffer<int>(ipiv);
+    exec_queue.submit([&](sycl::handler &cgh) {
+      auto from_acc = ipiv_buf.get_access<sycl::access_mode::read>(cgh);
+      auto to_acc = to_buffer.get_access<sycl::access_mode::write>(cgh);
+      cgh.parallel_for<dpct_kernel_name<class getrf_device_int64_to_int, T>>(
+          sycl::range<2>(batch_size, n), [=](sycl::id<2> id) {
+            to_acc[id.get(0) * n + id.get(1)] =
+                static_cast<int>(from_acc[id.get(0) * stride_ipiv + id.get(1)]);
+          });
+    });
+  }
+
+  // Copy back to the original buffers
+  std::vector<sycl::event> events;
+  for (std::int64_t i = 0; i < batch_size; ++i)
+    events.push_back(detail::dpct_memcpy(exec_queue, host_a[i],
+                                         a_buffer_ptr + i * stride_a,
+                                         n * lda * sizeof(T), automatic));
+
+  std::vector<void *> ptrs{host_a};
+  std::thread mem_free_thread(
+      [=](std::vector<void *> pointers_array,
+          std::vector<sycl::event> events_array) {
+        sycl::event::wait(events_array);
+        for (auto p : pointers_array)
+          std::free(p);
+      },
+      ptrs, events);
+  mem_free_thread.detach();
+#else
+  std::int64_t m_int64 = n;
+  std::int64_t n_int64 = n;
+  std::int64_t lda_int64 = lda;
+  std::int64_t group_sizes = batch_size;
+  std::int64_t scratchpad_size = oneapi::mkl::lapack::getrf_batch_scratchpad_size<Ty>(
+      exec_queue, &m_int64, &n_int64, &lda_int64, 1, &group_sizes);
+
+  Ty *scratchpad = sycl::malloc_device<Ty>(scratchpad_size, exec_queue);
+  std::int64_t *ipiv_int64 =
+      sycl::malloc_device<std::int64_t>(batch_size * n, exec_queue);
+  std::int64_t **ipiv_int64_ptr =
+      sycl::malloc_shared<std::int64_t *>(batch_size, exec_queue);
+  T **a_shared = sycl::malloc_shared<T *>(batch_size, exec_queue);
+  exec_queue.memcpy(a_shared, a, batch_size * sizeof(T *)).wait();
+  for (std::int64_t i = 0; i < batch_size; ++i)
+    ipiv_int64_ptr[i] = ipiv_int64 + n * i;
+
+  oneapi::mkl::lapack::getrf_batch(exec_queue, &m_int64, &n_int64, (Ty **)a_shared, &lda_int64,
+                           ipiv_int64_ptr, 1, &group_sizes, scratchpad,
+                           scratchpad_size);
+
+  sycl::event e = exec_queue.submit([&](sycl::handler &cgh) {
+    cgh.parallel_for<dpct_kernel_name<class getrf_device_int64_to_int, T>>(
+        sycl::range<1>(batch_size * n), [=](sycl::id<1> idx) {
+          ipiv[idx] = static_cast<int>(ipiv_int64[idx]);
+        });
+  });
+
+  std::vector<void *> ptrs{scratchpad, ipiv_int64, ipiv_int64_ptr, a_shared};
+  async_dpct_free(ptrs, {e}, exec_queue);
+#endif
+}
+
+/// Solves a system of linear equations with a batch of LU-factored square
+/// coefficient matrices, with multiple right-hand sides.
+/// \param [in] exec_queue The queue where the routine should be executed.
+/// \param [in] trans Indicates the form of the linear equations.
+/// \param [in] n The order of the matrices.
+/// \param [in] nrhs The number of right hand sides.
+/// \param [in] a Array of pointers to matrices.
+/// \param [in] lda The leading dimension of the matrices in \p a.
+/// \param [in] ipiv An array stores the pivots.
+/// \param [in, out] b Array of pointers to matrices, whose columns are
+/// the right-hand sides for the systems of equations.
+/// \param [in] ldb The leading dimension of the matrices in \p b.
+/// \param [out] info A value stores the error information.
+/// \param [in] batch_size The size of the batch.
+template <typename T>
+inline void getrs_batch_wrapper(sycl::queue &exec_queue,
+                                oneapi::mkl::transpose trans, int n, int nrhs,
+                                const T *a[], int lda, const int *ipiv, T *b[],
+                                int ldb, int *info, int batch_size) {
+  using Ty = typename DataType<T>::T2;
+  // Set the info value to 0
+  *info = 0;
+#ifdef DPCT_USM_LEVEL_NONE
+  std::int64_t stride_a = n * lda;
+  std::int64_t stride_b = nrhs * ldb;
+  std::int64_t stride_ipiv = n;
+  std::int64_t scratchpad_size = oneapi::mkl::lapack::getrs_batch_scratchpad_size<Ty>(
+      exec_queue, trans, n, nrhs, lda, stride_a, stride_ipiv, ldb, stride_b,
+      batch_size);
+
+  T *a_buffer_ptr, *b_buffer_ptr;
+  a_buffer_ptr = (T *)dpct_malloc(stride_a * batch_size * sizeof(T));
+  b_buffer_ptr = (T *)dpct_malloc(stride_b * batch_size * sizeof(T));
+
+  T **host_a = (T **)std::malloc(batch_size * sizeof(T *));
+  T **host_b = (T **)std::malloc(batch_size * sizeof(T *));
+  dpct_memcpy(host_a, a, batch_size * sizeof(T *));
+  dpct_memcpy(host_b, b, batch_size * sizeof(T *));
+  for (std::int64_t i = 0; i < batch_size; ++i) {
+    dpct_memcpy(a_buffer_ptr + i * stride_a, host_a[i], n * lda * sizeof(T));
+    dpct_memcpy(b_buffer_ptr + i * stride_b, host_b[i], nrhs * ldb * sizeof(T));
+  }
+
+  {
+    auto a_buffer = get_buffer<Ty>(a_buffer_ptr);
+    auto b_buffer = get_buffer<Ty>(b_buffer_ptr);
+    sycl::buffer<Ty, 1> scratchpad{sycl::range<1>(scratchpad_size)};
+    sycl::buffer<std::int64_t, 1> ipiv_buf(
+        sycl::range<1>(batch_size * stride_ipiv));
+    auto from_buf = get_buffer<int>(ipiv);
+    exec_queue.submit([&](sycl::handler &cgh) {
+      auto from_acc = from_buf.get_access<sycl::access_mode::read>(cgh);
+      auto to_acc = ipiv_buf.get_access<sycl::access_mode::write>(cgh);
+      cgh.parallel_for<dpct_kernel_name<class getrs_device_int64_to_int, T>>(
+          sycl::range<2>(batch_size, n), [=](sycl::id<2> id) {
+            to_acc[id.get(0) * stride_ipiv + id.get(1)] =
+                static_cast<std::int64_t>(from_acc[id.get(0) * n + id.get(1)]);
+          });
+    });
+
+    oneapi::mkl::lapack::getrs_batch(exec_queue, trans, n, nrhs, a_buffer, lda,
+                             stride_a, ipiv_buf, stride_ipiv, b_buffer, ldb,
+                             stride_b, batch_size, scratchpad, scratchpad_size);
+  }
+
+  // Copy back to the original buffers
+  std::vector<sycl::event> events;
+  for (std::int64_t i = 0; i < batch_size; ++i)
+    events.push_back(detail::dpct_memcpy(exec_queue, host_b[i],
+                                         b_buffer_ptr + i * stride_b,
+                                         nrhs * ldb * sizeof(T), automatic));
+  std::vector<void *> ptrs{host_a, host_b};
+  std::thread mem_free_thread(
+      [=](std::vector<void *> pointers_array,
+          std::vector<sycl::event> events_array) {
+        sycl::event::wait(events_array);
+        for (auto p : pointers_array)
+          std::free(p);
+      },
+      ptrs, events);
+  mem_free_thread.detach();
+#else
+  std::int64_t n_int64 = n;
+  std::int64_t nrhs_int64 = nrhs;
+  std::int64_t lda_int64 = lda;
+  std::int64_t ldb_int64 = ldb;
+  std::int64_t group_sizes = batch_size;
+  std::int64_t scratchpad_size = oneapi::mkl::lapack::getrs_batch_scratchpad_size<Ty>(
+      exec_queue, &trans, &n_int64, &nrhs_int64, &lda_int64, &ldb_int64, 1,
+      &group_sizes);
+
+  Ty *scratchpad = sycl::malloc_device<Ty>(scratchpad_size, exec_queue);
+  std::int64_t *ipiv_int64 =
+      sycl::malloc_device<std::int64_t>(batch_size * n, exec_queue);
+  std::int64_t **ipiv_int64_ptr =
+      sycl::malloc_shared<std::int64_t *>(batch_size, exec_queue);
+  T **a_shared = sycl::malloc_shared<T *>(batch_size, exec_queue);
+  T **b_shared = sycl::malloc_shared<T *>(batch_size, exec_queue);
+  exec_queue.memcpy(a_shared, a, batch_size * sizeof(T *));
+  exec_queue.memcpy(b_shared, b, batch_size * sizeof(T *));
+
+  exec_queue.submit([&](sycl::handler &cgh) {
+    cgh.parallel_for<dpct_kernel_name<class getrs_device_int64_to_int, T>>(
+        sycl::range<1>(batch_size * n), [=](sycl::id<1> idx) {
+          ipiv_int64[idx] = static_cast<std::int64_t>(ipiv[idx]);
+        });
+  }).wait();
+
+  for (std::int64_t i = 0; i < batch_size; ++i)
+    ipiv_int64_ptr[i] = ipiv_int64 + n * i;
+
+  sycl::event e = oneapi::mkl::lapack::getrs_batch(
+      exec_queue, &trans, &n_int64, &nrhs_int64, (Ty **)a_shared, &lda_int64,
+      ipiv_int64_ptr, (Ty **)b_shared, &ldb_int64, 1, &group_sizes, scratchpad,
+      scratchpad_size);
+
+  std::vector<void *> ptrs{scratchpad, ipiv_int64_ptr, ipiv_int64, a_shared, b_shared};
+  async_dpct_free(ptrs, {e}, exec_queue);
+#endif
+}
+
+/// Computes the inverses of a batch of LU-factored matrices.
+/// \param [in] exec_queue The queue where the routine should be executed.
+/// \param [in] n The order of the matrices.
+/// \param [in] a Array of pointers to matrices.
+/// \param [in] lda The leading dimension of the matrices in \p a.
+/// \param [in] ipiv An array stores the pivots.
+/// \param [out] b Array of pointers to inverse matrices.
+/// \param [in] ldb The leading dimension of the matrices in \p b.
+/// \param [out] info An array stores the error information.
+/// \param [in] batch_size The size of the batch.
+template <typename T>
+inline void getri_batch_wrapper(sycl::queue &exec_queue, int n,
+                                const T *a[], int lda, int *ipiv, T *b[],
+                                int ldb, int *info, int batch_size) {
+  using Ty = typename DataType<T>::T2;
+  // Set the info array value to 0
+  detail::dpct_memset<unsigned char>(exec_queue, info, 0, sizeof(int) * batch_size);
+#ifdef DPCT_USM_LEVEL_NONE
+  std::int64_t stride_b = n * ldb;
+  std::int64_t stride_ipiv = n;
+  std::int64_t scratchpad_size = oneapi::mkl::lapack::getri_batch_scratchpad_size<Ty>(
+      exec_queue, n, ldb, stride_b, stride_ipiv, batch_size);
+
+  T *b_buffer_ptr;
+  b_buffer_ptr = (T *)dpct_malloc(stride_b * batch_size * sizeof(T));
+
+  T **host_a = (T **)std::malloc(batch_size * sizeof(T *));
+  T **host_b = (T **)std::malloc(batch_size * sizeof(T *));
+  dpct_memcpy(host_a, a, batch_size * sizeof(T *));
+  dpct_memcpy(host_b, b, batch_size * sizeof(T *));
+
+  for (std::int64_t i = 0; i < batch_size; ++i) {
+    // Need to create a copy of input matrices "a" to keep them unchanged.
+    // Matrices "b" (copy of matrices "a") will be used as input and output
+    // parameter in oneapi::mkl::lapack::getri_batch call.
+    matrix_mem_copy(b_buffer_ptr + i * stride_b, host_a[i], ldb, lda, n, n,
+                    dpct::device_to_device, exec_queue);
+  }
+
+  {
+    auto b_buffer = get_buffer<Ty>(b_buffer_ptr);
+    sycl::buffer<Ty, 1> scratchpad{sycl::range<1>(scratchpad_size)};
+    sycl::buffer<std::int64_t, 1> ipiv_buf(
+        sycl::range<1>(batch_size * stride_ipiv));
+    auto from_buf = get_buffer<int>(ipiv);
+    exec_queue.submit([&](sycl::handler &cgh) {
+      auto from_acc = from_buf.get_access<sycl::access_mode::read>(cgh);
+      auto to_acc = ipiv_buf.get_access<sycl::access_mode::write>(cgh);
+      cgh.parallel_for<dpct_kernel_name<class getri_device_int64_to_int, T>>(
+          sycl::range<2>(batch_size, n), [=](sycl::id<2> id) {
+            to_acc[id.get(0) * stride_ipiv + id.get(1)] =
+                static_cast<std::int64_t>(from_acc[id.get(0) * n + id.get(1)]);
+          });
+    });
+
+    oneapi::mkl::lapack::getri_batch(exec_queue, n, b_buffer, ldb, stride_b, ipiv_buf,
+                             stride_ipiv, batch_size, scratchpad,
+                             scratchpad_size);
+  }
+
+  // Copy back to the original buffers
+  std::vector<sycl::event> events;
+  for (std::int64_t i = 0; i < batch_size; ++i)
+    events.push_back(detail::dpct_memcpy(exec_queue, host_b[i],
+                                         b_buffer_ptr + i * stride_b,
+                                         n * ldb * sizeof(T), automatic));
+  std::vector<void *> ptrs{host_a, host_b};
+  std::thread mem_free_thread(
+      [=](std::vector<void *> pointers_array,
+          std::vector<sycl::event> events_array) {
+        sycl::event::wait(events_array);
+        for (auto p : pointers_array)
+          std::free(p);
+      },
+      ptrs, events);
+  mem_free_thread.detach();
+#else
+  std::int64_t n_int64 = n;
+  std::int64_t ldb_int64 = ldb;
+  std::int64_t group_sizes = batch_size;
+  std::int64_t scratchpad_size = oneapi::mkl::lapack::getri_batch_scratchpad_size<Ty>(
+      exec_queue, &n_int64, &ldb_int64, 1, &group_sizes);
+
+  Ty *scratchpad = sycl::malloc_device<Ty>(scratchpad_size, exec_queue);
+  std::int64_t *ipiv_int64 =
+      sycl::malloc_device<std::int64_t>(batch_size * n, exec_queue);
+  std::int64_t **ipiv_int64_ptr =
+      sycl::malloc_shared<std::int64_t *>(batch_size, exec_queue);
+
+  exec_queue.submit([&](sycl::handler &cgh) {
+    cgh.parallel_for<dpct_kernel_name<class getri_device_int64_to_int, T>>(
+        sycl::range<1>(batch_size * n), [=](sycl::id<1> idx) {
+          ipiv_int64[idx] = static_cast<std::int64_t>(ipiv[idx]);
+        });
+  });
+
+  T **a_shared = sycl::malloc_shared<T *>(batch_size, exec_queue);
+  T **b_shared = sycl::malloc_shared<T *>(batch_size, exec_queue);
+  exec_queue.memcpy(a_shared, a, batch_size * sizeof(T *));
+  exec_queue.memcpy(b_shared, b, batch_size * sizeof(T *)).wait();
+  for (std::int64_t i = 0; i < batch_size; ++i) {
+    ipiv_int64_ptr[i] = ipiv_int64 + n * i;
+    // Need to create a copy of input matrices "a" to keep them unchanged.
+    // Matrices "b" (copy of matrices "a") will be used as input and output
+    // parameter in oneapi::mkl::lapack::getri_batch call.
+    matrix_mem_copy(b_shared[i], a_shared[i], ldb, lda, n, n, dpct::device_to_device,
+                    exec_queue);
+  }
+
+  sycl::event e = oneapi::mkl::lapack::getri_batch(
+      exec_queue, &n_int64, (Ty **)b_shared, &ldb_int64, ipiv_int64_ptr, 1,
+      &group_sizes, scratchpad, scratchpad_size);
+
+  std::vector<void *> ptrs{scratchpad, ipiv_int64_ptr, ipiv_int64, a_shared, b_shared};
+  async_dpct_free(ptrs, {e}, exec_queue);
+#endif
+}
+
+/// Computes the QR factorizations of a batch of general matrices.
+/// \param [in] exec_queue The queue where the routine should be executed.
+/// \param [in] m The number of rows in the matrices.
+/// \param [in] n The number of columns in the matrices.
+/// \param [in, out] a Array of pointers to matrices. These
+/// matrices will be overwritten by the factorization data.
+/// \param [in] lda The leading dimension of the matrices in \p a.
+/// \param [out] tau An array stores the scalars.
+/// \param [out] info A value stores the error information.
+/// \param [in] batch_size The size of the batch.
+template <typename T>
+inline void geqrf_batch_wrapper(sycl::queue exec_queue, int m, int n,
+                                T *a[], int lda, T *tau[], int *info,
+                                int batch_size) {
+  using Ty = typename DataType<T>::T2;
+  // Set the info value to 0
+  *info = 0;
+#ifdef DPCT_USM_LEVEL_NONE
+  std::int64_t stride_a = n * lda;
+  std::int64_t stride_tau = std::max(1, std::min(m, n));
+  std::int64_t scratchpad_size = oneapi::mkl::lapack::geqrf_batch_scratchpad_size<Ty>(
+      exec_queue, m, n, lda, stride_a, stride_tau, batch_size);
+
+  T *a_buffer_ptr, *tau_buffer_ptr;
+  a_buffer_ptr = (T *)dpct_malloc(stride_a * batch_size * sizeof(T));
+  tau_buffer_ptr = (T *)dpct_malloc(stride_tau * batch_size * sizeof(T));
+
+  T **host_a = (T **)std::malloc(batch_size * sizeof(T *));
+  T **host_tau = (T **)std::malloc(batch_size * sizeof(T *));
+  dpct_memcpy(host_a, a, batch_size * sizeof(T *));
+  dpct_memcpy(host_tau, tau, batch_size * sizeof(T *));
+
+  for (std::int64_t i = 0; i < batch_size; ++i)
+    dpct_memcpy(a_buffer_ptr + i * stride_a, host_a[i], n * lda * sizeof(T));
+  {
+    auto a_buffer = get_buffer<Ty>(a_buffer_ptr);
+    auto tau_buffer = get_buffer<Ty>(tau_buffer_ptr);
+    sycl::buffer<Ty, 1> scratchpad{sycl::range<1>(scratchpad_size)};
+    oneapi::mkl::lapack::geqrf_batch(exec_queue, m, n, a_buffer, lda, stride_a,
+                             tau_buffer, stride_tau, batch_size, scratchpad,
+                             scratchpad_size);
+  }
+
+  // Copy back to the original buffers
+  std::vector<sycl::event> events_a;
+  std::vector<sycl::event> events_tau;
+  for (std::int64_t i = 0; i < batch_size; ++i) {
+    events_a.push_back(detail::dpct_memcpy(exec_queue, host_a[i],
+                                           a_buffer_ptr + i * stride_a,
+                                           n * lda * sizeof(T), automatic));
+    events_tau.push_back(detail::dpct_memcpy(
+        exec_queue, host_tau[i], tau_buffer_ptr + i * stride_tau,
+        std::max(1, std::min(m, n)) * sizeof(T), automatic));
+  }
+  std::vector<void *> ptr_a{host_a};
+  std::vector<void *> ptr_tau{host_tau};
+  std::thread mem_free_thread_a(
+      [=](std::vector<void *> pointers_array,
+          std::vector<sycl::event> events_array) {
+        sycl::event::wait(events_array);
+        for (auto p : pointers_array)
+          std::free(p);
+      },
+      ptr_a, events_a);
+  std::thread mem_free_thread_tau(
+      [=](std::vector<void *> pointers_array,
+          std::vector<sycl::event> events_array) {
+        sycl::event::wait(events_array);
+        for (auto p : pointers_array)
+          std::free(p);
+      },
+      ptr_tau, events_tau);
+  mem_free_thread_a.detach();
+  mem_free_thread_tau.detach();
+#else
+  std::int64_t m_int64 = n;
+  std::int64_t n_int64 = n;
+  std::int64_t lda_int64 = lda;
+  std::int64_t group_sizes = batch_size;
+  std::int64_t scratchpad_size = oneapi::mkl::lapack::geqrf_batch_scratchpad_size<Ty>(
+      exec_queue, &m_int64, &n_int64, &lda_int64, 1, &group_sizes);
+
+  Ty *scratchpad = sycl::malloc_device<Ty>(scratchpad_size, exec_queue);
+  T **a_shared = sycl::malloc_shared<T *>(batch_size, exec_queue);
+  T **tau_shared = sycl::malloc_shared<T *>(batch_size, exec_queue);
+  exec_queue.memcpy(a_shared, a, batch_size * sizeof(T *));
+  exec_queue.memcpy(tau_shared, tau, batch_size * sizeof(T *)).wait();
+
+  sycl::event e = oneapi::mkl::lapack::geqrf_batch(
+      exec_queue, &m_int64, &n_int64, (Ty **)a_shared, &lda_int64, (Ty **)tau_shared, 1,
+      &group_sizes, scratchpad, scratchpad_size);
+
+  std::vector<void *> ptrs{scratchpad, a_shared, tau_shared};
+  async_dpct_free(ptrs, {e}, exec_queue);
+#endif
+}
+
+/// Computes the Euclidean norm of a vector.
+/// \param [in] q The queue where the routine should be executed.
+/// \param [in] n Number of elements in vector x.
+/// \param [in] x Input vector x.
+/// \param [in] x_type Data type of the vector x.
+/// \param [in] incx Stride of vector x.
+/// \param [out] result The result scalar.
+/// \param [in] result_type Data type of the result.
+inline void nrm2(sycl::queue &q, int n, const void *x, library_data_t x_type,
+                    int incx, void *result, library_data_t result_type) {
+  std::uint64_t key = detail::get_type_combination_id(x_type, result_type);
+  switch (key) {
+  case detail::get_type_combination_id(library_data_t::real_float,
+                       library_data_t::real_float): {
+    detail::nrm2_impl<float, float>(q, n, x, incx, result);
+    break;
+  }
+  case detail::get_type_combination_id(library_data_t::real_double,
+                       library_data_t::real_double): {
+    detail::nrm2_impl<double, double>(q, n, x, incx, result);
+    break;
+  }
+  case detail::get_type_combination_id(library_data_t::complex_float,
+                       library_data_t::real_float): {
+    detail::nrm2_impl<std::complex<float>, float>(
+        q, n, x, incx, result);
+    break;
+  }
+  case detail::get_type_combination_id(library_data_t::complex_double,
+                       library_data_t::real_double): {
+    detail::nrm2_impl<std::complex<double>, double>(
+        q, n, x, incx, result);
+    break;
+  }
+  case detail::get_type_combination_id(library_data_t::real_half,
+                       library_data_t::real_half): {
+    detail::nrm2_impl<sycl::half, sycl::half>(
+        q, n, x, incx, result);
+    break;
+  }
+  default:
+    throw std::runtime_error("the combination of data type is unsupported");
+  }
+}
+
+/// Computes the dot product of two vectors.
+/// \param [in] q The queue where the routine should be executed.
+/// \param [in] n Number of elements in vector x.
+/// \param [in] x Input vector x.
+/// \param [in] x_type Data type of the vector x.
+/// \param [in] incx Stride of vector x.
+/// \param [in] y Input vector y.
+/// \param [in] y_type Data type of the vector y.
+/// \param [in] incy Stride of vector y.
+/// \param [out] result The result scalar.
+/// \param [in] result_type Data type of the result.
+inline void dot(sycl::queue &q, int n, const void *x, library_data_t x_type,
+                   int incx, const void *y, library_data_t y_type, int incy,
+                   void *result, library_data_t result_type) {
+  detail::dotuc<false>(q, n, x, x_type, incx, y, y_type, incy, result,
+                          result_type);
+}
+
+/// Computes the dot product of two vectors, conjugating the first vector.
+/// \param [in] q The queue where the routine should be executed.
+/// \param [in] n Number of elements in vector x.
+/// \param [in] x Input vector x.
+/// \param [in] x_type Data type of the vector x.
+/// \param [in] incx Stride of vector x.
+/// \param [in] y Input vector y.
+/// \param [in] y_type Data type of the vector y.
+/// \param [in] incy Stride of vector y.
+/// \param [out] result The result scalar.
+/// \param [in] result_type Data type of the result.
+inline void dotc(sycl::queue &q, int n, const void *x, library_data_t x_type,
+                    int incx, const void *y, library_data_t y_type, int incy,
+                    void *result, library_data_t result_type) {
+  detail::dotuc<true>(q, n, x, x_type, incx, y, y_type, incy, result,
+                         result_type);
+}
+
+/// Computes the product of a vector by a scalar.
+/// \param [in] q The queue where the routine should be executed.
+/// \param [in] n Number of elements in vector x.
+/// \param [in] alpha The scale factor alpha.
+/// \param [in] alpha_type The data type of alpha.
+/// \param [in, out] x Input/Output vector x.
+/// \param [in] x_type Data type of the vector x.
+/// \param [in] incx Stride of vector x.
+inline void scal(sycl::queue &q, int n, const void *alpha,
+                    library_data_t alpha_type, void *x, library_data_t x_type,
+                    int incx) {
+  std::uint64_t key = detail::get_type_combination_id(x_type);
+  switch (key) {
+  case detail::get_type_combination_id(library_data_t::real_float): {
+    detail::scal_impl<float, float>(q, n, alpha, x, incx);
+    break;
+  }
+  case detail::get_type_combination_id(library_data_t::real_double): {
+    detail::scal_impl<double, double>(q, n, alpha, x, incx);
+    break;
+  }
+  case detail::get_type_combination_id(library_data_t::complex_float): {
+    detail::scal_impl<std::complex<float>, std::complex<float>>(q, n, alpha,
+                                                                   x, incx);
+    break;
+  }
+  case detail::get_type_combination_id(library_data_t::complex_double): {
+    detail::scal_impl<std::complex<double>, std::complex<double>>(
+        q, n, alpha, x, incx);
+    break;
+  }
+  case detail::get_type_combination_id(library_data_t::real_half): {
+    float alpha_value =
+        dpct::get_value(reinterpret_cast<const float *>(alpha), q);
+    sycl::half alaph_half(alpha_value);
+    detail::scal_impl<sycl::half, sycl::half>(q, n, &alaph_half, x, incx);
+    break;
+  }
+  default:
+    throw std::runtime_error("the combination of data type is unsupported");
+  }
+}
+
+/// Computes a vector-scalar product and adds the result to a vector.
+/// \param [in] q The queue where the routine should be executed.
+/// \param [in] n Number of elements in vector x.
+/// \param [in] alpha The scale factor alpha.
+/// \param [in] alpha_type The data type of alpha.
+/// \param [in] x Input vector x.
+/// \param [in] x_type Data type of the vector x.
+/// \param [in] incx Stride of vector x.
+/// \param [in, out] y Input/Output vector y.
+/// \param [in] y_type Data type of the vector y.
+/// \param [in] incy Stride of vector y.
+inline void axpy(sycl::queue &q, int n, const void *alpha,
+                    library_data_t alpha_type, const void *x, library_data_t x_type,
+                    int incx, void *y, library_data_t y_type, int incy) {
+  std::uint64_t key = detail::get_type_combination_id(x_type, alpha_type);
+  switch (key) {
+  case detail::get_type_combination_id(library_data_t::real_float,
+                       library_data_t::real_float): {
+    detail::axpy_impl<float, float>(q, n, alpha, x, incx, y, incy);
+    break;
+  }
+  case detail::get_type_combination_id(library_data_t::real_double,
+                       library_data_t::real_double): {
+    detail::axpy_impl<double, double>(q, n, alpha, x, incx, y, incy);
+    break;
+  }
+  case detail::get_type_combination_id(library_data_t::complex_float,
+                       library_data_t::complex_float): {
+    detail::axpy_impl<std::complex<float>, std::complex<float>>(
+        q, n, alpha, x, incx, y, incy);
+    break;
+  }
+  case detail::get_type_combination_id(library_data_t::complex_double,
+                       library_data_t::complex_double): {
+    detail::axpy_impl<std::complex<double>, std::complex<double>>(
+        q, n, alpha, x, incx, y, incy);
+    break;
+  }
+  case detail::get_type_combination_id(library_data_t::real_half,
+                       library_data_t::real_float): {
+    float alpha_value =
+        dpct::get_value(reinterpret_cast<const float *>(alpha), q);
+    sycl::half alaph_half(alpha_value);
+    detail::axpy_impl<sycl::half, sycl::half>(q, n, &alaph_half, x, incx, y, incy);
+    break;
+  }
+  default:
+    throw std::runtime_error("the combination of data type is unsupported");
+  }
+}
+
+/// Performs rotation of points in the plane.
+/// \param [in] q The queue where the routine should be executed.
+/// \param [in] n Number of elements in vector x.
+/// \param [in, out] x Input/Output vector x.
+/// \param [in] x_type Data type of the vector x.
+/// \param [in] incx Stride of vector x.
+/// \param [in, out] y Input/Output vector y.
+/// \param [in] y_type Data type of the vector y.
+/// \param [in] incy Stride of vector y.
+/// \param [in] c Scaling factor.
+/// \param [in] s Scaling factor.
+/// \param [in] cs_type Data type of the scaling factors.
+inline void rot(sycl::queue &q, int n, void *x, library_data_t x_type,
+                   int incx, void *y, library_data_t y_type, int incy,
+                   const void *c, const void *s, library_data_t cs_type) {
+  std::uint64_t key = detail::get_type_combination_id(x_type, cs_type);
+  switch (key) {
+  case detail::get_type_combination_id(library_data_t::real_float,
+                       library_data_t::real_float): {
+    detail::rot_impl<float, float, float>(q, n, x, incx, y, incy, c, s);
+    break;
+  }
+  case detail::get_type_combination_id(library_data_t::real_double,
+                       library_data_t::real_double): {
+    detail::rot_impl<double, double, double>(q, n, x, incx, y, incy, c, s);
+    break;
+  }
+  case detail::get_type_combination_id(library_data_t::complex_float,
+                       library_data_t::real_float): {
+    detail::rot_impl<std::complex<float>, float, float>(q, n, x, incx, y, incy, c,
+                                                    s);
+    break;
+  }
+  case detail::get_type_combination_id(library_data_t::complex_double,
+                       library_data_t::real_double): {
+    detail::rot_impl<std::complex<double>, double, double>(q, n, x, incx, y, incy, c,
+                                                      s);
+    break;
+  }
+  case detail::get_type_combination_id(library_data_t::complex_float,
+                       library_data_t::complex_float): {
+    detail::rot_impl<std::complex<float>, float, std::complex<float>>(q, n, x, incx, y, incy, c, s);
+    break;
+  }
+  case detail::get_type_combination_id(library_data_t::complex_double,
+                       library_data_t::complex_double): {
+    detail::rot_impl<std::complex<double>, double, std::complex<double>>(q, n, x, incx, y, incy, c, s);
+    break;
+  }
+  case detail::get_type_combination_id(library_data_t::real_half,
+                       library_data_t::real_half): {
+    detail::rot_impl<sycl::half, sycl::half, sycl::half>(q, n, x, incx, y, incy, c, s);
+    break;
+  }
+  case detail::get_type_combination_id(library_data_t::real_bfloat16,
+                       library_data_t::real_bfloat16): {
+    detail::rot_impl<oneapi::mkl::bfloat16, oneapi::mkl::bfloat16, oneapi::mkl::bfloat16>(q, n, x, incx, y, incy, c, s);
+    break;
+  }
+  default:
+    throw std::runtime_error("the combination of data type is unsupported");
+  }
+}
+
+/// Computes matrix-matrix product with general matrices.
+/// \param [in] q The queue where the routine should be executed.
+/// \param [in] a_trans Specifies the operation applied to A.
+/// \param [in] b_trans Specifies the operation applied to B.
+/// \param [in] m Specifies the number of rows of the matrix op(A) and of the matrix C.
+/// \param [in] n Specifies the number of columns of the matrix op(B) and of the matrix C.
+/// \param [in] k Specifies the number of columns of the matrix op(A) and the number of rows of the matrix op(B).
+/// \param [in] alpha Scaling factor for the matrix-matrix product.
+/// \param [in] a Input matrix A.
+/// \param [in] a_type Data type of the matrix A.
+/// \param [in] lda Leading dimension of A.
+/// \param [in] b Input matrix B.
+/// \param [in] b_type Data type of the matrix B.
+/// \param [in] ldb Leading dimension of B.
+/// \param [in] beta Scaling factor for matrix C.
+/// \param [in, out] c Input/Output matrix C.
+/// \param [in] c_type Data type of the matrix C.
+/// \param [in] ldc Leading dimension of C.
+/// \param [in] scaling_type Data type of the scaling factors.
+inline void gemm(sycl::queue &q, oneapi::mkl::transpose a_trans,
+                 oneapi::mkl::transpose b_trans, int m, int n, int k,
+                 const void *alpha, const void *a, library_data_t a_type,
+                 int lda, const void *b, library_data_t b_type, int ldb,
+                 const void *beta, void *c, library_data_t c_type, int ldc,
+                 library_data_t scaling_type) {
+  bool matched = false;
+  if (scaling_type == library_data_t::real_float &&
+      c_type == library_data_t::complex_float) {
+    scaling_type = library_data_t::complex_float;
+  } else if (scaling_type == library_data_t::real_double &&
+             c_type == library_data_t::complex_double) {
+    scaling_type = library_data_t::complex_double;
+  }
+
+  std::uint64_t key =
+      detail::get_type_combination_id(a_type, b_type, c_type, scaling_type);
+  switch (key) {
+  case detail::get_type_combination_id(
+      library_data_t::real_float, library_data_t::real_float,
+      library_data_t::real_float, library_data_t::real_float): {
+    detail::gemm_impl<float, float, float, float>(
+        q, a_trans, b_trans, m, n, k, alpha, a, lda, b, ldb, beta, c, ldc);
+    break;
+  }
+  case detail::get_type_combination_id(
+      library_data_t::real_double, library_data_t::real_double,
+      library_data_t::real_double, library_data_t::real_double): {
+    detail::gemm_impl<double, double, double, double>(
+        q, a_trans, b_trans, m, n, k, alpha, a, lda, b, ldb, beta, c, ldc);
+    break;
+  }
+  case detail::get_type_combination_id(
+      library_data_t::complex_float, library_data_t::complex_float,
+      library_data_t::complex_float, library_data_t::complex_float): {
+    detail::gemm_impl<std::complex<float>, std::complex<float>,
+                      std::complex<float>, std::complex<float>>(
+        q, a_trans, b_trans, m, n, k, alpha, a, lda, b, ldb, beta, c, ldc);
+    break;
+  }
+  case detail::get_type_combination_id(
+      library_data_t::complex_double, library_data_t::complex_double,
+      library_data_t::complex_double, library_data_t::complex_double): {
+    detail::gemm_impl<std::complex<double>, std::complex<double>,
+                      std::complex<double>, std::complex<double>>(
+        q, a_trans, b_trans, m, n, k, alpha, a, lda, b, ldb, beta, c, ldc);
+    break;
+  }
+  case detail::get_type_combination_id(
+      library_data_t::real_half, library_data_t::real_half,
+      library_data_t::real_half, library_data_t::real_half): {
+    detail::gemm_impl<sycl::half, sycl::half, sycl::half,
+                      sycl::half>(q, a_trans, b_trans, m, n, k, alpha, a,
+                                      lda, b, ldb, beta, c, ldc);
+    break;
+  }
+  case detail::get_type_combination_id(
+      library_data_t::real_bfloat16, library_data_t::real_bfloat16,
+      library_data_t::real_float, library_data_t::real_float): {
+    detail::gemm_impl<oneapi::mkl::bfloat16, oneapi::mkl::bfloat16, float,
+                      float>(q, a_trans, b_trans, m, n, k, alpha, a, lda, b,
+                             ldb, beta, c, ldc);
+    break;
+  }
+  case detail::get_type_combination_id(
+      library_data_t::real_half, library_data_t::real_half,
+      library_data_t::real_float, library_data_t::real_float): {
+    detail::gemm_impl<sycl::half, sycl::half, float, float>(
+        q, a_trans, b_trans, m, n, k, alpha, a, lda, b, ldb, beta, c, ldc);
+    break;
+  }
+  case detail::get_type_combination_id(
+      library_data_t::real_half, library_data_t::real_half,
+      library_data_t::real_half, library_data_t::real_float): {
+    float alpha_value =
+        dpct::get_value(reinterpret_cast<const float *>(alpha), q);
+    float beta_value =
+        dpct::get_value(reinterpret_cast<const float *>(beta), q);
+    sycl::half alpha_half(alpha_value);
+    sycl::half beta_half(beta_value);
+    detail::gemm_impl<sycl::half, sycl::half, sycl::half,
+                      sycl::half>(q, a_trans, b_trans, m, n, k, &alpha_half,
+                                      a, lda, b, ldb, &beta_half, c, ldc);
+    break;
+  }
+  case detail::get_type_combination_id(
+      library_data_t::real_int8, library_data_t::real_int8,
+      library_data_t::real_float, library_data_t::real_float): {
+    detail::gemm_impl<std::int8_t, std::int8_t, float, float>(
+        q, a_trans, b_trans, m, n, k, alpha, a, lda, b, ldb, beta, c, ldc);
+    break;
+  }
+  case detail::get_type_combination_id(
+      library_data_t::real_bfloat16, library_data_t::real_bfloat16,
+      library_data_t::real_bfloat16, library_data_t::real_float): {
+    detail::gemm_impl<oneapi::mkl::bfloat16, oneapi::mkl::bfloat16,
+                      oneapi::mkl::bfloat16, float>(
+        q, a_trans, b_trans, m, n, k, alpha, a, lda, b, ldb, beta, c, ldc);
+    break;
+  }
+  case detail::get_type_combination_id(
+      library_data_t::real_int8, library_data_t::real_int8,
+      library_data_t::real_int32, library_data_t::real_int32): {
+    float alpha_float =
+        dpct::get_value(reinterpret_cast<const std::int32_t *>(alpha), q);
+    float beta_float =
+        dpct::get_value(reinterpret_cast<const std::int32_t *>(beta), q);
+    detail::gemm_impl<std::int8_t, std::int8_t, std::int32_t, float>(
+        q, a_trans, b_trans, m, n, k, &alpha_float, a, lda, b, ldb, &beta_float, c, ldc);
+    break;
+  }
+  default:
+    throw std::runtime_error("the combination of data type is unsupported");
+  }
+}
+
+/// Computes a batch of matrix-matrix product with general matrices.
+/// \param [in] q The queue where the routine should be executed.
+/// \param [in] a_trans Specifies the operation applied to A.
+/// \param [in] b_trans Specifies the operation applied to B.
+/// \param [in] m Specifies the number of rows of the matrix op(A) and of the matrix C.
+/// \param [in] n Specifies the number of columns of the matrix op(B) and of the matrix C.
+/// \param [in] k Specifies the number of columns of the matrix op(A) and the number of rows of the matrix op(B).
+/// \param [in] alpha Scaling factor for the matrix-matrix product.
+/// \param [in] a Input matrix A.
+/// \param [in] a_type Data type of the matrix A.
+/// \param [in] lda Leading dimension of A.
+/// \param [in] b Input matrix B.
+/// \param [in] b_type Data type of the matrix B.
+/// \param [in] ldb Leading dimension of B.
+/// \param [in] beta Scaling factor for matrix C.
+/// \param [in, out] c Input/Output matrix C.
+/// \param [in] c_type Data type of the matrix C.
+/// \param [in] ldc Leading dimension of C.
+/// \param [in] batch_size Specifies the number of matrix multiply operations to perform.
+/// \param [in] scaling_type Data type of the scaling factors.
+inline void gemm_batch(sycl::queue &q, oneapi::mkl::transpose a_trans,
+                       oneapi::mkl::transpose b_trans, int m, int n, int k,
+                       const void *alpha, const void *a[],
+                       library_data_t a_type, int lda, const void *b[],
+                       library_data_t b_type, int ldb, const void *beta,
+                       void *c[], library_data_t c_type, int ldc,
+                       int batch_size, library_data_t scaling_type) {
+#ifdef DPCT_USM_LEVEL_NONE
+  throw std::runtime_error("this API is unsupported when USM level is none");
+#else
+  bool matched = false;
+  if (scaling_type == library_data_t::real_float &&
+      c_type == library_data_t::complex_float) {
+    scaling_type = library_data_t::complex_float;
+  } else if (scaling_type == library_data_t::real_double &&
+             c_type == library_data_t::complex_double) {
+    scaling_type = library_data_t::complex_double;
+  }
+
+  std::uint64_t key =
+      detail::get_type_combination_id(a_type, b_type, c_type, scaling_type);
+  switch (key) {
+  case detail::get_type_combination_id(
+      library_data_t::real_float, library_data_t::real_float,
+      library_data_t::real_float, library_data_t::real_float): {
+    detail::gemm_batch_impl<float, float, float, float>(
+        q, a_trans, b_trans, m, n, k, alpha, a, lda, b, ldb, beta, c, ldc,
+        batch_size);
+    break;
+  }
+  case detail::get_type_combination_id(
+      library_data_t::real_double, library_data_t::real_double,
+      library_data_t::real_double, library_data_t::real_double): {
+    detail::gemm_batch_impl<double, double, double, double>(
+        q, a_trans, b_trans, m, n, k, alpha, a, lda, b, ldb, beta, c, ldc,
+        batch_size);
+    break;
+  }
+  case detail::get_type_combination_id(
+      library_data_t::complex_float, library_data_t::complex_float,
+      library_data_t::complex_float, library_data_t::complex_float): {
+    detail::gemm_batch_impl<std::complex<float>, std::complex<float>,
+                            std::complex<float>, std::complex<float>>(
+        q, a_trans, b_trans, m, n, k, alpha, a, lda, b, ldb, beta, c, ldc,
+        batch_size);
+    break;
+  }
+  case detail::get_type_combination_id(
+      library_data_t::complex_double, library_data_t::complex_double,
+      library_data_t::complex_double, library_data_t::complex_double): {
+    detail::gemm_batch_impl<std::complex<double>, std::complex<double>,
+                            std::complex<double>, std::complex<double>>(
+        q, a_trans, b_trans, m, n, k, alpha, a, lda, b, ldb, beta, c, ldc,
+        batch_size);
+    break;
+  }
+  case detail::get_type_combination_id(
+      library_data_t::real_half, library_data_t::real_half,
+      library_data_t::real_half, library_data_t::real_half): {
+    detail::gemm_batch_impl<sycl::half, sycl::half, sycl::half,
+                            sycl::half>(q, a_trans, b_trans, m, n, k, alpha,
+                                            a, lda, b, ldb, beta, c, ldc,
+                                            batch_size);
+    break;
+  }
+#ifdef __INTEL_MKL__
+  case detail::get_type_combination_id(
+      library_data_t::real_bfloat16, library_data_t::real_bfloat16,
+      library_data_t::real_bfloat16, library_data_t::real_float): {
+    detail::gemm_batch_impl<oneapi::mkl::bfloat16, oneapi::mkl::bfloat16,
+                            oneapi::mkl::bfloat16, float>(
+        q, a_trans, b_trans, m, n, k, alpha, a, lda, b, ldb, beta, c, ldc,
+        batch_size);
+    break;
+  }
+  case detail::get_type_combination_id(
+      library_data_t::real_bfloat16, library_data_t::real_bfloat16,
+      library_data_t::real_float, library_data_t::real_float): {
+    detail::gemm_batch_impl<oneapi::mkl::bfloat16, oneapi::mkl::bfloat16, float,
+                            float>(q, a_trans, b_trans, m, n, k, alpha, a, lda,
+                                   b, ldb, beta, c, ldc, batch_size);
+    break;
+  }
+  case detail::get_type_combination_id(
+      library_data_t::real_int8, library_data_t::real_int8,
+      library_data_t::real_int32, library_data_t::real_int32): {
+    float alpha_float =
+        dpct::get_value(reinterpret_cast<const std::int32_t *>(alpha), q);
+    float beta_float =
+        dpct::get_value(reinterpret_cast<const std::int32_t *>(beta), q);
+    detail::gemm_batch_impl<std::int8_t, std::int8_t, std::int32_t,
+                            float>(q, a_trans, b_trans, m, n, k, &alpha_float,
+                                          a, lda, b, ldb, &beta_float, c, ldc,
+                                          batch_size);
+    break;
+  }
+  case detail::get_type_combination_id(
+      library_data_t::real_int8, library_data_t::real_int8,
+      library_data_t::real_float, library_data_t::real_float): {
+    detail::gemm_batch_impl<std::int8_t, std::int8_t, float, float>(
+        q, a_trans, b_trans, m, n, k, alpha, a, lda, b, ldb, beta, c, ldc,
+        batch_size);
+    break;
+  }
+  case detail::get_type_combination_id(
+      library_data_t::real_half, library_data_t::real_half,
+      library_data_t::real_float, library_data_t::real_float): {
+    detail::gemm_batch_impl<sycl::half, sycl::half, float, float>(
+        q, a_trans, b_trans, m, n, k, alpha, a, lda, b, ldb, beta, c, ldc,
+        batch_size);
+    break;
+  }
+#endif
+  case detail::get_type_combination_id(
+      library_data_t::real_half, library_data_t::real_half,
+      library_data_t::real_half, library_data_t::real_float): {
+    float alpha_value =
+        dpct::get_value(reinterpret_cast<const float *>(alpha), q);
+    float beta_value =
+        dpct::get_value(reinterpret_cast<const float *>(beta), q);
+    sycl::half alpha_half(alpha_value);
+    sycl::half beta_half(beta_value);
+    detail::gemm_batch_impl<sycl::half, sycl::half, sycl::half, sycl::half>(
+        q, a_trans, b_trans, m, n, k, &alpha_half, a, lda, b, ldb, &beta_half, c, ldc,
+        batch_size);
+    break;
+  }
+  default:
+    throw std::runtime_error("the combination of data type is unsupported");
+  }
+#endif
+}
+
+/// Computes a batch of matrix-matrix product with general matrices.
+/// \param [in] q The queue where the routine should be executed.
+/// \param [in] a_trans Specifies the operation applied to A.
+/// \param [in] b_trans Specifies the operation applied to B.
+/// \param [in] m Specifies the number of rows of the matrix op(A) and of the matrix C.
+/// \param [in] n Specifies the number of columns of the matrix op(B) and of the matrix C.
+/// \param [in] k Specifies the number of columns of the matrix op(A) and the number of rows of the matrix op(B).
+/// \param [in] alpha Scaling factor for the matrix-matrix product.
+/// \param [in] a Input matrix A.
+/// \param [in] a_type Data type of the matrix A.
+/// \param [in] lda Leading dimension of A.
+/// \param [in] stride_a Stride between the different A matrices.
+/// \param [in] b Input matrix B.
+/// \param [in] b_type Data type of the matrix B.
+/// \param [in] ldb Leading dimension of B.
+/// \param [in] stride_b Stride between the different B matrices.
+/// \param [in] beta Scaling factor for matrix C.
+/// \param [in, out] c Input/Output matrix C.
+/// \param [in] c_type Data type of the matrix C.
+/// \param [in] ldc Leading dimension of C.
+/// \param [in] stride_c Stride between the different C matrices.
+/// \param [in] batch_size Specifies the number of matrix multiply operations to perform.
+/// \param [in] scaling_type Data type of the scaling factors.
+inline void gemm_batch(sycl::queue &q, oneapi::mkl::transpose a_trans,
+                       oneapi::mkl::transpose b_trans, int m, int n, int k,
+                       const void *alpha, const void *a, library_data_t a_type,
+                       int lda, long long int stride_a, const void *b,
+                       library_data_t b_type, int ldb, long long int stride_b,
+                       const void *beta, void *c, library_data_t c_type,
+                       int ldc, long long int stride_c, int batch_size,
+                       library_data_t scaling_type) {
+  bool matched = false;
+  if (scaling_type == library_data_t::real_float &&
+      c_type == library_data_t::complex_float) {
+    scaling_type = library_data_t::complex_float;
+  } else if (scaling_type == library_data_t::real_double &&
+             c_type == library_data_t::complex_double) {
+    scaling_type = library_data_t::complex_double;
+  }
+
+  std::uint64_t key =
+      detail::get_type_combination_id(a_type, b_type, c_type, scaling_type);
+  switch (key) {
+  case detail::get_type_combination_id(
+      library_data_t::real_float, library_data_t::real_float,
+      library_data_t::real_float, library_data_t::real_float): {
+    detail::gemm_batch_impl<float, float, float, float>(
+        q, a_trans, b_trans, m, n, k, alpha, a, lda, stride_a, b, ldb, stride_b,
+        beta, c, ldc, stride_c, batch_size);
+    break;
+  }
+  case detail::get_type_combination_id(
+      library_data_t::real_double, library_data_t::real_double,
+      library_data_t::real_double, library_data_t::real_double): {
+    detail::gemm_batch_impl<double, double, double, double>(
+        q, a_trans, b_trans, m, n, k, alpha, a, lda, stride_a, b, ldb, stride_b,
+        beta, c, ldc, stride_c, batch_size);
+    break;
+  }
+  case detail::get_type_combination_id(
+      library_data_t::complex_float, library_data_t::complex_float,
+      library_data_t::complex_float, library_data_t::complex_float): {
+    detail::gemm_batch_impl<std::complex<float>, std::complex<float>,
+                            std::complex<float>, std::complex<float>>(
+        q, a_trans, b_trans, m, n, k, alpha, a, lda, stride_a, b, ldb, stride_b,
+        beta, c, ldc, stride_c, batch_size);
+    break;
+  }
+  case detail::get_type_combination_id(
+      library_data_t::complex_double, library_data_t::complex_double,
+      library_data_t::complex_double, library_data_t::complex_double): {
+    detail::gemm_batch_impl<std::complex<double>, std::complex<double>,
+                            std::complex<double>, std::complex<double>>(
+        q, a_trans, b_trans, m, n, k, alpha, a, lda, stride_a, b, ldb, stride_b,
+        beta, c, ldc, stride_c, batch_size);
+    break;
+  }
+  case detail::get_type_combination_id(
+      library_data_t::real_half, library_data_t::real_half,
+      library_data_t::real_half, library_data_t::real_half): {
+    detail::gemm_batch_impl<sycl::half, sycl::half, sycl::half,
+                            sycl::half>(q, a_trans, b_trans, m, n, k, alpha,
+                                            a, lda, stride_a, b, ldb, stride_b,
+                                            beta, c, ldc, stride_c, batch_size);
+    break;
+  }
+#ifdef __INTEL_MKL__
+  case detail::get_type_combination_id(
+      library_data_t::real_bfloat16, library_data_t::real_bfloat16,
+      library_data_t::real_bfloat16, library_data_t::real_float): {
+    detail::gemm_batch_impl<oneapi::mkl::bfloat16, oneapi::mkl::bfloat16,
+                            oneapi::mkl::bfloat16, float>(
+        q, a_trans, b_trans, m, n, k, alpha, a, lda, stride_a, b, ldb, stride_b,
+        beta, c, ldc, stride_c, batch_size);
+    break;
+  }
+  case detail::get_type_combination_id(
+      library_data_t::real_bfloat16, library_data_t::real_bfloat16,
+      library_data_t::real_float, library_data_t::real_float): {
+    detail::gemm_batch_impl<oneapi::mkl::bfloat16, oneapi::mkl::bfloat16, float,
+                            float>(q, a_trans, b_trans, m, n, k, alpha, a, lda,
+                                   stride_a, b, ldb, stride_b, beta, c, ldc,
+                                   stride_c, batch_size);
+    break;
+  }
+  case detail::get_type_combination_id(
+      library_data_t::real_int8, library_data_t::real_int8,
+      library_data_t::real_int32, library_data_t::real_int32): {
+    detail::gemm_batch_impl<std::int8_t, std::int8_t, std::int32_t,
+                            std::int32_t>(q, a_trans, b_trans, m, n, k, alpha,
+                                          a, lda, stride_a, b, ldb, stride_b,
+                                          beta, c, ldc, stride_c, batch_size);
+    break;
+  }
+  case detail::get_type_combination_id(
+      library_data_t::real_int8, library_data_t::real_int8,
+      library_data_t::real_float, library_data_t::real_float): {
+    detail::gemm_batch_impl<std::int8_t, std::int8_t, float, float>(
+        q, a_trans, b_trans, m, n, k, alpha, a, lda, stride_a, b, ldb, stride_b,
+        beta, c, ldc, stride_c, batch_size);
+    break;
+  }
+  case detail::get_type_combination_id(
+      library_data_t::real_half, library_data_t::real_half,
+      library_data_t::real_float, library_data_t::real_float): {
+    detail::gemm_batch_impl<sycl::half, sycl::half, float, float>(
+        q, a_trans, b_trans, m, n, k, alpha, a, lda, stride_a, b, ldb, stride_b,
+        beta, c, ldc, stride_c, batch_size);
+    break;
+  }
+#endif
+  case detail::get_type_combination_id(
+      library_data_t::real_half, library_data_t::real_half,
+      library_data_t::real_half, library_data_t::real_float): {
+    float alpha_value =
+        dpct::get_value(reinterpret_cast<const float *>(alpha), q);
+    float beta_value =
+        dpct::get_value(reinterpret_cast<const float *>(beta), q);
+    sycl::half alpha_half(alpha_value);
+    sycl::half beta_half(beta_value);
+    detail::gemm_batch_impl<sycl::half, sycl::half, sycl::half, sycl::half>(
+        q, a_trans, b_trans, m, n, k, &alpha_half, a, lda, stride_a, b, ldb, stride_b,
+        &beta_half, c, ldc, stride_c, batch_size);
+    break;
+  }
+  default:
+    throw std::runtime_error("the combination of data type is unsupported");
+  }
+}
+
+/// This routines perform a special rank-k update of a symmetric matrix C by
+/// general matrices A and B.
+/// \param [in] q The queue where the routine should be executed.
+/// \param [in] uplo Specifies whether C's data is stored in its upper or lower triangle.
+/// \param [in] trans Specifies the operation to apply.
+/// \param [in] n The number of rows and columns in C.
+/// \param [in] k The inner dimension of matrix multiplications.
+/// \param [in] alpha Scaling factor for the rank-k update.
+/// \param [in] a Input matrix A.
+/// \param [in] lda Leading dimension of A.
+/// \param [in] b Input matrix B.
+/// \param [in] ldb Leading dimension of B.
+/// \param [in] beta Scaling factor for the rank-k update.
+/// \param [in, out] c Input/Output matrix C.
+/// \param [in] ldc Leading dimension of C.
+template <class T>
+inline void syrk(sycl::queue &q, oneapi::mkl::uplo uplo,
+                  oneapi::mkl::transpose trans, int n, int k, const T *alpha,
+                  const T *a, int lda, const T *b, int ldb, const T *beta, T *c,
+                  int ldc) {
+  detail::rk_impl<false, T, T>(q, uplo, trans, n, k, alpha, a, lda, b,
+                                     ldb, beta, c, ldc);
+}
+
+/// This routines perform a special rank-k update of a Hermitian matrix C by
+/// general matrices A and B.
+/// \param [in] q The queue where the routine should be executed.
+/// \param [in] uplo Specifies whether C's data is stored in its upper or lower triangle.
+/// \param [in] trans Specifies the operation to apply.
+/// \param [in] n The number of rows and columns in C.
+/// \param [in] k The inner dimension of matrix multiplications.
+/// \param [in] alpha Scaling factor for the rank-k update.
+/// \param [in] a Input matrix A.
+/// \param [in] lda Leading dimension of A.
+/// \param [in] b Input matrix B.
+/// \param [in] ldb Leading dimension of B.
+/// \param [in] beta Scaling factor for the rank-k update.
+/// \param [in, out] c Input/Output matrix C.
+/// \param [in] ldc Leading dimension of C.
+template <class T, class Tbeta>
+inline void herk(sycl::queue &q, oneapi::mkl::uplo uplo,
+                 oneapi::mkl::transpose trans, int n, int k, const T *alpha,
+                 const T *a, int lda, const T *b, int ldb, const Tbeta *beta,
+                 T *c, int ldc) {
+  detail::rk_impl<true, T, Tbeta>(q, uplo, trans, n, k, alpha, a, lda, b,
+                                        ldb, beta, c, ldc);
+}
+
+/// This routine performs a group of trsm operations. Each trsm solves an
+/// equation of the form op(A) * X = alpha * B or X * op(A) = alpha * B.
+/// \param [in] q The queue where the routine should be executed.
+/// \param [in] left_right Specifies A multiplies X on the left or on the right.
+/// \param [in] upper_lower Specifies A is upper or lower triangular.
+/// \param [in] trans Specifies the operation applied to A.
+/// \param [in] unit_diag Specifies whether A is unit triangular.
+/// \param [in] m Number of rows of the B matrices.
+/// \param [in] n Number of columns of the B matrices.
+/// \param [in] alpha Scaling factor for the solutions.
+/// \param [in] a Input matrices A.
+/// \param [in] a_type Data type of the matrices A.
+/// \param [in] lda Leading dimension of the matrices A.
+/// \param [in, out] b Input and output matrices B.
+/// \param [in] b_type Data type of the matrices B.
+/// \param [in] ldb Leading dimension of the matrices B.
+/// \param [in] batch_size Specifies the number of trsm operations to perform.
+/// \param [in] scaling_type Data type of the scaling factors.
+inline void trsm_batch(sycl::queue &q, oneapi::mkl::side left_right,
+                       oneapi::mkl::uplo upper_lower,
+                       oneapi::mkl::transpose trans,
+                       oneapi::mkl::diag unit_diag, int m, int n,
+                       const void *alpha, const void **a, library_data_t a_type,
+                       int lda, void **b, library_data_t b_type, int ldb,
+                       int batch_size, library_data_t scaling_type) {
+#ifdef DPCT_USM_LEVEL_NONE
+  throw std::runtime_error("this API is unsupported when USM level is none");
+#else
+  std::uint64_t key =
+      detail::get_type_combination_id(a_type, b_type, scaling_type);
+  switch (key) {
+  case detail::get_type_combination_id(library_data_t::real_float,
+                                       library_data_t::real_float,
+                                       library_data_t::real_float): {
+    detail::trsm_batch_impl<float, float, float>(q, left_right, upper_lower,
+                                                 trans, unit_diag, m, n, alpha,
+                                                 a, lda, b, ldb, batch_size);
+    break;
+  }
+  case detail::get_type_combination_id(library_data_t::real_double,
+                                       library_data_t::real_double,
+                                       library_data_t::real_double): {
+    detail::trsm_batch_impl<double, double, double>(
+        q, left_right, upper_lower, trans, unit_diag, m, n, alpha, a, lda, b,
+        ldb, batch_size);
+    break;
+  }
+  case detail::get_type_combination_id(library_data_t::complex_float,
+                                       library_data_t::complex_float,
+                                       library_data_t::complex_float): {
+    detail::trsm_batch_impl<std::complex<float>, std::complex<float>,
+                            std::complex<float>>(q, left_right, upper_lower,
+                                                 trans, unit_diag, m, n, alpha,
+                                                 a, lda, b, ldb, batch_size);
+    break;
+  }
+  case detail::get_type_combination_id(library_data_t::complex_double,
+                                       library_data_t::complex_double,
+                                       library_data_t::complex_double): {
+    detail::trsm_batch_impl<std::complex<double>, std::complex<double>,
+                            std::complex<double>>(q, left_right, upper_lower,
+                                                  trans, unit_diag, m, n, alpha,
+                                                  a, lda, b, ldb, batch_size);
+    break;
+  }
+  default:
+    throw std::runtime_error("the combination of data type is unsupported");
+  }
+#endif
+}
+
+/// Computes a triangular matrix-general matrix product.
+/// \param [in] q The queue where the routine should be executed.
+/// \param [in] left_right Specifies A is on the left or right side of the
+/// multiplication.
+/// \param [in] upper_lower Specifies A is upper or lower triangular.
+/// \param [in] trans Specifies the operation applied to A.
+/// \param [in] unit_diag Specifies whether A is unit triangular.
+/// \param [in] m Number of rows of B.
+/// \param [in] n Number of columns of B.
+/// \param [in] alpha Scaling factor for the matrix-matrix product.
+/// \param [in] a Input matrices A.
+/// \param [in] lda Leading dimension of the matrices A.
+/// \param [in] b Input matrices B.
+/// \param [in] ldb Leading dimension of the matrices B.
+/// \param [out] c Output matrices C.
+/// \param [in] ldc Leading dimension of the matrices C.
+template <class T>
+inline void trmm(sycl::queue &q, oneapi::mkl::side left_right,
+                 oneapi::mkl::uplo upper_lower, oneapi::mkl::transpose trans,
+                 oneapi::mkl::diag unit_diag, int m, int n, const T *alpha,
+                 const T *a, int lda, const T *b, int ldb, T *c, int ldc) {
+  using Ty = typename DataType<T>::T2;
+  auto alpha_val = dpct::get_value(alpha, q);
+  if (b != c) {
+    dpct::matrix_mem_copy(c, b, ldc, ldb, m, n, dpct::device_to_device, q);
+  }
+  auto data_a = detail::get_memory<const Ty>(a);
+  auto data_c = detail::get_memory<Ty>(c);
+  oneapi::mkl::blas::column_major::trmm(q, left_right, upper_lower, trans,
+                                        unit_diag, m, n, alpha_val, data_a, lda,
+                                        data_c, ldc);
+}
+
+} // namespace dpct
+#endif // __DPCT_BLAS_UTILS_HPP__
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/ccl_utils.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/ccl_utils.hpp
new file mode 100644
index 0000000..07b3488
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/ccl_utils.hpp
@@ -0,0 +1,286 @@
+//==---- ccl_utils.hpp----------------------------*- C++ -*----------------==//
+//
+// Copyright (C) Intel Corporation
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+// See https://llvm.org/LICENSE.txt for license information.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef __DPCT_CCL_UTILS_HPP__
+#define __DPCT_CCL_UTILS_HPP__
+
+#include <sycl/sycl.hpp>
+#include <oneapi/ccl.hpp>
+#include <unordered_map>
+#include <memory>
+
+#include "device.hpp"
+
+namespace dpct {
+namespace ccl {
+namespace detail {
+
+/// Get stored kvs with specified kvs address.
+inline std::shared_ptr<oneapi::ccl::kvs> &
+get_kvs(const oneapi::ccl::kvs::address_type &addr) {
+  struct hash {
+    std::size_t operator()(const oneapi::ccl::kvs::address_type &in) const {
+      return std::hash<std::string_view>()(std::string_view(in.data(), in.size()));
+    }
+  };
+  static std::unordered_map<oneapi::ccl::kvs::address_type,
+                            std::shared_ptr<oneapi::ccl::kvs>, hash>
+      kvs_map;
+  return kvs_map[addr];
+}
+
+/// Help class to init ccl environment. 
+class ccl_init_helper {
+public:
+  ccl_init_helper() { oneapi::ccl::init(); }
+};
+
+} // namespace detail
+
+/// Get concatenated library version as an integer.
+static inline int get_version() {
+  oneapi::ccl::init();
+  auto ver = oneapi::ccl::get_library_version();
+  return ver.major * 10000 + ver.minor * 100 + ver.update;
+}
+
+/// Create main kvs and return its address.
+static inline oneapi::ccl::kvs::address_type create_kvs_address() {
+  oneapi::ccl::init();
+  auto ptr = oneapi::ccl::create_main_kvs();
+  auto addr = ptr->get_address();
+  detail::get_kvs(addr) = ptr;
+  return addr;
+}
+
+/// Get stored kvs with /p addr if exist. Otherwise, create kvs with /p addr.
+static inline std::shared_ptr<oneapi::ccl::kvs>
+create_kvs(const oneapi::ccl::kvs::address_type &addr) {
+  oneapi::ccl::init();
+  auto &ptr = detail::get_kvs(addr);
+  if (!ptr)
+    ptr = oneapi::ccl::create_kvs(addr);
+  return ptr;
+}
+
+/// dpct communicator extension
+class communicator_wrapper : public dpct::ccl::detail::ccl_init_helper {
+public:
+  communicator_wrapper(
+      int size, int rank, oneapi::ccl::kvs::address_type id,
+      const oneapi::ccl::comm_attr &attr = oneapi::ccl::default_comm_attr)
+      : _device_comm(oneapi::ccl::create_device(
+            static_cast<sycl::device &>(dpct::get_current_device()))),
+        _context_comm(oneapi::ccl::create_context(dpct::get_default_context())),
+        _comm(oneapi::ccl::create_communicator(
+            size, rank, _device_comm, _context_comm, dpct::ccl::create_kvs(id),
+            attr)) {
+    _queue_init = false;
+    _ccl_stream_ptr = nullptr;
+  }
+
+  ~communicator_wrapper() {
+    delete _ccl_stream_ptr;
+  };
+
+  /// Return the rank in a oneapi::ccl::communicator
+  /// \returns The rank corresponding to communicator object
+  int rank() const {
+    return _comm.rank();
+  }
+
+  /// Retrieves the number of rank in oneapi::ccl::communicator
+  /// \returns The number of the ranks
+  int size() const {
+    return _comm.size();
+  }
+
+  /// Return underlying native device, which was used in oneapi::ccl::communicator
+  sycl::device get_device() const {
+    return _comm.get_device().get_native();
+  }
+
+  /// \brief allreduce is a collective communication operation that performs the global reduction operation
+  ///       on values from all ranks of communicator and distributes the result back to all ranks.
+  /// \param sendbuff the buffer with @c count elements of @c dtype that stores local data to be reduced
+  /// \param recvbuff [out] the buffer to store reduced result, must have the same dimension as @c sendbuff
+  /// \param count the number of elements of type @c dtype in @c sendbuff and @c recvbuff
+  /// \param dtype the datatype of elements in @c sendbuff and @c recvbuff
+  /// \param rtype the type of the reduction operation to be applied
+  /// \param queue_ptr a sycl::queue ptr associated with the operation
+  /// \return @ref void
+  void allreduce(const void *sendbuff, void *recvbuff, size_t count,
+                 oneapi::ccl::datatype dtype, oneapi::ccl::reduction rtype,
+                 sycl::queue *queue_ptr) {
+    call_func_wrapper(
+        [=](const oneapi::ccl::stream &stream) {
+          return oneapi::ccl::allreduce(sendbuff, recvbuff, count, dtype, rtype,
+                                        _comm, stream);
+        },
+        queue_ptr);
+  }
+
+  /// \brief reduce is a collective communication operation that performs the
+  ///        global reduction operation on values from all ranks of the communicator
+  ///        and returns the result to the root rank.
+  /// \param sendbuff the buffer with @c count elements of @c dtype that stores
+  ///        local data to be reduced 
+  /// \param recvbuff [out] the buffer to store reduced result, 
+  ///        must have the same dimension as @c sendbuff 
+  /// \param count the number of elements of type @c dtype in @c sendbuff and @c recvbuff 
+  /// \param dtype the datatype of elements in @c sendbuff and @c recvbuff 
+  /// \param root the rank that gets the result of reduction 
+  /// \param rtype the type of the reduction operation to be applied 
+  /// \param queue_ptr a sycl::queue ptr associated with the operation 
+  /// \return @ref void
+  void reduce(const void *sendbuff, void *recvbuff, size_t count,
+              oneapi::ccl::datatype dtype, oneapi::ccl::reduction rtype,
+              int root, sycl::queue *queue_ptr) {
+    call_func_wrapper(
+        [=](const oneapi::ccl::stream &stream) {
+          return oneapi::ccl::reduce(sendbuff, recvbuff, count, dtype, rtype,
+                                     root, _comm, stream);
+        },
+        queue_ptr);
+  }
+
+  /// \brief broadcast is a collective communication operation that broadcasts data
+  ///        from one rank of communicator (denoted as root) to all other ranks.
+  ///        Only support in-place operation
+  /// \param sendbuff the buffer with @c count elements of @c dtype that stores
+  ///        local data to be reduced 
+  /// \param recvbuff [out] the buffer to store reduced result
+  /// \param count the number of elements of type @c dtype in @c buf 
+  /// \param dtype thedatatype of elements in @c buf 
+  /// \param root the rank that broadcasts @c buf
+  /// \param queue_ptr a sycl::queue ptr associated with the operation
+  /// \return @ref void
+  void broadcast(void *sendbuff, void *recvbuff, size_t count,
+                 oneapi::ccl::datatype dtype, int root,
+                 sycl::queue *queue_ptr) {
+    if (sendbuff != recvbuff) {
+      throw std::runtime_error(
+          "oneCCL broadcast only support in-place operation. "
+          "sendbuff and recvbuff must be same.");
+      return;
+    }
+    call_func_wrapper(
+        [=](const oneapi::ccl::stream &stream) {
+          return oneapi::ccl::broadcast(recvbuff, count, dtype, root, _comm,
+                                        stream);
+        },
+        queue_ptr);
+  }
+
+  /// \brief reduce_scatter is a collective communication operation that performs the global reduction operation
+  ///        on values from all ranks of the communicator and scatters the result in blocks back to all ranks.
+  /// \param sendbuff the buffer with @c count elements of @c dtype that stores local data to be reduced
+  /// \param recvbuff [out] the buffer to store reduced result, must have the same dimension as @c sendbuff
+  /// \param recv_count the number of elements of type @c dtype in receive block
+  /// \param dtype the datatype of elements in @c sendbuff and @c recvbuff
+  /// \param rtype the type of the reduction operation to be applied
+  /// \param queue_ptr a sycl::queue ptr associated with the operation
+  /// \return @ref void
+  void reduce_scatter(const void *sendbuff, void *recvbuff, size_t recv_count,
+                      oneapi::ccl::datatype dtype, oneapi::ccl::reduction rtype,
+                      sycl::queue *queue_ptr) {
+    call_func_wrapper(
+        [=](const oneapi::ccl::stream &stream) {
+          return oneapi::ccl::reduce_scatter(sendbuff, recvbuff, recv_count,
+                                             dtype, rtype, _comm, stream);
+        },
+        queue_ptr);
+  }
+
+  /// \brief send is a pt2pt communication operation that sends data from one rank of communicator.
+  /// \param sendbuff the buffer with @c count elements of @c dtype serves as send buffer for root
+  /// \param count the number of elements of type @c dtype in @c sendbuff
+  /// \param dtype the datatype of elements in @c sendbuff
+  /// \param peer the rank that receives @c sendbuff
+  /// \param queue_ptr a sycl::queue ptr associated with the operation
+  /// \return @ref void
+  void send(void *sendbuff, size_t count, oneapi::ccl::datatype dtype, int peer,
+            sycl::queue *queue_ptr) {
+    call_func_wrapper(
+        [=](const oneapi::ccl::stream &stream) {
+          return oneapi::ccl::send(sendbuff, count, dtype, peer, _comm, stream);
+        },
+        queue_ptr);
+  }
+
+  /// \brief recv is a pt2pt communication operation that sends data from one rank of communicator.
+  /// \param recvbuff the buffer with @c count elements of @c dtype serves as  receive buffer
+  /// \param count the number of elements of type @c dtype in @c recvbuff
+  /// \param dtype the datatype of elements in @c recvbuff
+  /// \param peer the rank that receives @c recvbuff
+  /// \param queue_ptr a sycl::queue ptr associated with the operation
+  /// \return @ref void
+  void recv(void *recvbuff, size_t count, oneapi::ccl::datatype dtype, int peer,
+            sycl::queue *queue_ptr) {
+    call_func_wrapper(
+        [=](const oneapi::ccl::stream &stream) {
+          return oneapi::ccl::recv(recvbuff, count, dtype, peer, _comm, stream);
+        },
+        queue_ptr);
+  }
+
+private:
+  oneapi::ccl::device _device_comm;
+  oneapi::ccl::context _context_comm;
+  oneapi::ccl::communicator _comm;
+  sycl::queue _queue;
+  bool _queue_init;
+  oneapi::ccl::stream *_ccl_stream_ptr;
+
+  template <class Fn>
+  void call_func_wrapper(Fn func, sycl::queue *qptr) {
+    if (_queue_init && *qptr != _queue) {
+      call_func_async(func, qptr);
+    } else {
+      if(!_queue_init) {
+        _queue = *qptr;
+        _queue_init = true;
+        _ccl_stream_ptr = new oneapi::ccl::stream(oneapi::ccl::create_stream(_queue));
+      }
+      std::invoke(func, *_ccl_stream_ptr);
+    }
+  }
+
+  class call_func_async {
+    sycl::queue *_q_ptr;
+    struct call_async_impl {
+      oneapi::ccl::stream _ccl_stream_impl;
+      oneapi::ccl::event _ccl_event_impl;
+      template <class Fn>
+      explicit call_async_impl(Fn func, sycl::queue *qptr)
+          : _ccl_stream_impl(oneapi::ccl::create_stream(*qptr)),
+            _ccl_event_impl(std::invoke(func, _ccl_stream_impl)) {}
+    };
+    call_async_impl *_imp;
+
+  public:
+    template <class Fn>
+    explicit call_func_async(Fn func, sycl::queue *qptr)
+        : _q_ptr(qptr),
+          _imp(new call_async_impl(func, qptr)) {}
+    ~call_func_async() {
+      _q_ptr->submit([&](sycl::handler &cgh)
+                     { cgh.host_task([=]
+                                     {
+        _imp->_ccl_event_impl.wait();
+        delete _imp; }); });
+    }
+  };
+};
+
+typedef dpct::ccl::communicator_wrapper *comm_ptr;
+
+} // namespace ccl
+} // namespace dpct
+
+#endif // __DPCT_CCL_UTILS_HPP__
\ No newline at end of file
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/device.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/device.hpp
new file mode 100644
index 0000000..729ebf6
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/device.hpp
@@ -0,0 +1,781 @@
+//==---- device.hpp -------------------------------*- C++ -*----------------==//
+//
+// Copyright (C) Intel Corporation
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+// See https://llvm.org/LICENSE.txt for license information.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef __DPCT_DEVICE_HPP__
+#define __DPCT_DEVICE_HPP__
+
+#include <sycl/sycl.hpp>
+#include <algorithm>
+#include <array>
+#include <cstring>
+#include <iostream>
+#include <mutex>
+#include <set>
+#include <sstream>
+#include <map>
+#include <vector>
+#include <thread>
+#if defined(__linux__)
+#include <unistd.h>
+#include <sys/syscall.h>
+#endif
+#if defined(_WIN64)
+#ifndef NOMINMAX
+#define NOMINMAX
+#endif
+#include <windows.h>
+#endif
+
+namespace dpct {
+namespace detail {
+static void get_version(const sycl::device &dev, int &major, int &minor) {
+  // Version string has the following format:
+  // a. OpenCL<space><major.minor><space><vendor-specific-information>
+  // b. <major.minor>
+  std::string ver;
+  ver = dev.get_info<sycl::info::device::version>();
+  std::string::size_type i = 0;
+  while (i < ver.size()) {
+    if (isdigit(ver[i]))
+      break;
+    i++;
+  }
+  major = std::stoi(&(ver[i]));
+  while (i < ver.size()) {
+    if (ver[i] == '.')
+      break;
+    i++;
+  }
+  i++;
+  minor = std::stoi(&(ver[i]));
+}
+} // namespace detail
+
+/// SYCL default exception handler
+inline auto exception_handler = [](sycl::exception_list exceptions) {
+  for (std::exception_ptr const &e : exceptions) {
+    try {
+      std::rethrow_exception(e);
+    } catch (sycl::exception const &e) {
+      std::cerr << "Caught asynchronous SYCL exception:" << std::endl
+                << e.what() << std::endl
+                << "Exception caught at file:" << __FILE__
+                << ", line:" << __LINE__ << std::endl;
+    }
+  }
+};
+
+typedef sycl::event *event_ptr;
+
+typedef sycl::queue *queue_ptr;
+
+typedef char *device_ptr;
+
+/// Destroy \p event pointed memory.
+///
+/// \param event Pointer to the sycl::event address.
+static void destroy_event(event_ptr event) {
+    delete event;
+}
+
+class device_info {
+public:
+  // get interface
+  const char *get_name() const { return _name; }
+  char *get_name() { return _name; }
+  template <typename WorkItemSizesTy = sycl::range<3>,
+            std::enable_if_t<std::is_same_v<WorkItemSizesTy, sycl::range<3>> ||
+                                 std::is_same_v<WorkItemSizesTy, int *>,
+                             int> = 0>
+  auto get_max_work_item_sizes() const {
+    if constexpr (std::is_same_v<WorkItemSizesTy, sycl::range<3>>)
+      return sycl::range<3>(_max_work_item_sizes_i[0],
+                            _max_work_item_sizes_i[1],
+                            _max_work_item_sizes_i[2]);
+    else {
+      return _max_work_item_sizes_i;
+    }  
+  }
+  template <typename WorkItemSizesTy = sycl::range<3>,
+            std::enable_if_t<std::is_same_v<WorkItemSizesTy, sycl::range<3>> ||
+                                 std::is_same_v<WorkItemSizesTy, int *>,
+                             int> = 0>
+  auto get_max_work_item_sizes() {
+    if constexpr (std::is_same_v<WorkItemSizesTy, sycl::range<3>>)
+      return sycl::range<3>(_max_work_item_sizes_i[0],
+                            _max_work_item_sizes_i[1],
+                            _max_work_item_sizes_i[2]);
+    else {
+      return _max_work_item_sizes_i;
+    }  
+  }
+  bool get_host_unified_memory() const { return _host_unified_memory; }
+  int get_major_version() const { return _major; }
+  int get_minor_version() const { return _minor; }
+  int get_integrated() const { return _integrated; }
+  int get_max_clock_frequency() const { return _frequency; }
+  int get_max_compute_units() const { return _max_compute_units; }
+  int get_max_work_group_size() const { return _max_work_group_size; }
+  int get_max_sub_group_size() const { return _max_sub_group_size; }
+  int get_max_work_items_per_compute_unit() const {
+    return _max_work_items_per_compute_unit;
+  }
+  int get_max_register_size_per_work_group() const {
+    return _max_register_size_per_work_group;
+  }
+  template <typename NDRangeSizeTy = size_t *,
+            std::enable_if_t<std::is_same_v<NDRangeSizeTy, size_t *> ||
+                                 std::is_same_v<NDRangeSizeTy, int *>,
+                             int> = 0>
+  auto get_max_nd_range_size() const {
+    if constexpr (std::is_same_v<NDRangeSizeTy, size_t *>)
+      return _max_nd_range_size;
+    else
+      return _max_nd_range_size_i;
+  }
+  template <typename NDRangeSizeTy = size_t *,
+            std::enable_if_t<std::is_same_v<NDRangeSizeTy, size_t *> ||
+                                 std::is_same_v<NDRangeSizeTy, int *>,
+                             int> = 0>
+  auto get_max_nd_range_size() {
+    if constexpr (std::is_same_v<NDRangeSizeTy, size_t *>)
+      return _max_nd_range_size;
+    else
+      return _max_nd_range_size_i;
+  }
+  size_t get_global_mem_size() const { return _global_mem_size; }
+  size_t get_local_mem_size() const { return _local_mem_size; }
+  /// Returns the maximum clock rate of device's global memory in kHz. If
+  /// compiler does not support this API then returns default value 3200000 kHz.
+  unsigned int get_memory_clock_rate() const { return _memory_clock_rate; }
+  /// Returns the maximum bus width between device and memory in bits. If
+  /// compiler does not support this API then returns default value 64 bits.
+  unsigned int get_memory_bus_width() const { return _memory_bus_width; }
+  uint32_t get_device_id() const { return _device_id; }
+  std::array<unsigned char, 16> get_uuid() const { return _uuid; }
+  /// Returns global memory cache size in bytes.
+  unsigned int get_global_mem_cache_size() const {
+    return _global_mem_cache_size;
+  }
+
+  // set interface
+  void set_name(const char* name) {
+    size_t length = strlen(name);
+    if (length < 256) {
+      std::memcpy(_name, name, length + 1);
+    } else {
+      std::memcpy(_name, name, 255);
+      _name[255] = '\0';
+    }
+  }
+  void set_max_work_item_sizes(const sycl::range<3> max_work_item_sizes) {
+    for (int i = 0; i < 3; ++i)
+      _max_work_item_sizes_i[i] = max_work_item_sizes[i];
+  }
+  [[deprecated]] void
+  set_max_work_item_sizes(const sycl::id<3> max_work_item_sizes) {
+    for (int i = 0; i < 3; ++i) {
+      _max_work_item_sizes_i[i] = max_work_item_sizes[i];
+    }
+  }
+  void set_host_unified_memory(bool host_unified_memory) {
+    _host_unified_memory = host_unified_memory;
+  }
+  void set_major_version(int major) { _major = major; }
+  void set_minor_version(int minor) { _minor = minor; }
+  void set_integrated(int integrated) { _integrated = integrated; }
+  void set_max_clock_frequency(int frequency) { _frequency = frequency; }
+  void set_max_compute_units(int max_compute_units) {
+    _max_compute_units = max_compute_units;
+  }
+  void set_global_mem_size(size_t global_mem_size) {
+    _global_mem_size = global_mem_size;
+  }
+  void set_local_mem_size(size_t local_mem_size) {
+    _local_mem_size = local_mem_size;
+  }
+  void set_max_work_group_size(int max_work_group_size) {
+    _max_work_group_size = max_work_group_size;
+  }
+  void set_max_sub_group_size(int max_sub_group_size) {
+    _max_sub_group_size = max_sub_group_size;
+  }
+  void
+  set_max_work_items_per_compute_unit(int max_work_items_per_compute_unit) {
+    _max_work_items_per_compute_unit = max_work_items_per_compute_unit;
+  }
+  void set_max_nd_range_size(int max_nd_range_size[]) {
+    for (int i = 0; i < 3; i++) {
+      _max_nd_range_size[i] = max_nd_range_size[i];
+      _max_nd_range_size_i[i] = max_nd_range_size[i];
+    }
+  }
+  void set_memory_clock_rate(unsigned int memory_clock_rate) {
+    _memory_clock_rate = memory_clock_rate;
+  }
+  void set_memory_bus_width(unsigned int memory_bus_width) {
+    _memory_bus_width = memory_bus_width;
+  }
+  void
+  set_max_register_size_per_work_group(int max_register_size_per_work_group) {
+    _max_register_size_per_work_group = max_register_size_per_work_group;
+  }
+  void set_device_id(uint32_t device_id) {
+    _device_id = device_id;
+  }
+  void set_uuid(std::array<unsigned char, 16> uuid) {
+    _uuid = std::move(uuid);
+  }
+  void set_global_mem_cache_size(unsigned int global_mem_cache_size) {
+    _global_mem_cache_size = global_mem_cache_size;
+  }
+
+private:
+  char _name[256];
+  int _max_work_item_sizes_i[3];
+  bool _host_unified_memory = false;
+  int _major;
+  int _minor;
+  int _integrated = 0;
+  int _frequency;
+  // Set estimated value 3200000 kHz as default value.
+  unsigned int _memory_clock_rate = 3200000;
+  // Set estimated value 64 bits as default value.
+  unsigned int _memory_bus_width = 64;
+  unsigned int _global_mem_cache_size;
+  int _max_compute_units;
+  int _max_work_group_size;
+  int _max_sub_group_size;
+  int _max_work_items_per_compute_unit;
+  int _max_register_size_per_work_group;
+  size_t _global_mem_size;
+  size_t _local_mem_size;
+  size_t _max_nd_range_size[3];
+  int _max_nd_range_size_i[3];
+  uint32_t _device_id;
+  std::array<unsigned char, 16> _uuid;
+};
+
+static int get_major_version(const sycl::device &dev) {
+  int major, minor;
+  detail::get_version(dev, major, minor);
+  return major;
+}
+
+static int get_minor_version(const sycl::device &dev) {
+  int major, minor;
+  detail::get_version(dev, major, minor);
+  return minor;
+}
+
+static void get_device_info(device_info &out, const sycl::device &dev) {
+  device_info prop;
+  prop.set_name(dev.get_info<sycl::info::device::name>().c_str());
+
+  int major, minor;
+  detail::get_version(dev, major, minor);
+  prop.set_major_version(major);
+  prop.set_minor_version(minor);
+
+  prop.set_max_work_item_sizes(
+#if (__SYCL_COMPILER_VERSION && __SYCL_COMPILER_VERSION < 20220902)
+      // oneAPI DPC++ compiler older than 2022/09/02, where max_work_item_sizes
+      // is an enum class element
+      dev.get_info<sycl::info::device::max_work_item_sizes>());
+#else
+      // SYCL 2020-conformant code, max_work_item_sizes is a struct templated by
+      // an int
+      dev.get_info<sycl::info::device::max_work_item_sizes<3>>());
+#endif
+  prop.set_host_unified_memory(dev.has(sycl::aspect::usm_host_allocations));
+
+  prop.set_max_clock_frequency(
+      dev.get_info<sycl::info::device::max_clock_frequency>() * 1000);
+
+  prop.set_max_compute_units(
+      dev.get_info<sycl::info::device::max_compute_units>());
+  prop.set_max_work_group_size(
+      dev.get_info<sycl::info::device::max_work_group_size>());
+  prop.set_global_mem_size(dev.get_info<sycl::info::device::global_mem_size>());
+  prop.set_local_mem_size(dev.get_info<sycl::info::device::local_mem_size>());
+
+#if (defined(SYCL_EXT_INTEL_DEVICE_INFO) && SYCL_EXT_INTEL_DEVICE_INFO >= 6)
+  if (dev.has(sycl::aspect::ext_intel_memory_clock_rate)) {
+    unsigned int tmp =
+        dev.get_info<sycl::ext::intel::info::device::memory_clock_rate>();
+    if (tmp != 0)
+      prop.set_memory_clock_rate(1000 * tmp);
+  }
+  if (dev.has(sycl::aspect::ext_intel_memory_bus_width)) {
+    prop.set_memory_bus_width(
+        dev.get_info<sycl::ext::intel::info::device::memory_bus_width>());
+  }
+  if (dev.has(sycl::aspect::ext_intel_device_id)) {
+    prop.set_device_id(
+        dev.get_info<sycl::ext::intel::info::device::device_id>());
+  }
+  if (dev.has(sycl::aspect::ext_intel_device_info_uuid)) {
+    prop.set_uuid(dev.get_info<sycl::ext::intel::info::device::uuid>());
+  }
+#elif defined(_MSC_VER) && !defined(__clang__)
+#pragma message("get_device_info: querying memory_clock_rate and \
+memory_bus_width are not supported by the compiler used. \
+Use 3200000 kHz as memory_clock_rate default value. \
+Use 64 bits as memory_bus_width default value.")
+#else
+#warning "get_device_info: querying memory_clock_rate and \
+memory_bus_width are not supported by the compiler used. \
+Use 3200000 kHz as memory_clock_rate default value. \
+Use 64 bits as memory_bus_width default value."
+#endif
+
+  size_t max_sub_group_size = 1;
+  std::vector<size_t> sub_group_sizes =
+      dev.get_info<sycl::info::device::sub_group_sizes>();
+
+  for (const auto &sub_group_size : sub_group_sizes) {
+    if (max_sub_group_size < sub_group_size)
+      max_sub_group_size = sub_group_size;
+  }
+
+  prop.set_max_sub_group_size(max_sub_group_size);
+
+  prop.set_max_work_items_per_compute_unit(
+      dev.get_info<sycl::info::device::max_work_group_size>());
+  int max_nd_range_size[] = {0x7FFFFFFF, 0x7FFFFFFF, 0x7FFFFFFF};
+  prop.set_max_nd_range_size(max_nd_range_size);
+
+  // Estimates max register size per work group, feel free to update the value
+  // according to device properties.
+  prop.set_max_register_size_per_work_group(65536);
+
+  prop.set_global_mem_cache_size(
+      dev.get_info<sycl::info::device::global_mem_cache_size>());
+  out = prop;
+}
+
+/// dpct device extension
+class device_ext : public sycl::device {
+  typedef std::mutex mutex_type;
+
+public:
+  device_ext() : sycl::device(), _ctx(*this) {}
+  ~device_ext() {
+    std::lock_guard<mutex_type> lock(m_mutex);
+    clear_queues();
+  }
+  device_ext(const sycl::device &base) : sycl::device(base), _ctx(*this) {
+    std::lock_guard<mutex_type> lock(m_mutex);
+    init_queues();
+  }
+
+  int is_native_atomic_supported() { return 0; }
+  int get_major_version() const {
+    return dpct::get_major_version(*this);
+  }
+
+  int get_minor_version() const {
+    return dpct::get_minor_version(*this);
+  }
+
+  int get_max_compute_units() const {
+    return get_device_info().get_max_compute_units();
+  }
+
+  /// Return the maximum clock frequency of this device in KHz.
+  int get_max_clock_frequency() const {
+    return get_device_info().get_max_clock_frequency();
+  }
+
+  int get_integrated() const { return get_device_info().get_integrated(); }
+
+  int get_max_sub_group_size() const {
+    return get_device_info().get_max_sub_group_size();
+  }
+
+  int get_max_register_size_per_work_group() const {
+    return get_device_info().get_max_register_size_per_work_group();
+  }
+
+  int get_max_work_group_size() const {
+    return get_device_info().get_max_work_group_size();
+  }
+
+  int get_mem_base_addr_align() const {
+    return get_info<sycl::info::device::mem_base_addr_align>();
+  }
+
+  size_t get_global_mem_size() const {
+    return get_device_info().get_global_mem_size();
+  }
+
+  /// Get the number of bytes of free and total memory on the SYCL device.
+  /// \param [out] free_memory The number of bytes of free memory on the SYCL device.
+  /// \param [out] total_memory The number of bytes of total memory on the SYCL device.
+  void get_memory_info(size_t &free_memory, size_t &total_memory) {
+#if (defined(__SYCL_COMPILER_VERSION) && __SYCL_COMPILER_VERSION >= 20221105)
+    if (!has(sycl::aspect::ext_intel_free_memory)) {
+      std::cerr << "get_memory_info: ext_intel_free_memory is not supported." << std::endl;
+      free_memory = 0;
+    } else {
+      free_memory = get_info<sycl::ext::intel::info::device::free_memory>();
+    }
+#else
+    std::cerr << "get_memory_info: ext_intel_free_memory is not supported." << std::endl;
+    free_memory = 0;
+#if defined(_MSC_VER) && !defined(__clang__)
+#pragma message("Querying the number of bytes of free memory is not supported")
+#else
+#warning "Querying the number of bytes of free memory is not supported"
+#endif
+#endif
+    total_memory = get_device_info().get_global_mem_size();
+  }
+
+  void get_device_info(device_info &out) const {
+    dpct::get_device_info(out, *this);
+  }
+
+  device_info get_device_info() const {
+    device_info prop;
+    dpct::get_device_info(prop, *this);
+    return prop;
+  }
+
+  void reset() {
+    std::lock_guard<mutex_type> lock(m_mutex);
+    clear_queues();
+    init_queues();
+  }
+
+  sycl::queue &in_order_queue() { return *_q_in_order; }
+
+  sycl::queue &out_of_order_queue() { return *_q_out_of_order; }
+
+  sycl::queue &default_queue() {
+#ifdef DPCT_USM_LEVEL_NONE
+    return out_of_order_queue();
+#else
+    return in_order_queue();
+#endif // DPCT_USM_LEVEL_NONE
+  }
+
+  void queues_wait_and_throw() {
+    std::unique_lock<mutex_type> lock(m_mutex);
+    std::vector<std::shared_ptr<sycl::queue>> current_queues(
+        _queues);
+    lock.unlock();
+    for (const auto &q : current_queues) {
+      q->wait_and_throw();
+    }
+    // Guard the destruct of current_queues to make sure the ref count is safe.
+    lock.lock();
+  }
+
+  sycl::queue *create_queue(bool enable_exception_handler = false) {
+#ifdef DPCT_USM_LEVEL_NONE
+    return create_out_of_order_queue(enable_exception_handler);
+#else
+    return create_in_order_queue(enable_exception_handler);
+#endif // DPCT_USM_LEVEL_NONE
+  }
+
+  sycl::queue *create_in_order_queue(bool enable_exception_handler = false) {
+    std::lock_guard<mutex_type> lock(m_mutex);
+    return create_queue_impl(enable_exception_handler,
+                             sycl::property::queue::in_order());
+  }
+
+  sycl::queue *create_out_of_order_queue(bool enable_exception_handler = false) {
+    std::lock_guard<mutex_type> lock(m_mutex);
+    return create_queue_impl(enable_exception_handler);
+  }
+
+  void destroy_queue(sycl::queue *&queue) {
+    std::lock_guard<mutex_type> lock(m_mutex);
+    _queues.erase(std::remove_if(_queues.begin(), _queues.end(),
+                                  [=](const std::shared_ptr<sycl::queue> &q) -> bool {
+                                    return q.get() == queue;
+                                  }),
+                   _queues.end());
+    queue = nullptr;
+  }
+  void set_saved_queue(sycl::queue* q) {
+    std::lock_guard<mutex_type> lock(m_mutex);
+    _saved_queue = q;
+  }
+  sycl::queue *get_saved_queue() const {
+    std::lock_guard<mutex_type> lock(m_mutex);
+    return _saved_queue;
+  }
+  sycl::context get_context() const { return _ctx; }
+
+private:
+  void clear_queues() {
+    _queues.clear();
+    _q_in_order = _q_out_of_order = _saved_queue = nullptr;
+  }
+
+  void init_queues() {
+    _q_in_order = create_queue_impl(true, sycl::property::queue::in_order());
+    _q_out_of_order = create_queue_impl(true);
+    _saved_queue = &default_queue();
+  }
+
+  /// Caller should acquire resource \p m_mutex before calling this function.
+  template <class... Properties>
+  sycl::queue *create_queue_impl(bool enable_exception_handler,
+                                 Properties... properties) {
+    sycl::async_handler eh = {};
+    if (enable_exception_handler) {
+      eh = exception_handler;
+    }
+    _queues.push_back(std::make_shared<sycl::queue>(
+        _ctx, *this, eh,
+        sycl::property_list(
+#ifdef DPCT_PROFILING_ENABLED
+            sycl::property::queue::enable_profiling(),
+#endif
+            properties...)));
+
+    return _queues.back().get();
+  }
+
+  void get_version(int &major, int &minor) const {
+    detail::get_version(*this, major, minor);
+  }
+  sycl::queue *_q_in_order, *_q_out_of_order;
+  sycl::queue *_saved_queue;
+  sycl::context _ctx;
+  std::vector<std::shared_ptr<sycl::queue>> _queues;
+  mutable mutex_type m_mutex;
+};
+
+static inline unsigned int get_tid() {
+#if defined(__linux__)
+  return syscall(SYS_gettid);
+#elif defined(_WIN64)
+  return GetCurrentThreadId();
+#else
+#error "Only support Windows and Linux."
+#endif
+}
+
+/// device manager
+class dev_mgr {
+public:
+  device_ext &current_device() {
+    unsigned int dev_id=current_device_id();
+    check_id(dev_id);
+    return *_devs[dev_id];
+  }
+  device_ext &cpu_device() const {
+    std::lock_guard<std::recursive_mutex> lock(m_mutex);
+    if (_cpu_device == -1) {
+      throw std::runtime_error("no valid cpu device");
+    } else {
+      return *_devs[_cpu_device];
+    }
+  }
+  device_ext &get_device(unsigned int id) const {
+    std::lock_guard<std::recursive_mutex> lock(m_mutex);
+    check_id(id);
+    return *_devs[id];
+  }
+  unsigned int current_device_id() const {
+   std::lock_guard<std::recursive_mutex> lock(m_mutex);
+   auto it=_thread2dev_map.find(get_tid());
+   if(it != _thread2dev_map.end())
+      return it->second;
+    return DEFAULT_DEVICE_ID;
+  }
+
+/// Select device with a device ID.
+/// \param [in] id The id of the device which can
+/// be obtained through get_device_id(const sycl::device).
+  void select_device(unsigned int id) {
+    std::lock_guard<std::recursive_mutex> lock(m_mutex);
+    check_id(id);
+    _thread2dev_map[get_tid()]=id;
+  }
+  unsigned int device_count() { return _devs.size(); }
+
+  unsigned int get_device_id(const sycl::device &dev) {
+    unsigned int id = 0;
+    for(auto dev_item : _devs) {
+      if (*dev_item == dev) {
+        break;
+      }
+      id++;
+    }
+    return id;
+  }
+
+  template <class DeviceSelector>
+  std::enable_if_t<
+      std::is_invocable_r_v<int, DeviceSelector, const sycl::device &>>
+  select_device(const DeviceSelector &selector = sycl::gpu_selector_v) {
+    sycl::device selected_device = sycl::device(selector);
+    unsigned int selected_device_id = get_device_id(selected_device);
+    select_device(selected_device_id);
+  }
+
+  /// Returns the instance of device manager singleton.
+  static dev_mgr &instance() {
+    static dev_mgr d_m;
+    return d_m;
+  }
+  dev_mgr(const dev_mgr &) = delete;
+  dev_mgr &operator=(const dev_mgr &) = delete;
+  dev_mgr(dev_mgr &&) = delete;
+  dev_mgr &operator=(dev_mgr &&) = delete;
+
+private:
+  mutable std::recursive_mutex m_mutex;
+  dev_mgr() {
+    sycl::device default_device =
+        sycl::device(sycl::default_selector_v);
+    _devs.push_back(std::make_shared<device_ext>(default_device));
+
+    std::vector<sycl::device> sycl_all_devs =
+        sycl::device::get_devices(sycl::info::device_type::all);
+    // Collect other devices except for the default device.
+    if (default_device.is_cpu())
+      _cpu_device = 0;
+    for (auto &dev : sycl_all_devs) {
+      if (dev == default_device) {
+        continue;
+      }
+      _devs.push_back(std::make_shared<device_ext>(dev));
+      if (_cpu_device == -1 && dev.is_cpu()) {
+        _cpu_device = _devs.size() - 1;
+      }
+    }
+  }
+  void check_id(unsigned int id) const {
+    if (id >= _devs.size()) {
+      throw std::runtime_error("invalid device id");
+    }
+  }
+  std::vector<std::shared_ptr<device_ext>> _devs;
+  /// DEFAULT_DEVICE_ID is used, if current_device_id() can not find current
+  /// thread id in _thread2dev_map, which means default device should be used
+  /// for the current thread.
+  const unsigned int DEFAULT_DEVICE_ID = 0;
+  /// thread-id to device-id map.
+  std::map<unsigned int, unsigned int> _thread2dev_map;
+  int _cpu_device = -1;
+};
+
+/// Util function to get the default queue of current selected device depends on
+/// the USM config. Return the default out-of-ordered queue when USM-none is
+/// enabled, otherwise return the default in-ordered queue.
+static inline sycl::queue &get_default_queue() {
+  return dev_mgr::instance().current_device().default_queue();
+}
+
+/// Util function to get the default in-ordered queue of current device in
+/// dpct device manager.
+static inline sycl::queue &get_in_order_queue() {
+  return dev_mgr::instance().current_device().in_order_queue();
+}
+
+/// Util function to get the default out-of-ordered queue of current device in
+/// dpct device manager.
+static inline sycl::queue &get_out_of_order_queue() {
+  return dev_mgr::instance().current_device().out_of_order_queue();
+}
+
+/// Util function to get the id of current device in
+/// dpct device manager.
+static inline unsigned int get_current_device_id() {
+  return dev_mgr::instance().current_device_id();
+}
+
+/// Util function to get the current device.
+static inline device_ext &get_current_device() {
+  return dev_mgr::instance().current_device();
+}
+
+/// Util function to get a device by id.
+static inline device_ext &get_device(unsigned int id) {
+  return dev_mgr::instance().get_device(id);
+}
+
+/// Util function to get the context of the default queue of current
+/// device in dpct device manager.
+static inline sycl::context get_default_context() {
+  return dpct::get_current_device().get_context();
+}
+
+/// Util function to get a CPU device.
+static inline device_ext &cpu_device() {
+  return dev_mgr::instance().cpu_device();
+}
+
+static inline unsigned int select_device(unsigned int id) {
+  dev_mgr::instance().select_device(id);
+  return id;
+}
+
+template <class DeviceSelector>
+static inline std::enable_if_t<
+    std::is_invocable_r_v<int, DeviceSelector, const sycl::device &>>
+select_device(const DeviceSelector &selector = sycl::gpu_selector_v) {
+  dev_mgr::instance().select_device(selector);
+}
+
+static inline unsigned int get_device_id(const sycl::device &dev){
+  return dev_mgr::instance().get_device_id(dev);
+}
+
+/// Util function to check whether a device supports some kinds of sycl::aspect.
+inline void
+has_capability_or_fail(const sycl::device &dev,
+                       const std::initializer_list<sycl::aspect> &props) {
+  for (const auto &it : props) {
+    if (dev.has(it))
+      continue;
+    switch (it) {
+    case sycl::aspect::fp64:
+      throw std::runtime_error("'double' is not supported in '" +
+                               dev.get_info<sycl::info::device::name>() +
+                               "' device");
+      break;
+    case sycl::aspect::fp16:
+      throw std::runtime_error("'half' is not supported in '" +
+                               dev.get_info<sycl::info::device::name>() +
+                               "' device");
+      break;
+    default:
+#define __SYCL_ASPECT(ASPECT, ID)                                              \
+  case sycl::aspect::ASPECT:                                                   \
+    return #ASPECT;
+#define __SYCL_ASPECT_DEPRECATED(ASPECT, ID, MESSAGE) __SYCL_ASPECT(ASPECT, ID)
+#define __SYCL_ASPECT_DEPRECATED_ALIAS(ASPECT, ID, MESSAGE)
+      auto getAspectNameStr = [](sycl::aspect AspectNum) -> std::string {
+        switch (AspectNum) {
+#include <sycl/info/aspects.def>
+#include <sycl/info/aspects_deprecated.def>
+        default:
+          return "unknown aspect";
+        }
+      };
+#undef __SYCL_ASPECT_DEPRECATED_ALIAS
+#undef __SYCL_ASPECT_DEPRECATED
+#undef __SYCL_ASPECT
+      throw std::runtime_error(
+          "'" + getAspectNameStr(it) + "' is not supported in '" +
+          dev.get_info<sycl::info::device::name>() + "' device");
+    }
+    break;
+  }
+}
+} // namespace dpct
+
+#endif // __DPCT_DEVICE_HPP__
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/dnnl_utils.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/dnnl_utils.hpp
new file mode 100644
index 0000000..c9876ba
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/dnnl_utils.hpp
@@ -0,0 +1,4917 @@
+//==---- dnnl_utils.hpp ---------------------------*- C++ -*----------------==//
+//
+// Copyright (C) Intel Corporation
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+// See https://llvm.org/LICENSE.txt for license information.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef __DPCT_DNNL_UTILS_HPP__
+#define __DPCT_DNNL_UTILS_HPP__
+
+#include <oneapi/dpl/algorithm>
+#include <oneapi/dpl/execution>
+#include <oneapi/dpl/numeric>
+#include <oneapi/mkl.hpp>
+#include <oneapi/mkl/rng/device.hpp>
+#include <sycl/sycl.hpp>
+#include <oneapi/dnnl/dnnl.hpp>
+#include <oneapi/dnnl/dnnl_sycl.hpp>
+#include <unordered_map>
+#include <algorithm>
+#include <list>
+
+#include "memory.hpp"
+#include "device.hpp"
+#include "lib_common_utils.hpp"
+
+namespace dpct {
+namespace dnnl {
+/// Get concatenated library version as an integer.
+static inline size_t get_version() {
+  const ::dnnl::version_t *ver = ::dnnl::version();
+  return ver->major * 1000 + ver->minor * 100 + ver->patch;
+}
+class engine_ext;
+typedef oneapi::mkl::rng::philox4x32x10 rng_engine_t;
+/// An enum class representing memory layout. Used by
+/// memory_desc_ext to create a memory with pre-defined layout.
+enum class memory_format_tag { nchw, nhwc, nchw_blocked };
+
+/// An enum class representing RNN data memory layout. Used by
+/// memory_desc_ext to create a memory with pre-defined layout.
+enum class rnn_memory_format_tag { tnc, ntc };
+
+/// A class holding the description of an N-dimensions memory.
+class memory_desc_ext {
+  ::dnnl::memory::desc _desc;
+public:
+  /// Convert dpct::library_data_t to dnnl::memory::data_type.
+  static ::dnnl::memory::data_type to_dnnl_data_type(dpct::library_data_t dt);
+  /// Convert dnnl::memory::data_type to dpct::library_data_t.
+  static dpct::library_data_t
+  to_dpct_library_data_t(::dnnl::memory::data_type dt, unsigned block_size);
+  /// Convert dpct::dnnl::memory_format_tag to dnnl::memory::format_tag.
+  static ::dnnl::memory::format_tag to_dnnl_format_tag(dpct::library_data_t dt,
+                                                       memory_format_tag tag);
+  memory_desc_ext() = default;
+  memory_desc_ext(::dnnl::memory::desc &desc) : _desc(desc) {}
+  memory_desc_ext(::dnnl::memory::desc &&desc) : _desc(std::move(desc)) {}
+  /// Setting a 4D memory with given parameters.
+  /// \param [in] tag Format tag.
+  /// \param [in] dt Data type.
+  /// \param [in] n Number of images.
+  /// \param [in] c Number of channels.
+  /// \param [in] h Height of images.
+  /// \param [in] w Width of images.
+  void set(memory_format_tag tag, dpct::library_data_t dt, int n, int c, int h,
+           int w);
+  /// Setting a 3D RNN data memory with given parameters.
+  /// \param [in] tag RNN data format tag.
+  /// \param [in] dt Data type.
+  /// \param [in] t Number of sequence length.
+  /// \param [in] n Number of batch.
+  /// \param [in] c Height of input channel.
+  void set(rnn_memory_format_tag tag, dpct::library_data_t dt, int t, int n, int c);
+  /// Setting a 4D memory with given parameters.
+  /// \param [in] dt Data type.
+  /// \param [in] n Number of images.
+  /// \param [in] c Number of channels.
+  /// \param [in] h Height of images.
+  /// \param [in] w Width of images.
+  /// \param [in] n_stride Stride between two continuous images.
+  /// \param [in] c_stride Stride between two continuous channels.
+  /// \param [in] h_stride Stride between two continuous rows.
+  /// \param [in] w_stride Stride between two continuous columns.
+  void set(dpct::library_data_t dt, int n, int c, int h, int w, int n_stride,
+           int c_stride, int h_stride, int w_stride);
+  /// Setting a ND memory with given parameters.
+  /// \param [in] dt Data type.
+  /// \param [in] ndims Dimension of the memory.
+  /// \param [in] dims Array of dimension ndims that contain the size of each
+  /// memory dimension. \param [in] strides Array of dimension ndims that
+  /// contain the stride of each memory dimension.
+  void set(dpct::library_data_t dt, int ndims, const int dims[],
+           const int strides[]);
+  /// Setting a ND memory with given parameters.
+  /// \param [in] tag Format tag.
+  /// \param [in] dt Data type.
+  /// \param [in] ndims Dimension of the memory.
+  /// \param [in] dims Array of dimension ndims that contain the size of each
+  /// memory dimension.
+  void set(memory_format_tag tag, dpct::library_data_t dt, int ndims,
+           const int dims[]);
+  /// Getting a ::dnnl::memory::desc from a memory_desc_ext.
+  /// \returns The ::dnnl::memory::desc.
+  const ::dnnl::memory::desc &get_desc() const { return _desc; }
+  /// Setting holding desc with given dnnl memory descriptor.
+  void set_desc(::dnnl::memory::desc desc) { _desc = desc; }
+  /// Getting a size of a memory_desc_ext in bytes.
+  /// \returns The size.
+  size_t get_size() const { return _desc.get_size(); }
+  /// Getting parameters from a 4D memory.
+  /// \param [out] dt Data type.
+  /// \param [out] n Number of images.
+  /// \param [out] c Number of channels.
+  /// \param [out] h Height of images.
+  /// \param [out] w Width of images.
+  /// \param [out] n_stride Stride between two continuous images.
+  /// \param [out] c_stride Stride between two continuous channels.
+  /// \param [out] h_stride Stride between two continuous rows.
+  /// \param [out] w_stride Stride between two continuous columns.
+  void get(dpct::library_data_t *dt, int *n, int *c, int *h, int *w,
+           int *n_stride, int *c_stride, int *h_stride, int *w_stride) const;
+  /// Getting parameters from a 4D memory.
+  /// \param [out] dt Data type.
+  /// \param [out] tag Format tag.
+  /// \param [out] n Number of images.
+  /// \param [out] c Number of channels.
+  /// \param [out] h Height of images.
+  /// \param [out] w Width of images.
+  void get(dpct::library_data_t *dt, memory_format_tag *tag, int *n, int *c,
+           int *h, int *w) const;
+  /// Getting parameters from a 3D RNN data memory.
+  /// \param [out] dt Data type.
+  /// \param [out] tag RNN data format tag.
+  /// \param [out] t Number of sequence length.
+  /// \param [out] n Number of batch.
+  /// \param [out] c Height of input channel.
+  void get(dpct::library_data_t *dt, rnn_memory_format_tag *tag, int *t, int *n,
+           int *c) const;
+  /// Getting parameters from a ND memory.
+  /// \param [in] requested_ndims Requested number of dimensions to get from a
+  /// given memory descriptor.
+  /// \param [out] dt Data type.
+  /// \param [out] ndims Dimension of the memory.
+  /// \param [out] dims Array of dimension requested_ndims that contain the 
+  /// size of each memory dimension.
+  /// \param [out] strides Array of dimension requested_ndims that contain the
+  /// stride of each memory dimension.
+  void get(int requested_ndims, dpct::library_data_t *dt, int *ndims,
+           int dims[], int strides[]) const;
+  /// Getting parameters from a ND memory.
+  /// \param [in] requested_ndims Requested number of dimensions to get from a
+  /// given memory descriptor.
+  /// \param [out] dt Data type.
+  /// \param [out] tag Format tag.
+  /// \param [out] ndims Dimension of the memory.
+  /// \param [out] dims Array of dimension requested_ndims that contain the 
+  /// size of each memory dimension.
+  void get(int requested_ndims, dpct::library_data_t *dt,
+           memory_format_tag *tag, int *ndims, int dims[]) const;
+  /// Getting dims from a ND memory.
+  /// \return The dims.
+  std::vector<int64_t> get_dims() const { return _desc.get_dims(); }
+  /// Getting strides from a ND memory.
+  /// \return The strides.
+  std::vector<int64_t> get_strides() const {
+    return _desc.get_strides();
+  }
+  /// Getting element num from a ND memory.
+  /// \return The element number.
+  size_t get_element_num() const {
+    auto dims = _desc.get_dims();
+    if (dims.empty()) {
+      return 0;
+    }
+    size_t result = 1;
+    for (auto &dim : dims) {
+      result *= dim;
+    }
+    return result;
+  }
+
+  operator bool() const {
+    return bool(_desc);
+  }
+
+  memory_desc_ext &operator=(std::nullptr_t) {
+    _desc.reset(nullptr);
+    return *this;
+  }
+};
+
+/// A class holding description for an activation operation.
+class activation_desc {
+  ::dnnl::algorithm _alg;
+  float _alpha;
+  float _beta;
+
+public:
+  /// Setting an activation descriptor with given parameters.
+  /// \param [in] alg Activation algorithm.
+  /// \param [in] alpha Value of alpha parameter.
+  void set(::dnnl::algorithm alg, float alpha) {
+    _alg = alg;
+    if(alg == ::dnnl::algorithm::eltwise_clip) {
+      _alpha = 0;
+      _beta = alpha;
+    } else {
+      _alpha = alpha;
+    }
+  }
+  /// Getting parameters form an activation descriptor.
+  /// \param [out] alg Activation algorithm.
+  /// \param [out] alpha Value of alpha parameter.
+  void get(::dnnl::algorithm *alg, float *alpha) const {
+    *alg = _alg;
+    if(_alg == ::dnnl::algorithm::eltwise_clip) {
+      *alpha = _beta;
+    } else {
+      *alpha = _alpha;
+    }
+  }
+  /// Setting the alpha parameter of an activation descriptor.
+  /// \param [in] alpha Value of alpha parameter.
+  void set_alpha(float alpha) { _alpha = alpha; }
+  /// Setting the beta parameter of an activation descriptor.
+  /// \param [in] beta Value of beta parameter.
+  void set_beta(float beta) { _beta = beta; }
+  /// Setting the algorithm parameter of an activation descriptor.
+  /// \param [in] alg Activation algorithm.
+  void set_algorithm(::dnnl::algorithm alg) { _alg = alg; }
+  /// Getting the alpha parameter from an activation descriptor.
+  /// \param [out] alpha Value of alpha parameter.
+  float get_alpha() const { return _alpha; }
+  /// Getting the beta parameter from an activation descriptor.
+  /// \param [out] beta Value of beta parameter.
+  float get_beta() const { return _beta; }
+  /// Getting the algorithm parameter from an activation descriptor.
+  /// \param [out] alg Activation algorithm.
+  ::dnnl::algorithm get_algorithm() const { return _alg; }
+};
+
+/// A class holding description for a local response normalization operation.
+class lrn_desc {
+  unsigned int _local_size;
+  float _alpha;
+  float _beta;
+  float _k;
+
+public:
+  /// Setting a local response normalization descriptor with given parameters.
+  /// \param [in] local_size Value of local_size parameter.
+  /// \param [in] alpha Value of alpha parameter.
+  /// \param [in] beta Value of beta parameter.
+  /// \param [in] k Value of k parameter.
+  void set(unsigned int local_size, float alpha, float beta, float k) {
+    _local_size = local_size;
+    _alpha = alpha;
+    _beta = beta;
+    _k = k;
+  }
+  /// Getting parameters form a local response normalization descriptor.
+  /// \param [out] local_size Value of local_size parameter.
+  /// \param [out] alpha Value of alpha parameter.
+  /// \param [out] beta Value of beta parameter.
+  /// \param [out] k Value of k parameter.
+  void get(unsigned int *local_size, float *alpha, float *beta,
+           float *k) const {
+    *local_size = _local_size;
+    *alpha = _alpha;
+    *beta = _beta;
+    *k = _k;
+  }
+  /// Setting the local size parameter of a local response normalization
+  /// descriptor.
+  /// \param [in] local_size Value of local_size parameter.
+  void set_local_size(unsigned int local_size) { _local_size = local_size; }
+  /// Setting the alpha parameter of a local response normalization descriptor.
+  /// \param [in] alpha Value of alpha parameter.
+  void set_alpha(float alpha) { _alpha = alpha; }
+  /// Setting the beta parameter of a local response normalization descriptor.
+  /// \param [in] beta Value of beta parameter.
+  void set_beta(float beta) { _beta = beta; }
+  /// Setting the k parameter of a local response normalization descriptor.
+  /// \param [in] k Value of k parameter.
+  void set_k(float k) { _k = k; }
+  /// Getting the local size parameter from a local response normalization
+  /// descriptor.
+  /// \param [out] local_size Value of local_size parameter.
+  unsigned int get_local_size() const { return _local_size; }
+  /// Getting the alpha parameter from a local response normalization
+  /// descriptor.
+  /// \param [out] alpha Value of alpha parameter.
+  float get_alpha() const { return _alpha; }
+  /// Getting the beta parameter from a local response normalization descriptor.
+  /// \param [out] beta Value of beta parameter.
+  float get_beta() const { return _beta; }
+  /// Getting the k parameter from a local response normalization descriptor.
+  /// \param [out] k Value of k parameter.
+  float get_k() const { return _k; }
+};
+
+/// An enum class representing softmax algorithm.
+enum class softmax_algorithm { normal, log };
+/// An enum class representing softmax mode.
+enum class softmax_mode { instance, channel };
+
+/// A class holding description for a pooling operation.
+class pooling_desc {
+  ::dnnl::algorithm _alg;
+  std::vector<int64_t> _stride;
+  std::vector<int64_t> _kernel;
+  std::vector<int64_t> _padding;
+
+public:
+  /// Setting a 2D pooling descriptor with given parameters.
+  /// \param [in] alg Pooling algorithm.
+  /// \param [in] kernel_h Value of height of kernel.
+  /// \param [in] kernel_w Value of width of kernel.
+  /// \param [in] padding_h Value of height of padding.
+  /// \param [in] padding_w Value of width of padding.
+  /// \param [in] stride_h Value of height of stride.
+  /// \param [in] stride_w Value of width of stride.
+  void set(::dnnl::algorithm alg, int kernel_h, int kernel_w, int padding_h,
+           int padding_w, int stride_h, int stride_w) {
+    _alg = alg;
+    _stride = {stride_h, stride_w};
+    _kernel = {kernel_h, kernel_w};
+    _padding = {padding_h, padding_w};
+  }
+  /// Setting a ND pooling descriptor with given parameters.
+  /// \param [in] alg Pooling algorithm.
+  /// \param [in] ndims Dimension of the pooling operation.
+  /// \param [in] kernel Array of dimension ndims containing the kernel size of
+  /// each dimension.
+  /// \param [in] padding Array of dimension ndims containing the padding size of
+  /// each dimension.
+  /// \param [in] stride Array of dimension ndims containing the stride size of
+  /// each dimension.
+  void set(::dnnl::algorithm alg, int ndims, int kernel[], int padding[],
+           int stride[]) {
+    _alg = alg;
+    _stride = std::vector<int64_t>(stride, stride + ndims);
+    _kernel = std::vector<int64_t>(kernel, kernel + ndims);
+    _padding = std::vector<int64_t>(padding, padding + ndims);
+  }
+  /// Getting parameters from a 2D pooling descriptor.
+  /// \param [out] alg Pooling algorithm.
+  /// \param [out] kernel_h Value of height of kernel.
+  /// \param [out] kernel_w Value of width of kernel.
+  /// \param [out] padding_h Value of height of padding.
+  /// \param [out] padding_w Value of width of padding.
+  /// \param [out] stride_h Value of height of stride.
+  /// \param [out] stride_w Value of width of stride.
+  void get(::dnnl::algorithm *alg, int *kernel_h, int *kernel_w, int *padding_h,
+           int *padding_w, int *stride_h, int *stride_w) const {
+    *alg = _alg;
+    *kernel_h = _kernel[0];
+    *kernel_w = _kernel[1];
+    *padding_h = _padding[0];
+    *padding_w = _padding[1];
+    *stride_h = _stride[0];
+    *stride_w = _stride[1];
+  }
+  /// Getting parameters from a ND pooling descriptor.
+  /// \param [in] requested_ndims Requested number of dimensions to get from a
+  /// given pooling descriptor.
+  /// \param [out] alg Pooling algorithm.
+  /// \param [out] ndims Dimension of the pooling operation.
+  /// \param [out] kernel Array of dimension ndims containing the kernel size of
+  /// each dimension.
+  /// \param [out] padding Array of dimension ndims containing the padding size
+  /// of each dimension.
+  /// \param [out] stride Array of dimension ndims containing the stride size of
+  /// each dimension.
+  void get(int requested_ndims, ::dnnl::algorithm *alg, int *ndims,
+           int kernel[], int padding[], int stride[]) const {
+    *alg = _alg;
+    *ndims = _stride.size();
+    for (int i = 0; i < requested_ndims; i++) {
+      kernel[i] = _kernel[i];
+      padding[i] = _padding[i];
+      stride[i] = _stride[i];
+    }
+  }
+  /// Setting the algorithm parameter of a pooling descriptor.
+  /// \param [in] alg Pooling algorithm.
+  void set_algorithm(::dnnl::algorithm alg) { _alg = alg; }
+  /// Setting the stride parameter of a pooling descriptor.
+  /// \param [in] stride Array of dimension ndims containing the stride size of
+  /// each dimension.
+  void set_stride(const std::vector<int64_t> &stride) { _stride = stride; }
+  /// Setting the kernel parameter of a pooling descriptor.
+  /// \param [in] kernel Array of dimension ndims containing the kernel size of
+  /// each dimension.
+  void set_kernel(const std::vector<int64_t> &kernel) { _kernel = kernel; }
+  /// Setting the padding parameter of a pooling descriptor.
+  /// \param [in] padding Array of dimension ndims containing the padding size
+  /// of each dimension.
+  void set_padding(const std::vector<int64_t> &padding) { _padding = padding; }
+
+  /// Getting the algorithm parameter from a pooling descriptor.
+  /// \param [out] alg Pooling algorithm.
+  ::dnnl::algorithm get_algorithm() const { return _alg; }
+  /// Getting the stride parameter from a pooling descriptor.
+  /// \returns Array of dimension ndims containing the stride size of each
+  /// dimension.
+  const std::vector<int64_t> &get_stride() const { return _stride; }
+  /// Getting the kernel parameter from a pooling descriptor.
+  /// \returns Array of dimension ndims containing the kernel size of each
+  /// dimension.
+  const std::vector<int64_t> &get_kernel() const { return _kernel; }
+  /// Getting the padding parameter from a pooling descriptor.
+  /// \returns Array of dimension ndims containing the padding size of each
+  /// dimension.
+  const std::vector<int64_t> &get_padding() const { return _padding; }
+  /// Getting the output dimensions of a memory after 2D pooling has been
+  /// applied.
+  /// \param [in] desc Input memory descriptor.
+  /// \param [out] out_n Number of images.
+  /// \param [out] out_c Number of channels.
+  /// \param [out] out_h Height of images.
+  /// \param [out] out_w Width of images.
+  void get_forward_output_dim(const memory_desc_ext &desc, int *out_n,
+                              int *out_c, int *out_h, int *out_w) const {
+    auto dims = desc.get_dims();
+    *out_n = dims[0];
+    *out_c = dims[1];
+    *out_h = 1 + (dims[2] + 2 * _padding[0] - _kernel[0]) / _stride[0];
+    *out_w = 1 + (dims[3] + 2 * _padding[1] - _kernel[1]) / _stride[1];
+  }
+  /// Getting the output dimensions of a memory after ND pooling has been
+  /// applied.
+  /// \param [in] desc Input memory descriptor.
+  /// \param [out] ndims Dimension of the memory.
+  /// \param [out] out_dims Array of dimension requested_ndims that contain
+  /// the size of each memory dimension.
+  void get_forward_output_dim(const memory_desc_ext &desc, int ndims,
+                              int out_dims[]) const {
+    assert(ndims >= 4 && "ndims is at least 4.");
+    auto dims = desc.get_dims();
+    out_dims[0] = dims[0];
+    out_dims[1] = dims[1];
+    for (int i = 2; i < ndims; i++) {
+      out_dims[i] =
+          1 + (dims[i] + 2 * _padding[i - 2] - _kernel[i - 2]) / _stride[i - 2];
+    }
+  }
+};
+
+/// An enum class representing reduction operations.
+enum class reduction_op {
+  max,
+  min,
+  sum,
+  mul,
+  mean,
+  amax,
+  mul_no_zeros,
+  norm1,
+  norm2
+};
+
+/// An enum class representing batch normalization mode.
+enum class batch_normalization_mode { per_activation, spatial };
+
+/// An enum class representing batch normalization operations.
+enum class batch_normalization_ops { none, activation, add_activation };
+
+/// An enum class representing binary operations.
+enum class binary_op { add, sub, mul, div, min, max, sqrt, neg };
+
+/// An struct representing convolution algorithm infomation.
+struct convolution_algorithm_info {
+  ::dnnl::algorithm algo = ::dnnl::algorithm::convolution_auto;
+  int status = 0;
+};
+
+/// A class holding description for a convolution operation.
+class convolution_desc {
+  std::vector<int64_t> _strides;
+  std::vector<int64_t> _dilates;
+  std::vector<int64_t> _paddings;
+  int _group_count = 1;
+  ::dnnl::fpmath_mode _math_mode = ::dnnl::fpmath_mode::strict;
+public:
+  /// Setting a group count to be used in the convolution.
+  /// \param [in] group_count Value of group count.
+  void set_group_count(int group_count) { _group_count = group_count; }
+  /// Getting a group count specified in the given convolution descriptor.
+  /// \returns Value of group count.
+  int get_group_count() { return _group_count; }
+  /// Setting floating point math mode to be used in the convolution.
+  /// \param [in] math_mode Value of math_mode.
+  void set_math_mode(::dnnl::fpmath_mode math_mode) { _math_mode = math_mode; }
+  /// Getting floating point math mode specified in the given convolution descriptor.
+  /// \returns Value of math mode.
+  ::dnnl::fpmath_mode get_math_mode() { return _math_mode; }
+  /// Setting a 2D convolution descriptor with given parameters.
+  /// \param [in] padding_h Value of height of padding.
+  /// \param [in] padding_w Value of width of padding.
+  /// \param [in] stride_h Value of height of stride.
+  /// \param [in] stride_w Value of width of stride.
+  /// \param [in] dilate_h Value of height of dilate.
+  /// \param [in] dilate_w Value of width of dilate.
+  void set(int padding_h, int padding_w, int stride_h, int stride_w,
+           int dilate_h, int dilate_w) {
+    _strides = {stride_h, stride_w};
+    _dilates = {dilate_h - 1, dilate_w - 1};
+    _paddings = {padding_h, padding_w};
+  }
+  /// Setting a ND convolution descriptor with given parameters.
+  /// \param [in] ndims Dimension of the convolution operation.
+  /// \param [in] paddings Array of dimension ndims containing the padding size of
+  /// each dimension.
+  /// \param [in] strides Array of dimension ndims containing the stride size of
+  /// each dimension.
+  /// \param [in] dilates Array of dimension ndims containing the kernel size of
+  /// each dimension.
+  void set(int ndims, int paddings[], int strides[], int dilates[]) {
+    _strides = std::vector<int64_t>(strides, strides + ndims);
+    _paddings = std::vector<int64_t>(paddings, paddings + ndims);
+    _dilates = std::vector<int64_t>(dilates, dilates + ndims);
+    for (auto &dilate : _dilates) {
+      dilate--;
+    }
+  }
+  /// Getting parameters from a 2D convolution descriptor.
+  /// \param [out] padding_h Value of height of padding.
+  /// \param [out] padding_w Value of width of padding.
+  /// \param [out] stride_h Value of height of stride.
+  /// \param [out] stride_w Value of width of stride.
+  /// \param [out] dilate_h Value of height of dilate.
+  /// \param [out] dilate_w Value of width of dilate.
+  void get(int *padding_h, int *padding_w, int *stride_h, int *stride_w,
+           int *dilate_h, int *dilate_w) const {
+    *dilate_h = _dilates[0];
+    *dilate_w = _dilates[1];
+    *padding_h = _paddings[0];
+    *padding_w = _paddings[1];
+    *stride_h = _strides[0];
+    *stride_w = _strides[1];
+  }
+  /// Getting parameters from a ND convolution descriptor.
+  /// \param [in] requested_ndims Requested number of dimensions to get from a
+  /// given convolution descriptor.
+  /// \param [out] ndims Dimension of the pooling operation.
+  /// \param [out] paddings Array of dimension ndims containing the padding size
+  /// of each dimension.
+  /// \param [out] strides Array of dimension ndims containing the stride size of
+  /// each dimension.
+  /// \param [out] dilates Array of dimension ndims containing the dilate size of
+  /// each dimension.
+  void get(int requested_ndims, int *ndims, int paddings[], int strides[],
+           int dilates[]) const {
+    *ndims = _strides.size();
+    for (int i = 0; i < requested_ndims; i++) {
+      dilates[i] = _dilates[i];
+      paddings[i] = _paddings[i];
+      strides[i] = _strides[i];
+    }
+  }
+  /// Getting the stride parameter from a convolution descriptor.
+  /// \returns Array of dimension ndims containing the stride size of each
+  /// dimension.
+  const std::vector<int64_t> &get_stride() const { return _strides; }
+  /// Getting the kernel parameter from a convolution descriptor.
+  /// \returns Array of dimension ndims containing the dilate size of each
+  /// dimension.
+  const std::vector<int64_t> &get_dilate() const { return _dilates; }
+  /// Getting the padding parameter from a convolution descriptor.
+  /// \returns Array of dimension ndims containing the padding size of each
+  /// dimension.
+  const std::vector<int64_t> &get_padding() const { return _paddings; }
+  /// Getting the output dimensions of a memory after 2D convolution has been
+  /// applied.
+  /// \param [in] desc Input memory descriptor.
+  /// \param [in] weight_desc Input weight memory descriptor.
+  /// \param [out] out_n Number of images.
+  /// \param [out] out_c Number of channels.
+  /// \param [out] out_h Height of images.
+  /// \param [out] out_w Width of images.
+  void get_forward_output_dim(const memory_desc_ext &desc,
+                              const memory_desc_ext &weight_desc, int *out_n,
+                              int *out_c, int *out_h, int *out_w) const {
+    auto dims = desc.get_dims();
+    auto weight_dims = weight_desc.get_dims();
+    *out_n = dims[0];
+    *out_c = weight_dims[0];
+    *out_h = 1 + (dims[2] + 2 * _paddings[0] -
+                  (1 + (_dilates[0] * (weight_dims[2] - 1)))) /
+                     _strides[0];
+    *out_w = 1 + (dims[3] + 2 * _paddings[1] -
+                  (1 + (_dilates[1] * (weight_dims[3] - 1)))) /
+                     _strides[1];
+  }
+  /// Getting the output dimensions of a memory after ND convolution has been
+  /// applied.
+  /// \param [in] desc Input memory descriptor.
+  /// \param [in] weight_desc Input weight memory descriptor.
+  /// \param [out] ndims Dimension of the memory.
+  /// \param [out] out_dims Array of dimension requested_ndims that contain
+  /// the size of each memory dimension.
+  void get_forward_output_dim(const memory_desc_ext &desc,
+                              const memory_desc_ext &weight_desc, int ndims,
+                              int out_dims[]) const {
+    assert(ndims >= 4 && "ndims is at least 4.");
+    auto dims = desc.get_dims();
+    auto weight_dims = weight_desc.get_dims();
+    out_dims[0] = dims[0];
+    out_dims[1] = weight_dims[1];
+    for (int i = 2; i < ndims; i++) {
+      out_dims[i] = 1 + (dims[i] + 2 * _paddings[i - 2] -
+                         (1 + (_dilates[i - 2] * (weight_dims[i] - 1)))) /
+                            _strides[i - 2];
+    }
+  }
+
+  convolution_desc &operator=(std::nullptr_t) {
+    return *this = convolution_desc();
+  }
+
+  operator bool() const {
+    return !(_strides.size() == 0
+             && _dilates.size() == 0
+             && _paddings.size() == 0);
+  }
+};
+
+/// An enum class representing rnn mode.
+enum class rnn_mode { vanilla_relu, vanilla_tanh, lstm, gru };
+
+/// An enum class representing rnn bias mode.
+enum class rnn_bias_mode { none, single };
+
+/// An enum class representing rnn direction.
+enum class rnn_direction {unidirectional, bidirectional};
+
+/// A class holding description for a RNN operation.
+class rnn_desc {
+  rnn_mode _mode;
+  rnn_bias_mode _bias_mode;
+  rnn_direction _direction;
+  dpct::library_data_t _dt;
+  int _input_size;
+  int _hidden_size;
+  int _projection_size;
+  int _layer_size;
+
+public:
+  void set(rnn_mode mode, rnn_bias_mode bias_mode, rnn_direction direction,
+           dpct::library_data_t dt, int input_size, int hidden_size,
+           int projection_size, int layer_size) {
+    _mode = mode;
+    _bias_mode = bias_mode;
+    _direction = direction;
+    _input_size = input_size;
+    _hidden_size = hidden_size;
+    _projection_size = projection_size;
+    _layer_size = layer_size;
+    _dt = dt;
+  }
+  void get(rnn_mode *mode, rnn_bias_mode *bias_mode, rnn_direction *direction,
+           dpct::library_data_t *dt, int *input_size, int *hidden_size,
+           int *projection_size, int *layer_size) const {
+    *mode = _mode;
+    *bias_mode = _bias_mode;
+    *direction = _direction;
+    *input_size = _input_size;
+    *hidden_size = _hidden_size;
+    *projection_size = _projection_size;
+    *layer_size = _layer_size;
+    *dt = _dt;
+  }
+};
+
+/// A class holding description for a Dropout operation.
+class dropout_desc {
+  struct dropout_desc_imp {
+    float _p = 0.5f;
+    unsigned long long _seed = 1;
+    void *_state = nullptr;
+    std::vector<std::uint8_t> _host_state;
+    rng_engine_t _rng_engine;
+    dropout_desc_imp() : _rng_engine(dpct::get_default_queue(), 1) {}
+  };
+  std::shared_ptr<dropout_desc_imp> _imp;
+
+  void generate(sycl::queue *q, std::int64_t required_state_size,
+                std::int64_t num, void *buffer) {
+#ifndef __INTEL_MKL__
+    throw std::runtime_error("The oneAPI Math Kernel Library (oneMKL) "
+                             "Interfaces Project does not support this API.");
+#else
+    sycl::event e_gen = oneapi::mkl::rng::generate(
+        oneapi::mkl::rng::bernoulli<std::int32_t>(1.f - _imp->_p),
+        _imp->_rng_engine, num, (std::int32_t *)buffer);
+    sycl::event e_save = q->submit([&](sycl::handler &cgh) {
+      cgh.depends_on(e_gen);
+      cgh.host_task([=] {
+        oneapi::mkl::rng::save_state(_imp->_rng_engine,
+                                     _imp->_host_state.data());
+      });
+    });
+    q->memcpy(_imp->_state, _imp->_host_state.data(), required_state_size,
+              e_save);
+#endif
+  }
+public:
+  operator bool() const {
+    return bool(_imp);
+  }
+  dropout_desc &operator=(std::nullptr_t) {
+    _imp.reset();
+    return *this;
+  }
+  /// Initializing a dropout descriptor.
+  void init(){
+    _imp = std::make_shared<dropout_desc_imp>();
+  }
+  /// Setting a dropout descriptor with given parameters.
+  /// \param [in] engine Engine of the dropout operation.
+  /// \param [in] p Probability of value set to zero.
+  /// \param [in] state Memory that store random generator state.
+  /// \param [in] state_size Required size to store random generator state.
+  /// \param [in] seed Seed to initialize conditions of the generator state.
+  void set(engine_ext &engine, float p, void *state, size_t state_size,
+           unsigned long long seed);
+  /// Getting parameters from a dropout descriptor.
+  /// \param [in] engine Engine of the dropout operation.
+  /// \param [in] p Probability of value set to zero.
+  /// \param [in] state Memory that store random generator state.
+  /// \param [in] seed Seed to initialize conditions of the generator state.
+  void get(float *p, void **states, unsigned long long *seed) const noexcept {
+    *seed = _imp->_seed;
+    *states = _imp->_state;
+    *p = _imp->_p;
+  }
+  /// Getting the probability of value set to zero.
+  /// \returns Probability.
+  float get_probability() const noexcept { return _imp->_p; }
+  /// Restoreing a dropout descriptor from stored state.
+  /// \param [in] engine Engine of the dropout operation.
+  /// \param [in] p Probability of value set to zero.
+  /// \param [in] state Memory that store random generator state.
+  /// \param [in] state_size Required size to store random generator state.
+  /// \param [in] seed Seed to initialize conditions of the generator state.
+  void restore(engine_ext &engine, float p, void *state, size_t state_size,
+               unsigned long long seed);
+  friend class engine_ext;
+};
+
+namespace detail {
+typedef std::string primitive_cache_key_type;
+typedef std::list<primitive_cache_key_type> usage_list_type;
+struct primitive_cache_value_type {
+  ::dnnl::primitive *_primitive;
+  std::unordered_map<int, ::dnnl::memory> *_args;
+  usage_list_type::iterator _usage_it;
+  std::function<void(::dnnl::primitive *)> _destructor;
+  sycl::event _e;
+  sycl::queue _q;
+  primitive_cache_value_type(
+      ::dnnl::primitive *primitive,
+      std::unordered_map<int, ::dnnl::memory> *args,
+      usage_list_type::iterator usage_it,
+      std::function<void(::dnnl::primitive *)> destructor, sycl::event e,
+      sycl::queue q)
+      : _primitive(primitive), _args(args), _usage_it(usage_it),
+        _destructor(destructor), _e(e), _q(q) {}
+};
+struct primitive_and_args {
+  ::dnnl::primitive *primitive;
+  std::unordered_map<int, ::dnnl::memory> *args;
+};
+typedef std::unordered_map<primitive_cache_key_type,
+                           std::shared_ptr<primitive_cache_value_type>>
+    cache_map_type;
+
+// The primitive cache uses LRU replacement policy, and the default cache
+// capacity is 1024.
+class primitive_cache {
+  int _capacity = 1024;
+  usage_list_type usage;
+  cache_map_type cache_map;
+  void touch(cache_map_type::iterator it, sycl::event e = {},
+             bool update_event = false) {
+    if (it->second->_usage_it != usage.begin()) {
+      const primitive_cache_key_type &key = it->first;
+      usage.erase(it->second->_usage_it);
+      usage.push_front(key);
+      it->second->_usage_it = usage.begin();
+    }
+    if (update_event) {
+      it->second->_e = e;
+    }
+  }
+
+public:
+  std::shared_ptr<primitive_cache_value_type>
+  get(const primitive_cache_key_type &key) {
+    auto it = cache_map.find(key);
+    if (it == cache_map.end()) {
+      return nullptr;
+    }
+    touch(it);
+    return it->second;
+  }
+  void put(const primitive_cache_key_type &key, ::dnnl::primitive *value,
+           std::unordered_map<int, ::dnnl::memory> *args,
+           std::function<void(::dnnl::primitive *)> destructor, sycl::event e,
+           sycl::queue *q) {
+    auto it = cache_map.find(key);
+    if (it != cache_map.end()) {
+      touch(it, e, true);
+    } else {
+      if (cache_map.size() == _capacity) {
+        auto v = *(cache_map.find(usage.back())->second);
+        v._q.submit([=](sycl::handler &cgh) {
+          cgh.depends_on(v._e);
+          cgh.host_task([=] {
+            delete v._args;
+            v._destructor(v._primitive);
+          });
+        });
+        cache_map.erase(usage.back());
+        usage.pop_back();
+      }
+      usage.push_front(key);
+      cache_map[key] = std::make_shared<primitive_cache_value_type>(
+          value, args, usage.begin(), destructor, e, *q);
+    }
+  }
+};
+} // namespace detail
+
+/// A class holding the oneDNN engine.
+class engine_ext {
+  struct output_argument_info {
+    float _alpha;
+    float _beta;
+    int _name;
+    memory_desc_ext _desc;
+    void *_data;
+    output_argument_info(float alpha, float beta, int name,
+                         memory_desc_ext desc, void *data)
+        : _alpha(alpha), _beta(beta), _name(name), _desc(desc), _data(data) {}
+    output_argument_info(float alpha, float beta, memory_desc_ext desc,
+                         void *data)
+        : _alpha(alpha), _beta(beta), _name(0), _desc(desc), _data(data) {}
+  };
+  struct buffer_info {
+    size_t capacity = 0;
+    uint8_t *buffer = nullptr;
+    size_t usage = 0;
+    sycl::queue q;
+    sycl::event deps;
+    size_t primitive_depth = 0;
+  };
+  struct internal_resource {
+    std::int64_t random_engine_state_size = -1;
+    buffer_info binfo;
+  };
+  std::shared_ptr<::dnnl::engine> _eng = nullptr;
+  std::shared_ptr<::dnnl::stream> _s = nullptr;
+  sycl::queue *_q = nullptr;
+  unsigned int _engine_id = 0;
+  static thread_local unsigned int _engine_count;
+  static thread_local std::map<void *, ::dnnl::memory> _workspace_map;
+  static thread_local std::map<sycl::queue *,
+                               std::shared_ptr<internal_resource>>
+      _internal_resource_cache;
+  static thread_local detail::primitive_cache _primitive_cache;
+  ::dnnl::memory &get_workspace(void *key) { return _workspace_map[key]; }
+  void insert_workspace(void *key, ::dnnl::memory workspace) {
+    _workspace_map[key] = workspace;
+  }
+  const ::dnnl::stream &get_stream() const { return *_s; }
+  const ::dnnl::engine &get_engine() const { return *_eng; }
+
+  void *allocate(const memory_desc_ext &desc, int count = 1);
+  void *allocate(size_t size);
+  std::shared_ptr<internal_resource> get_internal_resource(sycl::queue *q){
+    auto it = _internal_resource_cache.find(_q);
+    if (it == _internal_resource_cache.end()) {
+      return _internal_resource_cache[_q] = std::make_shared<internal_resource>();
+    }
+    return it->second;
+  }
+  void enter_primitive(size_t request_buffer_size = 0) {
+    auto &info = get_internal_resource(_q)->binfo;
+    if (info.primitive_depth == 0) {
+      info.usage = 0;
+      if (request_buffer_size > info.capacity) {
+        if (info.buffer && (info.capacity != 0)) {
+          auto ainfo = info;
+          ainfo.q.submit([=](sycl::handler &cgh) {
+            cgh.depends_on(ainfo.deps);
+            cgh.host_task([=] { sycl::free(ainfo.buffer, ainfo.q); });
+          });
+        }
+        size_t new_buffer_capacity =
+            std::max(request_buffer_size, info.capacity * 2);
+        info.capacity = new_buffer_capacity;
+        info.buffer = (uint8_t *)sycl::malloc_device(new_buffer_capacity, *_q);
+        info.q = *_q;
+        info.deps = sycl::event();
+      }
+    }
+    info.primitive_depth++;
+  }
+  sycl::event exit_primitive(const sycl::event &e) {
+    auto &info = get_internal_resource(_q)->binfo;
+    info.primitive_depth--;
+    if ((info.primitive_depth == 0) && info.usage) {
+      info.deps = e;
+    }
+    return e;
+  }
+  ::dnnl::memory::desc
+  compress_spatial_dimensions_to_channel(const ::dnnl::memory::desc &desc);
+  ::dnnl::memory::desc
+  get_bn_scale_bias_mean_var_desc(const ::dnnl::memory::desc &desc,
+                                  batch_normalization_mode mode);
+  sycl::event batch_normalization_backward_internal(
+      batch_normalization_mode mode, float epsilon, float alpha_data,
+      const memory_desc_ext &src_desc, void *src,
+      const memory_desc_ext &diff_dst_desc, void *diff_dst, float beta_data,
+      const memory_desc_ext &diff_src_desc, void *diff_src, float alpha_param,
+      const memory_desc_ext &diff_scale_bias_desc, void *scale, void *bias,
+      float beta_param, void *diff_scale, void *diff_bias,
+      const memory_desc_ext &mean_var_desc, void *saved_mean, void *saved_var);
+  sycl::event batch_normalization_forward_internal(
+      bool is_infer, batch_normalization_mode mode, float epsilon, float factor,
+      float alpha, const memory_desc_ext &src_desc, void *src, float beta,
+      const memory_desc_ext &dst_desc, void *dst,
+      const memory_desc_ext &scale_bias_desc, void *scale, void *bias,
+      const memory_desc_ext &mean_var_desc, void *saved_mean, void *saved_var,
+      void *running_mean, void *running_var);
+  ::dnnl::memory::desc
+  transfer_memory_desc_to_channel_major_format(const ::dnnl::memory::desc &desc);
+  ::dnnl::memory::desc
+  bn_reorder_memory_to_channel_major_format(
+      bool is_input, ::dnnl::memory::desc &desc, void *src, void **cache);
+  ::dnnl::memory::desc
+  transfer_memory_desc_to_format_tag_any(const ::dnnl::memory::desc &desc){
+    return ::dnnl::memory::desc(desc.get_dims(), desc.get_data_type(),
+                                ::dnnl::memory::format_tag::any);
+  }
+  void allocate_and_reorder_memory_to_optimal(::dnnl::memory::desc &from_desc,
+                                              void *&from,
+                                              ::dnnl::memory::desc &to_desc,
+                                              void *&to) {
+    if (from_desc != to_desc) {
+      to = allocate(to_desc);
+      async_reorder(1.f, from_desc, from, 0.f, to_desc, to);
+    }
+  }
+  template <typename primitive_type, typename... args_type>
+  std::pair<detail::primitive_cache_key_type, detail::primitive_and_args>
+  create_primitive_args_or_get(args_type &&...args);
+  template <typename primitive_type>
+  typename primitive_type::primitive_desc
+  get_primitive_desc(::dnnl::primitive *p);
+  template <typename primitive_type, typename... args_type>
+  typename primitive_type::primitive_desc
+  create_primitive_desc(args_type &&...args);
+  template <typename T>
+  void generate_cache_key(std::string &key_buffer, const T &arg);
+  template <typename T, typename... args_type>
+  void generate_cache_key(std::string &key_buffer, const T &first_arg,
+                          const args_type &...args);
+  void insert_arg(std::unordered_map<int, ::dnnl::memory> *args, int name,
+                  const ::dnnl::memory::desc &desc, void *data) {
+    auto it = args->find(name);
+    if (it != args->end()) {
+      it->second.set_data_handle(data);
+    } else {
+      args->insert({name, ::dnnl::memory(desc, *_eng, data)});
+    }
+  }
+  void insert_arg(std::unordered_map<int, ::dnnl::memory> *args, int name,
+                  const ::dnnl::memory &mem) {
+    (*args)[name] = mem;
+  }
+  sycl::event execute_rnn_forward_primitive(
+      rnn_mode mode, ::dnnl::prop_kind kind, ::dnnl::rnn_direction direction,
+      rnn_bias_mode bias_mode, ::dnnl::memory::data_type dt,
+      ::dnnl::memory::format_tag tag, int seq_length, int batch_size, int src_c,
+      int dst_c, int layer_size, int direction_num, int hidden_size,
+      int gate_num, int projection_size, std::vector<void *> &data,
+      std::vector<int> &offset, int iter_num, size_t *weight_size = nullptr,
+      size_t *workspace_size = nullptr, size_t *scratchpad_size = nullptr);
+
+  sycl::event rnn_forward_internal(
+      const rnn_desc &desc, ::dnnl::prop_kind kind,
+      const memory_desc_ext &src_desc, void *src,
+      const memory_desc_ext &dst_desc, void *dst,
+      const memory_desc_ext &iter_desc, void *src_iter, void *dst_iter,
+      const memory_desc_ext &iter_c_desc, void *src_iter_c, void *dst_iter_c,
+      size_t weight_size, void *weight, size_t workspace_size, void *workspace,
+      size_t scratchpad_size, void *scratchpad, bool is_get_execution_args,
+      size_t *weight_size_query, size_t *workspace_size_query,
+      size_t *scratchpad_size_query);
+
+  sycl::event execute_rnn_backward_primitive(
+      rnn_mode mode, ::dnnl::rnn_direction direction, rnn_bias_mode bias_mode,
+      ::dnnl::memory::data_type dt, ::dnnl::memory::format_tag tag,
+      int seq_length, int batch_size, int src_c, int dst_c, int layer_size,
+      int direction_num, int hidden_size, int gate_num, int projection_size,
+      std::vector<void *> &data, std::vector<int> &offset, int iter_num);
+  bool
+  scale_parameter_preprocess(const std::vector<output_argument_info> &args);
+  template <typename primitive_type>
+  sycl::event
+  execute_primitive(const std::pair<detail::primitive_cache_key_type,
+                                    detail::primitive_and_args> &primitive,
+                    const std::vector<output_argument_info> &extra_args = {});
+  template <typename T>
+  sycl::event fill_with_type(sycl::queue *q, void *src, const void *value,
+                             size_t size_with_byte) {
+    return q->fill<T>(static_cast<T *>(src), *static_cast<const T *>(value),
+                      size_with_byte / sizeof(T));
+  }
+  template <typename T> struct no_zero_op {
+    T operator()(T e) {
+      if (!e) {
+        return 1;
+      }
+      return e;
+    }
+  };
+  template <typename T>
+  void transform_no_zero_with_type(sycl::queue *q, void *src, void *dst,
+                                   size_t num) {
+    std::transform(oneapi::dpl::execution::make_device_policy(*q),
+                   static_cast<T *>(src), static_cast<T *>(src) + num,
+                   static_cast<T *>(dst), no_zero_op<T>());
+  }
+  void transform_no_zero(const memory_desc_ext &desc, void *src, void *dst);
+  ::dnnl::memory::desc get_group_weight_desc(int group_count,
+                                             const memory_desc_ext &weight_desc);
+  void get_rnn_configuration(const ::dnnl::memory::desc &desc,
+                             rnn_direction direction, rnn_mode mode,
+                             dpct::library_data_t dt, int hidden_size,
+                             ::dnnl::memory::data_type *dnnl_dt,
+                             ::dnnl::memory::format_tag *tag,
+                             int *projection_size, int *output_size,
+                             int *seq_length, int *batch_size,
+                             int *direction_num, int *gate_num);
+public:
+  engine_ext() {}
+  operator bool() const {
+    return bool(_eng) && bool(_s) && bool(_q);
+  }
+  engine_ext &operator=(std::nullptr_t) {
+    _eng = nullptr;
+    _s = nullptr;
+    _q = nullptr;
+    return *this;
+  }
+  /// Creating oneDNN engine.
+  void create_engine() {
+    _q = &dpct::get_current_device().default_queue();
+    _eng = std::make_shared<::dnnl::engine>(::dnnl::sycl_interop::make_engine(
+        dpct::get_current_device(), dpct::get_current_device().get_context()));
+    _s = std::make_shared<::dnnl::stream>(
+        ::dnnl::sycl_interop::make_stream(*_eng, *_q));
+    _engine_id = _engine_count++;
+  }
+  /// Setting the user's SYCL queue for an oneDNN engine.
+  /// \param [in] q Pointer to the SYCL queue.
+  void set_queue(sycl::queue *q) {
+    if (!q) {
+      throw std::runtime_error("set_queue: pointer must not be nullptr.");
+    }
+    if (!_eng) {
+      throw std::runtime_error("set_queue: current engine is invalid.");
+    }
+    if (q->get_context() != ::dnnl::sycl_interop::get_context(*_eng)) {
+      throw std::runtime_error(
+          "set_queue: queue is mismatch with current engine context.");
+    }
+    _q = q;
+    _s = std::make_shared<::dnnl::stream>(
+        ::dnnl::sycl_interop::make_stream(*_eng, *_q));
+  }
+  /// Retrieving the user's SYCL queue set in the oneDNN engine.
+  /// \returns Pointer to the SYCL queue.
+  sycl::queue *get_queue() const { return _q; }
+  /// Setting all elements of a memory to a given value.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] src Pointer to source data.
+  /// \param [in] valuePtr Pointer to a single value.
+  void fill(const memory_desc_ext &src_desc, void *src,
+                   const void *valuePtr);
+  /// Coping the scaled data from a memory to another memory with a different
+  /// description.
+  /// \param [in] alpha Value to scaling factors used to scale the computed
+  /// value.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] src Pointer to source data.
+  /// \param [in] beta Value to scaling factors used to scale the prior value
+  /// in the destination memory.
+  /// \param [in] dst_desc Destination memory descriptor.
+  /// \param [out] dst Pointer to destination data.
+  void reorder(float alpha, const memory_desc_ext &src_desc, void *src,
+                      float beta, const memory_desc_ext &dst_desc, void *dst);
+  /// Scaling all the elements of a memory by a given factor.
+  /// \param [in] alpha Value to scaling factors.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [out] src Pointer to source data.
+  void scale(float alpha, const memory_desc_ext &src_desc, void *src);
+  /// Adding the scaled values of a memory to another memory.
+  /// \param [in] alpha Value to scaling factors used to scale the computed
+  /// value.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] src Pointer to source data.
+  /// \param [in] beta Value to scaling factors used to scale the prior value
+  /// in the destination memory.
+  /// \param [in] dst_desc Destination memory descriptor.
+  /// \param [out] dst Pointer to destination data.
+  void sum(float alpha, const memory_desc_ext &src_desc, void *src,
+                  float beta, const memory_desc_ext &dst_desc, void *dst);
+  /// Computing a specified activation function value.
+  /// \param [in] desc Activation descriptor.
+  /// \param [in] alpha Value to scaling factors used to scale the computed
+  /// value.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] src Pointer to source data.
+  /// \param [in] beta Value to scaling factors used to scale the prior value
+  /// in the destination memory.
+  /// \param [in] dst_desc Destination memory descriptor.
+  /// \param [out] dst Pointer to destination data.
+  void activation_forward(activation_desc &desc, float alpha,
+                                 const memory_desc_ext &src_desc, void *src,
+                                 float beta, const memory_desc_ext &dst_desc,
+                                 void *dst);
+  /// Computing the gradient of a specified activation function.
+  /// \param [in] desc Activation descriptor.
+  /// \param [in] alpha Value to scaling factors used to scale the computed
+  /// value.
+  /// \param [in] dst_desc Destination memory descriptor.
+  /// \param [in] dst Pointer to destination data.
+  /// \param [in] diff_dst_desc Differential destination memory descriptor.
+  /// \param [in] diff_dst Pointer to differential destination data.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] src Pointer to source data.
+  /// \param [in] beta Value to scaling factors used to scale the prior value
+  /// in the differential destination memory.
+  /// \param [in] diff_src_desc Differential source memory descriptor.
+  /// \param [out] diff_src Pointer to differential source data.
+  void
+  activation_backward(activation_desc &desc, float alpha,
+                      const memory_desc_ext &dst_desc, void *dst,
+                      const memory_desc_ext &diff_dst_desc, void *diff_dst,
+                      const memory_desc_ext &src_desc, void *src, float beta,
+                      const memory_desc_ext &diff_src_desc, void *diff_src);
+  /// Computing a specified pooling function value.
+  /// \param [in] desc Pooling descriptor.
+  /// \param [in] alpha Value to scaling factors used to scale the computed
+  /// value.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] src Pointer to source data.
+  /// \param [in] beta Value to scaling factors used to scale the prior value
+  /// in the destination memory.
+  /// \param [in] dst_desc Destination memory descriptor.
+  /// \param [out] dst Pointer to destination data.
+  /// \param [out] workspace Pointer to workspace generated from forward propagation.
+  void pooling_forward(pooling_desc &desc, float alpha,
+                              const memory_desc_ext &src_desc, void *src,
+                              float beta, const memory_desc_ext &dst_desc,
+                              void *dst, ::dnnl::memory *workspace = nullptr);
+  /// Computing the gradient of a specified pooling function.
+  /// \param [in] desc Activation descriptor.
+  /// \param [in] alpha Value to scaling factors used to scale the computed
+  /// value.
+  /// \param [in] dst_desc Destination memory descriptor.
+  /// \param [in] dst Pointer to destination data.
+  /// \param [in] diff_dst_desc Differential destination memory descriptor.
+  /// \param [in] diff_dst Pointer to differential destination data.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] src Pointer to source data.
+  /// \param [in] beta Value to scaling factors used to scale the prior value
+  /// in the differential destination memory.
+  /// \param [in] diff_src_desc Differential source memory descriptor.
+  /// \param [out] diff_src Pointer to differential
+  /// source data. 
+  /// \param [in] workspace Pointer to workspace used for backward
+  /// propagation.
+  void pooling_backward(pooling_desc &desc, float alpha,
+                               const memory_desc_ext &dst_desc, void *dst,
+                               const memory_desc_ext &diff_dst_desc,
+                               void *diff_dst, const memory_desc_ext &src_desc,
+                               void *src, float beta,
+                               const memory_desc_ext &diff_src_desc,
+                               void *diff_src,
+                               ::dnnl::memory *workspace = nullptr);
+  /// Computing a specified softmax function value.
+  /// \param [in] alg Softmax algorithm.
+  /// \param [in] mode Softmax mode.
+  /// \param [in] alpha Value to scaling factors used to scale the computed
+  /// value. 
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] src Pointer to source data. 
+  /// \param [in] beta Value to scaling factors used to scale the prior value
+  /// in the destination memory.
+  /// \param [in] dst_desc Destination memory descriptor.
+  /// \param [out] dst Pointer to destination data.
+  void softmax_forward(softmax_algorithm alg, softmax_mode mode,
+                              float alpha, const memory_desc_ext &src_desc,
+                              void *src, float beta,
+                              const memory_desc_ext &dst_desc, void *dst);
+  /// Computing the gradient of a specified softmax function.
+  /// \param [in] alg Softmax algorithm.
+  /// \param [in] mode Softmax mode.
+  /// \param [in] alpha Value to scaling factors used to scale the computed
+  /// value. 
+  /// \param [in] dst_desc Destination memory descriptor.
+  /// \param [in] dst Pointer to destination data.
+  /// \param [in] diff_dst_desc Differential destination memory descriptor.
+  /// \param [in] diff_dst Pointer to differential destination data.
+  /// \param [in] beta Value to scaling factors used to scale the prior value
+  /// in the differential destination memory.
+  /// \param [in] diff_src_desc Differential source memory descriptor.
+  /// \param [out] diff_src Pointer to differential source data.
+  void softmax_backward(softmax_algorithm alg, softmax_mode mode,
+                               float alpha, const memory_desc_ext &dst_desc,
+                               void *dst, const memory_desc_ext &diff_dst_desc,
+                               void *diff_dst, float beta,
+                               const memory_desc_ext &diff_src_desc,
+                               void *diff_src);
+  /// Computing a specified local response normalization function value.
+  /// \param [in] desc Local response normalization descriptor.
+  /// \param [in] alpha Value to scaling factors used to scale the computed
+  /// value.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] src Pointer to source data.
+  /// \param [in] beta Value to scaling factors used to scale the prior value
+  /// in the destination memory.
+  /// \param [in] dst_desc Destination memory descriptor.
+  /// \param [out] dst Pointer to destination data.
+  /// \param [out] workspace Pointer to workspace generated from forward
+  /// propagation.
+  void lrn_forward(lrn_desc &desc, float alpha,
+                          const memory_desc_ext &src_desc, void *src,
+                          float beta, const memory_desc_ext &dst_desc,
+                          void *dst, ::dnnl::memory *workspace = nullptr);
+  /// Computing the gradient of a specified local response normalization
+  /// function.
+  /// \param [in] desc Local response normalization descriptor.
+  /// \param [in] alpha Value to scaling factors used to scale the computed value.
+  /// \param [in] dst_desc Destination memory descriptor.
+  /// \param [in] dst Pointer to destination data.
+  /// \param [in] diff_dst_desc Differential destination memory descriptor.
+  /// \param [in] diff_dst Pointer to differential destination data.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] src Pointer to source data.
+  /// \param [in] beta Value to scaling factors used to scale the prior value
+  /// in the differential destination memory.
+  /// \param [in] diff_src_desc Differential source memory descriptor.
+  /// \param [out] diff_src Pointer to differential source data.
+  /// \param [in] workspace Pointer to workspace used for backward propagation.
+  void lrn_backward(lrn_desc &desc, float alpha,
+                           const memory_desc_ext &dst_desc, void *dst,
+                           const memory_desc_ext &diff_dst_desc, void *diff_dst,
+                           const memory_desc_ext &src_desc, void *src,
+                           float beta, const memory_desc_ext &diff_src_desc,
+                           void *diff_src, ::dnnl::memory *workspace = nullptr);
+  /// Setting all elements of a memory to a given value asynchronously.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] src Pointer to source data.
+  /// \param [in] valuePtr Pointer to a single value.
+  /// \returns An event representing the fill operations.
+  sycl::event async_fill(const memory_desc_ext &src_desc, void *src,
+                   const void *valuePtr);
+  /// Coping the scaled data from a memory to another memory with a different
+  /// description asynchronously.
+  /// \param [in] alpha Value to scaling factors used to scale the computed
+  /// value.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] src Pointer to source data.
+  /// \param [in] beta Value to scaling factors used to scale the prior value
+  /// in the destination memory.
+  /// \param [in] dst_desc Destination memory descriptor.
+  /// \param [out] dst Pointer to destination data.
+  /// \returns An event representing the reorder operations.
+  sycl::event async_reorder(float alpha, const memory_desc_ext &src_desc, void *src,
+                      float beta, const memory_desc_ext &dst_desc, void *dst);
+  /// Scaling all the elements of a memory by a given factor asynchronously.
+  /// \param [in] alpha Value to scaling factors.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [out] src Pointer to source data.
+  /// \returns An event representing the scale operations.
+  sycl::event async_scale(float alpha, const memory_desc_ext &src_desc, void *src);
+  /// Adding the scaled values of a memory to another memory asynchronously.
+  /// \param [in] alpha Value to scaling factors used to scale the computed
+  /// value.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] src Pointer to source data.
+  /// \param [in] beta Value to scaling factors used to scale the prior value
+  /// in the destination memory.
+  /// \param [in] dst_desc Destination memory descriptor.
+  /// \param [out] dst Pointer to destination data.
+  /// \returns An event representing the sum operations.
+  sycl::event async_sum(float alpha, const memory_desc_ext &src_desc, void *src,
+                  float beta, const memory_desc_ext &dst_desc, void *dst);
+
+  /// Perform specified binary operation asynchronously.
+  /// \param [in] op Specified binary operation.
+  /// \param [in] alpha_0 Value to scaling factors used to scale the src_0
+  /// value.
+  /// \param [in] src_desc_0 Source 0 memory descriptor.
+  /// \param [in] src_0 Pointer to source 0 data.
+  /// \param [in] alpha_1 Value to scaling factors used to scale the src_1
+  /// value.
+  /// \param [in] src_desc_1 Source 1 memory descriptor.
+  /// \param [in] src_1 Pointer to source 1 data.
+  /// \param [in] beta Value to scaling factors used to scale the prior value
+  /// in the destination memory.
+  /// \param [in] dst_desc Destination memory descriptor.
+  /// \param [out] dst Pointer to destination data.
+  /// \returns An event representing the binary operations.
+  sycl::event async_binary(binary_op op, float alpha_0,
+                     const memory_desc_ext &src_desc_0, void *src_0,
+                     float alpha_1, const memory_desc_ext &src_desc_1,
+                     void *src_1, float beta, const memory_desc_ext &dst_desc,
+                     void *dst);
+
+  /// Perform specified binary operation asynchronously.
+  /// \param [in] op Specified reduction operation.
+  /// \param [in] alpha Value to scaling factors used to scale the data
+  /// value.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] src Pointer to source data.
+  /// \param [in] beta Value to scaling factors used to scale the prior value
+  /// in the destination memory.
+  /// \param [in] dst_desc Destination memory descriptor.
+  /// \param [out] dst Pointer to destination data.
+  /// \returns An event representing the reduction operations.
+  sycl::event async_reduction(reduction_op op, float alpha,
+                        const memory_desc_ext &src_desc, void *src, float beta,
+                        const memory_desc_ext &dst_desc, void *dst);
+  /// Computing a specified activation function value asynchronously.
+  /// \param [in] desc Activation descriptor.
+  /// \param [in] alpha Value to scaling factors used to scale the computed
+  /// value.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] src Pointer to source data.
+  /// \param [in] beta Value to scaling factors used to scale the prior value
+  /// in the destination memory.
+  /// \param [in] dst_desc Destination memory descriptor.
+  /// \param [out] dst Pointer to destination data.
+  /// \returns An event representing the activation forward operations.
+  sycl::event async_activation_forward(activation_desc &desc, float alpha,
+                                 const memory_desc_ext &src_desc, void *src,
+                                 float beta, const memory_desc_ext &dst_desc,
+                                 void *dst);
+  /// Computing the gradient of a specified activation function asynchronously.
+  /// \param [in] desc Activation descriptor.
+  /// \param [in] alpha Value to scaling factors used to scale the computed
+  /// value.
+  /// \param [in] dst_desc Destination memory descriptor.
+  /// \param [in] dst Pointer to destination data.
+  /// \param [in] diff_dst_desc Differential destination memory descriptor.
+  /// \param [in] diff_dst Pointer to differential destination data.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] src Pointer to source data.
+  /// \param [in] beta Value to scaling factors used to scale the prior value
+  /// in the differential destination memory.
+  /// \param [in] diff_src_desc Differential source memory descriptor.
+  /// \param [out] diff_src Pointer to differential source data.
+  /// \returns An event representing the activation backward operations.
+  sycl::event
+  async_activation_backward(activation_desc &desc, float alpha,
+                      const memory_desc_ext &dst_desc, void *dst,
+                      const memory_desc_ext &diff_dst_desc, void *diff_dst,
+                      const memory_desc_ext &src_desc, void *src, float beta,
+                      const memory_desc_ext &diff_src_desc, void *diff_src);
+  /// Computing a specified pooling function value asynchronously.
+  /// \param [in] desc Pooling descriptor.
+  /// \param [in] alpha Value to scaling factors used to scale the computed
+  /// value.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] src Pointer to source data.
+  /// \param [in] beta Value to scaling factors used to scale the prior value
+  /// in the destination memory.
+  /// \param [in] dst_desc Destination memory descriptor.
+  /// \param [out] dst Pointer to destination data.
+  /// \param [out] workspace Pointer to workspace generated from forward propagation.
+  /// \returns An event representing the pooling forward operations.
+  sycl::event async_pooling_forward(pooling_desc &desc, float alpha,
+                              const memory_desc_ext &src_desc, void *src,
+                              float beta, const memory_desc_ext &dst_desc,
+                              void *dst, ::dnnl::memory *workspace = nullptr);
+  /// Computing the gradient of a specified pooling function asynchronously.
+  /// \param [in] desc Activation descriptor.
+  /// \param [in] alpha Value to scaling factors used to scale the computed
+  /// value.
+  /// \param [in] dst_desc Destination memory descriptor.
+  /// \param [in] dst Pointer to destination data.
+  /// \param [in] diff_dst_desc Differential destination memory descriptor.
+  /// \param [in] diff_dst Pointer to differential destination data.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] src Pointer to source data.
+  /// \param [in] beta Value to scaling factors used to scale the prior value
+  /// in the differential destination memory.
+  /// \param [in] diff_src_desc Differential source memory descriptor.
+  /// \param [out] diff_src Pointer to differential
+  /// source data. 
+  /// \param [in] workspace Pointer to workspace used for backward
+  /// propagation.
+  /// \returns An event representing the pooling backward operations.
+  sycl::event async_pooling_backward(pooling_desc &desc, float alpha,
+                               const memory_desc_ext &dst_desc, void *dst,
+                               const memory_desc_ext &diff_dst_desc,
+                               void *diff_dst, const memory_desc_ext &src_desc,
+                               void *src, float beta,
+                               const memory_desc_ext &diff_src_desc,
+                               void *diff_src,
+                               ::dnnl::memory *workspace = nullptr);
+  /// Computing a specified softmax function value asynchronously.
+  /// \param [in] alg Softmax algorithm.
+  /// \param [in] mode Softmax mode.
+  /// \param [in] alpha Value to scaling factors used to scale the computed
+  /// value. 
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] src Pointer to source data. 
+  /// \param [in] beta Value to scaling factors used to scale the prior value
+  /// in the destination memory.
+  /// \param [in] dst_desc Destination memory descriptor.
+  /// \param [out] dst Pointer to destination data.
+  /// \returns An event representing the softmax forward operations.
+  sycl::event async_softmax_forward(softmax_algorithm alg, softmax_mode mode,
+                              float alpha, const memory_desc_ext &src_desc,
+                              void *src, float beta,
+                              const memory_desc_ext &dst_desc, void *dst);
+  /// Computing the gradient of a specified softmax function asynchronously.
+  /// \param [in] alg Softmax algorithm.
+  /// \param [in] mode Softmax mode.
+  /// \param [in] alpha Value to scaling factors used to scale the computed
+  /// value. 
+  /// \param [in] dst_desc Destination memory descriptor.
+  /// \param [in] dst Pointer to destination data.
+  /// \param [in] diff_dst_desc Differential destination memory descriptor.
+  /// \param [in] diff_dst Pointer to differential destination data.
+  /// \param [in] beta Value to scaling factors used to scale the prior value
+  /// in the differential destination memory.
+  /// \param [in] diff_src_desc Differential source memory descriptor.
+  /// \param [out] diff_src Pointer to differential source data.
+  /// \returns An event representing the softmax backward operations.
+  sycl::event async_softmax_backward(softmax_algorithm alg, softmax_mode mode,
+                               float alpha, const memory_desc_ext &dst_desc,
+                               void *dst, const memory_desc_ext &diff_dst_desc,
+                               void *diff_dst, float beta,
+                               const memory_desc_ext &diff_src_desc,
+                               void *diff_src);
+  /// Computing a specified local response normalization function value
+  /// asynchronously.
+  /// \param [in] desc Local response normalization descriptor.
+  /// \param [in] alpha Value to scaling factors used to scale the computed
+  /// value.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] src Pointer to source data.
+  /// \param [in] beta Value to scaling factors used to scale the prior value
+  /// in the destination memory.
+  /// \param [in] dst_desc Destination memory descriptor.
+  /// \param [out] dst Pointer to destination data.
+  /// \param [out] workspace Pointer to workspace generated from forward
+  /// propagation.
+  /// \returns An event representing the lrn forward operations.
+  sycl::event async_lrn_forward(lrn_desc &desc, float alpha,
+                          const memory_desc_ext &src_desc, void *src,
+                          float beta, const memory_desc_ext &dst_desc,
+                          void *dst, ::dnnl::memory *workspace = nullptr);
+  /// Computing the gradient of a specified local response normalization
+  /// function asynchronously.
+  /// \param [in] desc Local response normalization descriptor.
+  /// \param [in] alpha Value to scaling factors used to scale the computed value.
+  /// \param [in] dst_desc Destination memory descriptor.
+  /// \param [in] dst Pointer to destination data.
+  /// \param [in] diff_dst_desc Differential destination memory descriptor.
+  /// \param [in] diff_dst Pointer to differential destination data.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] src Pointer to source data.
+  /// \param [in] beta Value to scaling factors used to scale the prior value
+  /// in the differential destination memory.
+  /// \param [in] diff_src_desc Differential source memory descriptor.
+  /// \param [out] diff_src Pointer to differential source data.
+  /// \param [in] workspace Pointer to workspace used for backward propagation.
+  /// \returns An event representing the lrn backward operations.
+  sycl::event async_lrn_backward(lrn_desc &desc, float alpha,
+                           const memory_desc_ext &dst_desc, void *dst,
+                           const memory_desc_ext &diff_dst_desc, void *diff_dst,
+                           const memory_desc_ext &src_desc, void *src,
+                           float beta, const memory_desc_ext &diff_src_desc,
+                           void *diff_src, ::dnnl::memory *workspace = nullptr);
+
+  /// Derives a memory descriptor for the batch normalization scale, bias, mean,
+  /// variance from the source memory descriptor and batch normalization mode.
+  /// \param [out] desc Derived memory descriptor.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] mode Batch normalization mode.
+  static void derive_batch_normalization_memory_desc(memory_desc_ext &desc,
+                                              const memory_desc_ext &src_desc,
+                                              batch_normalization_mode mode);
+
+  /// Derives a memory descriptor for the batch normalization scale, bias, mean,
+  /// variance from the source memory descriptor and batch normalization mode.
+  /// \param [out] scale_bias_desc Derived scale and bias memory descriptor.
+  /// \param [out] mean_var_desc Derived mean and var memory descriptor.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] mode Batch normalization mode.
+  static void derive_batch_normalization_memory_desc(memory_desc_ext &scale_bias_desc,
+                                             memory_desc_ext &mean_var_desc,
+                                             const memory_desc_ext &src_desc,
+                                             batch_normalization_mode mode);
+
+  /// Get the size of workspace that needed by batch normalization. The data stored
+  /// in workspace must be preserved between forward and backward.
+  /// \param [in] ops Batch normalization operation mode. This mode can set to
+  /// perform only batch normalization, or batch normalization followed by
+  /// activation, or batch normalization followed by element-wise addition and
+  /// activation.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \returns Size of workspace.
+  size_t get_batch_normalization_workspace_size(
+    batch_normalization_ops ops, const memory_desc_ext &src_desc);
+
+  /// Computing a specified batch normalization inference stage function value
+  /// asynchronously.
+  /// \param [in] mode Batch normalization mode.
+  /// \param [in] epsilon Epsilon value used in computation.
+  /// \param [in] alpha Value to scaling factors used to scale the computed
+  /// value.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] src Pointer to source data.
+  /// \param [in] beta Value to scaling factors used to scale the prior value
+  /// in the destination memory.
+  /// \param [in] dst_desc Destination memory descriptor.
+  /// \param [out] dst Pointer to destination data.
+  /// \param [in] scale_bias_mean_var_desc Scale, bias, mean, variance memory
+  /// descriptor.
+  /// \param [in] scale Pointer to scale data.
+  /// \param [in] bias Pointer to bias data.
+  /// \param [in] mean Pointer to mean data.
+  /// \param [in] var Pointer to variance data.
+  /// \returns An event representing the batch normalization forward operations.
+  sycl::event async_batch_normalization_forward_inference(
+      batch_normalization_mode mode, float epsilon, float alpha,
+      const memory_desc_ext &src_desc, void *src, float beta,
+      const memory_desc_ext &dst_desc, void *dst,
+      const memory_desc_ext &scale_bias_mean_var_desc, void *scale, void *bias,
+      void *mean, void *var);
+
+  /// Computing a specified batch normalization inference stage function value
+  /// asynchronously.
+  /// \param [in] mode Batch normalization mode.
+  /// \param [in] ops Batch normalization operation mode. This mode can set to
+  /// perform only batch normalization, or batch normalization followed by
+  /// activation, or batch normalization followed by element-wise addition and
+  /// activation.
+  /// \param [in] adesc Activation operation descriptor.
+  /// \param [in] epsilon Epsilon value used in computation.
+  /// \param [in] alpha Value to scaling factors used to scale the computed
+  /// value.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] src Pointer to source data.
+  /// \param [in] beta Value to scaling factors used to scale the prior value
+  /// in the destination memory.
+  /// \param [in] dst_desc Destination memory descriptor.
+  /// \param [out] dst Pointer to destination data.
+  /// \param [in] summand_desc Summand memory descriptor.
+  /// \param [in] summand Pointer to summand data.
+  /// \param [in] scale_bias_desc Scale, bias memory descriptor.
+  /// \param [in] scale Pointer to scale data.
+  /// \param [in] bias Pointer to bias data.
+  /// \param [in] mean_var_desc Mean, variance memory descriptor.
+  /// \param [in] mean Pointer to mean data.
+  /// \param [in] var Pointer to variance data.
+  /// \returns An event representing the batch normalization forward operations.
+  sycl::event async_batch_normalization_forward_inference(
+      batch_normalization_mode mode, batch_normalization_ops ops,
+      activation_desc &adesc, float epsilon, float alpha,
+      const memory_desc_ext &src_desc, void *src, float beta,
+      const memory_desc_ext &dst_desc, void *dst,
+      const memory_desc_ext &summand_desc, void *summand,
+      const memory_desc_ext &scale_bias_desc, void *scale, void *bias,
+      const memory_desc_ext &mean_var_desc, void *mean, void *var);
+
+  /// Computing a specified batch normalization training stage function value
+  /// asynchronously.
+  /// \param [in] mode Batch normalization mode.
+  /// \param [in] epsilon Epsilon value used in computation.
+  /// \param [in] factor Factor value used in running mean and variance
+  /// computation.
+  /// \param [in] alpha Value to scaling factors used to scale the computed
+  /// value.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] src Pointer to source data.
+  /// \param [in] beta Value to scaling factors used to scale the prior value
+  /// in the destination memory.
+  /// \param [in] dst_desc Destination memory descriptor.
+  /// \param [out] dst Pointer to destination data.
+  /// \param [in] scale_bias_mean_var_desc Scale, bias, mean, variance memory
+  /// descriptor.
+  /// \param [in] scale Pointer to scale data.
+  /// \param [in] bias Pointer to bias data.
+  /// \param [out] running_mean Pointer to running mean data.
+  /// \param [out] running_var Pointer to running variance data.
+  /// \param [out] saved_mean Pointer to optional cache to save mean data.
+  /// \param [out] saved_var Pointer to optional cache to save variance data.
+  /// \returns An event representing the batch normalization forward operations.
+  sycl::event async_batch_normalization_forward_training(
+      batch_normalization_mode mode, float epsilon, float factor, float alpha,
+      const memory_desc_ext &src_desc, void *src, float beta,
+      const memory_desc_ext &dst_desc, void *dst,
+      const memory_desc_ext &scale_bias_mean_var_desc, void *scale, void *bias,
+      void *running_mean, void *running_var, void *saved_mean, void *saved_var);
+
+  /// Computing a specified batch normalization training stage function value
+  /// asynchronously.
+  /// \param [in] mode Batch normalization mode.
+  /// \param [in] ops Batch normalization operation mode. This mode can set to
+  /// perform only batch normalization, or batch normalization followed by
+  /// activation, or batch normalization followed by element-wise addition and
+  /// activation.
+  /// \param [in] adesc Activation operation descriptor.
+  /// \param [in] epsilon Epsilon value used in computation.
+  /// \param [in] factor Factor value used in running mean and variance
+  /// computation.
+  /// \param [in] alpha Value to scaling factors used to scale the computed
+  /// value.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] src Pointer to source data.
+  /// \param [in] beta Value to scaling factors used to scale the prior value
+  /// in the destination memory.
+  /// \param [in] dst_desc Destination memory descriptor.
+  /// \param [out] dst Pointer to destination data.
+  /// \param [in] summand_desc Summand memory descriptor.
+  /// \param [in] summand Pointer to summand data.
+  /// \param [in] scale_bias_mean_var_desc Scale, bias, mean, variance memory
+  /// descriptor.
+  /// \param [in] scale Pointer to scale data.
+  /// \param [in] bias Pointer to bias data.
+  /// \param [out] running_mean Pointer to running mean data.
+  /// \param [out] running_var Pointer to running variance data.
+  /// \param [out] saved_mean Pointer to optional cache to save mean data.
+  /// \param [out] saved_var Pointer to optional cache to save variance data.
+  /// \param [in] workspace_size Size of workspace.
+  /// \param [out] workspace Pointer to workspace generated from forward
+  /// propagation.
+  /// \returns An event representing the batch normalization forward operations.
+  sycl::event async_batch_normalization_forward_training(
+      batch_normalization_mode mode, batch_normalization_ops ops,
+      activation_desc &adesc, float epsilon, float factor, float alpha,
+      const memory_desc_ext &src_desc, void *src, float beta,
+      const memory_desc_ext &dst_desc, void *dst,
+      const memory_desc_ext &summand_desc, void *summand,
+      const memory_desc_ext &scale_bias_mean_var_desc, void *scale, void *bias,
+      void *running_mean, void *running_var, void *saved_mean, void *saved_var,
+      size_t workspace_size, void *workspace);
+
+  /// Computing a specified batch normalization training stage function value
+  /// asynchronously.
+  /// \param [in] mode Batch normalization mode.
+  /// \param [in] ops Batch normalization operation mode. This mode can set to
+  /// perform only batch normalization, or batch normalization followed by
+  /// activation, or batch normalization followed by element-wise addition and
+  /// activation.
+  /// \param [in] adesc Activation operation descriptor.
+  /// \param [in] epsilon Epsilon value used in computation.
+  /// \param [in] factor Factor value used in running mean and variance
+  /// computation.
+  /// \param [in] alpha Value to scaling factors used to scale the computed
+  /// value.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] src Pointer to source data.
+  /// \param [in] beta Value to scaling factors used to scale the prior value
+  /// in the destination memory.
+  /// \param [in] dst_desc Destination memory descriptor.
+  /// \param [out] dst Pointer to destination data.
+  /// \param [in] summand_desc Summand memory descriptor.
+  /// \param [in] summand Pointer to summand data.
+  /// \param [in] scale_bias_desc Scale, bias memory descriptor.
+  /// \param [in] scale Pointer to scale data.
+  /// \param [in] bias Pointer to bias data.
+  /// \param [in] mean_var_desc Mean, variance memory descriptor.
+  /// \param [out] running_mean Pointer to running mean data.
+  /// \param [out] running_var Pointer to running variance data.
+  /// \param [out] saved_mean Pointer to optional cache to save mean data.
+  /// \param [out] saved_var Pointer to optional cache to save variance data.
+  /// \param [in] workspace_size Size of workspace.
+  /// \param [out] workspace Pointer to workspace generated from forward
+  /// propagation.
+  /// \returns An event representing the batch normalization forward operations.
+  sycl::event async_batch_normalization_forward_training(
+      batch_normalization_mode mode, batch_normalization_ops ops,
+      activation_desc &adesc, float epsilon, float factor, float alpha,
+      const memory_desc_ext &src_desc, void *src, float beta,
+      const memory_desc_ext &dst_desc, void *dst,
+      const memory_desc_ext &summand_desc, void *summand,
+      const memory_desc_ext &scale_bias_desc, void *scale, void *bias,
+      const memory_desc_ext &mean_var_desc, void *running_mean, void *running_var,
+      void *saved_mean, void *saved_var, size_t workspace_size, void *workspace);
+
+  /// Computing the gradient of a specified batch normalization function asynchronously.
+  /// \param [in] mode Batch normalization mode.
+  /// \param [in] epsilon Epsilon value used in computation.
+  /// \param [in] alpha_data Value to scaling factors used to scale the computed
+  /// data value.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] src Pointer to source data.
+  /// \param [in] diff_dst_desc Differential destination memory descriptor.
+  /// \param [in] diff_dst Pointer to differential destination data.
+  /// \param [in] beta_data Value to scaling factors used to scale the prior value
+  /// in the data memory.
+  /// \param [in] diff_src_desc Differential source memory descriptor.
+  /// \param [out] diff_src Pointer to differential source data.
+  /// \param [in] alpha_param Value to scaling factors used to scale the computed
+  /// parameter value.
+  /// \param [in] diff_scale_bias_mean_var_desc Differential scale, bias, mean,
+  /// variance memory descriptor.
+  /// \param [in] scale Pointer to scale data.
+  /// \param [in] beta_param Value to scaling factors used to scale the prior value
+  /// in the parameter memory.
+  /// \param [in] diff_scale Pointer to differential scale data.
+  /// \param [in] diff_bias Pointer to differential bias data.
+  /// \param [in] saved_mean Pointer to optional cache saved mean data in forward.
+  /// \param [in] saved_var Pointer to optional cache saved variance data in forward.
+  /// \returns An event representing the batch normalization backward operations.
+  sycl::event async_batch_normalization_backward(
+      batch_normalization_mode mode, float epsilon, float alpha_data,
+      const memory_desc_ext &src_desc, void *src,
+      const memory_desc_ext &diff_dst_desc, void *diff_dst, float beta_data,
+      const memory_desc_ext &diff_src_desc, void *diff_src, float alpha_param,
+      const memory_desc_ext &diff_scale_bias_mean_var_desc, void *scale,
+      float beta_param, void *diff_scale, void *diff_bias, void *saved_mean,
+      void *saved_var);
+
+  /// Computing the gradient of a specified batch normalization function
+  /// asynchronously.
+  /// \param [in] mode Batch normalization mode.
+  /// \param [in] ops Batch normalization operation mode. This mode can set to
+  /// perform only batch normalization, or batch normalization followed by
+  /// activation, or batch normalization followed by element-wise addition and
+  /// activation.
+  /// \param [in] adesc Activation operation descriptor.
+  /// \param [in] epsilon Epsilon value used in computation.
+  /// \param [in] alpha_data Value to scaling factors used to scale the computed
+  /// data value.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] src Pointer to source data.
+  /// \param [in] dst_desc Destination memory descriptor.
+  /// \param [in] dst Pointer to destination data.
+  /// \param [in] diff_dst_desc Differential destination memory descriptor.
+  /// \param [in] diff_dst Pointer to differential destination data.
+  /// \param [in] beta_data Value to scaling factors used to scale the prior value
+  /// in the data memory.
+  /// \param [in] diff_src_desc Differential source memory descriptor.
+  /// \param [out] diff_src Pointer to differential source data.
+  /// \param [in] diff_summand_desc Differential summand memory descriptor.
+  /// \param [out] diff_summand Pointer to differential summand data.
+  /// \param [in] alpha_param Value to scaling factors used to scale the computed
+  /// parameter value.
+  /// \param [in] diff_scale_bias_mean_var_desc Differential scale, bias, mean,
+  /// variance memory descriptor.
+  /// \param [in] scale Pointer to scale data.
+  /// \param [in] bias Pointer to bias data.
+  /// \param [in] beta_param Value to scaling factors used to scale the prior value
+  /// in the parameter memory.
+  /// \param [out] diff_scale Pointer to differential scale data.
+  /// \param [out] diff_bias Pointer to differential bias data.
+  /// \param [in] saved_mean Pointer to optional cache saved mean data in forward.
+  /// \param [in] saved_var Pointer to optional cache saved variance data in forward.
+  /// \param [in] workspace_size Size of workspace.
+  /// \param [in] workspace Pointer to workspace used for backward propagation.
+  /// \returns An event representing the batch normalization backward operations.
+  sycl::event async_batch_normalization_backward(
+      batch_normalization_mode mode, batch_normalization_ops ops,
+      activation_desc &adesc, float epsilon, float alpha_data,
+      const memory_desc_ext &src_desc, void *src,
+      const memory_desc_ext &dst_desc, void *dst,
+      const memory_desc_ext &diff_dst_desc, void *diff_dst, float beta_data,
+      const memory_desc_ext &diff_src_desc, void *diff_src,
+      const memory_desc_ext &diff_summand_desc, void *diff_summand,
+      float alpha_param, const memory_desc_ext &diff_scale_bias_mean_var_desc,
+      void *scale, void *bias, float beta_param, void *diff_scale,
+      void *diff_bias, void *saved_mean, void *saved_var,
+      size_t workspace_size, void *workspace);
+
+  /// Computing the gradient of a specified batch normalization function
+  /// asynchronously.
+  /// \param [in] mode Batch normalization mode.
+  /// \param [in] ops Batch normalization operation mode. This mode can set to
+  /// perform only batch normalization, or batch normalization followed by
+  /// activation, or batch normalization followed by element-wise addition and
+  /// activation.
+  /// \param [in] adesc Activation operation descriptor.
+  /// \param [in] epsilon Epsilon value used in computation.
+  /// \param [in] alpha_data Value to scaling factors used to scale the computed
+  /// data value.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] src Pointer to source data.
+  /// \param [in] dst_desc Destination memory descriptor.
+  /// \param [in] dst Pointer to destination data.
+  /// \param [in] diff_dst_desc Differential destination memory descriptor.
+  /// \param [in] diff_dst Pointer to differential destination data.
+  /// \param [in] beta_data Value to scaling factors used to scale the prior value
+  /// in the data memory.
+  /// \param [in] diff_src_desc Differential source memory descriptor.
+  /// \param [out] diff_src Pointer to differential source data.
+  /// \param [in] diff_summand_desc Differential summand memory descriptor.
+  /// \param [out] diff_summand Pointer to differential summand data.
+  /// \param [in] alpha_param Value to scaling factors used to scale the computed
+  /// parameter value.
+  /// \param [in] diff_scale_bias_desc Differential scale, bias memory descriptor.
+  /// \param [in] scale Pointer to scale data.
+  /// \param [in] bias Pointer to bias data.
+  /// \param [in] beta_param Value to scaling factors used to scale the prior value
+  /// in the parameter memory.
+  /// \param [out] diff_scale Pointer to differential scale data.
+  /// \param [out] diff_bias Pointer to differential bias data.
+  /// \param [in] mean_var_desc Differential mean, variance memory descriptor.
+  /// \param [in] saved_mean Pointer to optional cache saved mean data in forward.
+  /// \param [in] saved_var Pointer to optional cache saved variance data in forward.
+  /// \param [in] workspace_size Size of workspace.
+  /// \param [in] workspace Pointer to workspace used for backward propagation.
+  /// \returns An event representing the batch normalization backward operations.
+  sycl::event async_batch_normalization_backward(
+      batch_normalization_mode mode, batch_normalization_ops ops,
+      activation_desc &adesc, float epsilon, float alpha_data,
+      const memory_desc_ext &src_desc, void *src, const memory_desc_ext &dst_desc,
+      void *dst, const memory_desc_ext &diff_dst_desc, void *diff_dst,
+      float beta_data, const memory_desc_ext &diff_src_desc, void *diff_src,
+      const memory_desc_ext &diff_summand_desc, void *diff_summand,
+      float alpha_param, const memory_desc_ext &diff_scale_bias_desc, void *scale,
+      void *bias, float beta_param, void *diff_scale, void *diff_bias,
+      const memory_desc_ext &mean_var_desc, void *saved_mean, void *saved_var,
+      size_t workspace_size, void *workspace);
+
+  /// Computing a specified convolution function value asynchronously.
+  /// \param [in] desc Convolution descriptor.
+  /// \param [in] alg Convolution algorithm.
+  /// \param [in] alpha Value to scaling factors used to scale the computed
+  /// value.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] src Pointer to source data.
+  /// \param [in] weight_desc Weight memory descriptor.
+  /// \param [in] weight Pointer to weight data.
+  /// \param [in] beta Value to scaling factors used to scale the prior value
+  /// in the destination memory.
+  /// \param [in] dst_desc Destination memory descriptor.
+  /// \param [out] dst Pointer to destination data.
+  /// \returns An event representing the convolution forward operations.
+  sycl::event async_convolution_forward(convolution_desc &desc, ::dnnl::algorithm alg,
+                                  float alpha, const memory_desc_ext &src_desc,
+                                  void *src, const memory_desc_ext &weight_desc,
+                                  void *weight, float beta,
+                                  const memory_desc_ext &dst_desc, void *dst);
+
+  /// Computing a specified convolution function value asynchronously.
+  /// \param [in] desc Convolution descriptor.
+  /// \param [in] alg Convolution algorithm.
+  /// \param [in] adesc Activation operation descriptor.
+  /// \param [in] alpha_0 Value to scaling factors used to scale the data
+  /// value.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] src Pointer to source data.
+  /// \param [in] weight_desc Weight memory descriptor.
+  /// \param [in] weight Pointer to weight data.
+  /// \param [in] alpha_1 Value to scaling factors used to scale the summand
+  /// value.
+  /// \param [in] summand_desc Summand memory descriptor.
+  /// \param [in] summand Pointer to summand data.
+  /// \param [in] bias_desc Bias memory descriptor.
+  /// \param [in] bias Pointer to bias data.
+  /// \param [in] dst_desc Destination memory descriptor.
+  /// \param [out] dst Pointer to destination data.
+  /// \returns An event representing the convolution forward operations.
+  sycl::event async_convolution_forward(
+      convolution_desc &desc, ::dnnl::algorithm alg, activation_desc &adesc,
+      float alpha_0, const memory_desc_ext &src_desc, void *src,
+      const memory_desc_ext &weight_desc, void *weight, float alpha_1,
+      const memory_desc_ext &summand_desc, void *summand,
+      const memory_desc_ext &bias_desc, void *bias,
+      const memory_desc_ext &dst_desc, void *dst);
+
+  /// Computing the data gradient of a specified convolution function asynchronously.
+  /// \param [in] desc Convolution descriptor.
+  /// \param [in] alg Convolution algorithm.
+  /// \param [in] alpha Value to scaling factors used to scale the computed
+  /// value.
+  /// \param [in] weight_desc Weight memory descriptor.
+  /// \param [in] weight Pointer to weight data.
+  /// \param [in] diff_dst_desc Differential destination memory descriptor.
+  /// \param [in] diff_dst Pointer to differential destination data.
+  /// \param [in] beta Value to scaling factors used to scale the prior value
+  /// in the destination memory.
+  /// \param [in] diff_src_desc Differential source memory descriptor.
+  /// \param [out] diff_src Pointer to differential source data.
+  /// \returns An event representing the convolution backward data operations.
+  sycl::event async_convolution_backward_data(
+      convolution_desc &desc, ::dnnl::algorithm alg, float alpha,
+      const memory_desc_ext &weight_desc, void *weight,
+      const memory_desc_ext &diff_dst_desc, void *diff_dst, float beta,
+      const memory_desc_ext &diff_src_desc, void *diff_src);
+
+  /// Computing the weight gradient of a specified convolution function
+  /// asynchronously.
+  /// \param [in] desc Convolution descriptor.
+  /// \param [in] alg Convolution algorithm.
+  /// \param [in] alpha Value to scaling factors used to scale the computed
+  /// value.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] src Pointer to source data.
+  /// \param [in] diff_dst_desc Differential destination memory descriptor.
+  /// \param [in] diff_dst Pointer to differential destination data.
+  /// \param [in] beta Value to scaling factors used to scale the prior value
+  /// in the destination memory.
+  /// \param [in] diff_weight_desc Differential weight memory descriptor.
+  /// \param [out] diff_weight Pointer to differential weight data.
+  /// \returns An event representing the convolution backward weight operations.
+  sycl::event async_convolution_backward_weight(
+      convolution_desc &desc, ::dnnl::algorithm alg, float alpha,
+      const memory_desc_ext &src_desc, void *src,
+      const memory_desc_ext &diff_dst_desc, void *diff_dst, float beta,
+      const memory_desc_ext &diff_weight_desc, void *diff_weight);
+
+  /// Computing the bias gradient of a specified convolution function
+  /// asynchronously.
+  /// \param [in] alpha Value to scaling factors used to scale the computed
+  /// value.
+  /// \param [in] diff_dst_desc Differential destination memory descriptor.
+  /// \param [in] diff_dst Pointer to differential destination data.
+  /// \param [in] beta Value to scaling factors used to scale the prior value
+  /// in the destination memory.
+  /// \param [in] diff_bias_desc Differential bias memory descriptor.
+  /// \param [out] diff_bias Pointer to differential bias data.
+  /// \returns An event representing the convolution backward bias operations.
+  sycl::event async_convolution_backward_bias(float alpha,
+                                        const memory_desc_ext &diff_dst_desc,
+                                        void *diff_dst, float beta,
+                                        const memory_desc_ext &diff_bias_desc,
+                                        void *diff_bias);
+
+  /// Getting the required weight space size for specified rnn operation.  
+  /// \param [in] desc RNN descriptor.
+  /// \param [out] weight_space_size Size of required weight space.
+  void rnn_get_weight_space_size(const rnn_desc &desc,
+                                 size_t *weight_space_size);
+
+  /// Getting the required scratchpad size and workspace size for specified rnn operation.  
+  /// \param [in] desc RNN descriptor.
+  /// \param [in] kind Propagation kind.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [out] scratchpad_size Size of required scratchpad.
+  /// \param [out] workspace_size Size of required workspace.
+  void rnn_get_scratchpad_workspace_size(const rnn_desc &desc, ::dnnl::prop_kind kind,
+                              const memory_desc_ext &src_desc,
+                              size_t *scratchpad_size, size_t *workspace_size);
+
+  /// Computing a specified rnn function value asynchronously.
+  /// \param [in] desc RNN descriptor.
+  /// \param [in] kind Propagation kind.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] src Pointer to source data.
+  /// \param [in] dst_desc Destination memory descriptor.
+  /// \param [out] dst Pointer to destination data.
+  /// \param [in] iter_desc Recurrent hidden state data memory descriptor.
+  /// \param [in] src_iter Pointer to input recurrent hidden state data.
+  /// \param [in] dst_iter Pointer to output recurrent hidden state data.
+  /// \param [in] iter_c_desc Recurrent cell state data memory descriptor.
+  /// \param [in] src_c_iter Pointer to input recurrent cell state data.
+  /// \param [in] dst_c_iter Pointer to output recurrent cell state data.
+  /// \param [in] weight_size Size of weight memory.
+  /// \param [in] weight Pointer to weight data.
+  /// \param [in] scratchpad_size Size of scratchpad memory.
+  /// \param [in] scratchpad Pointer to scratchpad data.
+  /// \param [in] workspace_size Size of workspace memory.
+  /// \param [in] workspace Pointer to workspace data.
+  /// \returns An event representing the status of rnn forward operations.
+  sycl::event async_rnn_forward(const rnn_desc &desc, ::dnnl::prop_kind kind,
+                               const memory_desc_ext &src_desc, void *src,
+                               const memory_desc_ext &dst_desc, void *dst,
+                               const memory_desc_ext &iter_desc, void *src_iter,
+                               void *dst_iter,
+                               const memory_desc_ext &iter_c_desc,
+                               void *src_iter_c, void *dst_iter_c,
+                               size_t weight_size, void *weight,
+                               size_t scratchpad_size, void *scratchpad,
+                               size_t workspace_size, void *workspace);
+
+  /// Computing the data and weight gradient of a specified rnn function
+  /// asynchronously.
+  /// \param [in] desc RNN descriptor.
+  /// \param [in] dst_desc Destination memory descriptor.
+  /// \param [in] dst Pointer to destination data.
+  /// \param [in] diff_dst Pointer to differential destination data.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] src Pointer to source data.
+  /// \param [out] diff_src Pointer to differential source data.
+  /// \param [in] iter_desc Recurrent hidden state data memory descriptor.
+  /// \param [in] src_iter Pointer to input recurrent hidden state data.
+  /// \param [in] diff_dst_iter Pointer to differential output recurrent hidden state data.
+  /// \param [out] diff_src_iter Pointer to differential input recurrent hidden state data.
+  /// \param [in] iter_c_desc Recurrent cell state data memory descriptor.
+  /// \param [in] src_c_iter Pointer to input recurrent cell state data.
+  /// \param [in] diff_dst_c_iter Pointer to differential output recurrent cell state data.
+  /// \param [out] diff_src_c_iter Pointer to differential input recurrent cell state data.
+  /// \param [in] weight_size Size of weight memory.
+  /// \param [in] weight Pointer to weight data.
+  /// \param [out] diff_weight Pointer to differential weight data.
+  /// \param [in] scratchpad_size Size of scratchpad memory.
+  /// \param [in] scratchpad Pointer to scratchpad data.
+  /// \param [in] workspace_size Size of workspace memory.
+  /// \param [in] workspace Pointer to workspace data.
+  /// \returns An event representing the status of rnn backward operations.
+  sycl::event async_rnn_backward(
+      const rnn_desc &desc, const memory_desc_ext &dst_desc, void *dst,
+      void *diff_dst, const memory_desc_ext &src_desc, void *src,
+      void *diff_src, const memory_desc_ext &iter_desc, void *src_iter,
+      void *diff_dst_iter, void *diff_src_iter,
+      const memory_desc_ext &iter_c_desc, void *src_iter_c,
+      void *diff_dst_iter_c, void *diff_src_iter_c, size_t weight_size,
+      void *weight, void *diff_weight, size_t scratchpad_size, void *scratchpad,
+      size_t workspace_size, void *workspace);
+
+  /// Getting the required state size for specified dropout operation.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \returns Required size of state.
+  size_t get_dropout_state_size();
+
+  /// Getting the required workspace size for dropout operation.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \returns Required size of workspace.
+  static size_t get_dropout_workspace_size(const memory_desc_ext &src_desc);
+
+  /// Computing a specified dropout function value asynchronously.
+  /// \param [in] desc Dropout descriptor.
+  /// \param [in] src_desc Source memory descriptor.
+  /// \param [in] src Pointer to source data.
+  /// \param [in] dst_desc Destination memory descriptor.
+  /// \param [out] dst Pointer to destination data.
+  /// \param [in] workspace Pointer to workspace data.
+  /// \param [in] workspace_size Size of workspace memory.
+  /// \returns An event representing the dropout forward operations.
+  sycl::event async_dropout_forward(dropout_desc &desc,
+                                    const memory_desc_ext &src_desc, void *src,
+                                    const memory_desc_ext &dst_desc, void *dst,
+                                    void *workspace, size_t workspace_size);
+
+  /// Computing the gradient of a specified dropout function asynchronously.
+  /// \param [in] desc Dropout descriptor.
+  /// \param [in] diff_dst_desc Differential destination memory descriptor.
+  /// \param [in] diff_dst Pointer to differential destination data.
+  /// \param [in] diff_src_desc Differential source memory descriptor.
+  /// \param [out] diff_src Pointer to differential source data.
+  /// \param [in] workspace Pointer to workspace data.
+  /// \param [in] workspace_size Size of workspace memory.
+  /// \returns An event representing the dropout backward operations.
+  sycl::event async_dropout_backward(dropout_desc &desc,
+                                     const memory_desc_ext &diff_dst_desc,
+                                     void *diff_dst,
+                                     const memory_desc_ext &diff_src_desc,
+                                     void *diff_src, void *workspace,
+                                     size_t workspace_size);
+};
+
+inline thread_local unsigned int engine_ext::_engine_count;
+inline thread_local detail::primitive_cache engine_ext::_primitive_cache;
+inline thread_local std::map<void *, ::dnnl::memory> engine_ext::_workspace_map;
+inline thread_local std::map<sycl::queue *,
+                             std::shared_ptr<engine_ext::internal_resource>>
+    engine_ext::_internal_resource_cache;
+
+inline
+void dropout_desc::restore(engine_ext &engine, float p, void *state,
+                                  size_t state_size, unsigned long long seed) {
+#ifndef __INTEL_MKL__
+    throw std::runtime_error("The oneAPI Math Kernel Library (oneMKL) "
+                             "Interfaces Project does not support this API.");
+#else
+  if (state) {
+    std::int64_t required_state_size = engine.get_dropout_state_size();
+    if (state_size < required_state_size) {
+      throw std::runtime_error("restore: state_size less than required state size.");
+    }
+    sycl::queue *q = engine.get_queue();
+    _imp->_p = p;
+    _imp->_seed = seed;
+    _imp->_state = state;
+    _imp->_host_state = std::vector<std::uint8_t>(required_state_size);
+    q->memcpy(_imp->_host_state.data(), _imp->_state, required_state_size).wait();
+    _imp->_rng_engine =
+        oneapi::mkl::rng::load_state<rng_engine_t>(
+            *q, _imp->_host_state.data());
+  }
+#endif
+}
+
+inline
+void dropout_desc::set(engine_ext &engine, float p, void *state,
+                              size_t state_size, unsigned long long seed) {
+#ifndef __INTEL_MKL__
+    throw std::runtime_error("The oneAPI Math Kernel Library (oneMKL) "
+                             "Interfaces Project does not support this API.");
+#else
+  _imp->_p = p;
+  if (state) {
+    std::int64_t required_state_size = engine.get_dropout_state_size();
+    if (state_size < required_state_size) {
+      throw std::runtime_error("set: no sufficient memory to save states.");
+    }
+    sycl::queue *q = engine.get_queue();
+    _imp->_seed = seed;
+    _imp->_state = state;
+    _imp->_host_state = std::vector<std::uint8_t>(required_state_size);
+    _imp->_rng_engine = rng_engine_t(*q, seed);
+    oneapi::mkl::rng::save_state(_imp->_rng_engine, _imp->_host_state.data());
+    q->memcpy(_imp->_state, _imp->_host_state.data(), required_state_size).wait();
+  }
+#endif
+}
+
+inline
+::dnnl::memory::data_type
+memory_desc_ext::to_dnnl_data_type(dpct::library_data_t dt) {
+  using dnnl_dt = ::dnnl::memory::data_type;
+  switch (dt) {
+  case dpct::library_data_t::real_half:
+    return dnnl_dt::f16;
+  case dpct::library_data_t::real_bfloat16:
+    return dnnl_dt::bf16;
+  case dpct::library_data_t::real_float:
+    return dnnl_dt::f32;
+  case dpct::library_data_t::real_int32:
+    return dnnl_dt::s32;
+  case dpct::library_data_t::real_int8:
+    return dnnl_dt::s8;
+  case dpct::library_data_t::real_uint8:
+    return dnnl_dt::u8;
+  case dpct::library_data_t::real_int8_4:
+    return dnnl_dt::s8;
+  case dpct::library_data_t::real_int8_32:
+    return dnnl_dt::s8;
+  case dpct::library_data_t::real_uint8_4:
+    return dnnl_dt::u8;
+  default:
+    throw std::runtime_error("to_dnnl_data_type: unsupported data type.");
+  }
+}
+
+inline
+dpct::library_data_t
+memory_desc_ext::to_dpct_library_data_t(::dnnl::memory::data_type dt,
+                                        unsigned block_size) {
+  using dpct_dt = dpct::library_data_t;
+  using dnnl_dt = ::dnnl::memory::data_type;
+  switch (dt) {
+  case dnnl_dt::f16:
+    return dpct_dt::real_half;
+  case dnnl_dt::bf16:
+    return dpct_dt::real_bfloat16;
+  case dnnl_dt::f32:
+    return dpct_dt::real_float;
+  case dnnl_dt::s32:
+    return dpct_dt::real_int32;
+  case dnnl_dt::s8:
+    if (block_size == 4) {
+      return dpct_dt::real_int8_4;
+    } else if (block_size == 32) {
+      return dpct_dt::real_int8_32;
+    } else {
+      return dpct_dt::real_int8;
+    }
+  case dnnl_dt::u8:
+    if (block_size == 4) {
+      return dpct_dt::real_uint8_4;
+    } else {
+      return dpct_dt::real_uint8;
+    }
+  default:
+    throw std::runtime_error("to_dpct_library_data_t: unsupported data type "
+                             "dnnl::memory::data_type::undef.");
+  }
+}
+
+inline
+::dnnl::memory::format_tag
+memory_desc_ext::to_dnnl_format_tag(dpct::library_data_t dt,
+                                    memory_format_tag tag) {
+  using dpct_dt = dpct::library_data_t;
+  using dpct_tag = memory_format_tag;
+  using dnnl_tag = ::dnnl::memory::format_tag;
+  switch (tag) {
+  case dpct_tag::nchw:
+    return dnnl_tag::nchw;
+  case dpct_tag::nhwc:
+    return dnnl_tag::nhwc;
+  default:
+    if (dt == dpct_dt::real_int8_32) {
+      return dnnl_tag::nChw32c;
+    } else {
+      return dnnl_tag::nChw4c;
+    }
+  }
+}
+
+inline
+void memory_desc_ext::set(memory_format_tag tag, dpct::library_data_t dt, int n,
+                          int c, int h, int w) {
+  _desc = ::dnnl::memory::desc({n, c, h, w}, to_dnnl_data_type(dt),
+                               to_dnnl_format_tag(dt, tag));
+}
+
+inline
+void memory_desc_ext::set(dpct::library_data_t dt, int n, int c, int h, int w,
+                          int n_stride, int c_stride, int h_stride,
+                          int w_stride) {
+  _desc = ::dnnl::memory::desc({n, c, h, w}, to_dnnl_data_type(dt),
+                               {n_stride, c_stride, h_stride, w_stride});
+}
+
+inline
+void memory_desc_ext::set(dpct::library_data_t dt, int ndims, const int dims[],
+                          const int strides[]) {
+  _desc = ::dnnl::memory::desc({dims, dims + ndims}, to_dnnl_data_type(dt),
+                               {strides, strides + ndims});
+}
+
+inline
+void memory_desc_ext::set(memory_format_tag tag, dpct::library_data_t dt,
+                          int ndims, const int dims[]) {
+  _desc = ::dnnl::memory::desc({dims, dims + ndims}, to_dnnl_data_type(dt),
+                               to_dnnl_format_tag(dt, tag));
+}
+
+inline
+void memory_desc_ext::set(rnn_memory_format_tag tag, dpct::library_data_t dt,
+                          int t, int n, int c) {
+  if (tag == rnn_memory_format_tag::tnc) {
+    _desc = ::dnnl::memory::desc({t, n, c}, to_dnnl_data_type(dt),
+                                 ::dnnl::memory::format_tag::tnc);
+  } else if(tag == rnn_memory_format_tag::ntc) {
+    _desc = ::dnnl::memory::desc({t, n, c}, to_dnnl_data_type(dt),
+                                 ::dnnl::memory::format_tag::ntc);
+  } else {
+    throw std::runtime_error("set: unsupported memory format tag.");
+  }
+}
+
+inline
+void memory_desc_ext::get(dpct::library_data_t *dt, int *n, int *c, int *h,
+                          int *w, int *n_stride, int *c_stride, int *h_stride,
+                          int *w_stride) const {
+  unsigned block_size = 1;
+  auto dims = _desc.get_dims();
+  auto inner_blks = _desc.get_inner_blks();
+  auto strides = _desc.get_strides();
+  if (!inner_blks.empty()) {
+    block_size = inner_blks[0];
+  }
+
+  *dt = to_dpct_library_data_t(_desc.get_data_type(), block_size);
+  *n = dims[0];
+  *c = dims[1];
+  *h = dims[2];
+  *w = dims[3];
+  *n_stride = strides[0] / block_size;
+  *c_stride = strides[1] / block_size;
+  *h_stride = strides[2] / block_size;
+  *w_stride = strides[3] / block_size;
+}
+
+inline
+void memory_desc_ext::get(dpct::library_data_t *dt, memory_format_tag *tag,
+                          int *n, int *c, int *h, int *w) const {
+  unsigned block_size = 1;
+  *tag = memory_format_tag::nchw;
+  auto dims = _desc.get_dims();
+  auto strides = _desc.get_strides();
+  auto inner_blks = _desc.get_inner_blks();
+  if (!inner_blks.empty()) {
+    block_size = inner_blks[0];
+    *tag = memory_format_tag::nchw_blocked;
+  }
+  if (strides[1] == 1 && dims[1] != 1) {
+    *tag = memory_format_tag::nhwc;
+  }
+  *dt = to_dpct_library_data_t(_desc.get_data_type(), block_size);
+  *n = dims[0];
+  *c = dims[1];
+  *h = dims[2];
+  *w = dims[3];
+}
+
+inline
+void memory_desc_ext::get(dpct::library_data_t *dt, rnn_memory_format_tag *tag,
+                          int *t, int *n, int *c) const {
+  auto dims = _desc.get_dims();
+  auto strides = _desc.get_strides();
+
+  if (strides[0] >= strides[1]) {
+    *tag = rnn_memory_format_tag::tnc;
+  } else {
+    *tag = rnn_memory_format_tag::ntc;
+  }
+
+  *dt = to_dpct_library_data_t(_desc.get_data_type(), 1);
+  *t = dims[0];
+  *n = dims[1];
+  *c = dims[2];
+}
+
+inline
+void memory_desc_ext::get(int requested_ndims, dpct::library_data_t *dt,
+                          int *ndims, int dims[], int strides[]) const {
+  unsigned block_size = 1;
+  auto inner_blks = _desc.get_inner_blks();
+  auto adims = _desc.get_dims();
+  auto astrides = _desc.get_strides();
+  if (!inner_blks.empty()) {
+    block_size = inner_blks[0];
+  }
+  *dt = to_dpct_library_data_t(_desc.get_data_type(), block_size);
+  *ndims = _desc.get_ndims();
+  for (int index = 0; index < requested_ndims; index++) {
+    dims[index] = adims[index];
+    strides[index] =
+        astrides[index] / block_size;
+  }
+}
+
+inline
+void memory_desc_ext::get(int requested_ndims, dpct::library_data_t *dt,
+                          memory_format_tag *tag, int *ndims,
+                          int dims[]) const {
+  unsigned block_size = 1;
+  *tag = memory_format_tag::nchw;
+  auto inner_blks = _desc.get_inner_blks();
+  auto adims = _desc.get_dims();
+  auto astrides = _desc.get_strides();
+  if (!inner_blks.empty()) {
+    block_size = inner_blks[0];
+    *tag = memory_format_tag::nchw_blocked;
+  }
+  if (astrides[1] == 1 &&
+      adims[1] != 1) {
+    *tag = memory_format_tag::nhwc;
+  }
+  *dt = to_dpct_library_data_t(_desc.get_data_type(), block_size);
+  *ndims = _desc.get_ndims();
+  for (int index = 0; index < requested_ndims; index++) {
+    dims[index] = adims[index];
+  }
+}
+
+inline
+void engine_ext::get_rnn_configuration(const ::dnnl::memory::desc &desc,
+                                       rnn_direction direction, rnn_mode mode,
+                                       dpct::library_data_t dt, int hidden_size,
+                                       ::dnnl::memory::data_type *dnnl_dt,
+                                       ::dnnl::memory::format_tag *tag,
+                                       int *projection_size, int *output_size,
+                                       int *seq_length, int *batch_size,
+                                       int *direction_num, int *gate_num) {
+  if (!desc.is_zero()) {
+    auto dims = desc.get_dims();
+    auto strides = desc.get_strides();
+    if (strides[0] >= strides[1]) {
+      *tag = ::dnnl::memory::format_tag::tnc;
+      *seq_length = dims[0];
+      *batch_size = dims[1];
+    } else {
+      *tag = ::dnnl::memory::format_tag::ntc;
+      *seq_length = dims[1];
+      *batch_size = dims[0];
+    }
+  }
+  if (direction == rnn_direction::bidirectional) {
+    *direction_num = 2;
+  } else {
+    *direction_num = 1;
+  }
+  if (mode == rnn_mode::lstm) {
+    *gate_num = 4;
+  } else if (mode == rnn_mode::gru) {
+    *gate_num = 3;
+  } else {
+    *gate_num = 1;
+  }
+  if (*projection_size != hidden_size) {
+    *output_size = *projection_size;
+  } else {
+    *projection_size = 0;
+    *output_size = hidden_size;
+  }
+  *dnnl_dt = memory_desc_ext::to_dnnl_data_type(dt);
+}
+
+inline
+void *engine_ext::allocate(const memory_desc_ext &data_desc, int count) {
+  return allocate(data_desc.get_size() * count);
+}
+
+inline
+void *engine_ext::allocate(size_t size) {
+  auto &Info = get_internal_resource(_q)->binfo;
+  uint8_t *result = Info.buffer + Info.usage;
+  Info.usage += size;
+  return result;
+}
+
+inline
+void engine_ext::transform_no_zero(const memory_desc_ext &desc, void *src, void *dst) {
+  ::dnnl::memory::data_type dt = desc.get_desc().get_data_type();
+  size_t element_num = desc.get_element_num();
+  switch (dt) {
+  case ::dnnl::memory::data_type::f32:
+    transform_no_zero_with_type<float>(_q, src, dst, element_num);
+    break;
+  case ::dnnl::memory::data_type::f16:
+    transform_no_zero_with_type<sycl::half>(_q, src, dst, element_num);
+    break;
+  case ::dnnl::memory::data_type::s32:
+    transform_no_zero_with_type<int32_t>(_q, src, dst, element_num);
+    break;
+  case ::dnnl::memory::data_type::s8:
+    transform_no_zero_with_type<int8_t>(_q, src, dst, element_num);
+    break;
+  case ::dnnl::memory::data_type::u8:
+    transform_no_zero_with_type<uint8_t>(_q, src, dst, element_num);
+    break;
+  default:
+    throw std::runtime_error("transform_no_zero: unsupported data type.");
+  }
+}
+
+inline
+::dnnl::memory::desc
+engine_ext::get_group_weight_desc(int group_count,
+                                  const memory_desc_ext &weight_desc) {
+  if (group_count == 1) {
+    return weight_desc.get_desc();
+  }
+  auto help_weight_desc = weight_desc.get_desc();
+  int ndims = help_weight_desc.get_ndims();
+  if (!help_weight_desc.get_inner_blks().empty()) {
+    throw std::runtime_error("get_group_weight_desc: group convolution with "
+                             "blocked weight memory unimplemented.");
+  }
+  std::vector<int64_t> new_size;
+  auto old_size = weight_desc.get_dims();
+  new_size.push_back(group_count);
+  new_size.push_back(old_size[0] / group_count);
+  for (int index = 1; index < old_size.size(); index++) {
+    new_size.push_back(old_size[index]);
+  }
+  std::vector<int64_t> strides = help_weight_desc.get_strides();
+  ::dnnl::memory::format_tag tag;
+  bool is_nhwc = (strides[1] == 1 && old_size[1] != 1);
+
+  if (ndims == 4) {
+    if (is_nhwc) {
+      tag = ::dnnl::memory::format_tag::gohwi;
+    } else {
+      tag = ::dnnl::memory::format_tag::goihw;
+    }
+  } else if (ndims == 5) {
+    if (is_nhwc) {
+      tag = ::dnnl::memory::format_tag::godhwi;
+    } else {
+      tag = ::dnnl::memory::format_tag::goidhw;
+    }
+  }
+
+  help_weight_desc =
+      ::dnnl::memory::desc(new_size, weight_desc.get_desc().get_data_type(), tag);
+  return help_weight_desc;
+}
+
+inline
+::dnnl::memory::desc engine_ext::compress_spatial_dimensions_to_channel(
+    const ::dnnl::memory::desc &desc) {
+  int ndims = desc.get_ndims();
+  auto dims = desc.get_dims();
+  auto inner_blks = desc.get_inner_blks();
+  assert(ndims >= 4 && "ndims is at least 4.");
+  std::vector<int64_t> compressed_dims(ndims);
+  compressed_dims[0] = dims[0];
+  compressed_dims[1] = dims[1];
+  for (int index = 2; index < ndims; index++) {
+    compressed_dims[1] = compressed_dims[1] * dims[index];
+    compressed_dims[index] = 1;
+  }
+  if (!inner_blks.empty() && inner_blks[0] == 4) {
+    return ::dnnl::memory::desc(compressed_dims, desc.get_data_type(),
+                                ::dnnl::memory::format_tag::nChw4c);
+  } else if (!inner_blks.empty() && inner_blks[0] == 32) {
+    return ::dnnl::memory::desc(compressed_dims, desc.get_data_type(),
+                                ::dnnl::memory::format_tag::nChw32c);
+  }
+  std::vector<int64_t> strides(ndims, 1);
+  strides[0] = compressed_dims[1];
+
+  return ::dnnl::memory::desc(compressed_dims, desc.get_data_type(), strides);
+}
+
+inline
+::dnnl::memory::desc
+engine_ext::get_bn_scale_bias_mean_var_desc(const ::dnnl::memory::desc &desc,
+                                            batch_normalization_mode mode) {
+  int ndims = desc.get_ndims();
+  auto dims = desc.get_dims();
+  assert(ndims >= 4 && "ndims is at least 4.");
+  int channel_num = 1;
+  if (mode == batch_normalization_mode::spatial) {
+    channel_num = dims[1];
+  } else {
+    for (int index = 1; index < ndims; index++) {
+      channel_num = channel_num * dims[index];
+    }
+  }
+  return ::dnnl::memory::desc({channel_num}, desc.get_data_type(),
+                              ::dnnl::memory::format_tag::a);
+}
+
+inline
+::dnnl::memory::desc engine_ext::transfer_memory_desc_to_channel_major_format(
+    const ::dnnl::memory::desc &desc) {
+  if (!desc.get_inner_blks().empty()) {
+    return desc;
+  }
+  int ndims = desc.get_ndims();
+  auto dims = desc.get_dims();
+  if (ndims == 4) {
+    return ::dnnl::memory::desc(dims, desc.get_data_type(),
+                                ::dnnl::memory::format_tag::nchw);
+  }
+  return ::dnnl::memory::desc(dims, desc.get_data_type(),
+                              ::dnnl::memory::format_tag::ncdhw);
+}
+
+/// If the alpha = 0 and beta = 1, then the destination (dst = alpha * out +
+/// beta * prior_dst) have no change. In this case this function returns true
+/// means the operation can exit directly.
+inline
+bool engine_ext::scale_parameter_preprocess(
+    const std::vector<output_argument_info> &args) {
+  bool direct_exit = true;
+  for (auto &arg : args) {
+    if (arg._alpha == 0.f) {
+      if (arg._beta != 1.f) {
+        async_scale(arg._beta, arg._desc, arg._data);
+      }
+    } else {
+      direct_exit = false;
+    }
+  }
+  return direct_exit;
+}
+
+inline
+void engine_ext::derive_batch_normalization_memory_desc(
+    memory_desc_ext &scale_bias_desc, memory_desc_ext &mean_var_desc,
+    const memory_desc_ext &src_desc, batch_normalization_mode mode) {
+    derive_batch_normalization_memory_desc(scale_bias_desc, src_desc, mode);
+    derive_batch_normalization_memory_desc(mean_var_desc, src_desc, mode);
+}
+
+inline
+void engine_ext::derive_batch_normalization_memory_desc(
+    memory_desc_ext &desc, const memory_desc_ext &src_desc,
+    batch_normalization_mode mode) {
+  int src_ndims = src_desc.get_desc().get_ndims();
+  auto inner_blks = src_desc.get_desc().get_inner_blks();
+  if (src_desc.get_desc().get_ndims() != 4 ||
+      src_desc.get_desc().get_ndims() != 5) {
+    throw std::runtime_error("derive_batch_normalization_memory_desc: only 4d "
+                             "and 5d memory descriptor supported.");
+  }
+  std::vector<int64_t> dims = src_desc.get_dims();
+  dims[0] = 1;
+  if (mode == batch_normalization_mode::spatial) {
+    dims[2] = 1;
+    dims[3] = 1;
+    if (src_ndims == 5) {
+      dims[4] = 1;
+    }
+  }
+  auto data_type = src_desc.get_desc().get_data_type();
+  if (data_type == ::dnnl::memory::data_type::f16) {
+    data_type = ::dnnl::memory::data_type::f32;
+  }
+  if (!inner_blks.empty() && inner_blks[0] == 4) {
+    desc.set_desc(::dnnl::memory::desc(dims, data_type,
+                                       ::dnnl::memory::format_tag::nChw4c));
+  } else if (!inner_blks.empty() && inner_blks[0] == 32) {
+    desc.set_desc(::dnnl::memory::desc(dims, data_type,
+                                       ::dnnl::memory::format_tag::nChw32c));
+  } else {
+    if (src_ndims == 4) {
+      desc.set_desc(::dnnl::memory::desc(dims, data_type,
+                                         ::dnnl::memory::format_tag::nchw));
+    } else {
+      desc.set_desc(::dnnl::memory::desc(dims, data_type,
+                                         ::dnnl::memory::format_tag::ncdhw));
+    }
+  }
+}
+
+template <typename primitive_type>
+sycl::event engine_ext::execute_primitive(
+    const std::pair<detail::primitive_cache_key_type, detail::primitive_and_args>
+        &primitive,
+    const std::vector<output_argument_info> &output_args) {
+  std::vector<void *> caches;
+  int output_arg_num = output_args.size();
+  for (int i = 0; i < output_arg_num; i++) {
+    if (output_args[i]._beta != 0.f) {
+      auto cache = allocate(output_args[i]._desc);
+      caches.push_back(cache);
+      (*primitive.second.args)[output_args[i]._name].set_data_handle(cache);
+    }
+  }
+
+  auto e = ::dnnl::sycl_interop::execute(
+      *(static_cast<primitive_type *>(primitive.second.primitive)), *_s,
+      *primitive.second.args);
+  _primitive_cache.put(
+      primitive.first, primitive.second.primitive, primitive.second.args,
+      [](::dnnl::primitive *p) { delete static_cast<primitive_type *>(p); }, e,
+      _q);
+  int cache_index = 0;
+  for (int i = 0; i < output_arg_num; i++) {
+    if (output_args[i]._beta != 0.f) {
+      e = async_sum(output_args[i]._alpha, output_args[i]._desc,
+                    caches[cache_index++], output_args[i]._beta,
+                    output_args[i]._desc, output_args[i]._data);
+    } else {
+      if (output_args[i]._alpha != 1.f) {
+        e = async_scale(output_args[i]._alpha, output_args[i]._desc,
+                        output_args[i]._data);
+      }
+    }
+  }
+  return e;
+}
+
+inline
+::dnnl::memory::desc engine_ext::bn_reorder_memory_to_channel_major_format(
+    bool is_input, ::dnnl::memory::desc &desc, void *src, void **cache) {
+  ::dnnl::memory::desc result;
+  result = transfer_memory_desc_to_channel_major_format(desc);
+  if ((result != desc) || !src) {
+    *cache = allocate(desc);
+    if (is_input && src) {
+      async_reorder(1.f, desc, src, 0.f, result, *cache);
+    }
+  }
+  return result;
+}
+
+inline
+sycl::event engine_ext::batch_normalization_backward_internal(
+    batch_normalization_mode mode, float epsilon, float alpha_data,
+    const memory_desc_ext &src_desc, void *src,
+    const memory_desc_ext &diff_dst_desc, void *diff_dst, float beta_data,
+    const memory_desc_ext &diff_src_desc, void *diff_src, float alpha_param,
+    const memory_desc_ext &diff_scale_bias_desc, void *scale, void *bias,
+    float beta_param, void *diff_scale, void *diff_bias,
+    const memory_desc_ext &mean_var_desc, void *saved_mean, void *saved_var) {
+  if (scale_parameter_preprocess(
+          {{alpha_data, beta_data, diff_src_desc, diff_src},
+           {alpha_param, beta_param, diff_scale_bias_desc, diff_scale},
+           {alpha_param, beta_param, diff_scale_bias_desc, diff_bias}})) {
+    return sycl::event();
+  }
+
+  void *reordered_src = nullptr, *reordered_diff_dst = nullptr,
+       *reordered_diff_src = nullptr, *reordered_scale = nullptr,
+       *reordered_bias = nullptr, *reordered_diff_scale = nullptr,
+       *reordered_diff_bias = nullptr, *reordered_saved_mean = nullptr,
+       *reordered_saved_var = nullptr;
+
+  ::dnnl::memory::desc help_src_desc = src_desc.get_desc();
+  ::dnnl::memory::desc help_diff_dst_desc = diff_dst_desc.get_desc();
+  ::dnnl::memory::desc help_diff_src_desc = diff_src_desc.get_desc();
+  ::dnnl::memory::desc help_diff_scale_bias_desc =
+      diff_scale_bias_desc.get_desc();
+  ::dnnl::memory::desc help_mean_var_desc = mean_var_desc.get_desc();
+  ::dnnl::memory::desc actual_diff_src_desc = help_diff_src_desc;
+  ::dnnl::memory::desc actual_diff_scale_bias_desc = help_diff_scale_bias_desc;
+  enter_primitive(
+      help_diff_scale_bias_desc.get_size() * 14 + help_src_desc.get_size() * 2 +
+      help_diff_dst_desc.get_size() * 7 + help_diff_src_desc.get_size() * 5 +
+      help_mean_var_desc.get_size() * 13);
+  if (mode == batch_normalization_mode::per_activation) {
+    help_src_desc = bn_reorder_memory_to_channel_major_format(true, help_src_desc, src,
+                                                       &reordered_src);
+    help_diff_dst_desc = bn_reorder_memory_to_channel_major_format(
+        true, help_diff_dst_desc, diff_dst, &reordered_diff_dst);
+    help_diff_src_desc = bn_reorder_memory_to_channel_major_format(
+        false, help_diff_src_desc, diff_src, &reordered_diff_src);
+    actual_diff_src_desc = help_diff_src_desc;
+    help_diff_scale_bias_desc = bn_reorder_memory_to_channel_major_format(
+        true, help_diff_scale_bias_desc, scale, &reordered_scale);
+    actual_diff_scale_bias_desc = help_diff_scale_bias_desc;
+    if (bias) {
+      bn_reorder_memory_to_channel_major_format(true, help_diff_scale_bias_desc, bias,
+                                         &reordered_bias);
+    }
+    bn_reorder_memory_to_channel_major_format(false, help_diff_scale_bias_desc,
+                                       diff_scale, &reordered_diff_scale);
+    bn_reorder_memory_to_channel_major_format(false, help_diff_scale_bias_desc,
+                                       diff_bias, &reordered_diff_bias);
+
+    help_mean_var_desc = bn_reorder_memory_to_channel_major_format(
+        true, help_mean_var_desc, saved_mean, &reordered_saved_mean);
+    bn_reorder_memory_to_channel_major_format(true, help_mean_var_desc, saved_var,
+                                       &reordered_saved_var);
+    help_src_desc = compress_spatial_dimensions_to_channel(help_src_desc);
+    help_diff_src_desc =
+        compress_spatial_dimensions_to_channel(help_diff_src_desc);
+    help_diff_dst_desc =
+        compress_spatial_dimensions_to_channel(help_diff_dst_desc);
+  } else {
+    if ((help_src_desc != help_diff_dst_desc) ||
+        (help_src_desc != help_diff_src_desc) ||
+        (help_diff_dst_desc != help_diff_src_desc)) {
+      help_src_desc = bn_reorder_memory_to_channel_major_format(
+          true, help_src_desc, src, &reordered_src);
+      help_diff_dst_desc = bn_reorder_memory_to_channel_major_format(
+          true, help_diff_dst_desc, diff_dst, &reordered_diff_dst);
+      help_diff_src_desc = bn_reorder_memory_to_channel_major_format(
+          false, help_diff_src_desc, diff_src, &reordered_diff_src);
+      actual_diff_src_desc = help_diff_src_desc;
+    }
+  }
+
+  help_diff_scale_bias_desc =
+      get_bn_scale_bias_mean_var_desc(help_diff_scale_bias_desc, mode);
+  help_mean_var_desc =
+      get_bn_scale_bias_mean_var_desc(help_mean_var_desc, mode);
+
+  auto forward_primitive =
+      create_primitive_desc<::dnnl::batch_normalization_forward>(
+          ::dnnl::prop_kind::forward_training, help_src_desc,
+          help_diff_dst_desc, epsilon,
+          ::dnnl::normalization_flags::use_scale |
+              ::dnnl::normalization_flags::use_shift);
+  auto primitive_args =
+      create_primitive_args_or_get<::dnnl::batch_normalization_backward>(
+          ::dnnl::prop_kind::backward, help_diff_src_desc, help_diff_dst_desc,
+          help_src_desc, epsilon,
+          ::dnnl::normalization_flags::use_scale |
+              ::dnnl::normalization_flags::use_shift, forward_primitive);
+
+  void *dst_cache = nullptr;
+  if (!saved_mean && !saved_var) {
+    dst_cache = allocate(diff_dst_desc);
+    if (!reordered_saved_mean) {
+      reordered_saved_mean = allocate(mean_var_desc);
+    }
+    if (!reordered_saved_var) {
+      reordered_saved_var = allocate(mean_var_desc);
+    }
+    if (!bias) {
+      _q->fill(reordered_bias, 0, diff_scale_bias_desc.get_size());
+    }
+
+    batch_normalization_forward_internal(
+        true, mode, epsilon, 0.f, 1.f, src_desc, src, 0.f, diff_dst_desc,
+        dst_cache, diff_scale_bias_desc, scale, bias ? bias : reordered_bias,
+        mean_var_desc, reordered_saved_mean, reordered_saved_var, nullptr,
+        nullptr);
+  }
+
+  insert_arg(primitive_args.second.args, DNNL_ARG_SRC, help_src_desc,
+             reordered_src ? reordered_src : src);
+  insert_arg(primitive_args.second.args, DNNL_ARG_SCALE,
+             help_diff_scale_bias_desc,
+             reordered_scale ? reordered_scale : scale);
+  insert_arg(primitive_args.second.args, DNNL_ARG_MEAN, help_mean_var_desc,
+             reordered_saved_mean ? reordered_saved_mean : saved_mean);
+  insert_arg(primitive_args.second.args, DNNL_ARG_VARIANCE, help_mean_var_desc,
+             reordered_saved_var ? reordered_saved_var : saved_var);
+  insert_arg(primitive_args.second.args, DNNL_ARG_DIFF_DST, help_diff_src_desc,
+             reordered_diff_dst ? reordered_diff_dst : diff_dst);
+  insert_arg(primitive_args.second.args, DNNL_ARG_DIFF_SRC, help_diff_src_desc,
+             reordered_diff_src ? reordered_diff_src : diff_src);
+  insert_arg(primitive_args.second.args, DNNL_ARG_DIFF_SCALE,
+             help_diff_scale_bias_desc,
+             reordered_diff_scale ? reordered_diff_scale : diff_scale);
+  insert_arg(primitive_args.second.args, DNNL_ARG_DIFF_SHIFT,
+             help_diff_scale_bias_desc,
+             reordered_diff_bias ? reordered_diff_bias : diff_bias);
+
+  sycl::event e = execute_primitive<::dnnl::batch_normalization_backward>(
+      primitive_args,
+      {{alpha_data, beta_data, DNNL_ARG_DIFF_SRC, help_diff_src_desc,
+        reordered_diff_src ? reordered_diff_src : diff_src},
+       {alpha_param, beta_param, DNNL_ARG_DIFF_SCALE, help_diff_scale_bias_desc,
+        reordered_diff_scale ? reordered_diff_scale : diff_scale},
+       {alpha_param, beta_param, DNNL_ARG_DIFF_SHIFT, help_diff_scale_bias_desc,
+        reordered_diff_bias ? reordered_diff_bias : diff_bias}});
+  if (actual_diff_src_desc != diff_src_desc.get_desc() && reordered_diff_src) {
+    e = async_reorder(1.f, actual_diff_src_desc, reordered_diff_src, 0.f,
+                diff_src_desc, diff_src);
+  }
+  if (actual_diff_scale_bias_desc != diff_scale_bias_desc.get_desc() &&
+      reordered_diff_scale && reordered_diff_bias) {
+    async_reorder(1.f, actual_diff_scale_bias_desc, reordered_diff_scale, 0.f,
+            diff_scale_bias_desc, diff_scale);
+    e = async_reorder(1.f, actual_diff_scale_bias_desc, reordered_diff_bias, 0.f,
+                diff_scale_bias_desc, diff_bias);
+  }
+  return exit_primitive(e);
+}
+
+inline
+sycl::event engine_ext::batch_normalization_forward_internal(
+    bool is_infer, batch_normalization_mode mode, float epsilon, float factor,
+    float alpha, const memory_desc_ext &src_desc, void *src, float beta,
+    const memory_desc_ext &dst_desc, void *dst,
+    const memory_desc_ext &scale_bias_desc, void *scale, void *bias,
+    const memory_desc_ext &mean_var_desc, void *saved_mean, void *saved_var,
+    void *running_mean, void *running_var) {
+  if (scale_parameter_preprocess({{alpha, beta, dst_desc, dst}})) {
+    return sycl::event();
+  }
+  enter_primitive(src_desc.get_size() + 5 * dst_desc.get_size() +
+                  scale_bias_desc.get_size() * 2 +
+                  mean_var_desc.get_size() * 9);
+  void *reordered_src = nullptr, *reordered_dst = nullptr,
+       *reordered_scale = nullptr, *reordered_bias = nullptr,
+       *reordered_saved_mean = nullptr, *reordered_saved_var = nullptr;
+  ::dnnl::memory::desc help_src_desc = src_desc.get_desc();
+  ::dnnl::memory::desc help_dst_desc = dst_desc.get_desc();
+  ::dnnl::memory::desc help_scale_bias_desc = scale_bias_desc.get_desc();
+  ::dnnl::memory::desc help_mean_var_desc = mean_var_desc.get_desc();
+  ::dnnl::memory::desc actual_dst_desc = help_dst_desc;
+  ::dnnl::memory::desc actual_mean_var_desc = help_mean_var_desc;
+
+  if (mode == batch_normalization_mode::per_activation) {
+    help_src_desc = bn_reorder_memory_to_channel_major_format(true, help_src_desc, src,
+                                                       &reordered_src);
+    help_dst_desc = bn_reorder_memory_to_channel_major_format(
+        false, help_dst_desc, dst, &reordered_dst);
+    actual_dst_desc = help_dst_desc;
+    help_scale_bias_desc = bn_reorder_memory_to_channel_major_format(
+        true, help_scale_bias_desc, scale, &reordered_scale);
+    bn_reorder_memory_to_channel_major_format(true, help_scale_bias_desc, bias,
+                                       &reordered_bias);
+    help_mean_var_desc = bn_reorder_memory_to_channel_major_format(
+        is_infer, help_mean_var_desc, saved_mean,
+        &reordered_saved_mean);
+    actual_mean_var_desc = help_mean_var_desc;
+    bn_reorder_memory_to_channel_major_format(is_infer,
+                                       help_mean_var_desc, saved_var,
+                                       &reordered_saved_var);
+    help_src_desc = compress_spatial_dimensions_to_channel(help_src_desc);
+    help_dst_desc = compress_spatial_dimensions_to_channel(help_dst_desc);
+  } else {
+    if (help_src_desc != help_dst_desc) {
+      help_src_desc = bn_reorder_memory_to_channel_major_format(
+          true, help_src_desc, src, &reordered_src);
+      help_dst_desc = bn_reorder_memory_to_channel_major_format(
+          false, help_dst_desc, dst, &reordered_dst);
+      actual_dst_desc = help_dst_desc;
+    }
+  }
+  help_scale_bias_desc =
+      get_bn_scale_bias_mean_var_desc(help_scale_bias_desc, mode);
+  help_mean_var_desc =
+      get_bn_scale_bias_mean_var_desc(help_mean_var_desc, mode);
+
+  ::dnnl::prop_kind kind;
+  ::dnnl::normalization_flags flag = ::dnnl::normalization_flags::use_scale |
+                                     ::dnnl::normalization_flags::use_shift;
+  if (is_infer) {
+    kind = ::dnnl::prop_kind::forward_inference;
+    flag = ::dnnl::normalization_flags::use_global_stats | flag;
+  } else {
+    kind = ::dnnl::prop_kind::forward_training;
+  }
+  auto primitive_args =
+      create_primitive_args_or_get<::dnnl::batch_normalization_forward>(
+          kind, help_src_desc, help_dst_desc, epsilon, flag);
+
+  insert_arg(primitive_args.second.args, DNNL_ARG_SRC, help_src_desc,
+             reordered_src ? reordered_src : src);
+  insert_arg(primitive_args.second.args, DNNL_ARG_SCALE, help_scale_bias_desc,
+             reordered_scale ? reordered_scale : scale);
+  insert_arg(primitive_args.second.args, DNNL_ARG_SHIFT, help_scale_bias_desc,
+             reordered_bias ? reordered_bias : bias);
+  insert_arg(primitive_args.second.args, DNNL_ARG_MEAN, help_mean_var_desc,
+             reordered_saved_mean ? reordered_saved_mean
+                                            : saved_mean);
+  insert_arg(primitive_args.second.args, DNNL_ARG_VARIANCE, help_mean_var_desc,
+             reordered_saved_var ? reordered_saved_var
+                                           : saved_var);
+  insert_arg(primitive_args.second.args, DNNL_ARG_DST, help_dst_desc,
+             reordered_dst ? reordered_dst : dst);
+  sycl::event e = execute_primitive<::dnnl::batch_normalization_forward>(primitive_args,
+                                    {{alpha, beta, DNNL_ARG_DST, help_dst_desc,
+                                      reordered_dst ? reordered_dst : dst}});
+
+  if (!is_infer && running_var) {
+    auto src_ndim = src_desc.get_desc().get_ndims();
+    auto src_dims = src_desc.get_dims();
+    int element_num = src_dims[0];
+    if (mode == batch_normalization_mode::spatial) {
+      for (int index = 2; index < src_ndim; index++) {
+        element_num *= src_dims[index];
+      }
+    }
+    float unbias_factor = element_num / (element_num - 1.f);
+    async_scale(1.f - factor, mean_var_desc, running_var);
+    e = async_sum(factor * unbias_factor, mean_var_desc,
+            reordered_saved_var ? reordered_saved_var : saved_var,
+            1.f, mean_var_desc, running_var);
+  }
+  if (!is_infer && running_mean) {
+    e = async_sum(factor, mean_var_desc,
+            reordered_saved_mean ? reordered_saved_mean : saved_mean,
+            (1.f - factor), mean_var_desc, running_mean);
+  }
+  if (reordered_dst && (actual_dst_desc != dst_desc.get_desc())) {
+    e = async_reorder(1.f, actual_dst_desc, reordered_dst, 0.f, dst_desc, dst);
+  }
+  if (!is_infer && reordered_saved_mean && reordered_saved_var && saved_mean &&
+      saved_var && (actual_mean_var_desc != mean_var_desc.get_desc())) {
+    e = async_reorder(1.f, actual_mean_var_desc, reordered_saved_mean, 0.f,
+                mean_var_desc, saved_mean);
+    e = async_reorder(1.f, actual_mean_var_desc, reordered_saved_var, 0.f,
+                mean_var_desc, saved_var);
+  }
+  return exit_primitive(e);
+}
+
+inline
+sycl::event engine_ext::rnn_forward_internal(
+    const rnn_desc &desc, ::dnnl::prop_kind kind,
+    const memory_desc_ext &src_desc, void *src, const memory_desc_ext &dst_desc,
+    void *dst, const memory_desc_ext &iter_desc, void *src_iter, void *dst_iter,
+    const memory_desc_ext &iter_c_desc, void *src_iter_c, void *dst_iter_c,
+    size_t weight_size, void *weight, size_t workspace_size, void *workspace,
+    size_t scratchpad_size, void *scratchpad, bool is_get_execution_args,
+    size_t *weight_size_query, size_t *workspace_size_query,
+    size_t *scratchpad_size_query) {
+  ::dnnl::memory::data_type src_dt;
+  ::dnnl::memory::format_tag src_format_tag;
+  rnn_mode mode;
+  rnn_bias_mode bias_mode;
+  rnn_direction direction;
+  dpct::library_data_t dt;
+  int direction_num = 1, input_size = 0, hidden_size = 0, projection_size = 0,
+      layer_size = 0, gate_num = 1, output_size = 0, data_type_size = 0,
+      seq_length = 1, batch_size = 1;
+  std::vector<void *> data = {src,        dst,        src_iter, dst_iter,
+                              src_iter_c, dst_iter_c, weight,   workspace,
+                              scratchpad};
+  std::vector<int> offset(6, 0);
+  void *input_layer_cache = nullptr, *hidden_layer_cache = nullptr;
+  sycl::event e;
+  enter_primitive(src_desc.get_size() * 2);
+  desc.get(&mode, &bias_mode, &direction, &dt, &input_size, &hidden_size,
+           &projection_size, &layer_size);
+
+  get_rnn_configuration(src_desc.get_desc(), direction, mode, dt, hidden_size,
+                        &src_dt, &src_format_tag, &projection_size,
+                        &output_size, &seq_length, &batch_size, &direction_num,
+                        &gate_num);
+
+  if (direction == rnn_direction::bidirectional) {
+    // Here to combine the oneDNN bidirectional_sum and 
+    // bidirectional_concat config, so call execute_rnn_forward_primitive
+    // twice.
+    if (layer_size > 1) {
+      if (!is_get_execution_args) {
+        input_layer_cache = allocate(src_desc);
+        hidden_layer_cache = allocate(src_desc);
+        _q->memcpy(input_layer_cache, src, src_desc.get_size());
+      }
+      data[0] = input_layer_cache;
+      data[1] = hidden_layer_cache;
+      e = execute_rnn_forward_primitive(
+          mode, kind, ::dnnl::rnn_direction::bidirectional_sum, bias_mode,
+          src_dt, src_format_tag, seq_length, batch_size, output_size,
+          output_size, 1, direction_num, hidden_size, gate_num, projection_size,
+          data, offset, layer_size - 1, weight_size_query, workspace_size_query,
+          scratchpad_size_query);
+      data[0] =
+          ((layer_size - 1) % 2 == 0) ? input_layer_cache : hidden_layer_cache;
+      data[1] = dst;
+    }
+    e = execute_rnn_forward_primitive(
+        mode, kind, ::dnnl::rnn_direction::bidirectional_concat, bias_mode,
+        src_dt, src_format_tag, seq_length, batch_size, output_size,
+        2 * output_size, 1, direction_num, hidden_size, gate_num,
+        projection_size, data, offset, 1, weight_size_query,
+        workspace_size_query, scratchpad_size_query);
+  } else {
+    e = execute_rnn_forward_primitive(
+        mode, kind, ::dnnl::rnn_direction::unidirectional_left2right, bias_mode,
+        src_dt, src_format_tag, seq_length, batch_size, output_size,
+        output_size, layer_size, direction_num, hidden_size, gate_num,
+        projection_size, data, offset, 1, weight_size_query,
+        workspace_size_query, scratchpad_size_query);
+  }
+
+  return exit_primitive(e);
+}
+
+inline
+sycl::event engine_ext::execute_rnn_forward_primitive(
+    rnn_mode mode, ::dnnl::prop_kind kind, ::dnnl::rnn_direction direction,
+    rnn_bias_mode bias_mode, ::dnnl::memory::data_type dt,
+    ::dnnl::memory::format_tag tag, int seq_length, int batch_size, int src_c,
+    int dst_c, int layer_size, int direction_num, int hidden_size, int gate_num,
+    int projection_size, std::vector<void *> &data, std::vector<int> &offset,
+    int iter_num, size_t *weight_size, size_t *workspace_size,
+    size_t *scratchpad_size) {
+
+  sycl::event e;
+  ::dnnl::primitive *p = nullptr;
+  std::unordered_map<int, ::dnnl::memory> *args = nullptr;
+  detail::primitive_cache_key_type key;
+  std::unordered_map<int, ::dnnl::memory> *execution_args;
+  ::dnnl::memory::desc bias_desc(
+      {layer_size, direction_num, gate_num, hidden_size}, dt,
+      ::dnnl::memory::format_tag::ldgo);
+  ::dnnl::memory::desc weight_layer_desc(
+      {layer_size, direction_num,
+       projection_size ? projection_size : hidden_size, gate_num, hidden_size},
+      dt, ::dnnl::memory::format_tag::ldigo);
+  ::dnnl::memory::desc weight_iter_desc(
+      {layer_size, direction_num,
+       projection_size ? projection_size : hidden_size, gate_num, hidden_size},
+      dt, ::dnnl::memory::format_tag::ldigo);
+  ::dnnl::memory::desc projection_desc;
+  if (projection_size) {
+    projection_desc = ::dnnl::memory::desc(
+        {layer_size, direction_num, hidden_size, projection_size}, dt,
+        ::dnnl::memory::format_tag::ldio);
+  }
+
+  if (weight_size) {
+    *weight_size +=
+        (weight_layer_desc.get_size() + weight_iter_desc.get_size() +
+         projection_desc.get_size() + bias_desc.get_size()) *
+        iter_num;
+    return e;
+  }
+
+  ::dnnl::memory::desc src_desc({seq_length, batch_size, src_c}, dt, tag);
+  ::dnnl::memory::desc dst_desc({seq_length, batch_size, dst_c}, dt, tag);
+  ::dnnl::memory::desc iter_desc(
+      {layer_size, direction_num, batch_size,
+       projection_size ? projection_size : hidden_size},
+      dt, ::dnnl::memory::format_tag::ldnc);
+  ::dnnl::memory::desc iter_c_desc(
+      {layer_size, direction_num, batch_size, hidden_size}, dt,
+      ::dnnl::memory::format_tag::ldnc);
+
+  ::dnnl::memory::desc workspace_desc;
+  ::dnnl::memory::desc scratchpad_desc;
+  ::dnnl::primitive_attr attr;
+  attr.set_scratchpad_mode(::dnnl::scratchpad_mode::user);
+
+  if (mode == rnn_mode::vanilla_relu || mode == rnn_mode::vanilla_tanh) {
+    auto primitive = create_primitive_args_or_get<::dnnl::vanilla_rnn_forward>(
+        kind,
+        mode == rnn_mode::vanilla_relu ? ::dnnl::algorithm::eltwise_relu
+                                       : ::dnnl::algorithm::eltwise_tanh,
+        direction, src_desc, iter_desc, weight_layer_desc, weight_iter_desc,
+        bias_desc, dst_desc, iter_desc, attr);
+
+    auto pd = get_primitive_desc<::dnnl::vanilla_rnn_forward>(
+        primitive.second.primitive);
+
+    workspace_desc = pd.workspace_desc();
+    scratchpad_desc = pd.scratchpad_desc();
+    if (workspace_size && scratchpad_size) {
+      *workspace_size += workspace_desc.get_size() * iter_num;
+      *scratchpad_size = scratchpad_desc.get_size() > *scratchpad_size
+                             ? scratchpad_desc.get_size()
+                             : *scratchpad_size;
+    } else {
+      key = primitive.first;
+      p = primitive.second.primitive;
+      args = primitive.second.args;
+    }
+  } else if (mode == rnn_mode::gru) {
+    auto primitive = create_primitive_args_or_get<::dnnl::gru_forward>(
+        kind, direction, src_desc, iter_desc, weight_layer_desc,
+        weight_iter_desc, bias_desc, dst_desc, iter_desc, attr);
+
+    auto pd =
+        get_primitive_desc<::dnnl::gru_forward>(primitive.second.primitive);
+
+    workspace_desc = pd.workspace_desc();
+    scratchpad_desc = pd.scratchpad_desc();
+    if (workspace_size && scratchpad_size) {
+      *workspace_size += workspace_desc.get_size() * iter_num;
+      *scratchpad_size = scratchpad_desc.get_size() > *scratchpad_size
+                             ? scratchpad_desc.get_size()
+                             : *scratchpad_size;
+    } else {
+      key = primitive.first;
+      p = primitive.second.primitive;
+      args = primitive.second.args;
+    }
+  } else if (mode == rnn_mode::lstm) {
+    auto primitive = create_primitive_args_or_get<::dnnl::lstm_forward>(
+        kind, direction, src_desc, iter_desc, iter_c_desc, weight_layer_desc,
+        weight_iter_desc, ::dnnl::memory::desc(), projection_desc, bias_desc,
+        dst_desc, iter_desc, iter_c_desc, attr);
+
+    auto pd =
+        get_primitive_desc<::dnnl::lstm_forward>(primitive.second.primitive);
+
+    workspace_desc = pd.workspace_desc();
+    scratchpad_desc = pd.scratchpad_desc();
+    if (workspace_size && scratchpad_size) {
+      *workspace_size += workspace_desc.get_size() * iter_num;
+      *scratchpad_size = scratchpad_desc.get_size() > *scratchpad_size
+                             ? scratchpad_desc.get_size()
+                             : *scratchpad_size;
+    } else {
+      key = primitive.first;
+      p = primitive.second.primitive;
+      args = primitive.second.args;
+    }
+  }
+
+  for (int i = 0; i < iter_num; i++) {
+    void *in_cache = data[0], *out_cache = data[1], *dst_iter_c_cache = nullptr,
+         *dst_iter_cache = ((uint8_t *)(data[3]) + offset[1]);
+    if (mode == rnn_mode::lstm) {
+      dst_iter_c_cache = (uint8_t *)(data[4]) + offset[2];
+    }
+    if (!workspace_size) {
+      insert_arg(args, DNNL_ARG_SRC_LAYER, src_desc, data[0]);
+      insert_arg(args, DNNL_ARG_DST_LAYER, dst_desc, data[1]);
+      insert_arg(args, DNNL_ARG_SCRATCHPAD, scratchpad_desc, data[8]);
+      auto insert_rnn_arg = [&](int arg_name, ::dnnl::memory::desc &d, void *data,
+                             int &offset) {
+        insert_arg(args, arg_name, d, (uint8_t *)data + offset);
+        offset += d.get_size();
+      };
+      insert_rnn_arg(DNNL_ARG_SRC_ITER, iter_desc, data[2], offset[0]);
+      insert_rnn_arg(DNNL_ARG_DST_ITER, iter_desc, data[3], offset[1]);
+
+      if (mode == rnn_mode::lstm) {
+        insert_rnn_arg(DNNL_ARG_SRC_ITER_C, iter_c_desc, data[4], offset[2]);
+        insert_rnn_arg(DNNL_ARG_DST_ITER_C, iter_c_desc, data[5], offset[3]);
+      }
+      insert_rnn_arg(DNNL_ARG_WEIGHTS_LAYER, weight_layer_desc, data[6],
+                  offset[4]);
+      insert_rnn_arg(DNNL_ARG_WEIGHTS_ITER, weight_iter_desc, data[6], offset[4]);
+      if (projection_size) {
+        insert_rnn_arg(DNNL_ARG_WEIGHTS_PROJECTION, projection_desc, data[6],
+                    offset[4]);
+      }
+      if (bias_mode == rnn_bias_mode::none) {
+        _q->memset((uint8_t *)(data[6]) + offset[4], 0, bias_desc.get_size());
+      }
+      insert_rnn_arg(DNNL_ARG_BIAS, bias_desc, data[6], offset[4]);
+      if (kind == ::dnnl::prop_kind::forward_training) {
+        insert_rnn_arg(DNNL_ARG_WORKSPACE, workspace_desc, data[7], offset[5]);
+      }
+      if (mode == rnn_mode::vanilla_relu || mode == rnn_mode::vanilla_tanh) {
+        execute_primitive<::dnnl::vanilla_rnn_forward>(
+            {key, {static_cast<::dnnl::vanilla_rnn_forward *>(p), args}});
+      } else if (mode == rnn_mode::gru) {
+        execute_primitive<::dnnl::gru_forward>(
+            {key, {static_cast<::dnnl::gru_forward *>(p), args}});
+      } else if (mode == rnn_mode::lstm) {
+        execute_primitive<::dnnl::lstm_forward>(
+            {key, {static_cast<::dnnl::lstm_forward *>(p), args}});
+      }
+      if (i != iter_num - 1) {
+        std::swap(data[0], data[1]);
+      }
+    }
+    if (kind == ::dnnl::prop_kind::forward_training) {
+      if (workspace_size) {
+        *workspace_size +=
+            (src_desc.get_size() + dst_desc.get_size() + iter_desc.get_size());
+        if (mode == rnn_mode::lstm) {
+          *workspace_size += iter_c_desc.get_size();
+        }
+      } else {
+        _q->memcpy((uint8_t *)(data[7]) + offset[5], in_cache,
+                   src_desc.get_size());
+        offset[5] += src_desc.get_size();
+        _q->memcpy((uint8_t *)(data[7]) + offset[5], out_cache,
+                   dst_desc.get_size());
+        offset[5] += dst_desc.get_size();
+        _q->memcpy((uint8_t *)(data[7]) + offset[5], dst_iter_cache,
+                   iter_desc.get_size());
+        offset[5] += iter_desc.get_size();
+        if (mode == rnn_mode::lstm) {
+          _q->memcpy((uint8_t *)(data[7]) + offset[5], dst_iter_c_cache,
+                     iter_c_desc.get_size());
+          offset[5] += iter_c_desc.get_size();
+        }
+      }
+    }
+  }
+  return e;
+}
+
+inline
+sycl::event engine_ext::execute_rnn_backward_primitive(
+    rnn_mode mode, ::dnnl::rnn_direction direction, rnn_bias_mode bias_mode,
+    ::dnnl::memory::data_type dt, ::dnnl::memory::format_tag tag,
+    int seq_length, int batch_size, int src_c, int dst_c, int layer_size,
+    int direction_num, int hidden_size, int gate_num, int projection_size,
+    std::vector<void *> &data, std::vector<int> &offset, int iter_num) {
+
+  sycl::event e;
+  ::dnnl::primitive *p = nullptr;
+  std::unordered_map<int, ::dnnl::memory> *args = nullptr;
+  detail::primitive_cache_key_type key;
+  ::dnnl::prop_kind fkind = ::dnnl::prop_kind::forward_training;
+  ::dnnl::prop_kind bkind = ::dnnl::prop_kind::backward;
+  ::dnnl::memory::desc bias_desc(
+      {layer_size, direction_num, gate_num, hidden_size}, dt,
+      ::dnnl::memory::format_tag::ldgo);
+  ::dnnl::memory::desc weight_layer_desc(
+      {layer_size, direction_num,
+       projection_size ? projection_size : hidden_size, gate_num, hidden_size},
+      dt, ::dnnl::memory::format_tag::ldigo);
+  ::dnnl::memory::desc weight_iter_desc(
+      {layer_size, direction_num,
+       projection_size ? projection_size : hidden_size, gate_num, hidden_size},
+      dt, ::dnnl::memory::format_tag::ldigo);
+  ::dnnl::memory::desc diff_weight_layer_desc(
+      {layer_size, direction_num,
+       projection_size ? projection_size : hidden_size, gate_num, hidden_size},
+      dt, ::dnnl::memory::format_tag::ldgoi);
+  ::dnnl::memory::desc diff_weight_iter_desc(
+      {layer_size, direction_num,
+       projection_size ? projection_size : hidden_size, gate_num, hidden_size},
+      dt, ::dnnl::memory::format_tag::ldgoi);
+  ::dnnl::memory::desc projection_desc, diff_projection_desc;
+  if (projection_size) {
+    projection_desc = ::dnnl::memory::desc(
+        {layer_size, direction_num, hidden_size, projection_size}, dt,
+        ::dnnl::memory::format_tag::ldio);
+    diff_projection_desc = ::dnnl::memory::desc(
+        {layer_size, direction_num, hidden_size, projection_size}, dt,
+        ::dnnl::memory::format_tag::ldoi);
+  }
+
+  ::dnnl::memory::desc src_desc({seq_length, batch_size, src_c}, dt, tag);
+  ::dnnl::memory::desc dst_desc({seq_length, batch_size, dst_c}, dt, tag);
+  ::dnnl::memory::desc iter_desc(
+      {layer_size, direction_num, batch_size,
+       projection_size ? projection_size : hidden_size},
+      dt, ::dnnl::memory::format_tag::ldnc);
+  ::dnnl::memory::desc iter_c_desc(
+      {layer_size, direction_num, batch_size, hidden_size}, dt,
+      ::dnnl::memory::format_tag::ldnc);
+
+  ::dnnl::memory::desc workspace_desc;
+  ::dnnl::memory::desc scratchpad_desc;
+  ::dnnl::primitive_attr attr;
+  attr.set_scratchpad_mode(::dnnl::scratchpad_mode::user);
+
+  if (mode == rnn_mode::vanilla_relu || mode == rnn_mode::vanilla_tanh) {
+    auto fpd = create_primitive_desc<::dnnl::vanilla_rnn_forward>(
+        fkind,
+        mode == rnn_mode::vanilla_relu ? ::dnnl::algorithm::eltwise_relu
+                                       : ::dnnl::algorithm::eltwise_tanh,
+        direction, src_desc, iter_desc, weight_layer_desc, weight_iter_desc,
+        bias_desc, dst_desc, iter_desc, attr);
+    auto primitive = create_primitive_args_or_get<::dnnl::vanilla_rnn_backward>(
+        bkind,
+        mode == rnn_mode::vanilla_relu ? ::dnnl::algorithm::eltwise_relu
+                                       : ::dnnl::algorithm::eltwise_tanh,
+        direction, src_desc, iter_desc, diff_weight_layer_desc,
+        diff_weight_iter_desc, bias_desc, dst_desc, iter_desc, src_desc,
+        iter_desc, weight_layer_desc, weight_iter_desc, bias_desc, dst_desc,
+        iter_desc, fpd, attr);
+    auto pd = get_primitive_desc<::dnnl::vanilla_rnn_backward>(
+        primitive.second.primitive);
+    workspace_desc = pd.workspace_desc();
+    scratchpad_desc = pd.scratchpad_desc();
+    key = primitive.first;
+    p = primitive.second.primitive;
+    args = primitive.second.args;
+  } else if (mode == rnn_mode::gru) {
+    auto fpd = create_primitive_desc<::dnnl::gru_forward>(
+        fkind, direction, src_desc, iter_desc, weight_layer_desc,
+        weight_iter_desc, bias_desc, dst_desc, iter_desc, attr);
+    auto primitive = create_primitive_args_or_get<::dnnl::gru_backward>(
+        bkind, direction, src_desc, iter_desc, diff_weight_layer_desc,
+        diff_weight_iter_desc, bias_desc, dst_desc, iter_desc, src_desc,
+        iter_desc, weight_layer_desc, weight_iter_desc, bias_desc, dst_desc,
+        iter_desc, fpd, attr);
+    auto pd =
+        get_primitive_desc<::dnnl::gru_backward>(primitive.second.primitive);
+    workspace_desc = pd.workspace_desc();
+    scratchpad_desc = pd.scratchpad_desc();
+    key = primitive.first;
+    p = primitive.second.primitive;
+    args = primitive.second.args;
+  } else if (mode == rnn_mode::lstm) {
+    auto fpd = create_primitive_desc<::dnnl::lstm_forward>(
+        fkind, direction, src_desc, iter_desc, iter_c_desc, weight_layer_desc,
+        weight_iter_desc, ::dnnl::memory::desc(), projection_desc, bias_desc,
+        dst_desc, iter_desc, iter_c_desc, attr);
+    auto primitive = create_primitive_args_or_get<::dnnl::lstm_backward>(
+        bkind, direction, src_desc, iter_desc, iter_c_desc,
+        diff_weight_layer_desc, diff_weight_iter_desc, ::dnnl::memory::desc(),
+        diff_projection_desc, bias_desc, dst_desc, iter_desc, iter_c_desc,
+        src_desc, iter_desc, iter_c_desc, weight_layer_desc, weight_iter_desc,
+        ::dnnl::memory::desc(), projection_desc, bias_desc, dst_desc, iter_desc,
+        iter_c_desc, fpd, attr);
+    auto pd =
+        get_primitive_desc<::dnnl::lstm_backward>(primitive.second.primitive);
+    workspace_desc = pd.workspace_desc();
+    scratchpad_desc = pd.scratchpad_desc();
+    key = primitive.first;
+    p = primitive.second.primitive;
+    args = primitive.second.args;
+  }
+
+  for (int i = 0; i < iter_num; i++) {
+    insert_arg(args, DNNL_ARG_DIFF_SRC_LAYER, src_desc, data[8]);
+    insert_arg(args, DNNL_ARG_DIFF_DST_LAYER, dst_desc, data[9]);
+    insert_arg(args, DNNL_ARG_SCRATCHPAD, scratchpad_desc, data[15]);
+    auto insert_rnn_arg = [&](int arg_name, ::dnnl::memory::desc &d, void *data,
+                           int &offset) {
+      offset += d.get_size();
+      insert_arg(args, arg_name, d, (uint8_t *)data - offset);
+    };
+    if (mode == rnn_mode::lstm) {
+      insert_rnn_arg(DNNL_ARG_DST_ITER_C, iter_c_desc, data[7], offset[0]);
+      insert_rnn_arg(DNNL_ARG_SRC_ITER_C, iter_c_desc, data[4], offset[2]);
+    }
+    insert_rnn_arg(DNNL_ARG_DST_ITER, iter_desc, data[7], offset[0]);
+    insert_rnn_arg(DNNL_ARG_DST_LAYER, dst_desc, data[7], offset[0]);
+    insert_rnn_arg(DNNL_ARG_SRC_LAYER, src_desc, data[7], offset[0]);
+    insert_rnn_arg(DNNL_ARG_WORKSPACE, workspace_desc, data[7], offset[0]);
+    insert_rnn_arg(DNNL_ARG_SRC_ITER, iter_desc, data[2], offset[1]);
+    insert_rnn_arg(DNNL_ARG_BIAS, bias_desc, data[6], offset[3]);
+    if (projection_size) {
+      insert_rnn_arg(DNNL_ARG_WEIGHTS_PROJECTION, diff_projection_desc, data[6],
+                  offset[3]);
+    }
+    insert_rnn_arg(DNNL_ARG_WEIGHTS_ITER, diff_weight_iter_desc, data[6],
+                offset[3]);
+    insert_rnn_arg(DNNL_ARG_WEIGHTS_LAYER, diff_weight_layer_desc, data[6],
+                offset[3]);
+    insert_rnn_arg(DNNL_ARG_DIFF_SRC_ITER, iter_desc, data[10], offset[4]);
+    insert_rnn_arg(DNNL_ARG_DIFF_DST_ITER, iter_desc, data[11], offset[5]);
+    if (mode == rnn_mode::lstm) {
+      insert_rnn_arg(DNNL_ARG_DIFF_SRC_ITER_C, iter_c_desc, data[12], offset[6]);
+      insert_rnn_arg(DNNL_ARG_DIFF_DST_ITER_C, iter_c_desc, data[13], offset[7]);
+    }
+    insert_rnn_arg(DNNL_ARG_DIFF_BIAS, bias_desc, data[14], offset[8]);
+    if (bias_mode == rnn_bias_mode::none) {
+      _q->memset((uint8_t *)(data[14]) - offset[8], 0, bias_desc.get_size());
+    }
+    if (projection_size) {
+      insert_rnn_arg(DNNL_ARG_DIFF_WEIGHTS_PROJECTION, projection_desc, data[14],
+                  offset[8]);
+    }
+    insert_rnn_arg(DNNL_ARG_DIFF_WEIGHTS_ITER, weight_iter_desc, data[14],
+                offset[8]);
+    insert_rnn_arg(DNNL_ARG_DIFF_WEIGHTS_LAYER, weight_layer_desc, data[14],
+                offset[8]);
+    if (mode == rnn_mode::vanilla_relu || mode == rnn_mode::vanilla_tanh) {
+      e = execute_primitive<::dnnl::vanilla_rnn_backward>(
+          {key, {static_cast<::dnnl::vanilla_rnn_backward *>(p), args}});
+    } else if (mode == rnn_mode::gru) {
+      e = execute_primitive<::dnnl::gru_backward>(
+          {key, {static_cast<::dnnl::gru_backward *>(p), args}});
+    } else if (mode == rnn_mode::lstm) {
+      e = execute_primitive<::dnnl::lstm_backward>(
+          {key, {static_cast<::dnnl::lstm_backward *>(p), args}});
+    }
+    if (i != iter_num - 1) {
+      std::swap(data[8], data[9]);
+    }
+  }
+  return e;
+}
+
+#define EMPTY_CACHE_KEY(type)                                                  \
+  template <>                                                                  \
+  inline void engine_ext::generate_cache_key<type>(std::string & key_buffer,   \
+                                                   const type &arg) {}
+
+EMPTY_CACHE_KEY(::dnnl::engine)
+EMPTY_CACHE_KEY(::dnnl::convolution_forward::primitive_desc)
+EMPTY_CACHE_KEY(::dnnl::eltwise_forward::primitive_desc)
+EMPTY_CACHE_KEY(::dnnl::softmax_forward::primitive_desc)
+EMPTY_CACHE_KEY(::dnnl::pooling_forward::primitive_desc)
+EMPTY_CACHE_KEY(::dnnl::lrn_forward::primitive_desc)
+EMPTY_CACHE_KEY(::dnnl::batch_normalization_forward::primitive_desc)
+EMPTY_CACHE_KEY(::dnnl::vanilla_rnn_forward::primitive_desc)
+EMPTY_CACHE_KEY(::dnnl::lstm_forward::primitive_desc)
+EMPTY_CACHE_KEY(::dnnl::gru_forward::primitive_desc)
+#undef EMPTY_CACHE_KEY
+
+template <>
+inline void engine_ext::generate_cache_key<std::vector<float>>(
+    std::string &key_buffer, const std::vector<float> &vec) {
+  key_buffer.append((char *)vec.data(), vec.size() * sizeof(float));
+}
+
+template <>
+inline void engine_ext::generate_cache_key<::dnnl::primitive_attr>(
+    std::string &key_buffer, const ::dnnl::primitive_attr &attr) {
+  if (!attr) {
+    return;
+  }
+  auto math_mode = (uint8_t)attr.get_fpmath_mode();
+  key_buffer.append((char *)&math_mode, sizeof(uint8_t));
+}
+
+template <>
+inline void engine_ext::generate_cache_key<::dnnl::memory::dims>(
+    std::string &key_buffer, const ::dnnl::memory::dims &dims) {
+  key_buffer.append((char *)dims.data(), dims.size() * sizeof(int64_t));
+}
+
+template <>
+inline void engine_ext::generate_cache_key<::dnnl::memory::desc>(
+    std::string &key_buffer, const ::dnnl::memory::desc &desc) {
+  uint8_t params[3] = {(uint8_t)desc.get_format_kind(),
+                       (uint8_t)desc.get_ndims(),
+                       (uint8_t)desc.get_data_type()};
+  generate_cache_key(key_buffer, desc.get_inner_blks());
+  generate_cache_key(key_buffer, desc.get_dims());
+  generate_cache_key(key_buffer, desc.get_strides());
+}
+
+template <typename T>
+void engine_ext::generate_cache_key(std::string &key_buffer, const T &arg) {
+  key_buffer.append((char *)&arg, sizeof(T));
+}
+
+template <typename T, typename... args_type>
+void engine_ext::generate_cache_key(std::string &key_buffer, const T &first_arg,
+                                    const args_type &...args) {
+  generate_cache_key(key_buffer, first_arg);
+  generate_cache_key(key_buffer, args...);
+}
+
+template <typename primitive_type, typename... args_type>
+std::pair<detail::primitive_cache_key_type, detail::primitive_and_args>
+engine_ext::create_primitive_args_or_get(args_type &&...args) {
+  std::string buffer;
+  buffer.reserve(512);
+  generate_cache_key(buffer, std::forward<args_type>(args)...);
+  buffer.append(std::to_string(_engine_id));
+  auto value = _primitive_cache.get(buffer);
+  primitive_type *p = nullptr;
+  std::unordered_map<int, ::dnnl::memory> *a = nullptr;
+  if (value) {
+    p = (primitive_type *)value->_primitive;
+    a = value->_args;
+  } else {
+    p = new primitive_type(create_primitive_desc<primitive_type>(
+        std::forward<args_type>(args)...));
+    a = new std::unordered_map<int, ::dnnl::memory>();
+  }
+  return {buffer, {p, a}};
+}
+
+template <typename primitive_type>
+typename primitive_type::primitive_desc
+engine_ext::get_primitive_desc(::dnnl::primitive *p) {
+  return typename primitive_type::primitive_desc(
+      const_cast<dnnl_primitive_desc_t>(p->get_primitive_desc()));
+}
+
+template <typename primitive_type, typename... args_type>
+typename primitive_type::primitive_desc
+engine_ext::create_primitive_desc(args_type &&...args) {
+  return typename primitive_type::primitive_desc(
+      *_eng, std::forward<args_type>(args)...);
+}
+
+inline
+void engine_ext::fill(const memory_desc_ext &src_desc, void *src,
+                      const void *valuePtr) {
+  async_fill(src_desc, src, valuePtr).wait();
+}
+
+inline
+void engine_ext::reorder(float alpha, const memory_desc_ext &src_desc,
+                         void *src, float beta, const memory_desc_ext &dst_desc,
+                         void *dst) {
+  async_reorder(alpha, src_desc, src, beta, dst_desc, dst).wait();
+}
+
+inline
+void engine_ext::scale(float alpha, const memory_desc_ext &src_desc,
+                       void *src) {
+  async_scale(alpha, src_desc, src).wait();
+}
+inline
+void engine_ext::sum(float alpha, const memory_desc_ext &src_desc, void *src,
+                     float beta, const memory_desc_ext &dst_desc, void *dst) {
+  async_sum(alpha, src_desc, src, beta, dst_desc, dst).wait();
+}
+inline
+void engine_ext::activation_forward(activation_desc &desc, float alpha,
+                                    const memory_desc_ext &src_desc, void *src,
+                                    float beta, const memory_desc_ext &dst_desc,
+                                    void *dst) {
+  async_activation_forward(desc, alpha, src_desc, src, beta, dst_desc, dst)
+      .wait();
+}
+inline
+void engine_ext::activation_backward(
+    activation_desc &desc, float alpha, const memory_desc_ext &dst_desc,
+    void *dst, const memory_desc_ext &diff_dst_desc, void *diff_dst,
+    const memory_desc_ext &src_desc, void *src, float beta,
+    const memory_desc_ext &diff_src_desc, void *diff_src) {
+  async_activation_backward(desc, alpha, dst_desc, dst, diff_dst_desc, diff_dst,
+                            src_desc, src, beta, diff_src_desc, diff_src)
+      .wait();
+}
+inline
+void engine_ext::pooling_forward(pooling_desc &desc, float alpha,
+                                 const memory_desc_ext &src_desc, void *src,
+                                 float beta, const memory_desc_ext &dst_desc,
+                                 void *dst,
+                                 ::dnnl::memory *workspace) {
+  async_pooling_forward(desc, alpha, src_desc, src, beta, dst_desc, dst,
+                        workspace).wait();
+}
+
+inline
+void engine_ext::pooling_backward(
+    pooling_desc &desc, float alpha, const memory_desc_ext &dst_desc, void *dst,
+    const memory_desc_ext &diff_dst_desc, void *diff_dst,
+    const memory_desc_ext &src_desc, void *src, float beta,
+    const memory_desc_ext &diff_src_desc, void *diff_src,
+    ::dnnl::memory *workspace) {
+  async_pooling_backward(desc, alpha, dst_desc, dst, diff_dst_desc, diff_dst,
+                         src_desc, src, beta, diff_src_desc, diff_src,
+                         workspace)
+      .wait();
+}
+
+inline
+void engine_ext::softmax_forward(softmax_algorithm alg, softmax_mode mode,
+                                 float alpha, const memory_desc_ext &src_desc,
+                                 void *src, float beta,
+                                 const memory_desc_ext &dst_desc, void *dst) {
+  async_softmax_forward(alg, mode, alpha, src_desc, src, beta, dst_desc, dst)
+      .wait();
+}
+
+inline
+void engine_ext::softmax_backward(softmax_algorithm alg, softmax_mode mode,
+                                  float alpha, const memory_desc_ext &dst_desc,
+                                  void *dst,
+                                  const memory_desc_ext &diff_dst_desc,
+                                  void *diff_dst, float beta,
+                                  const memory_desc_ext &diff_src_desc,
+                                  void *diff_src) {
+  async_softmax_backward(alg, mode, alpha, dst_desc, dst, diff_dst_desc,
+                         diff_dst, beta, diff_src_desc, diff_src)
+      .wait();
+}
+
+inline
+void engine_ext::lrn_forward(lrn_desc &desc, float alpha,
+                             const memory_desc_ext &src_desc, void *src,
+                             float beta, const memory_desc_ext &dst_desc,
+                             void *dst, ::dnnl::memory *workspace) {
+  async_lrn_forward(desc, alpha, src_desc, src, beta, dst_desc, dst, workspace)
+      .wait();
+}
+
+inline
+void engine_ext::lrn_backward(lrn_desc &desc, float alpha,
+                              const memory_desc_ext &dst_desc, void *dst,
+                              const memory_desc_ext &diff_dst_desc,
+                              void *diff_dst, const memory_desc_ext &src_desc,
+                              void *src, float beta,
+                              const memory_desc_ext &diff_src_desc,
+                              void *diff_src,
+                              ::dnnl::memory *workspace) {
+  async_lrn_backward(desc, alpha, dst_desc, dst, diff_dst_desc, diff_dst,
+                     src_desc, src, beta, diff_src_desc, diff_src, workspace)
+      .wait();
+}
+
+inline
+sycl::event engine_ext::async_fill(const memory_desc_ext &src_desc, void *src,
+                             const void *valuePtr) {
+  ::dnnl::memory::data_type dt = src_desc.get_desc().get_data_type();
+  unsigned mem_size = src_desc.get_size();
+  switch (dt) {
+  case ::dnnl::memory::data_type::f32:
+    return fill_with_type<float>(_q, src, valuePtr, mem_size);
+  case ::dnnl::memory::data_type::f16:
+    return fill_with_type<sycl::half>(_q, src, valuePtr, mem_size);
+  case ::dnnl::memory::data_type::s32:
+    return fill_with_type<int32_t>(_q, src, valuePtr, mem_size);
+  case ::dnnl::memory::data_type::s8:
+    return fill_with_type<int8_t>(_q, src, valuePtr, mem_size);
+  case ::dnnl::memory::data_type::u8:
+    return fill_with_type<uint8_t>(_q, src, valuePtr, mem_size);
+  default:
+    throw std::runtime_error("async_fill: unsupported data type.");
+  }
+}
+
+inline
+sycl::event engine_ext::async_reorder(float alpha, const memory_desc_ext &src_desc,
+                                void *src, float beta,
+                                const memory_desc_ext &dst_desc, void *dst) {
+  if (scale_parameter_preprocess({{alpha, beta, dst_desc, dst}})) {
+    return sycl::event();
+  }
+  enter_primitive(2 * dst_desc.get_size());
+
+  auto primitive_args = create_primitive_args_or_get<::dnnl::reorder>(
+      src_desc.get_desc(), *_eng, dst_desc.get_desc());
+
+  insert_arg(primitive_args.second.args, DNNL_ARG_SRC, src_desc.get_desc(),
+             src);
+  insert_arg(primitive_args.second.args, DNNL_ARG_DST, dst_desc.get_desc(),
+             dst);
+
+  return exit_primitive(execute_primitive<::dnnl::reorder>(
+      primitive_args, {{alpha, beta, DNNL_ARG_DST, dst_desc, dst}}));
+}
+
+inline
+sycl::event engine_ext::async_scale(float alpha, const memory_desc_ext &src_desc,
+                              void *src) {
+  if (alpha == 1.f) {
+    return sycl::event();
+  }
+  size_t cache_size = src_desc.get_size();
+  enter_primitive(cache_size);
+  void *src_cache = allocate(cache_size);
+  _q->memcpy(src_cache, src, cache_size);
+  auto primitive_args = create_primitive_args_or_get<::dnnl::eltwise_forward>(
+      ::dnnl::prop_kind::forward_inference, ::dnnl::algorithm::eltwise_linear,
+      src_desc.get_desc(), src_desc.get_desc(), alpha, 0.f);
+
+  insert_arg(primitive_args.second.args, DNNL_ARG_SRC, src_desc.get_desc(),
+             src_cache);
+  insert_arg(primitive_args.second.args, DNNL_ARG_DST, src_desc.get_desc(),
+             src);
+
+  return exit_primitive(
+      execute_primitive<::dnnl::eltwise_forward>(primitive_args));
+}
+
+inline sycl::event
+engine_ext::async_sum(float alpha, const memory_desc_ext &src_desc, void *src,
+                      float beta, const memory_desc_ext &dst_desc, void *dst) {
+  if (alpha == 0.f && beta == 1.f) {
+    return sycl::event();
+  }
+  size_t cache_size = dst_desc.get_size();
+  enter_primitive(cache_size);
+  void *dst_cache = allocate(dst_desc);
+  _q->memcpy(dst_cache, dst, cache_size);
+
+  auto primitive_args = create_primitive_args_or_get<::dnnl::sum>(
+      std::vector<float>{alpha, beta},
+      std::vector<::dnnl::memory::desc>{src_desc.get_desc(),
+                                        dst_desc.get_desc()});
+  insert_arg(primitive_args.second.args, DNNL_ARG_DST, dst_desc.get_desc(),
+             dst);
+  insert_arg(primitive_args.second.args, DNNL_ARG_MULTIPLE_SRC,
+             src_desc.get_desc(), src);
+  insert_arg(primitive_args.second.args, DNNL_ARG_MULTIPLE_SRC + 1,
+             dst_desc.get_desc(), dst_cache);
+
+  return exit_primitive(execute_primitive<::dnnl::sum>(primitive_args));
+}
+
+inline
+sycl::event engine_ext::async_binary(binary_op op, float alpha_0,
+                               const memory_desc_ext &src_desc_0, void *src_0,
+                               float alpha_1, const memory_desc_ext &src_desc_1,
+                               void *src_1, float beta,
+                               const memory_desc_ext &dst_desc, void *dst) {
+  ::dnnl::algorithm onednn_algorithm;
+  switch (op) {
+  case binary_op::max:
+    onednn_algorithm = ::dnnl::algorithm::binary_max;
+    break;
+  case binary_op::min:
+    onednn_algorithm = ::dnnl::algorithm::binary_min;
+    break;
+  case binary_op::add:
+    onednn_algorithm = ::dnnl::algorithm::binary_add;
+    break;
+  case binary_op::sub:
+    onednn_algorithm = ::dnnl::algorithm::binary_sub;
+    break;
+  case binary_op::mul:
+    onednn_algorithm = ::dnnl::algorithm::binary_mul;
+    break;
+  case binary_op::div:
+    onednn_algorithm = ::dnnl::algorithm::binary_div;
+    break;
+  case binary_op::sqrt:
+    onednn_algorithm = ::dnnl::algorithm::eltwise_sqrt;
+    break;
+  case binary_op::neg:
+    onednn_algorithm = ::dnnl::algorithm::eltwise_linear;
+    break;
+  }
+  size_t src0_cache_size = src_desc_0.get_size();
+  size_t src1_cache_size = src_desc_1.get_size();
+  size_t dst_cache_size = dst_desc.get_size();
+  enter_primitive(2 * src0_cache_size + 2 * src1_cache_size +
+                  5 * dst_cache_size);
+  if (onednn_algorithm == ::dnnl::algorithm::eltwise_sqrt ||
+      onednn_algorithm == ::dnnl::algorithm::eltwise_linear) {
+    void *src_cache = nullptr, *dst_cache = nullptr;
+    src_cache = allocate(src0_cache_size);
+    dst_cache = allocate(dst_cache_size);
+    _q->memcpy(src_cache, src_0, src0_cache_size);
+    _q->memcpy(dst_cache, dst, dst_cache_size);
+    async_scale(alpha_0, src_desc_0, src_cache);
+    async_scale(beta, dst_desc, dst_cache);
+
+    // Let the output = 1 - input to simulate the behavior of neg.
+    auto primitive_args = create_primitive_args_or_get<::dnnl::eltwise_forward>(
+        ::dnnl::prop_kind::forward_inference, onednn_algorithm,
+        src_desc_0.get_desc(), dst_desc.get_desc(), -1.f, 1.f);
+
+    insert_arg(primitive_args.second.args, DNNL_ARG_SRC, src_desc_0.get_desc(),
+               src_cache);
+    insert_arg(primitive_args.second.args, DNNL_ARG_DST, dst_desc.get_desc(),
+               dst);
+
+    execute_primitive<::dnnl::eltwise_forward>(
+        primitive_args, {{1.f, 0.f, DNNL_ARG_DST, dst_desc, dst}});
+    return exit_primitive(
+        async_sum(1.f, dst_desc, dst_cache, 1.f, dst_desc, dst));
+  }
+
+  void *src_0_cache = nullptr, *src_1_cache = nullptr, *dst_cache = nullptr;
+
+  src_0_cache = allocate(src0_cache_size);
+  src_1_cache = allocate(src1_cache_size);
+  dst_cache = allocate(dst_cache_size);
+
+  _q->memcpy(src_0_cache, src_0, src0_cache_size);
+  _q->memcpy(src_1_cache, src_1, src1_cache_size);
+  _q->memcpy(dst_cache, dst, dst_cache_size);
+
+  async_scale(alpha_0, src_desc_0, src_0_cache);
+  async_scale(alpha_1, src_desc_1, src_1_cache);
+  async_scale(beta, dst_desc, dst_cache);
+
+  auto primitive_args = create_primitive_args_or_get<::dnnl::binary>(
+      onednn_algorithm, src_desc_0.get_desc(), src_desc_1.get_desc(),
+      dst_desc.get_desc());
+
+  insert_arg(primitive_args.second.args, DNNL_ARG_SRC_0, src_desc_0.get_desc(),
+             src_0_cache);
+  insert_arg(primitive_args.second.args, DNNL_ARG_SRC_1, src_desc_1.get_desc(),
+             src_1_cache);
+  insert_arg(primitive_args.second.args, DNNL_ARG_DST, dst_desc.get_desc(),
+             dst);
+
+  execute_primitive<::dnnl::binary>(primitive_args,
+                                    {{1.f, 0.f, DNNL_ARG_DST, dst_desc, dst}});
+  return exit_primitive(
+      async_sum(1.f, dst_desc, dst_cache, 1.f, dst_desc, dst));
+}
+
+inline
+sycl::event engine_ext::async_reduction(reduction_op op, float alpha,
+                                  const memory_desc_ext &src_desc, void *src,
+                                  float beta, const memory_desc_ext &dst_desc,
+                                  void *dst) {
+  if (alpha == 0.f && beta == 1.f) {
+    return sycl::event();
+  }
+  size_t src_cache_size = src_desc.get_size();
+  size_t dst_cache_size = dst_desc.get_size();
+  enter_primitive(3 * src_cache_size + 2 * dst_cache_size);
+  float p = 2.f;
+  ::dnnl::algorithm onednn_algorithm;
+  void *cache = nullptr;
+  switch (op) {
+  case reduction_op::amax:
+    cache = allocate(src_cache_size);
+    activation_desc adesc;
+    adesc.set_algorithm(::dnnl::algorithm::eltwise_abs);
+    async_activation_forward(adesc, 1.f, src_desc, src, 0.f, src_desc, cache);
+    onednn_algorithm = ::dnnl::algorithm::reduction_max;
+    src = cache;
+    break;
+  case reduction_op::max:
+    onednn_algorithm = ::dnnl::algorithm::reduction_max;
+    break;
+  case reduction_op::min:
+    onednn_algorithm = ::dnnl::algorithm::reduction_min;
+    break;
+  case reduction_op::sum:
+    onednn_algorithm = ::dnnl::algorithm::reduction_sum;
+    break;
+  case reduction_op::mean:
+    onednn_algorithm = ::dnnl::algorithm::reduction_mean;
+    break;
+  case reduction_op::mul:
+    onednn_algorithm = ::dnnl::algorithm::reduction_mul;
+    break;
+  case reduction_op::mul_no_zeros:
+    cache = allocate(src_cache_size);
+    transform_no_zero(src_desc, src, cache);
+    onednn_algorithm = ::dnnl::algorithm::reduction_mul;
+    src = cache;
+    break;
+  case reduction_op::norm1:
+    p = 1.f;
+    onednn_algorithm = ::dnnl::algorithm::reduction_norm_lp_power_p_sum;
+    break;
+  case reduction_op::norm2:
+    onednn_algorithm = ::dnnl::algorithm::reduction_norm_lp_sum;
+    break;
+  }
+  auto primitive_args = create_primitive_args_or_get<::dnnl::reduction>(
+      onednn_algorithm, src_desc.get_desc(), dst_desc.get_desc(), p, 0.f);
+
+  insert_arg(primitive_args.second.args, DNNL_ARG_SRC, src_desc.get_desc(),
+             src);
+  insert_arg(primitive_args.second.args, DNNL_ARG_DST, dst_desc.get_desc(),
+             dst);
+
+  return exit_primitive(execute_primitive<::dnnl::reduction>(
+      primitive_args, {{alpha, beta, DNNL_ARG_DST, dst_desc, dst}}));
+}
+
+inline
+sycl::event engine_ext::async_activation_forward(activation_desc &desc, float alpha,
+                                           const memory_desc_ext &src_desc,
+                                           void *src, float beta,
+                                           const memory_desc_ext &dst_desc,
+                                           void *dst) {
+  if (scale_parameter_preprocess({{alpha, beta, dst_desc, dst}})) {
+    return sycl::event();
+  }
+  enter_primitive(2 * dst_desc.get_size());
+  auto primitive_args = create_primitive_args_or_get<::dnnl::eltwise_forward>(
+      ::dnnl::prop_kind::forward, desc.get_algorithm(), src_desc.get_desc(),
+      dst_desc.get_desc(), desc.get_alpha(), desc.get_beta());
+
+  insert_arg(primitive_args.second.args, DNNL_ARG_SRC, src_desc.get_desc(),
+             src);
+  insert_arg(primitive_args.second.args, DNNL_ARG_DST, dst_desc.get_desc(),
+             dst);
+
+  return exit_primitive(execute_primitive<::dnnl::eltwise_forward>(
+      primitive_args, {{alpha, beta, DNNL_ARG_DST, dst_desc, dst}}));
+}
+
+inline
+sycl::event engine_ext::async_activation_backward(
+    activation_desc &desc, float alpha, const memory_desc_ext &dst_desc,
+    void *dst, const memory_desc_ext &diff_dst_desc, void *diff_dst,
+    const memory_desc_ext &src_desc, void *src, float beta,
+    const memory_desc_ext &diff_src_desc, void *diff_src) {
+
+  if (scale_parameter_preprocess({{alpha, beta, diff_src_desc, diff_src}})) {
+    return sycl::event();
+  }
+  enter_primitive(2 * diff_src_desc.get_size());
+  ::dnnl::memory::desc data_desc = dst_desc.get_desc();
+  auto alg = desc.get_algorithm();
+  if ((alg == ::dnnl::algorithm::eltwise_clip) ||
+      (alg == ::dnnl::algorithm::eltwise_linear) ||
+      (alg == ::dnnl::algorithm::eltwise_swish)) {
+    data_desc = src_desc.get_desc();
+  }
+  auto primitive_args = create_primitive_args_or_get<::dnnl::eltwise_backward>(
+      alg, diff_src_desc.get_desc(), diff_dst_desc.get_desc(), data_desc,
+      desc.get_alpha(), desc.get_beta(),
+      create_primitive_desc<::dnnl::eltwise_forward>(
+          ::dnnl::prop_kind::forward, alg, src_desc.get_desc(),
+          dst_desc.get_desc(), desc.get_alpha(), desc.get_beta()));
+
+  insert_arg(primitive_args.second.args, DNNL_ARG_DST, dst_desc.get_desc(),
+             dst);
+  insert_arg(primitive_args.second.args, DNNL_ARG_SRC, src_desc.get_desc(),
+             src);
+  insert_arg(primitive_args.second.args, DNNL_ARG_DIFF_DST,
+             diff_dst_desc.get_desc(), diff_dst);
+  insert_arg(primitive_args.second.args, DNNL_ARG_DIFF_SRC,
+             diff_src_desc.get_desc(), diff_src);
+
+  return exit_primitive(execute_primitive<::dnnl::eltwise_backward>(
+      primitive_args,
+      {{alpha, beta, DNNL_ARG_DIFF_SRC, diff_src_desc, diff_src}}));
+}
+
+inline
+sycl::event engine_ext::async_pooling_forward(pooling_desc &desc, float alpha,
+                                        const memory_desc_ext &src_desc,
+                                        void *src, float beta,
+                                        const memory_desc_ext &dst_desc,
+                                        void *dst, ::dnnl::memory *workspace) {
+  if (scale_parameter_preprocess({{alpha, beta, dst_desc, dst}})) {
+    return sycl::event();
+  }
+  enter_primitive(2 * dst_desc.get_size());
+  int pooling_dim = desc.get_stride().size();
+  std::vector<int64_t> dilation(pooling_dim, 0);
+  auto primitive_args =
+      create_primitive_args_or_get<::dnnl::pooling_forward>(
+          ::dnnl::prop_kind::forward_training, desc.get_algorithm(),
+          src_desc.get_desc(), dst_desc.get_desc(), desc.get_stride(),
+          desc.get_kernel(), dilation, desc.get_padding(), desc.get_padding());
+  auto pd = get_primitive_desc<::dnnl::pooling_forward>(
+      primitive_args.second.primitive);
+  ::dnnl::memory ws_mem(pd.workspace_desc(), *_eng);
+  if (workspace) {
+    *workspace = ws_mem;
+  } else {
+    insert_workspace(src, ws_mem);
+  }
+  insert_arg(primitive_args.second.args, DNNL_ARG_SRC, src_desc.get_desc(),
+             src);
+  insert_arg(primitive_args.second.args, DNNL_ARG_WORKSPACE, ws_mem);
+  insert_arg(primitive_args.second.args, DNNL_ARG_DST, dst_desc.get_desc(),
+             dst);
+
+  return exit_primitive(execute_primitive<::dnnl::pooling_forward>(
+      primitive_args, {{alpha, beta, DNNL_ARG_DST, dst_desc, dst}}));
+}
+
+inline
+sycl::event engine_ext::async_pooling_backward(
+    pooling_desc &desc, float alpha, const memory_desc_ext &dst_desc, void *dst,
+    const memory_desc_ext &diff_dst_desc, void *diff_dst,
+    const memory_desc_ext &src_desc, void *src, float beta,
+    const memory_desc_ext &diff_src_desc, void *diff_src,
+    ::dnnl::memory *workspace) {
+  if (scale_parameter_preprocess({{alpha, beta, diff_src_desc, diff_src}})) {
+    return sycl::event();
+  }
+  enter_primitive(2 * diff_src_desc.get_size());
+  int pooling_dim = desc.get_stride().size();
+  std::vector<int64_t> dilation(pooling_dim, 0);
+  auto primitive_args = create_primitive_args_or_get<::dnnl::pooling_backward>(
+      desc.get_algorithm(), diff_src_desc.get_desc(), diff_dst_desc.get_desc(),
+      desc.get_stride(), desc.get_kernel(), dilation, desc.get_padding(),
+      desc.get_padding(),
+      create_primitive_desc<::dnnl::pooling_forward>(
+          ::dnnl::prop_kind::forward_training, desc.get_algorithm(),
+          src_desc.get_desc(), dst_desc.get_desc(), desc.get_stride(),
+          desc.get_kernel(), dilation, desc.get_padding(), desc.get_padding()));
+
+  insert_arg(primitive_args.second.args, DNNL_ARG_DST, dst_desc.get_desc(),
+             dst);
+  insert_arg(primitive_args.second.args, DNNL_ARG_SRC, src_desc.get_desc(),
+             src);
+  insert_arg(primitive_args.second.args, DNNL_ARG_DIFF_DST,
+             diff_dst_desc.get_desc(), diff_dst);
+  insert_arg(primitive_args.second.args, DNNL_ARG_DIFF_SRC,
+             diff_src_desc.get_desc(), diff_src);
+
+  if (workspace) {
+    insert_arg(primitive_args.second.args, DNNL_ARG_WORKSPACE, *workspace);
+  } else {
+    insert_arg(primitive_args.second.args, DNNL_ARG_WORKSPACE,
+               get_workspace(src));
+  }
+
+  return exit_primitive(execute_primitive<::dnnl::pooling_backward>(
+      primitive_args,
+      {{alpha, beta, DNNL_ARG_DIFF_SRC, diff_src_desc, diff_src}}));
+}
+
+inline
+sycl::event engine_ext::async_softmax_forward(softmax_algorithm alg,
+                                        softmax_mode mode, float alpha,
+                                        const memory_desc_ext &src_desc,
+                                        void *src, float beta,
+                                        const memory_desc_ext &dst_desc,
+                                        void *dst) {
+  if (scale_parameter_preprocess({{alpha, beta, dst_desc, dst}})) {
+    return sycl::event();
+  }
+
+  ::dnnl::memory::desc help_src_desc = src_desc.get_desc();
+  ::dnnl::memory::desc help_dst_desc = dst_desc.get_desc();
+  if (mode == softmax_mode::instance) {
+    help_src_desc = compress_spatial_dimensions_to_channel(help_src_desc);
+    help_dst_desc = compress_spatial_dimensions_to_channel(help_dst_desc);
+  }
+  enter_primitive(2 * help_dst_desc.get_size());
+
+  ::dnnl::algorithm softmax_alg = ::dnnl::algorithm::softmax_accurate;
+  if (alg == softmax_algorithm::log) {
+    softmax_alg = ::dnnl::algorithm::softmax_log;
+  }
+  auto primitive_args = create_primitive_args_or_get<::dnnl::softmax_forward>(
+      ::dnnl::prop_kind::forward, softmax_alg, help_src_desc, 
+      help_dst_desc, 1);
+
+  insert_arg(primitive_args.second.args, DNNL_ARG_DST, help_dst_desc, dst);
+  insert_arg(primitive_args.second.args, DNNL_ARG_SRC, help_src_desc, src);
+
+  return exit_primitive(execute_primitive<::dnnl::softmax_forward>(
+      primitive_args,
+      {{alpha, beta, DNNL_ARG_DST, memory_desc_ext(help_dst_desc), dst}}));
+}
+
+inline
+sycl::event engine_ext::async_softmax_backward(
+    softmax_algorithm alg, softmax_mode mode, float alpha,
+    const memory_desc_ext &dst_desc, void *dst,
+    const memory_desc_ext &diff_dst_desc, void *diff_dst, float beta,
+    const memory_desc_ext &diff_src_desc, void *diff_src) {
+  if (scale_parameter_preprocess({{alpha, beta, diff_src_desc, diff_src}})) {
+    return sycl::event();
+  }
+  ::dnnl::memory::desc help_diff_src_desc = diff_src_desc.get_desc();
+  ::dnnl::memory::desc help_dst_desc = dst_desc.get_desc();
+  ::dnnl::memory::desc help_diff_dst_desc = diff_dst_desc.get_desc();
+  if (mode == softmax_mode::instance) {
+    help_diff_src_desc =
+        compress_spatial_dimensions_to_channel(help_diff_src_desc);
+    help_dst_desc = compress_spatial_dimensions_to_channel(help_dst_desc);
+    help_diff_dst_desc =
+        compress_spatial_dimensions_to_channel(help_diff_dst_desc);
+  }
+  enter_primitive(2 * help_diff_src_desc.get_size());
+
+  ::dnnl::algorithm softmax_alg = ::dnnl::algorithm::softmax_accurate;
+  if (alg == softmax_algorithm::log) {
+    softmax_alg = ::dnnl::algorithm::softmax_log;
+  }
+
+  auto primitive_args = create_primitive_args_or_get<::dnnl::softmax_backward>(
+      softmax_alg, help_diff_src_desc, help_diff_dst_desc, help_dst_desc, 1,
+      create_primitive_desc<::dnnl::softmax_forward>(
+          ::dnnl::prop_kind::forward, softmax_alg, help_diff_src_desc,
+          help_dst_desc, 1));
+  insert_arg(primitive_args.second.args, DNNL_ARG_DST, help_dst_desc, dst);
+  insert_arg(primitive_args.second.args, DNNL_ARG_DIFF_DST, help_diff_dst_desc,
+             diff_dst);
+  insert_arg(primitive_args.second.args, DNNL_ARG_DIFF_SRC, help_diff_src_desc,
+             diff_src);
+
+  return exit_primitive(execute_primitive<::dnnl::softmax_backward>(
+      primitive_args, {{alpha, beta, DNNL_ARG_DIFF_SRC,
+                        memory_desc_ext(help_diff_src_desc), diff_src}}));
+}
+
+inline
+sycl::event engine_ext::async_lrn_forward(lrn_desc &desc, float alpha,
+                                    const memory_desc_ext &src_desc, void *src,
+                                    float beta, const memory_desc_ext &dst_desc,
+                                    void *dst, ::dnnl::memory *workspace) {
+
+  if (scale_parameter_preprocess({{alpha, beta, dst_desc, dst}})) {
+    return sycl::event();
+  }
+  enter_primitive(2 * dst_desc.get_size());
+  auto primitive_args = create_primitive_args_or_get<::dnnl::lrn_forward>(
+      ::dnnl::prop_kind::forward_training,
+      ::dnnl::algorithm::lrn_across_channels, src_desc.get_desc(),
+      dst_desc.get_desc(), desc.get_local_size(), desc.get_alpha(),
+      desc.get_beta(), desc.get_k());
+  auto pd =
+      get_primitive_desc<::dnnl::lrn_forward>(primitive_args.second.primitive);
+  ::dnnl::memory ws_mem(pd.workspace_desc(), *_eng);
+  if (workspace) {
+    *workspace = ws_mem;
+  } else {
+    insert_workspace(src, ws_mem);
+  }
+
+  insert_arg(primitive_args.second.args, DNNL_ARG_SRC, src_desc.get_desc(),
+             src);
+  insert_arg(primitive_args.second.args, DNNL_ARG_DST, dst_desc.get_desc(),
+             dst);
+  insert_arg(primitive_args.second.args, DNNL_ARG_WORKSPACE, ws_mem);
+
+  return exit_primitive(execute_primitive<::dnnl::lrn_forward>(
+      primitive_args, {{alpha, beta, DNNL_ARG_DST, dst_desc, dst}}));
+}
+
+inline
+sycl::event
+engine_ext::async_lrn_backward(lrn_desc &desc, float alpha,
+                         const memory_desc_ext &dst_desc, void *dst,
+                         const memory_desc_ext &diff_dst_desc, void *diff_dst,
+                         const memory_desc_ext &src_desc, void *src, float beta,
+                         const memory_desc_ext &diff_src_desc, void *diff_src,
+                         ::dnnl::memory *workspace) {
+
+  if (scale_parameter_preprocess({{alpha, beta, diff_src_desc, diff_src}})) {
+    return sycl::event();
+  }
+  enter_primitive(2 * diff_src_desc.get_size());
+  auto primitive_args = create_primitive_args_or_get<::dnnl::lrn_backward>(
+      ::dnnl::algorithm::lrn_across_channels, diff_src_desc.get_desc(),
+      diff_dst_desc.get_desc(), src_desc.get_desc(), desc.get_local_size(),
+      desc.get_alpha(), desc.get_beta(), desc.get_k(),
+      create_primitive_desc<::dnnl::lrn_forward>(
+          ::dnnl::prop_kind::forward_training,
+          ::dnnl::algorithm::lrn_across_channels, src_desc.get_desc(),
+          dst_desc.get_desc(), desc.get_local_size(), desc.get_alpha(),
+          desc.get_beta(), desc.get_k()));
+
+  insert_arg(primitive_args.second.args, DNNL_ARG_SRC, src_desc.get_desc(),
+             src);
+  insert_arg(primitive_args.second.args, DNNL_ARG_DST, dst_desc.get_desc(),
+             dst);
+  insert_arg(primitive_args.second.args, DNNL_ARG_DIFF_DST,
+             diff_dst_desc.get_desc(), diff_dst);
+  insert_arg(primitive_args.second.args, DNNL_ARG_DIFF_SRC,
+             diff_src_desc.get_desc(), diff_src);
+
+  if (workspace) {
+    insert_arg(primitive_args.second.args, DNNL_ARG_WORKSPACE, *workspace);
+  } else {
+    insert_arg(primitive_args.second.args, DNNL_ARG_WORKSPACE,
+               get_workspace(src));
+  }
+
+  return exit_primitive(execute_primitive<::dnnl::lrn_backward>(
+      primitive_args,
+      {{alpha, beta, DNNL_ARG_DIFF_SRC, diff_src_desc, diff_src}}));
+}
+
+inline
+size_t engine_ext::get_batch_normalization_workspace_size(
+    batch_normalization_ops ops, const memory_desc_ext &src_desc) {
+  if(ops == batch_normalization_ops::none) {
+    return 0;
+  }
+  return src_desc.get_size();
+}
+
+inline
+sycl::event engine_ext::async_batch_normalization_forward_inference(
+    batch_normalization_mode mode, float epsilon, float alpha,
+    const memory_desc_ext &src_desc, void *src, float beta,
+    const memory_desc_ext &dst_desc, void *dst,
+    const memory_desc_ext &scale_bias_mean_var_desc, void *scale, void *bias,
+    void *mean, void *var) {
+
+  return batch_normalization_forward_internal(
+      true, mode, epsilon, 0.f, alpha, src_desc, src, beta, dst_desc, dst,
+      scale_bias_mean_var_desc, scale, bias, scale_bias_mean_var_desc, mean,
+      var, nullptr, nullptr);
+}
+
+inline
+sycl::event engine_ext::async_batch_normalization_forward_inference(
+    batch_normalization_mode mode, batch_normalization_ops ops,
+    activation_desc &adesc, float epsilon, float alpha,
+    const memory_desc_ext &src_desc, void *src, float beta,
+    const memory_desc_ext &dst_desc, void *dst,
+    const memory_desc_ext &summand_desc, void *summand,
+    const memory_desc_ext &scale_bias_desc, void *scale, void *bias,
+    const memory_desc_ext &mean_var_desc, void *mean, void *var) {
+
+  bool has_post_op = (ops != batch_normalization_ops::none);
+  sycl::event e;
+  enter_primitive(src_desc.get_size() + dst_desc.get_size() * 4 +
+                  scale_bias_desc.get_size() * 2 +
+                  mean_var_desc.get_size() * 5);
+  if (has_post_op) {
+    void *dst_cache = allocate(dst_desc);
+    batch_normalization_forward_internal(
+        true, mode, epsilon, 0.f, 1.f, src_desc, src, 0.f, dst_desc, dst_cache,
+        scale_bias_desc, scale, bias, mean_var_desc, mean, var, nullptr,
+        nullptr);
+
+    if (ops == batch_normalization_ops::add_activation) {
+      async_sum(1.f, summand_desc, summand, 1.f, dst_desc, dst_cache);
+    }
+    async_activation_forward(adesc, 1.f, dst_desc, dst_cache, 0.f, dst_desc,
+                       dst_cache);
+    return exit_primitive(
+        async_sum(alpha, dst_desc, dst_cache, beta, dst_desc, dst));
+  }
+  return exit_primitive(batch_normalization_forward_internal(
+      true, mode, epsilon, 0.f, alpha, src_desc, src, beta, dst_desc, dst,
+      scale_bias_desc, scale, bias, mean_var_desc, mean, var, nullptr,
+      nullptr));
+}
+
+inline
+sycl::event engine_ext::async_batch_normalization_forward_training(
+    batch_normalization_mode mode, float epsilon, float factor, float alpha,
+    const memory_desc_ext &src_desc, void *src, float beta,
+    const memory_desc_ext &dst_desc, void *dst,
+    const memory_desc_ext &scale_bias_mean_var_desc, void *scale, void *bias,
+    void *running_mean, void *running_var, void *saved_mean, void *saved_var) {
+  return batch_normalization_forward_internal(
+      false, mode, epsilon, factor, alpha, src_desc, src, beta, dst_desc, dst,
+      scale_bias_mean_var_desc, scale, bias, scale_bias_mean_var_desc,
+      saved_mean, saved_var, running_mean, running_var);
+}
+
+inline
+sycl::event engine_ext::async_batch_normalization_forward_training(
+    batch_normalization_mode mode, batch_normalization_ops ops,
+    activation_desc &adesc, float epsilon, float factor, float alpha,
+    const memory_desc_ext &src_desc, void *src, float beta,
+    const memory_desc_ext &dst_desc, void *dst,
+    const memory_desc_ext &summand_desc, void *summand,
+    const memory_desc_ext &scale_bias_desc, void *scale, void *bias,
+    const memory_desc_ext &mean_var_desc, void *running_mean, void *running_var,
+    void *saved_mean, void *saved_var, size_t workspace_size,
+    void *workspace) {
+  enter_primitive(src_desc.get_size() + dst_desc.get_size() * 3 +
+                  mean_var_desc.get_size() * 5 +
+                  scale_bias_desc.get_size() * 2);
+  bool has_post_op = (ops != batch_normalization_ops::none);
+  sycl::event e;
+  if (has_post_op) {
+    if(workspace_size < dst_desc.get_desc().get_size()) {
+      throw std::runtime_error("async_batch_normalization_forward_training_ex: "
+        "no sufficient workspace.");
+    }
+    batch_normalization_forward_internal(
+        false, mode, epsilon, factor, 1.f, src_desc, src, 0.f, dst_desc,
+        workspace, scale_bias_desc, scale, bias, mean_var_desc,
+        saved_mean, saved_var, running_mean, running_var);
+    if (ops == batch_normalization_ops::add_activation) {
+      async_sum(1.f, summand_desc, summand, 1.f, dst_desc,
+          workspace);
+    }
+    return exit_primitive(async_activation_forward(
+        adesc, alpha, dst_desc, workspace, beta, dst_desc, dst));
+  }
+  return exit_primitive(batch_normalization_forward_internal(
+      false, mode, epsilon, factor, alpha, src_desc, src, beta, dst_desc, dst,
+      scale_bias_desc, scale, bias, mean_var_desc, saved_mean, saved_var,
+      running_mean, running_var));
+}
+
+inline
+sycl::event engine_ext::async_batch_normalization_forward_training(
+    batch_normalization_mode mode, batch_normalization_ops ops,
+    activation_desc &adesc, float epsilon, float factor, float alpha,
+    const memory_desc_ext &src_desc, void *src, float beta,
+    const memory_desc_ext &dst_desc, void *dst,
+    const memory_desc_ext &summand_desc, void *summand,
+    const memory_desc_ext &scale_bias_mean_var_desc, void *scale, void *bias,
+    void *running_mean, void *running_var, void *saved_mean, void *saved_var,
+    size_t workspace_size, void *workspace) {
+  return async_batch_normalization_forward_training(
+      mode, ops, adesc, epsilon, factor, alpha, src_desc, src, beta, dst_desc,
+      dst, summand_desc, summand, scale_bias_mean_var_desc, scale, bias,
+      scale_bias_mean_var_desc, running_mean, running_var, saved_mean,
+      saved_var, workspace_size, workspace);
+}
+
+inline
+sycl::event engine_ext::async_batch_normalization_backward(
+    batch_normalization_mode mode, float epsilon, float alpha_data,
+    const memory_desc_ext &src_desc, void *src,
+    const memory_desc_ext &diff_dst_desc, void *diff_dst, float beta_data,
+    const memory_desc_ext &diff_src_desc, void *diff_src, float alpha_param,
+    const memory_desc_ext &diff_scale_bias_mean_var_desc, void *scale,
+    float beta_param, void *diff_scale, void *diff_bias, void *saved_mean,
+    void *saved_var) {
+
+  return batch_normalization_backward_internal(
+      mode, epsilon, alpha_data, src_desc, src, diff_dst_desc, diff_dst,
+      beta_data, diff_src_desc, diff_src, alpha_param,
+      diff_scale_bias_mean_var_desc, scale, nullptr, beta_param, diff_scale,
+      diff_bias, diff_scale_bias_mean_var_desc, saved_mean, saved_var);
+}
+
+inline
+sycl::event engine_ext::async_batch_normalization_backward(
+    batch_normalization_mode mode, batch_normalization_ops ops,
+    activation_desc &adesc, float epsilon, float alpha_data,
+    const memory_desc_ext &src_desc, void *src, const memory_desc_ext &dst_desc,
+    void *dst, const memory_desc_ext &diff_dst_desc, void *diff_dst,
+    float beta_data, const memory_desc_ext &diff_src_desc, void *diff_src,
+    const memory_desc_ext &diff_summand_desc, void *diff_summand,
+    float alpha_param, const memory_desc_ext &diff_scale_bias_desc, void *scale,
+    void *bias, float beta_param, void *diff_scale, void *diff_bias,
+    const memory_desc_ext &mean_var_desc, void *saved_mean, void *saved_var,
+    size_t workspace_size, void *workspace) {
+  std::vector<void *> caches;
+  ::dnnl::memory::desc real_diff_dst_desc = diff_dst_desc.get_desc();
+  void *real_diff_dst = diff_dst;
+
+  if (ops != batch_normalization_ops::none &&
+      workspace_size < dst_desc.get_desc().get_size()) {
+    throw std::runtime_error("async_batch_normalization_backward_ex: "
+                             "no sufficient workspace.");
+  }
+  enter_primitive(diff_scale_bias_desc.get_size() * 8 +
+                  src_desc.get_size() * 3 + diff_dst_desc.get_size() * 5 +
+                  diff_src_desc.get_size() + mean_var_desc.get_size() * 9 +
+                  diff_summand_desc.get_size());
+  if (ops == batch_normalization_ops::add_activation) {
+    void *diff_summand_cache = allocate(diff_summand_desc);
+    async_activation_backward(adesc, 1.f, dst_desc, dst, diff_dst_desc, diff_dst,
+                        dst_desc, workspace, 0.f,
+                        diff_summand_desc, diff_summand_cache);
+    async_sum(alpha_data, diff_summand_desc, diff_summand_cache, beta_data,
+        diff_summand_desc, diff_summand);
+    real_diff_dst_desc = diff_summand_desc.get_desc();
+    real_diff_dst = diff_summand_cache;
+  } else if (ops == batch_normalization_ops::activation) {
+    void *diff_dst_cache = allocate(diff_dst_desc);
+    async_activation_backward(adesc, 1.f, dst_desc, dst, diff_dst_desc,
+                        diff_dst, dst_desc, workspace,
+                        0.f, diff_dst_desc, diff_dst_cache);
+    real_diff_dst = diff_dst_cache;
+  }
+
+  return exit_primitive(batch_normalization_backward_internal(
+      mode, epsilon, alpha_data, src_desc, src, real_diff_dst_desc,
+      real_diff_dst, beta_data, diff_src_desc, diff_src, alpha_param,
+      diff_scale_bias_desc, scale, bias, beta_param, diff_scale, diff_bias,
+      mean_var_desc, saved_mean, saved_var));
+}
+
+inline
+sycl::event engine_ext::async_batch_normalization_backward(
+    batch_normalization_mode mode, batch_normalization_ops ops,
+    activation_desc &adesc, float epsilon, float alpha_data,
+    const memory_desc_ext &src_desc, void *src, const memory_desc_ext &dst_desc,
+    void *dst, const memory_desc_ext &diff_dst_desc, void *diff_dst,
+    float beta_data, const memory_desc_ext &diff_src_desc, void *diff_src,
+    const memory_desc_ext &diff_summand_desc, void *diff_summand,
+    float alpha_param, const memory_desc_ext &diff_scale_bias_mean_var_desc,
+    void *scale, void *bias, float beta_param, void *diff_scale,
+    void *diff_bias, void *saved_mean, void *saved_var,
+    size_t workspace_size, void *workspace) {
+
+  return async_batch_normalization_backward(
+      mode, ops, adesc, epsilon, alpha_data, src_desc, src, dst_desc, dst,
+      diff_dst_desc, diff_dst, beta_data, diff_src_desc, diff_src,
+      diff_summand_desc, diff_summand, alpha_param,
+      diff_scale_bias_mean_var_desc, scale, bias, beta_param, diff_scale,
+      diff_bias, diff_scale_bias_mean_var_desc, saved_mean, saved_var,
+      workspace_size, workspace);
+}
+
+inline
+sycl::event
+engine_ext::async_convolution_forward(convolution_desc &desc, ::dnnl::algorithm alg,
+                                float alpha, const memory_desc_ext &src_desc,
+                                void *src, const memory_desc_ext &weight_desc,
+                                void *weight, float beta,
+                                const memory_desc_ext &dst_desc, void *dst) {
+  if (scale_parameter_preprocess({{alpha, beta, dst_desc, dst}})) {
+    return sycl::event();
+  }
+  auto help_weight_desc =
+      get_group_weight_desc(desc.get_group_count(), weight_desc);
+
+  ::dnnl::primitive_attr attr;
+  attr.set_fpmath_mode(desc.get_math_mode());
+
+  auto origin_src_md = src_desc.get_desc();
+  auto origin_dst_md = dst_desc.get_desc();
+  auto origin_weight_md = help_weight_desc;
+  auto src_md = transfer_memory_desc_to_format_tag_any(origin_src_md);
+  auto dst_md = transfer_memory_desc_to_format_tag_any(origin_dst_md);
+  auto weight_md = transfer_memory_desc_to_format_tag_any(origin_weight_md);
+
+  auto primitive_args =
+      create_primitive_args_or_get<::dnnl::convolution_forward>(
+          ::dnnl::prop_kind::forward_training, alg, src_md, weight_md, dst_md,
+          desc.get_stride(), desc.get_dilate(), desc.get_padding(),
+          desc.get_padding(), attr);
+
+  auto pd = get_primitive_desc<::dnnl::convolution_forward>(
+      primitive_args.second.primitive);
+  auto optimal_src_md = pd.src_desc();
+  auto optimal_dst_md = pd.dst_desc();
+  auto optimal_weight_md = pd.weights_desc();
+
+  enter_primitive(
+      optimal_src_md.get_size() * 3 + optimal_dst_md.get_size() * 5 +
+      optimal_weight_md.get_size() * 3 + origin_dst_md.get_size() * 2);
+
+  void *optimal_src = src, *optimal_dst = dst, *optimal_weight = weight;
+  allocate_and_reorder_memory_to_optimal(origin_src_md, src, optimal_src_md,
+                                         optimal_src);
+  allocate_and_reorder_memory_to_optimal(origin_weight_md, weight,
+                                         optimal_weight_md, optimal_weight);
+
+  if (beta == 0.f) {
+    if(origin_dst_md != optimal_dst_md) {
+      optimal_dst = allocate(optimal_dst_md);
+    }
+  } else {
+    allocate_and_reorder_memory_to_optimal(origin_dst_md, dst, optimal_dst_md,
+                                           optimal_dst);
+  }
+
+  insert_arg(primitive_args.second.args, DNNL_ARG_SRC, optimal_src_md,
+             optimal_src);
+  insert_arg(primitive_args.second.args, DNNL_ARG_WEIGHTS, optimal_weight_md,
+             optimal_weight);
+  insert_arg(primitive_args.second.args, DNNL_ARG_DST, optimal_dst_md,
+             optimal_dst);
+
+  auto e = execute_primitive<::dnnl::convolution_forward>(
+      primitive_args,
+      {{alpha, beta, DNNL_ARG_DST, optimal_dst_md, optimal_dst}});
+
+  if (origin_dst_md != optimal_dst_md) {
+    e = async_reorder(1.f, optimal_dst_md, optimal_dst, 0.f, origin_dst_md,
+                      dst);
+  }
+  return exit_primitive(e);
+}
+
+inline
+sycl::event engine_ext::async_convolution_forward(
+    convolution_desc &desc, ::dnnl::algorithm alg, activation_desc &adesc,
+    float alpha_0, const memory_desc_ext &src_desc, void *src,
+    const memory_desc_ext &weight_desc, void *weight, float alpha_1,
+    const memory_desc_ext &summand_desc, void *summand,
+    const memory_desc_ext &bias_desc, void *bias,
+    const memory_desc_ext &dst_desc, void *dst) {
+
+  int channel_num = bias_desc.get_element_num();
+  auto help_weight_desc =
+      get_group_weight_desc(desc.get_group_count(), weight_desc);
+  ::dnnl::memory::desc help_bias_desc = {{channel_num},
+                                         bias_desc.get_desc().get_data_type(),
+                                         ::dnnl::memory::format_tag::a};
+  auto origin_weight_md = help_weight_desc;
+  auto origin_bias_md = help_bias_desc;
+  auto origin_src_md = src_desc.get_desc();
+  auto origin_dst_md = dst_desc.get_desc();
+  auto src_md = transfer_memory_desc_to_format_tag_any(origin_src_md);
+  auto dst_md = transfer_memory_desc_to_format_tag_any(origin_dst_md);
+  auto weight_md = transfer_memory_desc_to_format_tag_any(origin_weight_md);
+  auto bias_md = transfer_memory_desc_to_format_tag_any(origin_bias_md);
+
+  ::dnnl::primitive_attr attr;
+  attr.set_fpmath_mode(desc.get_math_mode());
+
+  auto primitive_args =
+      create_primitive_args_or_get<::dnnl::convolution_forward>(
+          ::dnnl::prop_kind::forward_training, alg, src_md, weight_md, bias_md,
+          dst_md, desc.get_stride(), desc.get_dilate(), desc.get_padding(),
+          desc.get_padding(), attr);
+
+  auto pd = get_primitive_desc<::dnnl::convolution_forward>(
+      primitive_args.second.primitive);
+  auto optimal_src_md = pd.src_desc();
+  auto optimal_dst_md = pd.dst_desc();
+  auto optimal_weight_md = pd.weights_desc();
+  auto optimal_bias_md = pd.bias_desc();
+
+  enter_primitive(optimal_src_md.get_size() + 3 * optimal_weight_md.get_size() +
+                  optimal_bias_md.get_size() + 7 * optimal_dst_md.get_size() +
+                  summand_desc.get_size());
+
+  void *optimal_src = src, *optimal_dst = dst, *optimal_weight = weight,
+       *optimal_bias = bias;
+  allocate_and_reorder_memory_to_optimal(origin_src_md, src, optimal_src_md,
+                                         optimal_src);
+  allocate_and_reorder_memory_to_optimal(origin_weight_md, weight,
+                                         optimal_weight_md, optimal_weight);
+  allocate_and_reorder_memory_to_optimal(origin_bias_md, bias, optimal_bias_md,
+                                         optimal_bias);
+  if (origin_dst_md != optimal_dst_md) {
+    optimal_dst = allocate(optimal_dst_md);
+  }
+
+  insert_arg(primitive_args.second.args, DNNL_ARG_SRC, optimal_src_md,
+             optimal_src);
+  insert_arg(primitive_args.second.args, DNNL_ARG_BIAS, optimal_bias_md,
+             optimal_bias);
+  insert_arg(primitive_args.second.args, DNNL_ARG_DST, optimal_dst_md,
+             optimal_dst);
+
+  void *cache = nullptr;
+  if (alpha_0 != 1.f) {
+    cache = allocate(optimal_weight_md);
+    _q->memcpy(cache, optimal_weight, optimal_weight_md.get_size());
+    async_scale(alpha_0, optimal_weight_md, cache);
+    insert_arg(primitive_args.second.args, DNNL_ARG_WEIGHTS, optimal_weight_md,
+               cache);
+    execute_primitive<::dnnl::convolution_forward>(
+        primitive_args,
+        {{1.f, 0.f, DNNL_ARG_DST, optimal_dst_md, optimal_dst}});
+  } else {
+    insert_arg(primitive_args.second.args, DNNL_ARG_WEIGHTS, optimal_weight_md,
+               optimal_weight);
+    execute_primitive<::dnnl::convolution_forward>(
+        primitive_args,
+        {{1.f, 0.f, DNNL_ARG_DST, optimal_dst_md, optimal_dst}});
+  }
+  if (origin_dst_md != optimal_dst_md) {
+    async_reorder(1.f, optimal_dst_md, optimal_dst, 0.f, origin_dst_md, dst);
+  }
+  async_sum(alpha_1, summand_desc, summand, 1.f, dst_desc, dst);
+  return exit_primitive(
+      async_activation_forward(adesc, 1.f, dst_desc, dst, 0.f, dst_desc, dst));
+}
+
+inline
+sycl::event engine_ext::async_convolution_backward_data(
+    convolution_desc &desc, ::dnnl::algorithm alg, float alpha,
+    const memory_desc_ext &weight_desc, void *weight,
+    const memory_desc_ext &diff_dst_desc, void *diff_dst, float beta,
+    const memory_desc_ext &diff_src_desc, void *diff_src) {
+
+  if (scale_parameter_preprocess({{alpha, beta, diff_dst_desc, diff_dst}})) {
+    return sycl::event();
+  }
+
+  auto help_weight_desc =
+      get_group_weight_desc(desc.get_group_count(), weight_desc);
+
+  auto origin_weight_md = help_weight_desc;
+  auto origin_diff_src_md = diff_src_desc.get_desc();
+  auto origin_diff_dst_md = diff_dst_desc.get_desc();
+  auto diff_src_md = transfer_memory_desc_to_format_tag_any(origin_diff_src_md);
+  auto diff_dst_md = transfer_memory_desc_to_format_tag_any(origin_diff_dst_md);
+  auto weight_md = transfer_memory_desc_to_format_tag_any(origin_weight_md);
+
+  ::dnnl::primitive_attr attr;
+  attr.set_fpmath_mode(desc.get_math_mode());
+
+  auto forward_primitive = create_primitive_desc<::dnnl::convolution_forward>(
+      ::dnnl::prop_kind::forward_training, ::dnnl::algorithm::convolution_auto,
+      diff_src_md, weight_md, diff_dst_md, desc.get_stride(), desc.get_dilate(),
+      desc.get_padding(), desc.get_padding(), attr);
+
+  auto primitive_args =
+      create_primitive_args_or_get<::dnnl::convolution_backward_data>(
+          ::dnnl::algorithm::convolution_auto, diff_src_md, weight_md,
+          diff_dst_md, desc.get_stride(), desc.get_dilate(), desc.get_padding(),
+          desc.get_padding(), forward_primitive, attr);
+
+  auto pd = get_primitive_desc<::dnnl::convolution_backward_data>(
+      primitive_args.second.primitive);
+  auto optimal_diff_src_md = pd.diff_src_desc();
+  auto optimal_diff_dst_md = pd.diff_dst_desc();
+  auto optimal_weight_md = pd.weights_desc();
+
+  enter_primitive(5 * optimal_diff_src_md.get_size() +
+                  optimal_diff_dst_md.get_size() +
+                  optimal_weight_md.get_size());
+
+  void *optimal_diff_src = diff_src, *optimal_diff_dst = diff_dst,
+       *optimal_weight = weight;
+  allocate_and_reorder_memory_to_optimal(origin_diff_dst_md, diff_dst,
+                                         optimal_diff_dst_md, optimal_diff_dst);
+  allocate_and_reorder_memory_to_optimal(origin_weight_md, weight,
+                                         optimal_weight_md, optimal_weight);
+  if (beta == 0.f) {
+    if (origin_diff_src_md != optimal_diff_src_md) {
+      optimal_diff_src = allocate(optimal_diff_src_md);
+    }
+  } else {
+    allocate_and_reorder_memory_to_optimal(
+        origin_diff_src_md, diff_src, optimal_diff_src_md, optimal_diff_src);
+  }
+
+  insert_arg(primitive_args.second.args, DNNL_ARG_DIFF_DST, optimal_diff_dst_md,
+             optimal_diff_dst);
+  insert_arg(primitive_args.second.args, DNNL_ARG_WEIGHTS, optimal_weight_md,
+             optimal_weight);
+  insert_arg(primitive_args.second.args, DNNL_ARG_DIFF_SRC, optimal_diff_src_md,
+             optimal_diff_src);
+
+  auto e = execute_primitive<::dnnl::convolution_backward_data>(
+      primitive_args,
+      {{alpha, beta, DNNL_ARG_DIFF_SRC, optimal_diff_src_md, optimal_diff_src}});
+
+  if (origin_diff_src_md != optimal_diff_src_md) {
+    e = async_reorder(1.f, optimal_diff_src_md, optimal_diff_src, 0.f,
+                      origin_diff_src_md, diff_src);
+  }
+  return exit_primitive(e);
+}
+
+inline
+sycl::event engine_ext::async_convolution_backward_weight(
+    convolution_desc &desc, ::dnnl::algorithm alg, float alpha,
+    const memory_desc_ext &src_desc, void *src,
+    const memory_desc_ext &diff_dst_desc, void *diff_dst, float beta,
+    const memory_desc_ext &diff_weight_desc, void *diff_weight) {
+
+  if (scale_parameter_preprocess(
+          {{alpha, beta, diff_weight_desc, diff_weight}})) {
+    return sycl::event();
+  }
+
+  auto help_diff_weight_desc =
+      get_group_weight_desc(desc.get_group_count(), diff_weight_desc);
+
+  ::dnnl::primitive_attr attr;
+  attr.set_fpmath_mode(desc.get_math_mode());
+
+  auto origin_diff_weight_md = help_diff_weight_desc;
+  auto origin_src_md = src_desc.get_desc();
+  auto origin_diff_dst_md = diff_dst_desc.get_desc();
+  auto src_md = transfer_memory_desc_to_format_tag_any(origin_src_md);
+  auto diff_dst_md = transfer_memory_desc_to_format_tag_any(origin_diff_dst_md);
+  auto diff_weight_md =
+      transfer_memory_desc_to_format_tag_any(origin_diff_weight_md);
+
+  auto forward_primitive = create_primitive_desc<::dnnl::convolution_forward>(
+      ::dnnl::prop_kind::forward_training, ::dnnl::algorithm::convolution_auto,
+      src_md, diff_weight_md, diff_dst_md, desc.get_stride(), desc.get_dilate(),
+      desc.get_padding(), desc.get_padding(), attr);
+
+  auto primitive_args =
+      create_primitive_args_or_get<::dnnl::convolution_backward_weights>(
+          ::dnnl::algorithm::convolution_auto, src_md, diff_weight_md,
+          diff_dst_md, desc.get_stride(), desc.get_dilate(), desc.get_padding(),
+          desc.get_padding(), forward_primitive, attr);
+
+  auto pd = get_primitive_desc<::dnnl::convolution_backward_weights>(
+      primitive_args.second.primitive);
+  auto optimal_src_md = pd.src_desc();
+  auto optimal_diff_dst_md = pd.diff_dst_desc();
+  auto optimal_diff_weight_md = pd.diff_weights_desc();
+
+  enter_primitive(optimal_diff_weight_md.get_size() * 5 +
+                  optimal_diff_dst_md.get_size() + optimal_src_md.get_size());
+
+  void *optimal_src = src, *optimal_diff_dst = diff_dst,
+       *optimal_diff_weight = diff_weight;
+  allocate_and_reorder_memory_to_optimal(origin_diff_dst_md, diff_dst,
+                                         optimal_diff_dst_md, optimal_diff_dst);
+  allocate_and_reorder_memory_to_optimal(origin_src_md, src, optimal_src_md,
+                                         optimal_src);
+  if (beta == 0.f) {
+    if (origin_diff_weight_md != optimal_diff_weight_md) {
+      optimal_diff_weight = allocate(optimal_diff_weight_md);
+    }
+  } else {
+    allocate_and_reorder_memory_to_optimal(origin_diff_weight_md, diff_weight,
+                                           optimal_diff_weight_md,
+                                           optimal_diff_weight);
+  }
+
+  insert_arg(primitive_args.second.args, DNNL_ARG_SRC, optimal_src_md,
+             optimal_src);
+  insert_arg(primitive_args.second.args, DNNL_ARG_DIFF_DST, optimal_diff_dst_md,
+             optimal_diff_dst);
+  insert_arg(primitive_args.second.args, DNNL_ARG_DIFF_WEIGHTS,
+             optimal_diff_weight_md, optimal_diff_weight);
+
+  auto e = execute_primitive<::dnnl::convolution_backward_weights>(
+      primitive_args, {{alpha, beta, DNNL_ARG_DIFF_WEIGHTS,
+                        optimal_diff_weight_md, optimal_diff_weight}});
+
+  if (origin_diff_weight_md != optimal_diff_weight_md) {
+    e = async_reorder(1.f, optimal_diff_weight_md, optimal_diff_weight, 0.f,
+                      origin_diff_weight_md, diff_weight);
+  }
+  return exit_primitive(e);
+}
+
+inline
+sycl::event engine_ext::async_convolution_backward_bias(
+    float alpha, const memory_desc_ext &diff_dst_desc, void *diff_dst,
+    float beta, const memory_desc_ext &diff_bias_desc, void *diff_bias) {
+  return async_reduction(reduction_op::sum, alpha, diff_dst_desc, diff_dst, beta,
+                   diff_bias_desc, diff_bias);
+}
+
+inline
+void engine_ext::rnn_get_weight_space_size(const rnn_desc &desc,
+                                           size_t *weight_space_size) {
+  *weight_space_size = 0;
+  rnn_forward_internal(desc, ::dnnl::prop_kind::forward_inference,
+                       memory_desc_ext(), nullptr, memory_desc_ext(), nullptr,
+                       memory_desc_ext(), nullptr, nullptr, memory_desc_ext(),
+                       nullptr, nullptr, 0, nullptr, 0, nullptr, 0, nullptr, true,
+                       weight_space_size, nullptr, nullptr);
+  return;
+}
+
+inline
+void engine_ext::rnn_get_scratchpad_workspace_size(
+    const rnn_desc &desc, ::dnnl::prop_kind kind,
+    const memory_desc_ext &src_desc, size_t *scratchpad_size,
+    size_t *workspace_size) {
+  *workspace_size = 0;
+  *scratchpad_size = 0;
+  rnn_forward_internal(desc, kind, src_desc, nullptr, memory_desc_ext(),
+                       nullptr, memory_desc_ext(), nullptr, nullptr,
+                       memory_desc_ext(), nullptr, nullptr, 0, nullptr, 0,
+                       nullptr, 0, nullptr, true, nullptr, workspace_size,
+                       scratchpad_size);
+  return;
+}
+
+inline
+sycl::event engine_ext::async_rnn_forward(
+    const rnn_desc &desc, ::dnnl::prop_kind kind,
+    const memory_desc_ext &src_desc, void *src, const memory_desc_ext &dst_desc,
+    void *dst, const memory_desc_ext &iter_desc, void *src_iter, void *dst_iter,
+    const memory_desc_ext &iter_c_desc, void *src_iter_c, void *dst_iter_c,
+    size_t weight_size, void *weight, size_t scratchpad_size, void *scratchpad,
+    size_t workspace_size, void *workspace) {
+
+  return rnn_forward_internal(
+      desc, kind, src_desc, src, dst_desc, dst, iter_desc, src_iter, dst_iter,
+      iter_c_desc, src_iter_c, dst_iter_c, weight_size, weight, workspace_size,
+      workspace, scratchpad_size, scratchpad, false, nullptr, nullptr,
+      nullptr);
+}
+
+inline
+sycl::event engine_ext::async_rnn_backward(
+    const rnn_desc &desc, const memory_desc_ext &dst_desc, void *dst,
+    void *diff_dst, const memory_desc_ext &src_desc, void *src, void *diff_src,
+    const memory_desc_ext &iter_desc, void *src_iter, void *diff_dst_iter,
+    void *diff_src_iter, const memory_desc_ext &iter_c_desc, void *src_iter_c,
+    void *diff_dst_iter_c, void *diff_src_iter_c, size_t weight_size,
+    void *weight, void *diff_weight, size_t scratchpad_size, void *scratchpad,
+    size_t workspace_size, void *workspace) {
+  ::dnnl::memory::data_type src_dt;
+  ::dnnl::memory::format_tag src_format_tag;
+  rnn_mode mode;
+  rnn_memory_format_tag format_tag;
+  rnn_bias_mode bias_mode;
+  rnn_direction direction;
+  dpct::library_data_t dt;
+  int direction_num = 1, input_size = 0, hidden_size = 0, projection_size = 0,
+      layer_size = 0, gate_num = 1, output_size = 0, data_type_size = 0,
+      seq_length = 1, batch_size = 1;
+  void *last_layer_cache = nullptr;
+  void *hidden_layer_cache = nullptr;
+  sycl::event e;
+  enter_primitive(src_desc.get_size() * 2);
+  std::vector<int> offset(9, 0);
+  std::vector<void *> data = {
+      src,
+      dst,
+      (uint8_t *)src_iter + iter_desc.get_size(),
+      nullptr,
+      (uint8_t *)src_iter_c + iter_c_desc.get_size(),
+      nullptr,
+      (uint8_t *)weight + weight_size,
+      (uint8_t *)workspace + workspace_size,
+      diff_src,
+      diff_dst,
+      (uint8_t *)diff_src_iter + iter_desc.get_size(),
+      (uint8_t *)diff_dst_iter + iter_desc.get_size(),
+      (uint8_t *)diff_src_iter_c + iter_c_desc.get_size(),
+      (uint8_t *)diff_dst_iter_c + iter_c_desc.get_size(),
+      (uint8_t *)diff_weight + weight_size,
+      scratchpad};
+
+  desc.get(&mode, &bias_mode, &direction, &dt, &input_size, &hidden_size,
+           &projection_size, &layer_size);
+
+  get_rnn_configuration(src_desc.get_desc(), direction, mode, dt, hidden_size,
+                        &src_dt, &src_format_tag, &projection_size,
+                        &output_size, &seq_length, &batch_size, &direction_num,
+                        &gate_num);
+
+  if (direction == rnn_direction::bidirectional) {
+    if (layer_size > 1) {
+      last_layer_cache = allocate(src_desc);
+      hidden_layer_cache = allocate(src_desc);
+      data[8] = last_layer_cache;
+    }
+    e = execute_rnn_backward_primitive(
+        mode, ::dnnl::rnn_direction::bidirectional_concat, bias_mode, src_dt,
+        src_format_tag, seq_length, batch_size, output_size, 2 * output_size, 1,
+        direction_num, hidden_size, gate_num, projection_size, data, offset, 1);
+    if (layer_size > 1) {
+      data[8] = hidden_layer_cache;
+      data[9] = last_layer_cache;
+      e = execute_rnn_backward_primitive(
+          mode, ::dnnl::rnn_direction::bidirectional_sum, bias_mode, src_dt,
+          src_format_tag, seq_length, batch_size, output_size, output_size, 1,
+          direction_num, hidden_size, gate_num, projection_size, data, offset,
+          layer_size - 1);
+      _q->memcpy(diff_src,
+                 ((layer_size - 1) % 2 == 0) ? last_layer_cache
+                                             : hidden_layer_cache,
+                 src_desc.get_size());
+    }
+  } else {
+    e = execute_rnn_backward_primitive(
+        mode, ::dnnl::rnn_direction::unidirectional_left2right, bias_mode,
+        src_dt, src_format_tag, seq_length, batch_size, output_size,
+        output_size, layer_size, direction_num, hidden_size, gate_num,
+        projection_size, data, offset, 1);
+  }
+
+  return exit_primitive(e);
+}
+
+inline
+size_t engine_ext::get_dropout_state_size(){
+#ifndef __INTEL_MKL__
+  throw std::runtime_error("The oneAPI Math Kernel Library (oneMKL) "
+                           "Interfaces Project does not support this API.");
+#else
+  auto r = get_internal_resource(_q);
+  if(r->random_engine_state_size == -1){
+    auto rand_engine = rng_engine_t(*_q, 0);
+    r->random_engine_state_size =
+        oneapi::mkl::rng::get_state_size(rand_engine);
+  }
+  return r->random_engine_state_size;
+#endif
+}
+
+inline size_t
+engine_ext::get_dropout_workspace_size(const memory_desc_ext &src_desc) {
+  return src_desc.get_size();
+}
+
+inline
+sycl::event engine_ext::async_dropout_forward(dropout_desc &desc,
+                                              const memory_desc_ext &src_desc,
+                                              void *src,
+                                              const memory_desc_ext &dst_desc,
+                                              void *dst, void *workspace,
+                                              size_t workspace_size) {
+  if (workspace_size < src_desc.get_size()) {
+    throw std::runtime_error("async_dropout_forward: no sufficient workspace.");
+  }
+  enter_primitive(src_desc.get_size() * 2 + dst_desc.get_size() * 2);
+  float p = desc.get_probability();
+  if (p == 1.f) {
+    return _q->memset(dst, 0, dst_desc.get_size());
+  } else if (p == 0.f) {
+    return async_reorder(1.f, src_desc, src, 0.f, dst_desc, dst);
+  }
+
+  float scale_factor = 1.f / (1.f - p);
+  void *cache = workspace;
+
+  memory_desc_ext rng_data_desc(
+      ::dnnl::memory::desc(src_desc.get_dims(), ::dnnl::memory::data_type::s32,
+                           src_desc.get_strides()));
+  if (src_desc.get_desc().get_data_type() != ::dnnl::memory::data_type::s32) {
+    cache = allocate(rng_data_desc);
+  }
+
+  desc.generate(_q, get_dropout_state_size(), rng_data_desc.get_element_num(),
+                (std::int32_t *)cache);
+
+  if (cache == workspace) {
+    async_scale(scale_factor, src_desc, workspace);
+  } else {
+    async_reorder(scale_factor, rng_data_desc, cache, 0.f, src_desc, workspace);
+  }
+
+  auto primitive_args = create_primitive_args_or_get<::dnnl::binary>(
+      ::dnnl::algorithm::binary_mul, src_desc.get_desc(), src_desc.get_desc(),
+      dst_desc.get_desc());
+
+  insert_arg(primitive_args.second.args, DNNL_ARG_SRC_0, src_desc.get_desc(),
+             src);
+  insert_arg(primitive_args.second.args, DNNL_ARG_SRC_1, src_desc.get_desc(),
+             workspace);
+  insert_arg(primitive_args.second.args, DNNL_ARG_DST, dst_desc.get_desc(),
+             dst);
+
+  return exit_primitive(execute_primitive<::dnnl::binary>(primitive_args));
+}
+
+inline
+sycl::event engine_ext::async_dropout_backward(
+    dropout_desc &desc, const memory_desc_ext &diff_dst_desc,
+    void *diff_dst, const memory_desc_ext &diff_src_desc, void *diff_src,
+    void *workspace, size_t workspace_size) {
+  enter_primitive(2 * diff_src_desc.get_size());
+  float p = desc.get_probability();
+  if (p == 1.f) {
+    return _q->memset(diff_src, 0, diff_src_desc.get_size());
+  } else if (p == 0.f) {
+    return async_reorder(1.f, diff_dst_desc, diff_dst, 0.f, diff_src_desc,
+                         diff_src);
+  }
+
+  auto primitive_args = create_primitive_args_or_get<::dnnl::binary>(
+      ::dnnl::algorithm::binary_mul, diff_dst_desc.get_desc(),
+      diff_dst_desc.get_desc(), diff_src_desc.get_desc());
+
+  insert_arg(primitive_args.second.args, DNNL_ARG_SRC_0,
+             diff_dst_desc.get_desc(), diff_dst);
+  insert_arg(primitive_args.second.args, DNNL_ARG_SRC_1,
+             diff_dst_desc.get_desc(), workspace);
+  insert_arg(primitive_args.second.args, DNNL_ARG_DST, diff_src_desc.get_desc(),
+             diff_src);
+
+  return exit_primitive(execute_primitive<::dnnl::binary>(primitive_args));
+}
+} // namespace dnnl
+} // namespace dpct
+
+#endif // __DPCT_DNNL_UTILS_HPP__
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/dpct.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/dpct.hpp
new file mode 100644
index 0000000..6880d1a
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/dpct.hpp
@@ -0,0 +1,62 @@
+//==---- dpct.hpp ---------------------------------*- C++ -*----------------==//
+//
+// Copyright (C) Intel Corporation
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+// See https://llvm.org/LICENSE.txt for license information.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef __DPCT_HPP__
+#define __DPCT_HPP__
+
+#include <sycl/sycl.hpp>
+#include <iostream>
+#include <limits.h>
+#include <math.h>
+
+template <class... Args> class dpct_kernel_name;
+template <int Arg> class dpct_kernel_scalar;
+
+#include "atomic.hpp"
+#include "device.hpp"
+#include "image.hpp"
+#include "kernel.hpp"
+#include "math.hpp"
+#include "memory.hpp"
+#include "util.hpp"
+
+#if defined(_MSC_VER)
+#define __dpct_align__(n) __declspec(align(n))
+#define __dpct_inline__ __forceinline
+#else
+#define __dpct_align__(n) __attribute__((aligned(n)))
+#define __dpct_inline__ __inline__ __attribute__((always_inline))
+#endif
+
+#if defined(_MSC_VER)
+#define __dpct_noinline__ __declspec(noinline)
+#else
+#define __dpct_noinline__ __attribute__((noinline))
+#endif
+
+#define DPCT_COMPATIBILITY_TEMP (600)
+
+namespace dpct{
+enum error_code { success = 0, default_error = 999 };
+}
+
+#define DPCT_CHECK_ERROR(expr)                                                 \
+  [&]() {                                                                      \
+    try {                                                                      \
+      expr;                                                                    \
+      return dpct::success;                                                    \
+    } catch (std::exception const &e) {                                        \
+      std::cerr << e.what() << std::endl;                                      \
+      return dpct::default_error;                                              \
+    }                                                                          \
+  }()
+
+#define DPCT_PI_F (3.14159274101257f)
+#define DPCT_PI (3.141592653589793115998)
+
+#endif // __DPCT_HPP__
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/dpl_utils.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/dpl_utils.hpp
new file mode 100644
index 0000000..79a6e74
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/dpl_utils.hpp
@@ -0,0 +1,26 @@
+//==---- dpl_utils.hpp ----------------------------*- C++ -*----------------==//
+//
+// Copyright (C) Intel Corporation
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+// See https://llvm.org/LICENSE.txt for license information.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef __DPCT_DPL_UTILS_HPP__
+#define __DPCT_DPL_UTILS_HPP__
+
+#define ONEDPL_USE_DPCPP_BACKEND 1
+#define __USE_DPCT 1
+
+#include <oneapi/dpl/execution>
+#include <oneapi/dpl/algorithm>
+#include <oneapi/dpl/numeric>
+
+#include "dpl_extras/memory.h"
+#include "dpl_extras/algorithm.h"
+#include "dpl_extras/numeric.h"
+#include "dpl_extras/iterators.h"
+#include "dpl_extras/vector.h"
+#include "dpl_extras/dpcpp_extensions.h"
+
+#endif // __DPCT_DPL_UTILS_HPP__
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/fft_utils.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/fft_utils.hpp
new file mode 100644
index 0000000..cba1b25
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/fft_utils.hpp
@@ -0,0 +1,1376 @@
+//==---- fft_utils.hpp ----------------------------*- C++ -*----------------==//
+//
+// Copyright (C) Intel Corporation
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+// See https://llvm.org/LICENSE.txt for license information.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef __DPCT_FFT_UTILS_HPP__
+#define __DPCT_FFT_UTILS_HPP__
+
+#include <sycl/sycl.hpp>
+#include <oneapi/mkl.hpp>
+#include <optional>
+#include <utility>
+#include "lib_common_utils.hpp"
+
+namespace dpct {
+namespace fft {
+/// An enumeration type to describe the FFT direction is forward or backward.
+enum fft_direction : int {
+  forward = 0,
+  backward
+};
+/// An enumeration type to describe the types of FFT input and output data.
+enum fft_type : int {
+  real_float_to_complex_float = 0,
+  complex_float_to_real_float,
+  real_double_to_complex_double,
+  complex_double_to_real_double,
+  complex_float_to_complex_float,
+  complex_double_to_complex_double,
+};
+
+/// A class to perform FFT calculation.
+class fft_engine {
+public:
+  /// Default constructor.
+  fft_engine() {}
+  /// Commit the configuration to calculate n-D FFT.
+  /// \param [in] exec_queue The queue where the calculation should be executed.
+  /// \param [in] dim Dimension number of the data.
+  /// \param [in] n Pointer to an array containing each dimension's size.
+  /// \param [in] inembed Pointer to an array containing each dimension's size
+  /// of the embedded input data.
+  /// \param [in] istride Stride size of the input data.
+  /// \param [in] idist Distance between the two batches of the input data.
+  /// \param [in] input_type Input data type.
+  /// \param [in] onembed Pointer to an array containing each dimension's size
+  /// of the embedded output data.
+  /// \param [in] ostride Stride size of the output data.
+  /// \param [in] odist Distance between the two batches of the output data.
+  /// \param [in] output_type Output data type.
+  /// \param [in] batch The number of FFT operations to perform.
+  /// \param [out] scratchpad_size The workspace size required for this FFT.
+  /// If this value is used to allocate memory, \p direction_and_placement need
+  /// to be specified explicitly to get correct result.
+  /// \param [in] direction_and_placement Explicitly specify the FFT direction
+  /// and placement info. If this value is specified, the direction parameter
+  /// will be ignored in the fft_engine::compute function. If it is not set,
+  /// forward direction(if current FFT is complex-to-complex) and out-of-place
+  /// (false) are set by default.
+  void commit(sycl::queue *exec_queue, int dim, long long *n,
+              long long *inembed, long long istride, long long idist,
+              library_data_t input_type, long long *onembed, long long ostride,
+              long long odist, library_data_t output_type, long long batch,
+              size_t *scratchpad_size,
+              std::optional<std::pair<fft_direction, bool /*is_inplace*/>>
+                  direction_and_placement = std::nullopt) {
+    _q = exec_queue;
+    init<long long>(dim, n, inembed, istride, idist, input_type, onembed,
+                    ostride, odist, output_type, batch,
+                    direction_and_placement);
+    if (scratchpad_size) {
+      if (_is_estimate_call)
+        *scratchpad_size = _workspace_estimate_bytes;
+      else
+        *scratchpad_size = _workspace_bytes;
+    }
+  }
+  /// Commit the configuration to calculate n-D FFT.
+  /// \param [in] exec_queue The queue where the calculation should be executed.
+  /// \param [in] dim Dimension number of the data.
+  /// \param [in] n Pointer to an array containing each dimension's size.
+  /// \param [in] inembed Pointer to an array containing each dimension's size
+  /// of the embedded input data.
+  /// \param [in] istride Stride size of the input data.
+  /// \param [in] idist Distance between the two batches of the input data.
+  /// \param [in] input_type Input data type.
+  /// \param [in] onembed Pointer to an array containing each dimension's size
+  /// of the embedded output data.
+  /// \param [in] ostride Stride size of the output data.
+  /// \param [in] odist Distance between the two batches of the output data.
+  /// \param [in] output_type Output data type.
+  /// \param [in] batch The number of FFT operations to perform.
+  /// \param [out] scratchpad_size The workspace size required for this FFT.
+  /// If this value is used to allocate memory, \p direction_and_placement need
+  /// to be specified explicitly to get correct result.
+  /// \param [in] direction_and_placement Explicitly specify the FFT direction
+  /// and placement info. If this value is specified, the direction parameter
+  /// will be ignored in the fft_engine::compute function. If it is not set,
+  /// forward direction(if current FFT is complex-to-complex) and out-of-place
+  /// (false) are set by default.
+  void commit(sycl::queue *exec_queue, int dim, int *n, int *inembed,
+              int istride, int idist, library_data_t input_type, int *onembed,
+              int ostride, int odist, library_data_t output_type, int batch,
+              size_t *scratchpad_size,
+              std::optional<std::pair<fft_direction, bool /*is_inplace*/>>
+                  direction_and_placement = std::nullopt) {
+    _q = exec_queue;
+    init<int>(dim, n, inembed, istride, idist, input_type, onembed, ostride,
+              odist, output_type, batch, direction_and_placement);
+    if (scratchpad_size) {
+      if (_is_estimate_call)
+        *scratchpad_size = _workspace_estimate_bytes;
+      else
+        *scratchpad_size = _workspace_bytes;
+    }
+  }
+  /// Commit the configuration to calculate n-D FFT.
+  /// \param [in] exec_queue The queue where the calculation should be executed.
+  /// \param [in] dim Dimension number of the data.
+  /// \param [in] n Pointer to an array containing each dimension's size.
+  /// \param [in] inembed Pointer to an array containing each dimension's size
+  /// of the embedded input data.
+  /// \param [in] istride Stride size of the input data.
+  /// \param [in] idist Distance between the two batches of the input data.
+  /// \param [in] onembed Pointer to an array containing each dimension's size
+  /// of the embedded output data.
+  /// \param [in] ostride Stride size of the output data.
+  /// \param [in] odist Distance between the two batches of the output data.
+  /// \param [in] type The FFT type.
+  /// \param [in] batch The number of FFT operations to perform.
+  /// \param [out] scratchpad_size The workspace size required for this FFT.
+  /// If this value is used to allocate memory, \p direction_and_placement need
+  /// to be specified explicitly to get correct result.
+  /// \param [in] direction_and_placement Explicitly specify the FFT direction
+  /// and placement info. If this value is specified, the direction parameter
+  /// will be ignored in the fft_engine::compute function. If it is not set,
+  /// forward direction(if current FFT is complex-to-complex) and out-of-place
+  /// (false) are set by default.
+  void commit(sycl::queue *exec_queue, int dim, long long *n,
+              long long *inembed, long long istride, long long idist,
+              long long *onembed, long long ostride, long long odist,
+              fft_type type, long long batch, size_t *scratchpad_size,
+              std::optional<std::pair<fft_direction, bool /*is_inplace*/>>
+                  direction_and_placement = std::nullopt) {
+    commit(exec_queue, dim, n, inembed, istride, idist,
+           fft_type_to_data_type(type).first, onembed, ostride, odist,
+           fft_type_to_data_type(type).second, batch, scratchpad_size,
+           direction_and_placement);
+  }
+  /// Commit the configuration to calculate n-D FFT.
+  /// \param [in] exec_queue The queue where the calculation should be executed.
+  /// \param [in] dim Dimension number of the data.
+  /// \param [in] n Pointer to an array containing each dimension's size.
+  /// \param [in] inembed Pointer to an array containing each dimension's size
+  /// of the embedded input data.
+  /// \param [in] istride Stride size of the input data.
+  /// \param [in] idist Distance between the two batches of the input data.
+  /// \param [in] onembed Pointer to an array containing each dimension's size
+  /// of the embedded output data.
+  /// \param [in] ostride Stride size of the output data.
+  /// \param [in] odist Distance between the two batches of the output data.
+  /// \param [in] type The FFT type.
+  /// \param [in] batch The number of FFT operations to perform.
+  /// \param [out] scratchpad_size The workspace size required for this FFT.
+  /// If this value is used to allocate memory, \p direction_and_placement need
+  /// to be specified explicitly to get correct result.
+  /// \param [in] direction_and_placement Explicitly specify the FFT direction
+  /// and placement info. If this value is specified, the direction parameter
+  /// will be ignored in the fft_engine::compute function. If it is not set,
+  /// forward direction(if current FFT is complex-to-complex) and out-of-place
+  /// (false) are set by default.
+  void commit(sycl::queue *exec_queue, int dim, int *n, int *inembed,
+              int istride, int idist, int *onembed, int ostride, int odist,
+              fft_type type, int batch, size_t *scratchpad_size,
+              std::optional<std::pair<fft_direction, bool /*is_inplace*/>>
+                  direction_and_placement = std::nullopt) {
+    commit(exec_queue, dim, n, inembed, istride, idist,
+           fft_type_to_data_type(type).first, onembed, ostride, odist,
+           fft_type_to_data_type(type).second, batch, scratchpad_size,
+           direction_and_placement);
+  }
+  /// Commit the configuration to calculate 1-D FFT.
+  /// \param [in] exec_queue The queue where the calculation should be executed.
+  /// \param [in] n1 The size of the dimension of the data.
+  /// \param [in] type The FFT type.
+  /// \param [in] batch The number of FFT operations to perform.
+  /// \param [out] scratchpad_size The workspace size required for this FFT.
+  /// If this value is used to allocate memory, \p direction_and_placement need
+  /// to be specified explicitly to get correct result.
+  /// \param [in] direction_and_placement Explicitly specify the FFT direction
+  /// and placement info. If this value is specified, the direction parameter
+  /// will be ignored in the fft_engine::compute function. If it is not set,
+  /// forward direction(if current FFT is complex-to-complex) and out-of-place
+  /// (false) are set by default.
+  void commit(sycl::queue *exec_queue, int n1, fft_type type, int batch,
+              size_t *scratchpad_size,
+              std::optional<std::pair<fft_direction, bool /*is_inplace*/>>
+                  direction_and_placement = std::nullopt) {
+    _q = exec_queue;
+    _n.resize(1);
+    _n[0] = n1;
+    std::tie(_input_type, _output_type) = fft_type_to_data_type(type);
+    _dim = 1;
+    _batch = batch;
+    _is_basic = true;
+    if (direction_and_placement.has_value()) {
+      _is_user_specified_dir_and_placement = true;
+      _direction = direction_and_placement->first;
+      _is_inplace = direction_and_placement->second;
+    }
+    config_and_commit_basic();
+    if (scratchpad_size) {
+      if (_is_estimate_call)
+        *scratchpad_size = _workspace_estimate_bytes;
+      else
+        *scratchpad_size = _workspace_bytes;
+    }
+  }
+  /// Commit the configuration to calculate 2-D FFT.
+  /// \param [in] exec_queue The queue where the calculation should be executed.
+  /// \param [in] n2 The size of the 2nd dimension (outermost) of the data.
+  /// \param [in] n1 The size of the 1st dimension (innermost) of the data.
+  /// \param [in] type The FFT type.
+  /// \param [out] scratchpad_size The workspace size required for this FFT.
+  /// If this value is used to allocate memory, \p direction_and_placement need
+  /// to be specified explicitly to get correct result.
+  /// \param [in] direction_and_placement Explicitly specify the FFT direction
+  /// and placement info. If this value is specified, the direction parameter
+  /// will be ignored in the fft_engine::compute function. If it is not set,
+  /// forward direction(if current FFT is complex-to-complex) and out-of-place
+  /// (false) are set by default.
+  void commit(sycl::queue *exec_queue, int n2, int n1, fft_type type,
+              size_t *scratchpad_size,
+              std::optional<std::pair<fft_direction, bool /*is_inplace*/>>
+                  direction_and_placement = std::nullopt) {
+    _q = exec_queue;
+    _n.resize(2);
+    _n[0] = n2;
+    _n[1] = n1;
+    std::tie(_input_type, _output_type) = fft_type_to_data_type(type);
+    _dim = 2;
+    _is_basic = true;
+    if (direction_and_placement.has_value()) {
+      _is_user_specified_dir_and_placement = true;
+      _direction = direction_and_placement->first;
+      _is_inplace = direction_and_placement->second;
+    }
+    config_and_commit_basic();
+    if (scratchpad_size) {
+      if (_is_estimate_call)
+        *scratchpad_size = _workspace_estimate_bytes;
+      else
+        *scratchpad_size = _workspace_bytes;
+    }
+  }
+  /// Commit the configuration to calculate 3-D FFT.
+  /// \param [in] exec_queue The queue where the calculation should be executed.
+  /// \param [in] n3 The size of the 3rd dimension (outermost) of the data.
+  /// \param [in] n2 The size of the 2nd dimension of the data.
+  /// \param [in] n1 The size of the 1st dimension (innermost) of the data.
+  /// \param [in] type The FFT type.
+  /// \param [out] scratchpad_size The workspace size required for this FFT.
+  /// If this value is used to allocate memory, \p direction_and_placement need
+  /// to be specified explicitly to get correct result.
+  /// \param [in] direction_and_placement Explicitly specify the FFT direction
+  /// and placement info. If this value is specified, the direction parameter
+  /// will be ignored in the fft_engine::compute function. If it is not set,
+  /// forward direction(if current FFT is complex-to-complex) and out-of-place
+  /// (false) are set by default.
+  void commit(sycl::queue *exec_queue, int n3, int n2, int n1, fft_type type,
+              size_t *scratchpad_size,
+              std::optional<std::pair<fft_direction, bool /*is_inplace*/>>
+                  direction_and_placement = std::nullopt) {
+    _q = exec_queue;
+    _n.resize(3);
+    _n[0] = n3;
+    _n[1] = n2;
+    _n[2] = n1;
+    std::tie(_input_type, _output_type) = fft_type_to_data_type(type);
+    _dim = 3;
+    _is_basic = true;
+    if (direction_and_placement.has_value()) {
+      _is_user_specified_dir_and_placement = true;
+      _direction = direction_and_placement->first;
+      _is_inplace = direction_and_placement->second;
+    }
+    config_and_commit_basic();
+    if (scratchpad_size) {
+      if (_is_estimate_call)
+        *scratchpad_size = _workspace_estimate_bytes;
+      else
+        *scratchpad_size = _workspace_bytes;
+    }
+  }
+
+  /// Create the class for calculate 1-D FFT.
+  /// \param [in] exec_queue The queue where the calculation should be executed.
+  /// \param [in] n1 The size of the dimension of the data.
+  /// \param [in] type The FFT type.
+  /// \param [in] batch The number of FFT operations to perform.
+  /// \param [in] direction_and_placement Explicitly specify the FFT direction
+  /// and placement info. If this value is specified, the direction parameter
+  /// will be ignored in the fft_engine::compute function. If it is not set,
+  /// forward direction(if current FFT is complex-to-complex) and out-of-place
+  /// (false) are set by default.
+  static fft_engine *
+  create(sycl::queue *exec_queue, int n1, fft_type type, int batch,
+         std::optional<std::pair<fft_direction, bool /*is_inplace*/>>
+             direction_and_placement = std::nullopt) {
+    fft_engine *engine = new fft_engine();
+    engine->commit(exec_queue, n1, type, batch, nullptr,
+                   direction_and_placement);
+    return engine;
+  }
+  /// Create the class for calculate 2-D FFT.
+  /// \param [in] exec_queue The queue where the calculation should be executed.
+  /// \param [in] n2 The size of the 2nd dimension (outermost) of the data.
+  /// \param [in] n1 The size of the 1st dimension (innermost) of the data.
+  /// \param [in] type The FFT type.
+  /// \param [in] direction_and_placement Explicitly specify the FFT direction
+  /// and placement info. If this value is specified, the direction parameter
+  /// will be ignored in the fft_engine::compute function. If it is not set,
+  /// forward direction(if current FFT is complex-to-complex) and out-of-place
+  /// (false) are set by default.
+  static fft_engine *
+  create(sycl::queue *exec_queue, int n2, int n1, fft_type type,
+         std::optional<std::pair<fft_direction, bool /*is_inplace*/>>
+             direction_and_placement = std::nullopt) {
+    fft_engine *engine = new fft_engine();
+    engine->commit(exec_queue, n2, n1, type, nullptr, direction_and_placement);
+    return engine;
+  }
+  /// Create the class for calculate 3-D FFT.
+  /// \param [in] exec_queue The queue where the calculation should be executed.
+  /// \param [in] n3 The size of the 3rd dimension (outermost) of the data.
+  /// \param [in] n2 The size of the 2nd dimension of the data.
+  /// \param [in] n1 The size of the 1st dimension (innermost) of the data.
+  /// \param [in] type The FFT type.
+  /// \param [in] direction_and_placement Explicitly specify the FFT direction
+  /// and placement info. If this value is specified, the direction parameter
+  /// will be ignored in the fft_engine::compute function. If it is not set,
+  /// forward direction(if current FFT is complex-to-complex) and out-of-place
+  /// (false) are set by default.
+  static fft_engine *
+  create(sycl::queue *exec_queue, int n3, int n2, int n1, fft_type type,
+         std::optional<std::pair<fft_direction, bool /*is_inplace*/>>
+             direction_and_placement = std::nullopt) {
+    fft_engine *engine = new fft_engine();
+    engine->commit(exec_queue, n3, n2, n1, type, nullptr,
+                   direction_and_placement);
+    return engine;
+  }
+  /// Create the class for calculate n-D FFT.
+  /// \param [in] exec_queue The queue where the calculation should be executed.
+  /// \param [in] dim Dimension number of the data.
+  /// \param [in] n Pointer to an array containing each dimension's size.
+  /// \param [in] inembed Pointer to an array containing each dimension's size
+  /// of the embedded input data.
+  /// \param [in] istride Stride size of the input data.
+  /// \param [in] idist Distance between the two batches of the input data.
+  /// \param [in] onembed Pointer to an array containing each dimension's size
+  /// of the embedded output data.
+  /// \param [in] ostride Stride size of the output data.
+  /// \param [in] odist Distance between the two batches of the output data.
+  /// \param [in] type The FFT type.
+  /// \param [in] batch The number of FFT operations to perform.
+  /// \param [in] direction_and_placement Explicitly specify the FFT direction
+  /// and placement info. If this value is specified, the direction parameter
+  /// will be ignored in the fft_engine::compute function. If it is not set,
+  /// forward direction(if current FFT is complex-to-complex) and out-of-place
+  /// (false) are set by default.
+  static fft_engine *
+  create(sycl::queue *exec_queue, int dim, int *n, int *inembed, int istride,
+         int idist, int *onembed, int ostride, int odist, fft_type type,
+         int batch,
+         std::optional<std::pair<fft_direction, bool /*is_inplace*/>>
+             direction_and_placement = std::nullopt) {
+    fft_engine *engine = new fft_engine();
+    engine->commit(exec_queue, dim, n, inembed, istride, idist, onembed,
+                   ostride, odist, type, batch, nullptr,
+                   direction_and_placement);
+    return engine;
+  }
+  /// Create the class for calculate FFT without commit any config.
+  static fft_engine *create() {
+    fft_engine *engine = new fft_engine();
+    return engine;
+  }
+  /// Destroy the class for calculate FFT.
+  /// \param [in] engine Pointer returned from fft_engine::craete.
+  static void destroy(fft_engine *engine) { delete engine; }
+
+#ifdef __INTEL_MKL__
+  /// Estimates the workspace size for calculating n-D FFT.
+  /// \param [in] dim Dimension number of the data.
+  /// \param [in] n Pointer to an array containing each dimension's size.
+  /// \param [in] inembed Pointer to an array containing each dimension's size
+  /// of the embedded input data.
+  /// \param [in] istride Stride size of the input data.
+  /// \param [in] idist Distance between the two batches of the input data.
+  /// \param [in] onembed Pointer to an array containing each dimension's size
+  /// of the embedded output data.
+  /// \param [in] ostride Stride size of the output data.
+  /// \param [in] odist Distance between the two batches of the output data.
+  /// \param [in] type The FFT type.
+  /// \param [in] batch The number of FFT operations to perform.
+  /// \param [out] estimated_scratchpad_size The estimated workspace size
+  /// required for this FFT. If this value is used to allocate memory,
+  /// \p direction_and_placement need to be specified explicitly to get correct
+  /// result.
+  /// \param [in] direction_and_placement Explicitly specify the FFT
+  /// direction and placement info. If it is not set, forward direction(if
+  /// current FFT is complex-to-complex) and out-of-place (false) are set by default.
+  static void
+  estimate_size(int dim, long long *n, long long *inembed, long long istride,
+                long long idist, long long *onembed, long long ostride,
+                long long odist, fft_type type, long long batch,
+                size_t *estimated_scratchpad_size,
+                std::optional<std::pair<fft_direction, bool /*is_inplace*/>>
+                    direction_and_placement = std::nullopt) {
+    fft_engine *engine = fft_engine::create();
+    engine->_is_estimate_call = true;
+    engine->commit(&dpct::get_default_queue(), dim, n, inembed, istride, idist,
+                   fft_type_to_data_type(type).first, onembed, ostride, odist,
+                   fft_type_to_data_type(type).second, batch,
+                   estimated_scratchpad_size, direction_and_placement);
+    fft_engine::destroy(engine);
+  }
+  /// Estimates the workspace size for calculating n-D FFT.
+  /// \param [in] dim Dimension number of the data.
+  /// \param [in] n Pointer to an array containing each dimension's size.
+  /// \param [in] inembed Pointer to an array containing each dimension's size
+  /// of the embedded input data.
+  /// \param [in] istride Stride size of the input data.
+  /// \param [in] idist Distance between the two batches of the input data.
+  /// \param [in] onembed Pointer to an array containing each dimension's size
+  /// of the embedded output data.
+  /// \param [in] ostride Stride size of the output data.
+  /// \param [in] odist Distance between the two batches of the output data.
+  /// \param [in] type The FFT type.
+  /// \param [in] batch The number of FFT operations to perform.
+  /// \param [out] estimated_scratchpad_size The estimated workspace size
+  /// required for this FFT. If this value is used to allocate memory,
+  /// \p direction_and_placement need to be specified explicitly to get correct
+  /// result.
+  /// \param [in] direction_and_placement Explicitly specify the FFT
+  /// direction and placement info. If it is not set, forward direction(if
+  /// current FFT is complex-to-complex) and out-of-place (false) are set by default.
+  static void
+  estimate_size(int dim, int *n, int *inembed, int istride, int idist,
+                int *onembed, int ostride, int odist, fft_type type, int batch,
+                size_t *estimated_scratchpad_size,
+                std::optional<std::pair<fft_direction, bool /*is_inplace*/>>
+                    direction_and_placement = std::nullopt) {
+    fft_engine *engine = fft_engine::create();
+    engine->_is_estimate_call = true;
+    engine->commit(&dpct::get_default_queue(), dim, n, inembed, istride, idist,
+                   fft_type_to_data_type(type).first, onembed, ostride, odist,
+                   fft_type_to_data_type(type).second, batch,
+                   estimated_scratchpad_size, direction_and_placement);
+    fft_engine::destroy(engine);
+  }
+  /// Estimates the workspace size for calculating 1-D FFT.
+  /// \param [in] n1 The size of the dimension of the data.
+  /// \param [in] type The FFT type.
+  /// \param [in] batch The number of FFT operations to perform.
+  /// \param [out] estimated_scratchpad_size The estimated workspace size
+  /// required for this FFT. If this value is used to allocate memory,
+  /// \p direction_and_placement need to be specified explicitly to get correct
+  /// result.
+  /// \param [in] direction_and_placement Explicitly specify the FFT direction
+  /// and placement info. If it is not set, forward direction(if current FFT is
+  /// complex-to-complex) and out-of-place (false) are set by default.
+  static void
+  estimate_size(int n1, fft_type type, int batch,
+                size_t *estimated_scratchpad_size,
+                std::optional<std::pair<fft_direction, bool /*is_inplace*/>>
+                    direction_and_placement = std::nullopt) {
+    fft_engine *engine = fft_engine::create();
+    engine->_is_estimate_call = true;
+    engine->commit(&dpct::get_default_queue(), n1, type, batch,
+                   estimated_scratchpad_size, direction_and_placement);
+    fft_engine::destroy(engine);
+  }
+  /// Estimates the workspace size for calculating 2-D FFT.
+  /// \param [in] n2 The size of the 2nd dimension (outermost) of the data.
+  /// \param [in] n1 The size of the 1st dimension (innermost) of the data.
+  /// \param [in] type The FFT type.
+  /// \param [out] estimated_scratchpad_size The estimated workspace size
+  /// required for this FFT. If this value is used to allocate memory,
+  /// \p direction_and_placement need to be specified explicitly to get correct
+  /// result.
+  /// \param [in] direction_and_placement Explicitly specify the FFT
+  /// direction and placement info. If it is not set, forward direction(if
+  /// current FFT is complex-to-complex) and out-of-place (false) are set by default.
+  static void
+  estimate_size(int n2, int n1, fft_type type,
+                size_t *estimated_scratchpad_size,
+                std::optional<std::pair<fft_direction, bool /*is_inplace*/>>
+                    direction_and_placement = std::nullopt) {
+    fft_engine *engine = fft_engine::create();
+    engine->_is_estimate_call = true;
+    engine->commit(&dpct::get_default_queue(), n2, n1, type,
+                   estimated_scratchpad_size, direction_and_placement);
+    fft_engine::destroy(engine);
+  }
+  /// Estimates the workspace size for calculating 3-D FFT.
+  /// \param [in] n3 The size of the 3rd dimension (outermost) of the data.
+  /// \param [in] n2 The size of the 2nd dimension of the data.
+  /// \param [in] n1 The size of the 1st dimension (innermost) of the data.
+  /// \param [in] type The FFT type.
+  /// \param [out] estimated_scratchpad_size The estimated workspace size
+  /// required for this FFT. If this value is used to allocate memory,
+  /// \p direction_and_placement need to be specified explicitly to get correct
+  /// result.
+  /// \param [in] direction_and_placement Explicitly specify the FFT
+  /// direction and placement info. If it is not set, forward direction(if
+  /// current FFT is complex-to-complex) and out-of-place (false) are set by default.
+  static void
+  estimate_size(int n3, int n2, int n1, fft_type type,
+                size_t *estimated_scratchpad_size,
+                std::optional<std::pair<fft_direction, bool /*is_inplace*/>>
+                    direction_and_placement = std::nullopt) {
+    fft_engine *engine = fft_engine::create();
+    engine->_is_estimate_call = true;
+    engine->commit(&dpct::get_default_queue(), n3, n2, n1, type,
+                   estimated_scratchpad_size, direction_and_placement);
+    fft_engine::destroy(engine);
+  }
+#endif
+
+  /// Execute the FFT calculation.
+  /// \param [in] input Pointer to the input data.
+  /// \param [out] output Pointer to the output data.
+  /// \param [in] direction The FFT direction.
+  template <typename input_t, typename output_t>
+  void compute(input_t *input, output_t *output, fft_direction direction) {
+    if (_input_type == library_data_t::complex_float &&
+        _output_type == library_data_t::complex_float) {
+      compute_complex<float, oneapi::mkl::dft::precision::SINGLE>(
+          (float *)input, (float *)output, direction);
+    } else if (_input_type == library_data_t::complex_double &&
+               _output_type == library_data_t::complex_double) {
+      compute_complex<double, oneapi::mkl::dft::precision::DOUBLE>(
+          (double *)input, (double *)output, direction);
+    } else if (_input_type == library_data_t::real_float &&
+               _output_type == library_data_t::complex_float) {
+      _direction = direction;
+      compute_real<float, oneapi::mkl::dft::precision::SINGLE>((float *)input,
+                                                               (float *)output);
+    } else if (_input_type == library_data_t::complex_float &&
+               _output_type == library_data_t::real_float) {
+      _direction = direction;
+      compute_real<float, oneapi::mkl::dft::precision::SINGLE>((float *)input,
+                                                               (float *)output);
+    } else if (_input_type == library_data_t::real_double &&
+               _output_type == library_data_t::complex_double) {
+      _direction = direction;
+      compute_real<double, oneapi::mkl::dft::precision::DOUBLE>(
+          (double *)input, (double *)output);
+    } else if (_input_type == library_data_t::complex_double &&
+               _output_type == library_data_t::real_double) {
+      _direction = direction;
+      compute_real<double, oneapi::mkl::dft::precision::DOUBLE>(
+          (double *)input, (double *)output);
+    }
+  }
+  template <>
+  void compute(float *input, sycl::float2 *output, fft_direction direction) {
+    _direction = direction;
+    compute_real<float, oneapi::mkl::dft::precision::SINGLE>((float *)input,
+                                                             (float *)output);
+  }
+  template <>
+  void compute(sycl::float2 *input, float *output, fft_direction direction) {
+    _direction = direction;
+    compute_real<float, oneapi::mkl::dft::precision::SINGLE>((float *)input,
+                                                             (float *)output);
+  }
+  template <>
+  void compute(double *input, sycl::double2 *output, fft_direction direction) {
+    _direction = direction;
+    compute_real<double, oneapi::mkl::dft::precision::DOUBLE>((double *)input,
+                                                              (double *)output);
+  }
+  template <>
+  void compute(sycl::double2 *input, double *output, fft_direction direction) {
+    _direction = direction;
+    compute_real<double, oneapi::mkl::dft::precision::DOUBLE>((double *)input,
+                                                              (double *)output);
+  }
+  template <>
+  void compute(sycl::float2 *input, sycl::float2 *output,
+               fft_direction direction) {
+    compute_complex<float, oneapi::mkl::dft::precision::SINGLE>(
+        (float *)input, (float *)output, direction);
+  }
+  template <>
+  void compute(sycl::double2 *input, sycl::double2 *output,
+               fft_direction direction) {
+    compute_complex<double, oneapi::mkl::dft::precision::DOUBLE>(
+        (double *)input, (double *)output, direction);
+  }
+  /// Setting the user's SYCL queue for calculation.
+  /// \param [in] q Pointer to the SYCL queue.
+  void set_queue(sycl::queue *q) { _q = q; }
+#ifdef __INTEL_MKL__
+  /// Setting whether to use external or internal workspace.
+  /// \param [in] flag True means using internal workspace. False means using
+  /// external workspace.
+  void use_internal_workspace(bool flag = true) {
+    _use_external_workspace = !flag;
+  }
+  /// Specify the external workspace.
+  /// \param [in] ptr Pointer to the workspace.
+  void set_workspace(void *ptr) {
+    if (!_use_external_workspace) {
+      return;
+    }
+    if (_input_type == library_data_t::complex_float &&
+        _output_type == library_data_t::complex_float) {
+      if (_q->get_device().is_gpu()) {
+        auto data = dpct::detail::get_memory<float>(ptr);
+        _desc_sc->set_workspace(data);
+      }
+    } else if (_input_type == library_data_t::complex_double &&
+               _output_type == library_data_t::complex_double) {
+      if (_q->get_device().is_gpu()) {
+        auto data = dpct::detail::get_memory<double>(ptr);
+        _desc_dc->set_workspace(data);
+      }
+    } else if ((_input_type == library_data_t::real_float &&
+                _output_type == library_data_t::complex_float) ||
+               (_input_type == library_data_t::complex_float &&
+                _output_type == library_data_t::real_float)) {
+      if (_q->get_device().is_gpu()) {
+        auto data = dpct::detail::get_memory<float>(ptr);
+        _desc_sr->set_workspace(data);
+      }
+    } else if ((_input_type == library_data_t::real_double &&
+                _output_type == library_data_t::complex_double) ||
+               (_input_type == library_data_t::complex_double &&
+                _output_type == library_data_t::real_double)) {
+      if (_q->get_device().is_gpu()) {
+        auto data = dpct::detail::get_memory<double>(ptr);
+        _desc_dr->set_workspace(data);
+      }
+    } else {
+      throw sycl::exception(sycl::make_error_code(sycl::errc::invalid),
+                            "invalid fft type");
+    }
+  }
+#endif
+  /// Get the workspace size.
+  /// \param [out] scratchpad_size Workspace size in bytes.
+  void get_workspace_size(size_t *scratchpad_size) {
+    if (scratchpad_size) {
+      *scratchpad_size = _workspace_bytes;
+    }
+  }
+
+private:
+  static std::pair<library_data_t, library_data_t>
+  fft_type_to_data_type(fft_type type) {
+    switch (type) {
+    case fft_type::real_float_to_complex_float: {
+      return std::make_pair(library_data_t::real_float,
+                            library_data_t::complex_float);
+    }
+    case fft_type::complex_float_to_real_float: {
+      return std::make_pair(library_data_t::complex_float,
+                            library_data_t::real_float);
+    }
+    case fft_type::real_double_to_complex_double: {
+      return std::make_pair(library_data_t::real_double,
+                            library_data_t::complex_double);
+    }
+    case fft_type::complex_double_to_real_double: {
+      return std::make_pair(library_data_t::complex_double,
+                            library_data_t::real_double);
+    }
+    case fft_type::complex_float_to_complex_float: {
+      return std::make_pair(library_data_t::complex_float,
+                            library_data_t::complex_float);
+    }
+    case fft_type::complex_double_to_complex_double: {
+      return std::make_pair(library_data_t::complex_double,
+                            library_data_t::complex_double);
+    }
+    }
+  }
+
+  void config_and_commit_basic() {
+    if (_input_type == library_data_t::complex_float &&
+        _output_type == library_data_t::complex_float) {
+      _desc_sc = std::make_shared<
+          oneapi::mkl::dft::descriptor<oneapi::mkl::dft::precision::SINGLE,
+                                       oneapi::mkl::dft::domain::COMPLEX>>(_n);
+      std::int64_t distance = 1;
+      for (auto i : _n)
+        distance = distance * i;
+      _fwd_dist = distance;
+      _bwd_dist = distance;
+      _desc_sc->set_value(oneapi::mkl::dft::config_param::FWD_DISTANCE,
+                          distance);
+      _desc_sc->set_value(oneapi::mkl::dft::config_param::BWD_DISTANCE,
+                          distance);
+      _desc_sc->set_value(oneapi::mkl::dft::config_param::NUMBER_OF_TRANSFORMS,
+                          _batch);
+#ifdef __INTEL_MKL__
+      if (_is_user_specified_dir_and_placement && _is_inplace)
+        _desc_sc->set_value(oneapi::mkl::dft::config_param::PLACEMENT,
+                            DFTI_CONFIG_VALUE::DFTI_INPLACE);
+      else
+        _desc_sc->set_value(oneapi::mkl::dft::config_param::PLACEMENT,
+                            DFTI_CONFIG_VALUE::DFTI_NOT_INPLACE);
+      if (_use_external_workspace) {
+        if (_q->get_device().is_gpu()) {
+          _desc_sc->set_value(
+              oneapi::mkl::dft::config_param::WORKSPACE,
+              oneapi::mkl::dft::config_value::WORKSPACE_EXTERNAL);
+        }
+      }
+      if (_is_estimate_call) {
+        if (_q->get_device().is_gpu()) {
+          _desc_sc->get_value(
+              oneapi::mkl::dft::config_param::WORKSPACE_ESTIMATE_BYTES,
+              &_workspace_estimate_bytes);
+        }
+      } else {
+        _desc_sc->commit(*_q);
+        if (_q->get_device().is_gpu()) {
+          _desc_sc->get_value(oneapi::mkl::dft::config_param::WORKSPACE_BYTES,
+                              &_workspace_bytes);
+        }
+      }
+#else
+      if (_is_user_specified_dir_and_placement && _is_inplace)
+        _desc_sc->set_value(oneapi::mkl::dft::config_param::PLACEMENT,
+                            oneapi::mkl::dft::config_value::INPLACE);
+      else
+        _desc_sc->set_value(oneapi::mkl::dft::config_param::PLACEMENT,
+                            oneapi::mkl::dft::config_value::NOT_INPLACE);
+      _desc_sc->commit(*_q);
+#endif
+    } else if (_input_type == library_data_t::complex_double &&
+               _output_type == library_data_t::complex_double) {
+      _desc_dc = std::make_shared<
+          oneapi::mkl::dft::descriptor<oneapi::mkl::dft::precision::DOUBLE,
+                                       oneapi::mkl::dft::domain::COMPLEX>>(_n);
+      std::int64_t distance = 1;
+      for (auto i : _n)
+        distance = distance * i;
+      _fwd_dist = distance;
+      _bwd_dist = distance;
+      _desc_dc->set_value(oneapi::mkl::dft::config_param::FWD_DISTANCE,
+                          distance);
+      _desc_dc->set_value(oneapi::mkl::dft::config_param::BWD_DISTANCE,
+                          distance);
+      _desc_dc->set_value(oneapi::mkl::dft::config_param::NUMBER_OF_TRANSFORMS,
+                          _batch);
+#ifdef __INTEL_MKL__
+      if (_is_user_specified_dir_and_placement && _is_inplace)
+        _desc_dc->set_value(oneapi::mkl::dft::config_param::PLACEMENT,
+                            DFTI_CONFIG_VALUE::DFTI_INPLACE);
+      else
+        _desc_dc->set_value(oneapi::mkl::dft::config_param::PLACEMENT,
+                            DFTI_CONFIG_VALUE::DFTI_NOT_INPLACE);
+      if (_use_external_workspace) {
+        if (_q->get_device().is_gpu()) {
+          _desc_dc->set_value(
+              oneapi::mkl::dft::config_param::WORKSPACE,
+              oneapi::mkl::dft::config_value::WORKSPACE_EXTERNAL);
+        }
+      }
+      if (_is_estimate_call) {
+        if (_q->get_device().is_gpu()) {
+          _desc_dc->get_value(
+              oneapi::mkl::dft::config_param::WORKSPACE_ESTIMATE_BYTES,
+              &_workspace_estimate_bytes);
+        }
+      } else {
+        _desc_dc->commit(*_q);
+        if (_q->get_device().is_gpu()) {
+          _desc_dc->get_value(oneapi::mkl::dft::config_param::WORKSPACE_BYTES,
+                              &_workspace_bytes);
+        }
+      }
+#else
+      if (_is_user_specified_dir_and_placement && _is_inplace)
+        _desc_dc->set_value(oneapi::mkl::dft::config_param::PLACEMENT,
+                            oneapi::mkl::dft::config_value::INPLACE);
+      else
+        _desc_dc->set_value(oneapi::mkl::dft::config_param::PLACEMENT,
+                            oneapi::mkl::dft::config_value::NOT_INPLACE);
+      _desc_dc->commit(*_q);
+#endif
+    } else if ((_input_type == library_data_t::real_float &&
+                _output_type == library_data_t::complex_float) ||
+               (_input_type == library_data_t::complex_float &&
+                _output_type == library_data_t::real_float)) {
+      _desc_sr = std::make_shared<oneapi::mkl::dft::descriptor<
+          oneapi::mkl::dft::precision::SINGLE, oneapi::mkl::dft::domain::REAL>>(
+          _n);
+      if (_input_type == library_data_t::real_float &&
+          _output_type == library_data_t::complex_float)
+        _direction = fft_direction::forward;
+      else
+        _direction = fft_direction::backward;
+      _desc_sr->set_value(oneapi::mkl::dft::config_param::NUMBER_OF_TRANSFORMS,
+                          _batch);
+#ifdef __INTEL_MKL__
+      if (_is_user_specified_dir_and_placement && _is_inplace) {
+        _desc_sr->set_value(oneapi::mkl::dft::config_param::PLACEMENT,
+                            DFTI_CONFIG_VALUE::DFTI_INPLACE);
+        set_stride_and_distance_basic<true>(_desc_sr);
+      } else {
+        _desc_sr->set_value(oneapi::mkl::dft::config_param::PLACEMENT,
+                            DFTI_CONFIG_VALUE::DFTI_NOT_INPLACE);
+        set_stride_and_distance_basic<false>(_desc_sr);
+      }
+      if (_use_external_workspace) {
+        if (_q->get_device().is_gpu()) {
+          _desc_sr->set_value(
+              oneapi::mkl::dft::config_param::WORKSPACE,
+              oneapi::mkl::dft::config_value::WORKSPACE_EXTERNAL);
+        }
+      }
+      if (_is_estimate_call) {
+        if (_q->get_device().is_gpu()) {
+          _desc_sr->get_value(
+              oneapi::mkl::dft::config_param::WORKSPACE_ESTIMATE_BYTES,
+              &_workspace_estimate_bytes);
+        }
+      } else {
+        _desc_sr->commit(*_q);
+        if (_q->get_device().is_gpu()) {
+          _desc_sr->get_value(oneapi::mkl::dft::config_param::WORKSPACE_BYTES,
+                              &_workspace_bytes);
+        }
+      }
+#else
+      if (_is_user_specified_dir_and_placement && _is_inplace) {
+        _desc_sr->set_value(oneapi::mkl::dft::config_param::PLACEMENT,
+                            oneapi::mkl::dft::config_value::INPLACE);
+        set_stride_and_distance_basic<true>(_desc_sr);
+      } else {
+        _desc_sr->set_value(oneapi::mkl::dft::config_param::PLACEMENT,
+                            oneapi::mkl::dft::config_value::NOT_INPLACE);
+        set_stride_and_distance_basic<false>(_desc_sr);
+      }
+      _desc_sr->commit(*_q);
+#endif
+    } else if ((_input_type == library_data_t::real_double &&
+                _output_type == library_data_t::complex_double) ||
+               (_input_type == library_data_t::complex_double &&
+                _output_type == library_data_t::real_double)) {
+      _desc_dr = std::make_shared<oneapi::mkl::dft::descriptor<
+          oneapi::mkl::dft::precision::DOUBLE, oneapi::mkl::dft::domain::REAL>>(
+          _n);
+      if (_input_type == library_data_t::real_double &&
+          _output_type == library_data_t::complex_double)
+        _direction = fft_direction::forward;
+      else
+        _direction = fft_direction::backward;
+      _desc_dr->set_value(oneapi::mkl::dft::config_param::NUMBER_OF_TRANSFORMS,
+                          _batch);
+#ifdef __INTEL_MKL__
+      if (_is_user_specified_dir_and_placement && _is_inplace) {
+        _desc_dr->set_value(oneapi::mkl::dft::config_param::PLACEMENT,
+                            DFTI_CONFIG_VALUE::DFTI_INPLACE);
+        set_stride_and_distance_basic<true>(_desc_dr);
+      } else {
+        _desc_dr->set_value(oneapi::mkl::dft::config_param::PLACEMENT,
+                            DFTI_CONFIG_VALUE::DFTI_NOT_INPLACE);
+        set_stride_and_distance_basic<false>(_desc_dr);
+      }
+      if (_use_external_workspace) {
+        if (_q->get_device().is_gpu()) {
+          _desc_dr->set_value(
+              oneapi::mkl::dft::config_param::WORKSPACE,
+              oneapi::mkl::dft::config_value::WORKSPACE_EXTERNAL);
+        }
+      }
+      if (_is_estimate_call) {
+        if (_q->get_device().is_gpu()) {
+          _desc_dr->get_value(
+              oneapi::mkl::dft::config_param::WORKSPACE_ESTIMATE_BYTES,
+              &_workspace_estimate_bytes);
+        }
+      } else {
+        _desc_dr->commit(*_q);
+        if (_q->get_device().is_gpu()) {
+          _desc_dr->get_value(oneapi::mkl::dft::config_param::WORKSPACE_BYTES,
+                              &_workspace_bytes);
+        }
+      }
+#else
+      if (_is_user_specified_dir_and_placement && _is_inplace) {
+        _desc_dr->set_value(oneapi::mkl::dft::config_param::PLACEMENT,
+                            oneapi::mkl::dft::config_value::INPLACE);
+        set_stride_and_distance_basic<true>(_desc_dr);
+      } else {
+        _desc_dr->set_value(oneapi::mkl::dft::config_param::PLACEMENT,
+                            oneapi::mkl::dft::config_value::NOT_INPLACE);
+        set_stride_and_distance_basic<false>(_desc_dr);
+      }
+      _desc_dr->commit(*_q);
+#endif
+    } else {
+      throw sycl::exception(sycl::make_error_code(sycl::errc::invalid),
+                            "invalid fft type");
+    }
+  }
+
+  void config_and_commit_advanced() {
+#ifdef __INTEL_MKL__
+#define CONFIG_AND_COMMIT(DESC, PREC, DOM, TYPE)                               \
+  {                                                                            \
+    DESC = std::make_shared<oneapi::mkl::dft::descriptor<                      \
+        oneapi::mkl::dft::precision::PREC, oneapi::mkl::dft::domain::DOM>>(    \
+        _n);                                                                   \
+    set_stride_advanced(DESC);                                                 \
+    DESC->set_value(oneapi::mkl::dft::config_param::FWD_DISTANCE, _fwd_dist);  \
+    DESC->set_value(oneapi::mkl::dft::config_param::BWD_DISTANCE, _bwd_dist);  \
+    DESC->set_value(oneapi::mkl::dft::config_param::NUMBER_OF_TRANSFORMS,      \
+                    _batch);                                                   \
+    if (_is_user_specified_dir_and_placement && _is_inplace)                   \
+      DESC->set_value(oneapi::mkl::dft::config_param::PLACEMENT,               \
+                      DFTI_CONFIG_VALUE::DFTI_INPLACE);                        \
+    else                                                                       \
+      DESC->set_value(oneapi::mkl::dft::config_param::PLACEMENT,               \
+                      DFTI_CONFIG_VALUE::DFTI_NOT_INPLACE);                    \
+    if (_use_external_workspace) {                                             \
+      DESC->set_value(oneapi::mkl::dft::config_param::WORKSPACE,               \
+                      oneapi::mkl::dft::config_value::WORKSPACE_EXTERNAL);     \
+    }                                                                          \
+    if (_is_estimate_call) {                                                   \
+      if (_q->get_device().is_gpu()) {                                         \
+        DESC->get_value(                                                       \
+            oneapi::mkl::dft::config_param::WORKSPACE_ESTIMATE_BYTES,          \
+            &_workspace_estimate_bytes);                                       \
+      }                                                                        \
+    } else {                                                                   \
+      DESC->commit(*_q);                                                       \
+      if (_is_estimate_call) {                                                 \
+        DESC->get_value(oneapi::mkl::dft::config_param::WORKSPACE_BYTES,       \
+                        &_workspace_bytes);                                    \
+      }                                                                        \
+    }                                                                          \
+  }
+#else
+#define CONFIG_AND_COMMIT(DESC, PREC, DOM, TYPE)                               \
+  {                                                                            \
+    DESC = std::make_shared<oneapi::mkl::dft::descriptor<                      \
+        oneapi::mkl::dft::precision::PREC, oneapi::mkl::dft::domain::DOM>>(    \
+        _n);                                                                   \
+    set_stride_advanced(DESC);                                                 \
+    DESC->set_value(oneapi::mkl::dft::config_param::FWD_DISTANCE, _fwd_dist);  \
+    DESC->set_value(oneapi::mkl::dft::config_param::BWD_DISTANCE, _bwd_dist);  \
+    DESC->set_value(oneapi::mkl::dft::config_param::NUMBER_OF_TRANSFORMS,      \
+                    _batch);                                                   \
+    if (_is_user_specified_dir_and_placement && _is_inplace)                   \
+      DESC->set_value(oneapi::mkl::dft::config_param::PLACEMENT,               \
+                      oneapi::mkl::dft::config_value::INPLACE);                \
+    else                                                                       \
+      DESC->set_value(oneapi::mkl::dft::config_param::PLACEMENT,               \
+                      oneapi::mkl::dft::config_value::NOT_INPLACE);            \
+    DESC->commit(*_q);                                                         \
+  }
+#endif
+
+    if (_input_type == library_data_t::complex_float &&
+        _output_type == library_data_t::complex_float) {
+      CONFIG_AND_COMMIT(_desc_sc, SINGLE, COMPLEX, float);
+    } else if (_input_type == library_data_t::complex_double &&
+               _output_type == library_data_t::complex_double) {
+      CONFIG_AND_COMMIT(_desc_dc, DOUBLE, COMPLEX, double);
+    } else if ((_input_type == library_data_t::real_float &&
+                _output_type == library_data_t::complex_float) ||
+               (_input_type == library_data_t::complex_float &&
+                _output_type == library_data_t::real_float)) {
+      CONFIG_AND_COMMIT(_desc_sr, SINGLE, REAL, float);
+    } else if ((_input_type == library_data_t::real_double &&
+                _output_type == library_data_t::complex_double) ||
+               (_input_type == library_data_t::complex_double &&
+                _output_type == library_data_t::real_double)) {
+      CONFIG_AND_COMMIT(_desc_dr, DOUBLE, REAL, double);
+    } else {
+      throw sycl::exception(sycl::make_error_code(sycl::errc::invalid),
+                            "invalid fft type");
+    }
+#undef CONFIG_AND_COMMIT
+  }
+
+  template <typename T>
+  void init(int dim, T *n, T *inembed, T istride, T idist,
+            library_data_t input_type, T *onembed, T ostride, T odist,
+            library_data_t output_type, T batch,
+            std::optional<std::pair<fft_direction, bool /*is_inplace*/>>
+                direction_and_placement) {
+    if (direction_and_placement.has_value()) {
+      _is_user_specified_dir_and_placement = true;
+      _direction = direction_and_placement->first;
+      _is_inplace = direction_and_placement->second;
+    }
+    _n.resize(dim);
+    _inembed.resize(dim);
+    _onembed.resize(dim);
+    _input_type = input_type;
+    _output_type = output_type;
+    for (int i = 0; i < dim; i++) {
+      _n[i] = n[i];
+    }
+    if (inembed && onembed) {
+      for (int i = 0; i < dim; i++) {
+        _inembed[i] = inembed[i];
+        _onembed[i] = onembed[i];
+      }
+      _istride = istride;
+      _ostride = ostride;
+
+      if ((_input_type == library_data_t::real_float &&
+           _output_type == library_data_t::complex_float) ||
+          (_input_type == library_data_t::real_double &&
+           _output_type == library_data_t::complex_double)) {
+        _fwd_dist = idist;
+        _bwd_dist = odist;
+      } else if ((_output_type == library_data_t::real_float &&
+                  _input_type == library_data_t::complex_float) ||
+                 (_output_type == library_data_t::real_double &&
+                  _input_type == library_data_t::complex_double)) {
+        _fwd_dist = odist;
+        _bwd_dist = idist;
+      } else {
+        if (_is_user_specified_dir_and_placement &&
+            (_direction == fft_direction::backward)) {
+          _fwd_dist = odist;
+          _bwd_dist = idist;
+        } else {
+          _fwd_dist = idist;
+          _bwd_dist = odist;
+        }
+      }
+    } else {
+      _is_basic = true;
+    }
+    _batch = batch;
+    _dim = dim;
+
+    if (_is_basic)
+      config_and_commit_basic();
+    else
+      config_and_commit_advanced();
+  }
+  template <class Desc_t>
+  void set_stride_advanced(std::shared_ptr<Desc_t> desc) {
+    if (_dim == 1) {
+      std::int64_t input_stride[2] = {0, _istride};
+      std::int64_t output_stride[2] = {0, _ostride};
+      desc->set_value(oneapi::mkl::dft::config_param::INPUT_STRIDES,
+                      input_stride);
+      desc->set_value(oneapi::mkl::dft::config_param::OUTPUT_STRIDES,
+                      output_stride);
+    } else if (_dim == 2) {
+      std::int64_t input_stride[3] = {0, _inembed[1] * _istride, _istride};
+      std::int64_t output_stride[3] = {0, _onembed[1] * _ostride, _ostride};
+      desc->set_value(oneapi::mkl::dft::config_param::INPUT_STRIDES,
+                      input_stride);
+      desc->set_value(oneapi::mkl::dft::config_param::OUTPUT_STRIDES,
+                      output_stride);
+    } else if (_dim == 3) {
+      std::int64_t input_stride[4] = {0, _inembed[2] * _inembed[1] * _istride,
+                                      _inembed[2] * _istride, _istride};
+      std::int64_t output_stride[4] = {0, _onembed[2] * _onembed[1] * _ostride,
+                                       _onembed[2] * _ostride, _ostride};
+      desc->set_value(oneapi::mkl::dft::config_param::INPUT_STRIDES,
+                      input_stride);
+      desc->set_value(oneapi::mkl::dft::config_param::OUTPUT_STRIDES,
+                      output_stride);
+    }
+  }
+
+  template <class Desc_t> void swap_distance(std::shared_ptr<Desc_t> desc) {
+    desc->set_value(oneapi::mkl::dft::config_param::FWD_DISTANCE, _bwd_dist);
+    desc->set_value(oneapi::mkl::dft::config_param::BWD_DISTANCE, _fwd_dist);
+    std::int64_t temp = _bwd_dist;
+    _bwd_dist = _fwd_dist;
+    _fwd_dist = temp;
+  }
+
+  template <bool Is_inplace, class Desc_t>
+  void set_stride_and_distance_basic(std::shared_ptr<Desc_t> desc) {
+    std::int64_t forward_distance = 0;
+    std::int64_t backward_distance = 0;
+
+#define SET_STRIDE                                                             \
+  {                                                                            \
+    if (_direction == fft_direction::forward) {                                \
+      desc->set_value(oneapi::mkl::dft::config_param::INPUT_STRIDES,           \
+                      real_stride);                                            \
+      desc->set_value(oneapi::mkl::dft::config_param::OUTPUT_STRIDES,          \
+                      complex_stride);                                         \
+    } else {                                                                   \
+      desc->set_value(oneapi::mkl::dft::config_param::INPUT_STRIDES,           \
+                      complex_stride);                                         \
+      desc->set_value(oneapi::mkl::dft::config_param::OUTPUT_STRIDES,          \
+                      real_stride);                                            \
+    }                                                                          \
+  }
+    if (_dim == 1) {
+      if constexpr (Is_inplace) {
+        std::int64_t real_stride[2] = {0, 1};
+        std::int64_t complex_stride[2] = {0, 1};
+        SET_STRIDE;
+        forward_distance = 2 * (_n[0] / 2 + 1);
+        backward_distance = _n[0] / 2 + 1;
+      } else {
+        std::int64_t real_stride[2] = {0, 1};
+        std::int64_t complex_stride[2] = {0, 1};
+        SET_STRIDE;
+        forward_distance = _n[0];
+        backward_distance = _n[0] / 2 + 1;
+      }
+    } else if (_dim == 2) {
+      if constexpr (Is_inplace) {
+        std::int64_t complex_stride[3] = {0, _n[1] / 2 + 1, 1};
+        std::int64_t real_stride[3] = {0, 2 * (_n[1] / 2 + 1), 1};
+        SET_STRIDE;
+        forward_distance = _n[0] * 2 * (_n[1] / 2 + 1);
+        backward_distance = _n[0] * (_n[1] / 2 + 1);
+      } else {
+        std::int64_t complex_stride[3] = {0, _n[1] / 2 + 1, 1};
+        std::int64_t real_stride[3] = {0, _n[1], 1};
+        SET_STRIDE;
+        forward_distance = _n[0] * _n[1];
+        backward_distance = _n[0] * (_n[1] / 2 + 1);
+      }
+    } else if (_dim == 3) {
+      if constexpr (Is_inplace) {
+        std::int64_t complex_stride[4] = {0, _n[1] * (_n[2] / 2 + 1),
+                                          _n[2] / 2 + 1, 1};
+        std::int64_t real_stride[4] = {0, _n[1] * 2 * (_n[2] / 2 + 1),
+                                       2 * (_n[2] / 2 + 1), 1};
+        SET_STRIDE;
+        forward_distance = _n[0] * _n[1] * 2 * (_n[2] / 2 + 1);
+        backward_distance = _n[0] * _n[1] * (_n[2] / 2 + 1);
+      } else {
+        std::int64_t complex_stride[4] = {0, _n[1] * (_n[2] / 2 + 1),
+                                          _n[2] / 2 + 1, 1};
+        std::int64_t real_stride[4] = {0, _n[1] * _n[2], _n[2], 1};
+        SET_STRIDE;
+        forward_distance = _n[0] * _n[1] * _n[2];
+        backward_distance = _n[0] * _n[1] * (_n[2] / 2 + 1);
+      }
+    }
+#undef SET_STRIDE
+    desc->set_value(oneapi::mkl::dft::config_param::FWD_DISTANCE,
+                    forward_distance);
+    desc->set_value(oneapi::mkl::dft::config_param::BWD_DISTANCE,
+                    backward_distance);
+  }
+
+#define COMPUTE(DESC)                                                          \
+  {                                                                            \
+    if (_is_inplace) {                                                         \
+      auto data_input = dpct::detail::get_memory<T>(input);                    \
+      if (_direction == fft_direction::forward) {                              \
+        oneapi::mkl::dft::compute_forward<                                     \
+            std::remove_reference_t<decltype(*DESC)>, T>(*DESC, data_input);   \
+      } else {                                                                 \
+        oneapi::mkl::dft::compute_backward<                                    \
+            std::remove_reference_t<decltype(*DESC)>, T>(*DESC, data_input);   \
+      }                                                                        \
+    } else {                                                                   \
+      auto data_input = dpct::detail::get_memory<T>(input);                    \
+      auto data_output = dpct::detail::get_memory<T>(output);                  \
+      if (_direction == fft_direction::forward) {                              \
+        oneapi::mkl::dft::compute_forward<                                     \
+            std::remove_reference_t<decltype(*DESC)>, T, T>(*DESC, data_input, \
+                                                            data_output);      \
+      } else {                                                                 \
+        oneapi::mkl::dft::compute_backward<                                    \
+            std::remove_reference_t<decltype(*DESC)>, T, T>(*DESC, data_input, \
+                                                            data_output);      \
+      }                                                                        \
+    }                                                                          \
+  }
+
+  template <class T, oneapi::mkl::dft::precision Precision>
+  void compute_complex(T *input, T *output, fft_direction direction) {
+    bool is_this_compute_inplace = input == output;
+
+    if (!_is_user_specified_dir_and_placement) {
+      // The complex domain descriptor need different config values if the
+      // FFT direction or placement is different.
+      // Here we check the conditions, and new config values are set and
+      // re-committed if needed.
+      if (direction != _direction || is_this_compute_inplace != _is_inplace) {
+        if constexpr (Precision == oneapi::mkl::dft::precision::SINGLE) {
+          if (direction != _direction) {
+            swap_distance(_desc_sc);
+            _direction = direction;
+          }
+          if (is_this_compute_inplace != _is_inplace) {
+            _is_inplace = is_this_compute_inplace;
+#ifdef __INTEL_MKL__
+            if (_is_inplace) {
+              _desc_sc->set_value(oneapi::mkl::dft::config_param::PLACEMENT,
+                                  DFTI_CONFIG_VALUE::DFTI_INPLACE);
+            } else {
+              _desc_sc->set_value(oneapi::mkl::dft::config_param::PLACEMENT,
+                                  DFTI_CONFIG_VALUE::DFTI_NOT_INPLACE);
+            }
+#else
+            if (_is_inplace) {
+              _desc_sc->set_value(oneapi::mkl::dft::config_param::PLACEMENT,
+                                  oneapi::mkl::dft::config_value::INPLACE);
+            } else {
+              _desc_sc->set_value(oneapi::mkl::dft::config_param::PLACEMENT,
+                                  oneapi::mkl::dft::config_value::NOT_INPLACE);
+            }
+#endif
+          }
+          _desc_sc->commit(*_q);
+        } else {
+          if (direction != _direction) {
+            swap_distance(_desc_dc);
+            _direction = direction;
+          }
+          if (is_this_compute_inplace != _is_inplace) {
+            _is_inplace = is_this_compute_inplace;
+#ifdef __INTEL_MKL__
+            if (_is_inplace) {
+              _desc_dc->set_value(oneapi::mkl::dft::config_param::PLACEMENT,
+                                  DFTI_CONFIG_VALUE::DFTI_INPLACE);
+            } else {
+              _desc_dc->set_value(oneapi::mkl::dft::config_param::PLACEMENT,
+                                  DFTI_CONFIG_VALUE::DFTI_NOT_INPLACE);
+            }
+#else
+            if (_is_inplace) {
+              _desc_dc->set_value(oneapi::mkl::dft::config_param::PLACEMENT,
+                                  oneapi::mkl::dft::config_value::INPLACE);
+            } else {
+              _desc_dc->set_value(oneapi::mkl::dft::config_param::PLACEMENT,
+                                  oneapi::mkl::dft::config_value::NOT_INPLACE);
+            }
+#endif
+          }
+          _desc_dc->commit(*_q);
+        }
+      }
+    }
+
+    if constexpr (Precision == oneapi::mkl::dft::precision::SINGLE) {
+      COMPUTE(_desc_sc);
+    } else {
+      COMPUTE(_desc_dc);
+    }
+  }
+
+  template <class T, oneapi::mkl::dft::precision Precision>
+  void compute_real(T *input, T *output) {
+    bool is_this_compute_inplace = input == output;
+
+    if (!_is_user_specified_dir_and_placement) {
+      // The real domain descriptor need different config values if the
+      // FFT placement is different.
+      // Here we check the condition, and new config values are set and
+      // re-committed if needed.
+      if (is_this_compute_inplace != _is_inplace) {
+        if constexpr (Precision == oneapi::mkl::dft::precision::SINGLE) {
+          _is_inplace = is_this_compute_inplace;
+          if (_is_inplace) {
+#ifdef __INTEL_MKL__
+            _desc_sr->set_value(oneapi::mkl::dft::config_param::PLACEMENT,
+                                DFTI_CONFIG_VALUE::DFTI_INPLACE);
+#else
+            _desc_sr->set_value(oneapi::mkl::dft::config_param::PLACEMENT,
+                                oneapi::mkl::dft::config_value::INPLACE);
+#endif
+            if (_is_basic)
+              set_stride_and_distance_basic<true>(_desc_sr);
+          } else {
+#ifdef __INTEL_MKL__
+            _desc_sr->set_value(oneapi::mkl::dft::config_param::PLACEMENT,
+                                DFTI_CONFIG_VALUE::DFTI_NOT_INPLACE);
+#else
+            _desc_sr->set_value(oneapi::mkl::dft::config_param::PLACEMENT,
+                                oneapi::mkl::dft::config_value::NOT_INPLACE);
+#endif
+            if (_is_basic)
+              set_stride_and_distance_basic<false>(_desc_sr);
+          }
+          _desc_sr->commit(*_q);
+        } else {
+          _is_inplace = is_this_compute_inplace;
+          if (_is_inplace) {
+#ifdef __INTEL_MKL__
+            _desc_dr->set_value(oneapi::mkl::dft::config_param::PLACEMENT,
+                                DFTI_CONFIG_VALUE::DFTI_INPLACE);
+#else
+            _desc_dr->set_value(oneapi::mkl::dft::config_param::PLACEMENT,
+                                oneapi::mkl::dft::config_value::INPLACE);
+#endif
+            if (_is_basic)
+              set_stride_and_distance_basic<true>(_desc_dr);
+          } else {
+#ifdef __INTEL_MKL__
+            _desc_dr->set_value(oneapi::mkl::dft::config_param::PLACEMENT,
+                                DFTI_CONFIG_VALUE::DFTI_NOT_INPLACE);
+#else
+            _desc_dr->set_value(oneapi::mkl::dft::config_param::PLACEMENT,
+                                oneapi::mkl::dft::config_value::NOT_INPLACE);
+#endif
+            if (_is_basic)
+              set_stride_and_distance_basic<false>(_desc_dr);
+          }
+          _desc_dr->commit(*_q);
+        }
+      }
+    }
+
+    if constexpr (Precision == oneapi::mkl::dft::precision::SINGLE) {
+      COMPUTE(_desc_sr);
+    } else {
+      COMPUTE(_desc_dr);
+    }
+  }
+#undef COMPUTE
+
+private:
+  sycl::queue *_q = nullptr;
+  int _dim;
+  std::vector<std::int64_t> _n;
+  std::vector<std::int64_t> _inembed;
+  std::int64_t _istride;
+  std::int64_t _fwd_dist;
+  library_data_t _input_type;
+  std::vector<std::int64_t> _onembed;
+  std::int64_t _ostride;
+  std::int64_t _bwd_dist;
+  library_data_t _output_type;
+  std::int64_t _batch = 1;
+  bool _is_basic = false;
+  bool _is_inplace = false;
+  fft_direction _direction = fft_direction::forward;
+  bool _is_user_specified_dir_and_placement = false;
+  bool _use_external_workspace = false;
+  void *_external_workspace_ptr = nullptr;
+  size_t _workspace_bytes = 0;
+  bool _is_estimate_call = false;
+  size_t _workspace_estimate_bytes = 0;
+  std::shared_ptr<oneapi::mkl::dft::descriptor<
+      oneapi::mkl::dft::precision::SINGLE, oneapi::mkl::dft::domain::REAL>>
+      _desc_sr;
+  std::shared_ptr<oneapi::mkl::dft::descriptor<
+      oneapi::mkl::dft::precision::DOUBLE, oneapi::mkl::dft::domain::REAL>>
+      _desc_dr;
+  std::shared_ptr<oneapi::mkl::dft::descriptor<
+      oneapi::mkl::dft::precision::SINGLE, oneapi::mkl::dft::domain::COMPLEX>>
+      _desc_sc;
+  std::shared_ptr<oneapi::mkl::dft::descriptor<
+      oneapi::mkl::dft::precision::DOUBLE, oneapi::mkl::dft::domain::COMPLEX>>
+      _desc_dc;
+};
+
+using fft_engine_ptr = fft_engine *;
+} // namespace fft
+} // namespace dpct
+
+#endif // __DPCT_FFT_UTILS_HPP__
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/image.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/image.hpp
new file mode 100644
index 0000000..d7add8f
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/image.hpp
@@ -0,0 +1,891 @@
+//==---- image.hpp --------------------------------*- C++ -*----------------==//
+//
+// Copyright (C) Intel Corporation
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+// See https://llvm.org/LICENSE.txt for license information.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef __DPCT_IMAGE_HPP__
+#define __DPCT_IMAGE_HPP__
+
+#include <sycl/sycl.hpp>
+
+#include "memory.hpp"
+#include "util.hpp"
+
+namespace dpct {
+
+enum class image_channel_data_type {
+  signed_int,
+  unsigned_int,
+  fp,
+};
+
+class image_channel;
+class image_wrapper_base;
+namespace detail {
+/// Image object type traits, with accessor type and sampled data type defined.
+/// The data type of an image accessor must be one of sycl::int4, sycl::uint4,
+/// sycl::float4 and sycl::half4. The data type of accessors with 8bits/16bits
+/// channel width will be 32 bits. sycl::half is an exception.
+template <class T> struct image_trait {
+  using acc_data_t = sycl::vec<T, 4>;
+  template <int dimensions>
+  using accessor_t =
+      sycl::accessor<acc_data_t, dimensions, sycl::access_mode::read,
+                         sycl::access::target::image>;
+  template <int dimensions>
+  using array_accessor_t =
+      sycl::accessor<acc_data_t, dimensions, sycl::access_mode::read,
+                         sycl::access::target::image_array>;
+  using data_t = T;
+  using elem_t = T;
+  static constexpr image_channel_data_type data_type =
+      std::is_integral<T>::value
+          ? (std::is_signed<T>::value ? image_channel_data_type::signed_int
+                                      : image_channel_data_type::unsigned_int)
+          : image_channel_data_type::fp;
+  static constexpr int channel_num = 1;
+};
+template <>
+struct image_trait<std::uint8_t> : public image_trait<std::uint32_t> {
+  using data_t = std::uint8_t;
+  using elem_t = data_t;
+};
+template <>
+struct image_trait<std::uint16_t>
+    : public image_trait<std::uint32_t> {
+  using data_t = std::uint16_t;
+  using elem_t = data_t;
+};
+template <>
+struct image_trait<std::int8_t> : public image_trait<std::int32_t> {
+  using data_t = std::int8_t;
+  using elem_t = data_t;
+};
+template <>
+struct image_trait<std::int16_t> : public image_trait<std::int32_t> {
+  using data_t = std::int16_t;
+  using elem_t = data_t;
+};
+template <>
+struct image_trait<char>
+    : public image_trait<typename std::conditional<
+          std::is_signed<char>::value, signed char, unsigned char>::type> {};
+
+template <class T>
+struct image_trait<sycl::vec<T, 1>> : public image_trait<T> {};
+
+template <class T>
+struct image_trait<sycl::vec<T, 2>> : public image_trait<T> {
+  using data_t = sycl::vec<T, 2>;
+  static constexpr int channel_num = 2;
+};
+
+template <class T>
+struct image_trait<sycl::vec<T, 3>>
+    : public image_trait<sycl::vec<T, 4>> {
+  static constexpr int channel_num = 3;
+};
+
+template <class T>
+struct image_trait<sycl::vec<T, 4>> : public image_trait<T> {
+  using data_t = sycl::vec<T, 4>;
+  static constexpr int channel_num = 4;
+};
+
+/// Functor to fetch data from read result of an image accessor.
+template <class T> struct fetch_data {
+  using return_t = typename image_trait<T>::data_t;
+  using acc_data_t = typename image_trait<T>::acc_data_t;
+
+  return_t operator()(acc_data_t &&original_data) {
+    return (return_t)original_data.r();
+  }
+};
+template <class T>
+struct fetch_data<sycl::vec<T, 1>> : public fetch_data<T> {};
+template <class T> struct fetch_data<sycl::vec<T, 2>> {
+  using return_t = typename image_trait<sycl::vec<T, 2>>::data_t;
+  using acc_data_t = typename image_trait<sycl::vec<T, 2>>::acc_data_t;
+
+  return_t operator()(acc_data_t &&origin_data) {
+    return return_t(origin_data.r(), origin_data.g());
+  }
+};
+template <class T>
+struct fetch_data<sycl::vec<T, 3>>
+    : public fetch_data<sycl::vec<T, 4>> {};
+template <class T> struct fetch_data<sycl::vec<T, 4>> {
+  using return_t = typename image_trait<sycl::vec<T, 4>>::data_t;
+  using acc_data_t = typename image_trait<sycl::vec<T, 4>>::acc_data_t;
+
+  return_t operator()(acc_data_t &&origin_data) {
+    return return_t(origin_data.r(), origin_data.g(), origin_data.b(),
+                    origin_data.a());
+  }
+};
+
+/// Create image according with given type \p T and \p dims.
+template <class T> static image_wrapper_base *create_image_wrapper(int dims);
+
+/// Create image with given data type \p T, channel order and dims
+template <class T>
+static image_wrapper_base *create_image_wrapper(unsigned channel_num, int dims);
+
+/// Create image with channel info and specified dimensions.
+static image_wrapper_base *create_image_wrapper(image_channel channel, int dims);
+
+} // namespace detail
+
+/// Image channel info, include channel number, order, data width and type
+class image_channel {
+  image_channel_data_type _type = image_channel_data_type::signed_int;
+  /// Number of channels.
+  unsigned _channel_num = 0;
+  /// Total size of all channels in bytes.
+  unsigned _total_size = 0;
+  /// Size of each channel in bytes.
+  unsigned _channel_size = 0;
+
+public:
+  /// Create image channel info according to template argument \p T.
+  template <class T> static image_channel create() {
+    image_channel channel;
+    channel.set_channel_size(detail::image_trait<T>::channel_num,
+                             sizeof(typename detail::image_trait<T>::elem_t) *
+                                 8);
+    channel.set_channel_data_type(detail::image_trait<T>::data_type);
+    return channel;
+  }
+
+  image_channel() = default;
+
+  image_channel_data_type get_channel_data_type() { return _type; }
+  void set_channel_data_type(image_channel_data_type type) { _type = type; }
+
+  unsigned get_total_size() { return _total_size; }
+
+  unsigned get_channel_num() { return _channel_num; }
+  void set_channel_num(unsigned channel_num) {
+    _channel_num = channel_num;
+    _total_size = _channel_size * _channel_num;
+  }
+
+  /// image_channel constructor.
+  /// \param r Channel r width in bits.
+  /// \param g Channel g width in bits. Should be same with \p r, or zero.
+  /// \param b Channel b width in bits. Should be same with \p g, or zero.
+  /// \param a Channel a width in bits. Should be same with \p b, or zero.
+  /// \param data_type Image channel data type: signed_nt, unsigned_int or fp.
+  image_channel(int r, int g, int b, int a, image_channel_data_type data_type) {
+    _type = data_type;
+    if (a) {
+      assert(r == a && "SYCL doesn't support different channel size");
+      assert(r == b && "SYCL doesn't support different channel size");
+      assert(r == g && "SYCL doesn't support different channel size");
+      set_channel_size(4, a);
+    } else if (b) {
+      assert(r == b && "SYCL doesn't support different channel size");
+      assert(r == g && "SYCL doesn't support different channel size");
+      set_channel_size(3, b);
+    } else if (g) {
+      assert(r == g && "SYCL doesn't support different channel size");
+      set_channel_size(2, g);
+    } else {
+      set_channel_size(1, r);
+    }
+  }
+
+  sycl::image_channel_type get_channel_type() const {
+    if (_channel_size == 4) {
+      if (_type == image_channel_data_type::signed_int)
+        return sycl::image_channel_type::signed_int32;
+      else if (_type == image_channel_data_type::unsigned_int)
+        return sycl::image_channel_type::unsigned_int32;
+      else if (_type == image_channel_data_type::fp)
+        return sycl::image_channel_type::fp32;
+    } else if (_channel_size == 2) {
+      if (_type == image_channel_data_type::signed_int)
+        return sycl::image_channel_type::signed_int16;
+      else if (_type == image_channel_data_type::unsigned_int)
+        return sycl::image_channel_type::unsigned_int16;
+      else if (_type == image_channel_data_type::fp)
+        return sycl::image_channel_type::fp16;
+    } else {
+      if (_type == image_channel_data_type::signed_int)
+        return sycl::image_channel_type::signed_int8;
+      else if (_type == image_channel_data_type::unsigned_int)
+        return sycl::image_channel_type::unsigned_int8;
+    }
+    assert(false && "unexpected channel data kind and channel size");
+    return sycl::image_channel_type::signed_int32;
+  }
+  void set_channel_type(sycl::image_channel_type type) {
+    switch (type) {
+    case sycl::image_channel_type::unsigned_int8:
+      _type = image_channel_data_type::unsigned_int;
+      _channel_size = 1;
+      break;
+    case sycl::image_channel_type::unsigned_int16:
+      _type = image_channel_data_type::unsigned_int;
+      _channel_size = 2;
+      break;
+    case sycl::image_channel_type::unsigned_int32:
+      _type = image_channel_data_type::unsigned_int;
+      _channel_size = 4;
+      break;
+    case sycl::image_channel_type::signed_int8:
+      _type = image_channel_data_type::signed_int;
+      _channel_size = 1;
+      break;
+    case sycl::image_channel_type::signed_int16:
+      _type = image_channel_data_type::signed_int;
+      _channel_size = 2;
+      break;
+    case sycl::image_channel_type::signed_int32:
+      _type = image_channel_data_type::signed_int;
+      _channel_size = 4;
+      break;
+    case sycl::image_channel_type::fp16:
+      _type = image_channel_data_type::fp;
+      _channel_size = 2;
+      break;
+    case sycl::image_channel_type::fp32:
+      _type = image_channel_data_type::fp;
+      _channel_size = 4;
+      break;
+    default:
+      break;
+    }
+    _total_size = _channel_size * _channel_num;
+  }
+
+  sycl::image_channel_order get_channel_order() const {
+    switch (_channel_num) {
+    case 1:
+      return sycl::image_channel_order::r;
+    case 2:
+      return sycl::image_channel_order::rg;
+    case 3:
+      return sycl::image_channel_order::rgb;
+    case 4:
+      return sycl::image_channel_order::rgba;
+    default:
+      return sycl::image_channel_order::r;
+    }
+  }
+  /// Get the size for each channel in bits.
+  unsigned get_channel_size() const { return _channel_size * 8; }
+
+  /// Set channel size.
+  /// \param in_channel_num Channels number to set.
+  /// \param channel_size Size for each channel in bits.
+  void set_channel_size(unsigned in_channel_num,
+                        unsigned channel_size) {
+    if (in_channel_num < _channel_num)
+      return;
+    _channel_num = in_channel_num;
+    _channel_size = channel_size / 8;
+    _total_size = _channel_size * _channel_num;
+  }
+};
+
+/// 2D or 3D matrix data for image.
+class image_matrix {
+  image_channel _channel;
+  int _range[3] = {1, 1, 1};
+  int _dims = 0;
+  void *_host_data = nullptr;
+
+  /// Set range of each dimension.
+  template <int dimensions> void set_range(sycl::range<dimensions> range) {
+    for (int i = 0; i < dimensions; ++i)
+      _range[i] = range[i];
+    _dims = dimensions;
+  }
+
+  template <int... DimIdx>
+  sycl::range<sizeof...(DimIdx)> get_range(integer_sequence<DimIdx...>) {
+    return sycl::range<sizeof...(DimIdx)>(_range[DimIdx]...);
+  }
+
+public:
+  /// Constructor with channel info and dimension size info.
+  template <int dimensions>
+  image_matrix(image_channel channel, sycl::range<dimensions> range)
+      : _channel(channel) {
+    set_range(range);
+    _host_data = std::malloc(range.size() * _channel.get_total_size());
+  }
+  image_matrix(sycl::image_channel_type channel_type, unsigned channel_num,
+               size_t x, size_t y) {
+    _channel.set_channel_type(channel_type);
+    _channel.set_channel_num(channel_num);
+    _dims = 1;
+    _range[0] = x;
+    if (y) {
+      _dims = 2;
+      _range[1] = y;
+    }
+    _host_data = std::malloc(_range[0] * _range[1] * _channel.get_total_size());
+  }
+
+  /// Construct a new image class with the matrix data.
+  template <int dimensions> sycl::image<dimensions> *create_image() {
+    return create_image<dimensions>(_channel);
+  }
+  /// Construct a new image class with the matrix data.
+  template <int dimensions>
+  sycl::image<dimensions> *create_image(image_channel channel) {
+    return new sycl::image<dimensions>(
+        _host_data, channel.get_channel_order(), channel.get_channel_type(),
+        get_range(make_index_sequence<dimensions>()),
+        sycl::property::image::use_host_ptr());
+  }
+
+  /// Get channel info.
+  inline image_channel get_channel() { return _channel; }
+  /// Get range of the image.
+  sycl::range<3> get_range() {
+    return sycl::range<3>(_range[0], _range[1], _range[2]);
+  }
+  /// Get matrix dims.
+  inline int get_dims() { return _dims; }
+  /// Convert to pitched data.
+  pitched_data to_pitched_data() {
+    return pitched_data(_host_data, _range[0], _range[0], _range[1]);
+  }
+
+  ~image_matrix() {
+    if (_host_data)
+      std::free(_host_data);
+    _host_data = nullptr;
+  }
+};
+using image_matrix_p = image_matrix *;
+
+enum class image_data_type { matrix, linear, pitch, unsupport };
+
+/// Image data info.
+class image_data {
+public:
+  image_data() { _type = image_data_type::unsupport; }
+  image_data(image_matrix_p matrix_data) { set_data(matrix_data); }
+  image_data(void *data_ptr, size_t x_size, image_channel channel) {
+    set_data(data_ptr, x_size, channel);
+  }
+  image_data(void *data_ptr, size_t x_size, size_t y_size, size_t pitch_size,
+             image_channel channel) {
+    set_data(data_ptr, x_size, y_size, pitch_size, channel);
+  }
+  void set_data(image_matrix_p matrix_data) {
+    _type = image_data_type::matrix;
+    _data = matrix_data;
+    _channel = matrix_data->get_channel();
+  }
+  void set_data(void *data_ptr, size_t x_size, image_channel channel) {
+    _type = image_data_type::linear;
+    _data = data_ptr;
+    _x = x_size;
+    _channel = channel;
+  }
+  void set_data(void *data_ptr, size_t x_size, size_t y_size, size_t pitch_size,
+                image_channel channel) {
+    _type = image_data_type::pitch;
+    _data = data_ptr;
+    _x = x_size;
+    _y = y_size;
+    _pitch = pitch_size;
+    _channel = channel;
+  }
+
+  image_data_type get_data_type() const { return _type; }
+  void set_data_type(image_data_type type) { _type = type; }
+
+  void *get_data_ptr() const { return _data; }
+  void set_data_ptr(void *data) { _data = data; }
+
+  size_t get_x() const { return _x; }
+  void set_x(size_t x) { _x = x; }
+
+  size_t get_y() const { return _y; }
+  void set_y(size_t y) { _y = y; }
+
+  size_t get_pitch() const { return _pitch; }
+  void set_pitch(size_t pitch) { _pitch = pitch; }
+
+  image_channel get_channel() const { return _channel; }
+  void set_channel(image_channel channel) { _channel = channel; }
+
+  image_channel_data_type get_channel_data_type() {
+    return _channel.get_channel_data_type();
+  }
+  void set_channel_data_type(image_channel_data_type type) {
+    _channel.set_channel_data_type(type);
+  }
+
+  unsigned get_channel_size() { return _channel.get_channel_size(); }
+  void set_channel_size(unsigned channel_num, unsigned channel_size) {
+    return _channel.set_channel_size(channel_num, channel_size);
+  }
+
+  unsigned get_channel_num() { return _channel.get_channel_num(); }
+  void set_channel_num(unsigned num) {
+    return _channel.set_channel_num(num);
+  }
+
+  sycl::image_channel_type get_channel_type() {
+    return _channel.get_channel_type();
+  }
+  void set_channel_type(sycl::image_channel_type type) {
+    return _channel.set_channel_type(type);
+  }
+
+private:
+  image_data_type _type;
+  void *_data = nullptr;
+  size_t _x, _y, _pitch;
+  image_channel _channel;
+};
+
+/// Image sampling info, include addressing mode, filtering mode and
+/// normalization info.
+class sampling_info {
+  sycl::addressing_mode _addressing_mode =
+      sycl::addressing_mode::clamp_to_edge;
+  sycl::filtering_mode _filtering_mode = sycl::filtering_mode::nearest;
+  sycl::coordinate_normalization_mode _coordinate_normalization_mode =
+      sycl::coordinate_normalization_mode::unnormalized;
+
+public:
+  sycl::addressing_mode get_addressing_mode() { return _addressing_mode; }
+  void set(sycl::addressing_mode addressing_mode) { _addressing_mode = addressing_mode; }
+
+  sycl::filtering_mode get_filtering_mode() { return _filtering_mode; }
+  void set(sycl::filtering_mode filtering_mode) { _filtering_mode = filtering_mode; }
+
+  sycl::coordinate_normalization_mode get_coordinate_normalization_mode() {
+    return _coordinate_normalization_mode;
+  }
+  void set(sycl::coordinate_normalization_mode coordinate_normalization_mode) {
+    _coordinate_normalization_mode = coordinate_normalization_mode;
+  }
+
+  bool is_coordinate_normalized() {
+    return _coordinate_normalization_mode ==
+           sycl::coordinate_normalization_mode::normalized;
+  }
+  void set_coordinate_normalization_mode(int is_normalized) {
+    _coordinate_normalization_mode =
+        is_normalized ? sycl::coordinate_normalization_mode::normalized
+                      : sycl::coordinate_normalization_mode::unnormalized;
+  }
+  void
+  set(sycl::addressing_mode addressing_mode,
+      sycl::filtering_mode filtering_mode,
+      sycl::coordinate_normalization_mode coordinate_normalization_mode) {
+    set(addressing_mode);
+    set(filtering_mode);
+    set(coordinate_normalization_mode);
+  }
+  void set(sycl::addressing_mode addressing_mode,
+           sycl::filtering_mode filtering_mode, int is_normalized) {
+    set(addressing_mode);
+    set(filtering_mode);
+    set_coordinate_normalization_mode(is_normalized);
+  }
+
+  sycl::sampler get_sampler() {
+    return sycl::sampler(_coordinate_normalization_mode, _addressing_mode,
+                             _filtering_mode);
+  }
+};
+
+/// Image base class.
+class image_wrapper_base {
+  sampling_info _sampling_info;
+  image_data _data;
+
+public:
+  virtual ~image_wrapper_base() = 0;
+
+  void attach(image_data data) { set_data(data); }
+  /// Attach matrix data to this class.
+  void attach(image_matrix *matrix) {
+    detach();
+    image_wrapper_base::set_data(image_data(matrix));
+  }
+  /// Attach matrix data to this class.
+  void attach(image_matrix *matrix, image_channel channel) {
+    attach(matrix);
+    image_wrapper_base::set_channel(channel);
+  }
+  /// Attach linear data to this class.
+  void attach(const void *ptr, size_t count) {
+    attach(ptr, count, get_channel());
+  }
+  /// Attach linear data to this class.
+  void attach(const void *ptr, size_t count, image_channel channel) {
+    detach();
+    image_wrapper_base::set_data(image_data(const_cast<void *>(ptr), count, channel));
+  }
+  /// Attach 2D data to this class.
+  void attach(const void *data, size_t x, size_t y, size_t pitch) {
+    attach(data, x, y, pitch, get_channel());
+  }
+  /// Attach 2D data to this class.
+  void attach(const void *data, size_t x, size_t y, size_t pitch,
+              image_channel channel) {
+    detach();
+    image_wrapper_base::set_data(
+        image_data(const_cast<void *>(data), x, y, pitch, channel));
+  }
+  /// Detach data.
+  virtual void detach() {}
+
+  sampling_info get_sampling_info() { return _sampling_info; }
+  void set_sampling_info(sampling_info info) {
+    _sampling_info = info;
+  }
+  const image_data &get_data() { return _data; }
+  void set_data(image_data data) { _data = data; }
+
+  image_channel get_channel() { return _data.get_channel(); }
+  void set_channel(image_channel channel) { _data.set_channel(channel); }
+
+  image_channel_data_type get_channel_data_type() {
+    return _data.get_channel_data_type();
+  }
+  void set_channel_data_type(image_channel_data_type type) {
+    _data.set_channel_data_type(type);
+  }
+
+  unsigned get_channel_size() { return _data.get_channel_size(); }
+  void set_channel_size(unsigned channel_num, unsigned channel_size) {
+    return _data.set_channel_size(channel_num, channel_size);
+  }
+
+  sycl::addressing_mode get_addressing_mode() {
+    return _sampling_info.get_addressing_mode();
+  }
+  void set(sycl::addressing_mode addressing_mode) {
+    _sampling_info.set(addressing_mode);
+  }
+
+  sycl::filtering_mode get_filtering_mode() {
+    return _sampling_info.get_filtering_mode();
+  }
+  void set(sycl::filtering_mode filtering_mode) {
+    _sampling_info.set(filtering_mode);
+  }
+
+  sycl::coordinate_normalization_mode get_coordinate_normalization_mode() {
+    return _sampling_info.get_coordinate_normalization_mode();
+  }
+  void
+  set(sycl::coordinate_normalization_mode coordinate_normalization_mode) {
+    _sampling_info.set(coordinate_normalization_mode);
+  }
+
+  bool is_coordinate_normalized() {
+    return _sampling_info.is_coordinate_normalized();
+  }
+  void set_coordinate_normalization_mode(int is_normalized) {
+    _sampling_info.set_coordinate_normalization_mode(is_normalized);
+  }
+  void
+  set(sycl::addressing_mode addressing_mode,
+      sycl::filtering_mode filtering_mode,
+      sycl::coordinate_normalization_mode coordinate_normalization_mode) {
+    set(addressing_mode);
+    set(filtering_mode);
+    set(coordinate_normalization_mode);
+  }
+  void set(sycl::addressing_mode addressing_mode,
+           sycl::filtering_mode filtering_mode, int is_normalized) {
+    set(addressing_mode);
+    set(filtering_mode);
+    set_coordinate_normalization_mode(is_normalized);
+  }
+
+  unsigned get_channel_num() { return _data.get_channel_num(); }
+  void set_channel_num(unsigned num) {
+    return _data.set_channel_num(num);
+  }
+
+  sycl::image_channel_type get_channel_type() {
+    return _data.get_channel_type();
+  }
+  void set_channel_type(sycl::image_channel_type type) {
+    return _data.set_channel_type(type);
+  }
+
+  sycl::sampler get_sampler() { return _sampling_info.get_sampler(); }
+};
+inline image_wrapper_base::~image_wrapper_base() {}
+using image_wrapper_base_p = image_wrapper_base *;
+
+template <class T, int dimensions, bool IsImageArray> class image_accessor_ext;
+
+/// Image class, wrapper of sycl::image.
+template <class T, int dimensions, bool IsImageArray = false> class image_wrapper : public image_wrapper_base {
+  sycl::image<dimensions> *_image = nullptr;
+
+#ifndef DPCT_USM_LEVEL_NONE
+  std::vector<char> _host_buffer;
+#endif
+
+  void create_image(sycl::queue q) {
+    auto &data = get_data();
+    if (data.get_data_type() == image_data_type::matrix) {
+      _image = static_cast<image_matrix_p>(data.get_data_ptr())
+          ->create_image<dimensions>(data.get_channel());
+      return;
+    }
+    auto ptr = data.get_data_ptr();
+    auto channel = data.get_channel();
+
+    if (detail::get_pointer_attribute(q, ptr) == detail::pointer_access_attribute::device_only) {
+#ifdef DPCT_USM_LEVEL_NONE
+      ptr = get_buffer(ptr)
+                .template get_access<sycl::access_mode::read_write>()
+                .get_pointer();
+#else
+      auto sz = data.get_x();
+      if (data.get_data_type() == image_data_type::pitch)
+        sz *= channel.get_total_size() * data.get_y();
+      _host_buffer.resize(sz);
+      q.memcpy(_host_buffer.data(), ptr, sz).wait();
+      ptr = _host_buffer.data();
+#endif
+    }
+
+    if constexpr (dimensions == 1) {
+      assert(data.get_data_type() == image_data_type::linear);
+      _image = new sycl::image<1>(
+        ptr, channel.get_channel_order(), channel.get_channel_type(),
+        sycl::range<1>(data.get_x() / channel.get_total_size()));
+    } else if constexpr (dimensions == 2) {
+      assert(data.get_data_type() == image_data_type::pitch);
+      _image = new sycl::image<2>(ptr, channel.get_channel_order(),
+                                  channel.get_channel_type(),
+                                  sycl::range<2>(data.get_x(), data.get_y()),
+                                  sycl::range<1>(data.get_pitch()));
+    } else {
+      throw std::runtime_error("3D image only support matrix data");
+    }
+    return;
+  }
+
+public:
+  using acc_data_t = typename detail::image_trait<T>::acc_data_t;
+  using accessor_t =
+      typename image_accessor_ext<T, IsImageArray ? (dimensions - 1) : dimensions,
+                              IsImageArray>::accessor_t;
+
+  image_wrapper() { set_channel(image_channel::create<T>()); }
+  ~image_wrapper() { detach(); }
+
+  /// Get image accessor.
+  accessor_t get_access(sycl::handler &cgh, sycl::queue &q = get_default_queue()) {
+    if (!_image)
+      create_image(q);
+    return accessor_t(*_image, cgh);
+  }
+
+  /// Detach data.
+  void detach() override {
+    if (_image)
+      delete _image;
+    _image = nullptr;
+  }
+};
+
+/// Wrap sampler and image accessor together.
+template <class T, int dimensions, bool IsImageArray = false>
+class image_accessor_ext {
+public:
+  using accessor_t =
+      typename detail::image_trait<T>::template accessor_t<dimensions>;
+  using data_t = typename detail::image_trait<T>::data_t;
+  sycl::sampler _sampler;
+  accessor_t _img_acc;
+
+public:
+  image_accessor_ext(sycl::sampler sampler, accessor_t acc)
+      : _sampler(sampler), _img_acc(acc) {}
+
+  /// Read data from accessor.
+  template <bool Available = dimensions == 3>
+  typename std::enable_if<Available, data_t>::type read(float x, float y,
+                                                        float z) {
+    return detail::fetch_data<T>()(
+        _img_acc.read(sycl::float4(x, y, z, 0), _sampler));
+  }
+  /// Read data from accessor.
+  template <class Coord0, class Coord1, class Coord2,
+            bool Available = dimensions == 3 &&
+                             std::is_integral<Coord0>::value
+                                 &&std::is_integral<Coord1>::value
+                                     &&std::is_integral<Coord2>::value>
+  typename std::enable_if<Available, data_t>::type read(Coord0 x, Coord1 y,
+                                                        Coord2 z) {
+    return detail::fetch_data<T>()(
+        _img_acc.read(sycl::int4(x, y, z, 0), _sampler));
+  }
+  /// Read data from accessor.
+  template <bool Available = dimensions == 2>
+  typename std::enable_if<Available, data_t>::type read(float x, float y) {
+    return detail::fetch_data<T>()(
+        _img_acc.read(sycl::float2(x, y), _sampler));
+  }
+  /// Read data from accessor.
+  template <class Coord0, class Coord1,
+            bool Available = dimensions == 2 &&
+                             std::is_integral<Coord0>::value
+                                 &&std::is_integral<Coord1>::value>
+  typename std::enable_if<Available, data_t>::type read(Coord0 x, Coord1 y) {
+    return detail::fetch_data<T>()(
+        _img_acc.read(sycl::int2(x, y), _sampler));
+  }
+  /// Read data from accessor.
+  template <bool Available = dimensions == 1>
+  typename std::enable_if<Available, data_t>::type read(float x) {
+    return detail::fetch_data<T>()(_img_acc.read(x, _sampler));
+  }
+  /// Read data from accessor.
+  template <class CoordT,
+            bool Available = dimensions == 1 && std::is_integral<CoordT>::value>
+  typename std::enable_if<Available, data_t>::type read(CoordT x) {
+    return detail::fetch_data<T>()(_img_acc.read(x, _sampler));
+  }
+};
+
+template <class T, int dimensions> class image_accessor_ext<T, dimensions, true> {
+public:
+  using accessor_t =
+      typename detail::image_trait<T>::template array_accessor_t<dimensions>;
+  using data_t = typename detail::image_trait<T>::data_t;
+  sycl::sampler _sampler;
+  accessor_t _img_acc;
+
+public:
+  image_accessor_ext(sycl::sampler sampler, accessor_t acc)
+      : _sampler(sampler), _img_acc(acc) {}
+
+  /// Read data from accessor.
+  template <bool Available = dimensions == 2>
+  typename std::enable_if<Available, data_t>::type read(int index, float x,
+                                                        float y) {
+    return detail::fetch_data<T>()(
+        _img_acc[index].read(sycl::float2(x, y), _sampler));
+  }
+  /// Read data from accessor.
+  template <bool Available = dimensions == 2>
+  typename std::enable_if<Available, data_t>::type read(int index, int x, int y) {
+    return detail::fetch_data<T>()(
+        _img_acc[index].read(sycl::int2(x, y), _sampler));
+  }
+  /// Read data from accessor.
+  template <bool Available = dimensions == 1>
+  typename std::enable_if<Available, data_t>::type read(int index, float x) {
+    return detail::fetch_data<T>()(
+        _img_acc[index].read(x, _sampler));
+  }
+  /// Read data from accessor.
+  template <bool Available = dimensions == 1>
+  typename std::enable_if<Available, data_t>::type read(int index, int x) {
+    return detail::fetch_data<T>()(
+        _img_acc[index].read(x, _sampler));
+  }
+};
+
+/// Create image wrapper according to image data and sampling info.
+/// \return Pointer to image wrapper base class.
+/// \param data Image data used to create image wrapper.
+/// \param info Image sampling info used to create image wrapper.
+/// \returns Pointer to base class of created image wrapper object.
+static inline image_wrapper_base *create_image_wrapper(image_data data,
+                              sampling_info info) {
+  image_channel channel;
+  int dims = 1;
+  if (data.get_data_type() == image_data_type::matrix) {
+    auto matrix = (image_matrix_p)data.get_data_ptr();
+    channel = matrix->get_channel();
+    dims = matrix->get_dims();
+  } else {
+    if (data.get_data_type() == image_data_type::pitch) {
+      dims = 2;
+    }
+    channel = data.get_channel();
+  }
+
+  if (auto ret = detail::create_image_wrapper(channel, dims)) {
+    ret->set_sampling_info(info);
+    ret->set_data(data);
+    return ret;
+  }
+  return nullptr;
+}
+
+namespace detail {
+/// Create image according with given type \p T and \p dims.
+template <class T> static image_wrapper_base *create_image_wrapper(int dims) {
+  switch (dims) {
+  case 1:
+    return new image_wrapper<T, 1>();
+  case 2:
+    return new image_wrapper<T, 2>();
+  case 3:
+    return new image_wrapper<T, 3>();
+  default:
+    return nullptr;
+  }
+}
+/// Create image with given data type \p T, channel order and dims
+template <class T>
+static image_wrapper_base *create_image_wrapper(unsigned channel_num, int dims) {
+  switch (channel_num) {
+  case 1:
+    return create_image_wrapper<T>(dims);
+  case 2:
+    return create_image_wrapper<sycl::vec<T, 2>>(dims);
+  case 3:
+    return create_image_wrapper<sycl::vec<T, 3>>(dims);
+  case 4:
+    return create_image_wrapper<sycl::vec<T, 4>>(dims);
+  default:
+    return nullptr;
+  }
+}
+
+/// Create image with channel info and specified dimensions.
+static image_wrapper_base *create_image_wrapper(image_channel channel, int dims) {
+  switch (channel.get_channel_type()) {
+  case sycl::image_channel_type::fp16:
+    return create_image_wrapper<sycl::half>(channel.get_channel_num(), dims);
+  case sycl::image_channel_type::fp32:
+    return create_image_wrapper<float>(channel.get_channel_num(), dims);
+  case sycl::image_channel_type::signed_int8:
+    return create_image_wrapper<std::int8_t>(channel.get_channel_num(), dims);
+  case sycl::image_channel_type::signed_int16:
+    return create_image_wrapper<std::int16_t>(channel.get_channel_num(), dims);
+  case sycl::image_channel_type::signed_int32:
+    return create_image_wrapper<std::int32_t>(channel.get_channel_num(), dims);
+  case sycl::image_channel_type::unsigned_int8:
+    return create_image_wrapper<std::uint8_t>(channel.get_channel_num(), dims);
+  case sycl::image_channel_type::unsigned_int16:
+    return create_image_wrapper<std::uint16_t>(channel.get_channel_num(), dims);
+  case sycl::image_channel_type::unsigned_int32:
+    return create_image_wrapper<std::uint32_t>(channel.get_channel_num(), dims);
+  default:
+    return nullptr;
+  }
+}
+} // namespace detail
+
+} // namespace dpct
+
+#endif // !__DPCT_IMAGE_HPP__
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/kernel.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/kernel.hpp
new file mode 100644
index 0000000..11d1321
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/kernel.hpp
@@ -0,0 +1,459 @@
+//==---- kernel.hpp -------------------------------*- C++ -*----------------==//
+//
+// Copyright (C) Intel Corporation
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+// See https://llvm.org/LICENSE.txt for license information.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef __DPCT_KERNEL_HPP__
+#define __DPCT_KERNEL_HPP__
+
+#include <sycl/sycl.hpp>
+#ifdef _WIN32
+#include <unordered_set>
+#include <windows.h>
+#else
+#include <dlfcn.h>
+#endif
+
+#if defined(__has_include) && __has_include(<filesystem>)
+#include <filesystem>
+#elif defined(__has_include) && __has_include(<experimental/filesystem>)
+#include <experimental/filesystem>
+#else
+#error "SYCLomatic runtime requires C++ filesystem support"
+#endif
+
+#include <fstream>
+#include <image.hpp>
+#include <random>
+
+namespace dpct {
+
+typedef void (*kernel_functor)(sycl::queue &, const sycl::nd_range<3> &,
+                               unsigned int, void **, void **);
+
+struct kernel_function_info {
+  int max_work_group_size = 0;
+};
+
+static inline void get_kernel_function_info(kernel_function_info *kernel_info,
+                                            const void *function) {
+  kernel_info->max_work_group_size =
+      dpct::dev_mgr::instance()
+          .current_device()
+          .get_info<sycl::info::device::max_work_group_size>();
+}
+static inline kernel_function_info
+get_kernel_function_info(const void *function) {
+  kernel_function_info kernel_info;
+  kernel_info.max_work_group_size =
+      dpct::dev_mgr::instance()
+          .current_device()
+          .get_info<sycl::info::device::max_work_group_size>();
+  return kernel_info;
+}
+
+
+namespace detail {
+
+#if defined(__has_include) && __has_include(<filesystem>)
+namespace fs = std::filesystem;
+#else
+namespace fs = std::experimental::filesystem;
+#endif
+
+/// Write data to temporary file and return absolute path to temporary file.
+/// Temporary file is created in a temporary directory both of which have random
+/// names with only the user having access permissions.  Only one temporary file
+/// will be created in the temporary directory.
+static inline fs::path write_data_to_file(char const *const data, size_t size) {
+  std::error_code ec;
+
+  if (sizeof(size_t) >= sizeof(std::streamsize) &&
+      size > (std::numeric_limits<std::streamsize>::max)())
+    throw std::runtime_error("data file too large");
+
+  // random number generator
+  std::random_device dev;
+  std::mt19937 prng(dev());
+  std::uniform_int_distribution<uint64_t> rand(0);
+
+  // find temporary directory
+  auto tmp_dir = fs::temp_directory_path(ec);
+  if (ec)
+    throw std::runtime_error("could not find temporary directory");
+
+  // create private directory
+  std::stringstream directory;
+  fs::path directory_path;
+  constexpr int max_attempts = 5;
+  int i;
+
+  for (i = 0; i < max_attempts; i++) {
+    directory << std::hex << rand(prng);
+    directory_path = tmp_dir / directory.str();
+    if (fs::create_directory(directory_path)) {
+      break;
+    }
+  }
+  if (i == max_attempts)
+    throw std::runtime_error("could not create directory");
+
+  // only allow owner permissions to private directory
+  fs::permissions(directory_path, fs::perms::owner_all, ec);
+  if (ec)
+    throw std::runtime_error("could not set directory permissions");
+
+  // random filename in private directory
+  std::stringstream filename;
+  filename << std::hex << rand(prng);
+#ifdef _WIN32
+  auto filepath = directory_path / (filename.str() + ".dll");
+#else
+  auto filepath = directory_path / filename.str();
+#endif
+
+  // write data to temporary file
+  auto outfile = std::ofstream(filepath, std::ios::out | std::ios::binary);
+  if (outfile) {
+    // only allow program to write file
+    fs::permissions(filepath, fs::perms::owner_write, ec);
+    if (ec)
+      throw std::runtime_error("could not set permissions");
+
+    outfile.write(data, size);
+    if (!outfile.good())
+      throw std::runtime_error("could not write data");
+    outfile.close();
+
+    // only allow program to read/execute file
+    fs::permissions(filepath, fs::perms::owner_read | fs::perms::owner_exec,
+                    ec);
+    if (ec)
+      throw std::runtime_error("could not set permissions");
+  } else
+    throw std::runtime_error("could not write data");
+
+  // check temporary file contents
+  auto infile = std::ifstream(filepath, std::ios::in | std::ios::binary);
+  if (infile) {
+    bool mismatch = false;
+    size_t cnt = 0;
+
+    while (1) {
+      char c;
+      infile.get(c);
+      if (infile.eof())
+        break;
+      if (c != data[cnt++])
+        mismatch = true;
+    }
+    if (cnt != size || mismatch)
+      throw std::runtime_error("file contents not written correctly");
+  } else
+    throw std::runtime_error("could not validate file");
+
+  if (!filepath.is_absolute())
+    throw std::runtime_error("temporary filepath is not absolute");
+
+  return filepath;
+}
+
+static inline uint16_t extract16(unsigned char const *const ptr) {
+  uint16_t ret = 0;
+
+  ret |= static_cast<uint16_t>(ptr[0]) << 0;
+  ret |= static_cast<uint16_t>(ptr[1]) << 8;
+
+  return (ret);
+}
+
+static inline uint32_t extract32(unsigned char const *const ptr) {
+  uint32_t ret = 0;
+
+  ret |= static_cast<uint32_t>(ptr[0]) << 0;
+  ret |= static_cast<uint32_t>(ptr[1]) << 8;
+  ret |= static_cast<uint32_t>(ptr[2]) << 16;
+  ret |= static_cast<uint32_t>(ptr[3]) << 24;
+
+  return (ret);
+}
+
+static inline uint64_t extract64(unsigned char const *const ptr) {
+  uint64_t ret = 0;
+
+  ret |= static_cast<uint64_t>(ptr[0]) << 0;
+  ret |= static_cast<uint64_t>(ptr[1]) << 8;
+  ret |= static_cast<uint64_t>(ptr[2]) << 16;
+  ret |= static_cast<uint64_t>(ptr[3]) << 24;
+  ret |= static_cast<uint64_t>(ptr[4]) << 32;
+  ret |= static_cast<uint64_t>(ptr[5]) << 40;
+  ret |= static_cast<uint64_t>(ptr[6]) << 48;
+  ret |= static_cast<uint64_t>(ptr[7]) << 56;
+
+  return (ret);
+}
+
+static inline uint64_t get_lib_size(char const *const blob) {
+#ifdef _WIN32
+  ///////////////////////////////////////////////////////////////////////
+  // Analyze DOS stub
+  unsigned char const *const ublob =
+      reinterpret_cast<unsigned char const *const>(blob);
+  if (ublob[0] != 0x4d || ublob[1] != 0x5a) {
+    throw std::runtime_error("Blob is not a Windows DLL.");
+  }
+  uint32_t pe_header_offset = extract32(ublob + 0x3c);
+
+  ///////////////////////////////////////////////////////////////////////
+  // Ananlyze PE-header
+  unsigned char const *const pe_header = ublob + pe_header_offset;
+
+  // signature
+  uint32_t pe_signature = extract32(pe_header + 0);
+  if (pe_signature != 0x00004550) {
+    throw std::runtime_error("PE-header signature is not 0x00004550");
+  }
+
+  // machine
+  uint16_t machine = extract16(pe_header + 4);
+  if (machine != 0x8664) {
+    throw std::runtime_error("Only DLLs for x64 supported");
+  }
+
+  // number of sections
+  uint16_t number_of_sections = extract16(pe_header + 6);
+
+  // sizeof optional header
+  uint16_t sizeof_optional_header = extract16(pe_header + 20);
+
+  // magic
+  uint16_t magic = extract16(pe_header + 24);
+  if (magic != 0x10b && magic != 0x20b) {
+    throw std::runtime_error("MAGIC is not 0x010b or 0x020b");
+  }
+
+  ///////////////////////////////////////////////////////////////////////
+  // Analyze tail of optional header
+  constexpr int coff_header_size = 24;
+
+  unsigned char const *const tail_of_optional_header =
+      pe_header + coff_header_size + sizeof_optional_header;
+  if (extract64(tail_of_optional_header - 8) != 0) {
+    throw std::runtime_error("Optional header not zero-padded");
+  }
+
+  ///////////////////////////////////////////////////////////////////////
+  // Analyze last section header
+  constexpr int section_header_size = 40;
+  unsigned char const *const last_section_header =
+      tail_of_optional_header + section_header_size * (number_of_sections - 1);
+
+  uint32_t sizeof_raw_data = extract32(last_section_header + 16);
+  uint32_t pointer_to_raw_data = extract32(last_section_header + 20);
+
+  return sizeof_raw_data + pointer_to_raw_data;
+#else
+  if (blob[0] != 0x7F || blob[1] != 'E' || blob[2] != 'L' || blob[3] != 'F')
+    throw std::runtime_error("Blob is not in ELF format");
+
+  if (blob[4] != 0x02)
+    throw std::runtime_error("Only 64-bit headers are supported");
+
+  if (blob[5] != 0x01)
+    throw std::runtime_error("Only little-endian headers are supported");
+
+  unsigned char const *const ublob =
+      reinterpret_cast<unsigned char const *const>(blob);
+  uint64_t e_shoff = extract64(ublob + 0x28);
+  uint16_t e_shentsize = extract16(ublob + 0x3A);
+  uint16_t e_shnum = extract16(ublob + 0x3C);
+
+  return e_shoff + (e_shentsize * e_shnum);
+#endif
+}
+
+#ifdef _WIN32
+class path_lib_record {
+public:
+  void operator=(const path_lib_record &) = delete;
+  ~path_lib_record() {
+    for (auto entry : lib_to_path) {
+      FreeLibrary(static_cast<HMODULE>(entry.first));
+      fs::permissions(entry.second, fs::perms::owner_all);
+      fs::remove_all(entry.second.remove_filename());
+    }
+  }
+  static void record_lib_path(fs::path path, void *library) {
+    lib_to_path[library] = path;
+  }
+  static void remove_lib(void *library) {
+    auto path = lib_to_path[library];
+    std::error_code ec;
+
+    FreeLibrary(static_cast<HMODULE>(library));
+    fs::permissions(path, fs::perms::owner_all);
+    if (fs::remove_all(path.remove_filename(), ec) != 2 || ec)
+      // one directory and one temporary file should have been deleted
+      throw std::runtime_error("Directory delete failed");
+
+    lib_to_path.erase(library);
+  }
+
+private:
+  static inline std::unordered_map<void *, fs::path> lib_to_path;
+};
+#endif
+
+} // namespace detail
+
+class kernel_library {
+public:
+  kernel_library() : ptr{nullptr} {}
+  kernel_library(void *ptr) : ptr{ptr} {}
+
+  operator void *() const { return ptr; }
+
+private:
+  void *ptr;
+#ifdef _WIN32
+  static inline detail::path_lib_record single_instance_to_trigger_destructor;
+#endif
+};
+
+namespace detail {
+
+static inline kernel_library load_dl_from_data(char const *const data,
+                                               size_t size) {
+  fs::path filename = write_data_to_file(data, size);
+#ifdef _WIN32
+  void *so = LoadLibraryW(filename.wstring().c_str());
+#else
+  void *so = dlopen(filename.c_str(), RTLD_LAZY);
+#endif
+  if (so == nullptr)
+    throw std::runtime_error("Failed to load kernel library");
+
+#ifdef _WIN32
+  detail::path_lib_record::record_lib_path(filename, so);
+#else
+  std::error_code ec;
+
+  // Windows DLL cannot be deleted while in use
+  if (fs::remove_all(filename.remove_filename(), ec) != 2 || ec)
+    // one directory and one temporary file should have been deleted
+    throw std::runtime_error("Directory delete failed");
+#endif
+
+  return so;
+}
+
+} // namespace detail
+
+/// Load kernel library and return a handle to use the library.
+/// \param [in] name The name of the library.
+static inline kernel_library load_kernel_library(const std::string &name) {
+  std::ifstream ifs;
+  ifs.open(name, std::ios::in | std::ios::binary);
+
+  std::stringstream buffer;
+  buffer << ifs.rdbuf();
+
+  const std::string buffer_string = buffer.str();
+  return detail::load_dl_from_data(buffer_string.c_str(), buffer_string.size());
+}
+
+/// Load kernel library whose image is alreay in memory and return a handle to
+/// use the library.
+/// \param [in] image A pointer to the image in memory.
+static inline kernel_library load_kernel_library_mem(char const *const image) {
+  const size_t size = detail::get_lib_size(image);
+
+  return detail::load_dl_from_data(image, size);
+}
+
+/// Unload kernel library.
+/// \param [in,out] library Handle to the library to be closed.
+static inline void unload_kernel_library(const kernel_library &library) {
+#ifdef _WIN32
+  detail::path_lib_record::remove_lib(library);
+#else
+  dlclose(library);
+#endif
+}
+
+class kernel_function {
+public:
+  kernel_function() : ptr{nullptr} {}
+  kernel_function(dpct::kernel_functor ptr) : ptr{ptr} {}
+
+  operator void *() const { return ((void *)ptr); }
+
+  void operator()(sycl::queue &q, const sycl::nd_range<3> &range,
+                  unsigned int a, void **args, void **extra) {
+    ptr(q, range, a, args, extra);
+  }
+
+private:
+  dpct::kernel_functor ptr;
+};
+
+/// Find kernel function in a kernel library and return its address.
+/// \param [in] library Handle to the kernel library.
+/// \param [in] name Name of the kernel function.
+static inline dpct::kernel_function
+get_kernel_function(kernel_library &library, const std::string &name) {
+#ifdef _WIN32
+  dpct::kernel_functor fn = reinterpret_cast<dpct::kernel_functor>(
+      GetProcAddress(static_cast<HMODULE>(static_cast<void *>(library)),
+                     (name + std::string("_wrapper")).c_str()));
+#else
+  dpct::kernel_functor fn = reinterpret_cast<dpct::kernel_functor>(
+      dlsym(library, (name + std::string("_wrapper")).c_str()));
+#endif
+  if (fn == nullptr)
+    throw std::runtime_error("Failed to get function");
+  return fn;
+}
+
+/// Invoke a kernel function.
+/// \param [in] function kernel function.
+/// \param [in] queue SYCL queue used to execute kernel
+/// \param [in] groupRange SYCL group range
+/// \param [in] localRange SYCL local range
+/// \param [in] localMemSize The size of local memory required by the kernel
+///             function.
+/// \param [in] kernelParams Array of pointers to kernel arguments.
+/// \param [in] extra Extra arguments.
+static inline void invoke_kernel_function(dpct::kernel_function &function,
+                                          sycl::queue &queue,
+                                          sycl::range<3> groupRange,
+                                          sycl::range<3> localRange,
+                                          unsigned int localMemSize,
+                                          void **kernelParams, void **extra) {
+  function(queue, sycl::nd_range<3>(groupRange * localRange, localRange),
+           localMemSize, kernelParams, extra);
+}
+
+/// Find image wrapper in a kernel library and return its address.
+/// \param [in] library Handle to the kernel library.
+/// \param [in] name Name of the target image wrapper.
+static inline dpct::image_wrapper_base_p
+get_image_wrapper(dpct::kernel_library &library, const std::string &name) {
+#ifdef _WIN32
+  dpct::image_wrapper_base_p fn =
+      reinterpret_cast<dpct::image_wrapper_base_p>(GetProcAddress(
+          static_cast<HMODULE>(static_cast<void *>(library)), name.c_str()));
+#else
+  dpct::image_wrapper_base_p fn = reinterpret_cast<dpct::image_wrapper_base_p>(
+      dlsym(library, name.c_str()));
+#endif
+  if (fn == nullptr)
+    throw std::runtime_error("Failed to get image");
+  return fn;
+}
+
+} // namespace dpct
+#endif // __DPCT_KERNEL_HPP__
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/lapack_utils.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/lapack_utils.hpp
new file mode 100644
index 0000000..dac77d5
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/lapack_utils.hpp
@@ -0,0 +1,1953 @@
+//==---- lapack_utils.hpp -------------------------*- C++ -*----------------==//
+//
+// Copyright (C) Intel Corporation
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+// See https://llvm.org/LICENSE.txt for license information.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef __DPCT_LAPACK_UTILS_HPP__
+#define __DPCT_LAPACK_UTILS_HPP__
+
+#include "memory.hpp"
+#include "util.hpp"
+#include "lib_common_utils.hpp"
+
+#include <oneapi/mkl.hpp>
+#include <sycl/sycl.hpp>
+
+namespace dpct {
+namespace lapack {
+/// Computes all eigenvalues and, optionally, eigenvectors of a real generalized
+/// symmetric definite eigenproblem using a divide and conquer method.
+/// \return Returns 0 if no synchronous exception, otherwise returns 1.
+/// \param [in] queue Device queue where calculations will be performed. It must
+/// have the in_order property when using the USM mode (DPCT_USM_LEVEL_NONE is
+/// not defined).
+/// \param [in] itype Must be 1 or 2 or 3. Specifies the problem type to be solved.
+/// \param [in] jobz Must be job::novec or job::vec.
+/// \param [in] uplo Must be uplo::upper or uplo::lower.
+/// \param [in] n The order of the matrices A and B.
+/// \param [in,out] a The symmetric matrix A.
+/// \param [in] lda The leading dimension of matrix A.
+/// \param [in,out] b The symmetric matrix B.
+/// \param [in] ldb The leading dimension of matrix B.
+/// \param [out] w Eigenvalues.
+/// \param [in] scratchpad Scratchpad memory to be used by the routine
+/// for storing intermediate results.
+/// \param [in] scratchpad_size Size of scratchpad memory as a number of
+/// floating point elements of type T.
+/// \param [out] info If lapack synchronous exception is caught, the value
+/// returned from info() method of the exception is set to \p info.
+template <typename T>
+inline int sygvd(sycl::queue &queue, std::int64_t itype, oneapi::mkl::job jobz,
+                 oneapi::mkl::uplo uplo, int n, T *a, int lda, T *b, int ldb,
+                 T *w, T *scratchpad, int scratchpad_size, int *info) {
+#ifdef DPCT_USM_LEVEL_NONE
+  auto info_buf = get_buffer<int>(info);
+  auto a_buffer = get_buffer<T>(a);
+  auto b_buffer = get_buffer<T>(b);
+  auto w_buffer = get_buffer<T>(w);
+  auto scratchpad_buffer = get_buffer<T>(scratchpad);
+  int info_val = 0;
+  int ret_val = 0;
+  try {
+    oneapi::mkl::lapack::sygvd(queue, itype, jobz, uplo, n, a_buffer, lda,
+                               b_buffer, ldb, w_buffer, scratchpad_buffer,
+                               scratchpad_size);
+  } catch (oneapi::mkl::lapack::exception const& e) {
+    std::cerr << "Unexpected exception caught during call to LAPACK API: sygvd"
+              << std::endl
+              << "reason: " << e.what() << std::endl
+              << "info: " << e.info() << std::endl;
+    info_val = static_cast<int>(e.info());
+    ret_val = 1;
+  } catch (sycl::exception const& e) {
+    std::cerr << "Caught synchronous SYCL exception:" << std::endl
+              << "reason: " << e.what() << std::endl;
+    ret_val = 1;
+  }
+  queue.submit([&, info_val](sycl::handler &cgh) {
+    auto info_acc = info_buf.get_access<sycl::access_mode::write>(cgh);
+    cgh.single_task<dpct_kernel_name<class sygvd_set_info, T>>(
+        [=]() { info_acc[0] = info_val; });
+  });
+  return ret_val;
+#else
+  try {
+    oneapi::mkl::lapack::sygvd(queue, itype, jobz, uplo, n, a, lda, b, ldb, w,
+                               scratchpad, scratchpad_size);
+  } catch (oneapi::mkl::lapack::exception const& e) {
+    std::cerr << "Unexpected exception caught during call to LAPACK API: sygvd"
+              << std::endl
+              << "reason: " << e.what() << std::endl
+              << "info: " << e.info() << std::endl;
+    int info_val = static_cast<int>(e.info());
+    queue.memcpy(info, &info_val, sizeof(int)).wait();
+    return 1;
+  } catch (sycl::exception const& e) {
+    std::cerr << "Caught synchronous SYCL exception:" << std::endl
+              << "reason: " << e.what() << std::endl;
+    queue.memset(info, 0, sizeof(int)).wait();
+    return 1;
+  }
+  queue.memset(info, 0, sizeof(int));
+  return 0;
+#endif
+}
+/// Computes all the eigenvalues, and optionally, the eigenvectors of a complex
+/// generalized Hermitian positive-definite eigenproblem using a divide and
+/// conquer method.
+/// \return Returns 0 if no synchronous exception, otherwise returns 1.
+/// \param [in] queue Device queue where calculations will be performed. It must
+/// have the in_order property when using the USM mode (DPCT_USM_LEVEL_NONE is
+/// not defined).
+/// \param [in] itype Must be 1 or 2 or 3. Specifies the problem type to be solved.
+/// \param [in] jobz Must be job::novec or job::vec.
+/// \param [in] uplo Must be uplo::upper or uplo::lower.
+/// \param [in] n The order of the matrices A and B.
+/// \param [in,out] a The Hermitian matrix A.
+/// \param [in] lda The leading dimension of matrix A.
+/// \param [in,out] b The Hermitian matrix B.
+/// \param [in] ldb The leading dimension of matrix B.
+/// \param [in] w Eigenvalues.
+/// \param [in] scratchpad Scratchpad memory to be used by the routine
+/// for storing intermediate results.
+/// \param [in] scratchpad_size Size of scratchpad memory as a number of
+/// floating point elements of type T.
+/// \param [out] info If lapack synchronous exception is caught, the value
+/// returned from info() method of the exception is set to \p info.
+template <typename T, typename Tw>
+inline int hegvd(sycl::queue &queue, std::int64_t itype, oneapi::mkl::job jobz,
+                 oneapi::mkl::uplo uplo, int n, T *a, int lda, T *b, int ldb,
+                 Tw *w, T *scratchpad, int scratchpad_size, int *info) {
+  using Ty = typename DataType<T>::T2;
+#ifdef DPCT_USM_LEVEL_NONE
+  auto info_buf = get_buffer<int>(info);
+  auto a_buffer = get_buffer<Ty>(a);
+  auto b_buffer = get_buffer<Ty>(b);
+  auto w_buffer = get_buffer<Tw>(w);
+  auto scratchpad_buffer = get_buffer<Ty>(scratchpad);
+  int info_val = 0;
+  int ret_val = 0;
+  try {
+    oneapi::mkl::lapack::hegvd(queue, itype, jobz, uplo, n, a_buffer, lda,
+                               b_buffer, ldb, w_buffer, scratchpad_buffer,
+                               scratchpad_size);
+  } catch (oneapi::mkl::lapack::exception const& e) {
+    std::cerr << "Unexpected exception caught during call to LAPACK API: hegvd"
+              << std::endl
+              << "reason: " << e.what() << std::endl
+              << "info: " << e.info() << std::endl;
+    info_val = static_cast<int>(e.info());
+    ret_val = 1;
+  } catch (sycl::exception const& e) {
+    std::cerr << "Caught synchronous SYCL exception:" << std::endl
+              << "reason: " << e.what() << std::endl;
+    ret_val = 1;
+  }
+  queue.submit([&, info_val](sycl::handler &cgh) {
+    auto info_acc = info_buf.get_access<sycl::access_mode::write>(cgh);
+    cgh.single_task<dpct_kernel_name<class hegvd_set_info, T>>(
+        [=]() { info_acc[0] = info_val; });
+  });
+  return ret_val;
+#else
+  try {
+    oneapi::mkl::lapack::hegvd(queue, itype, jobz, uplo, n, (Ty *)a, lda, (Ty *)b,
+                               ldb, w, (Ty *)scratchpad, scratchpad_size);
+  } catch (oneapi::mkl::lapack::exception const& e) {
+    std::cerr << "Unexpected exception caught during call to LAPACK API: hegvd"
+              << std::endl
+              << "reason: " << e.what() << std::endl
+              << "info: " << e.info() << std::endl;
+    int info_val = static_cast<int>(e.info());
+    queue.memcpy(info, &info_val, sizeof(int)).wait();
+    return 1;
+  } catch (sycl::exception const& e) {
+    std::cerr << "Caught synchronous SYCL exception:" << std::endl
+              << "reason: " << e.what() << std::endl;
+    queue.memset(info, 0, sizeof(int)).wait();
+    return 1;
+  }
+  queue.memset(info, 0, sizeof(int));
+  return 0;
+#endif
+}
+/// Computes the Cholesky factorizations of a batch of symmetric (or Hermitian,
+/// for complex data) positive-definite matrices.
+/// \return Returns 0 if no synchronous exception, otherwise returns 1.
+/// \param [in] queue Device queue where calculations will be performed. It must
+/// have the in_order property when using the USM mode (DPCT_USM_LEVEL_NONE is
+/// not defined).
+/// \param [in] uplo Must be uplo::upper or uplo::lower.
+/// \param [in] n The order of the matrix A.
+/// \param [in,out] a Array of pointers to matrix A.
+/// \param [in] lda The leading dimension of matrix A.
+/// \param [out] info If lapack synchronous exception is caught, the value
+/// returned from info() method of the exception is set to \p info.
+/// \param [in] group_size The batch size.
+template <typename T>
+inline int potrf_batch(sycl::queue &queue, oneapi::mkl::uplo uplo, int n,
+                       T *a[], int lda, int *info, int group_size) {
+#ifdef DPCT_USM_LEVEL_NONE
+  throw std::runtime_error("this API is unsupported when USM level is none");
+#else
+  using Ty = typename DataType<T>::T2;
+  struct matrix_info_t {
+    oneapi::mkl::uplo uplo_info;
+    std::int64_t n_info;
+    std::int64_t lda_info;
+    std::int64_t group_size_info;
+  };
+  matrix_info_t *matrix_info =
+      (matrix_info_t *)std::malloc(sizeof(matrix_info_t));
+  matrix_info->uplo_info = uplo;
+  matrix_info->n_info = n;
+  matrix_info->lda_info = lda;
+  matrix_info->group_size_info = group_size;
+  std::int64_t scratchpad_size = 0;
+  sycl::event e;
+  Ty *scratchpad = nullptr;
+  try {
+    scratchpad_size = oneapi::mkl::lapack::potrf_batch_scratchpad_size<Ty>(
+        queue, &(matrix_info->uplo_info), &(matrix_info->n_info),
+        &(matrix_info->lda_info), 1, &(matrix_info->group_size_info));
+    scratchpad = sycl::malloc_device<Ty>(scratchpad_size, queue);
+    e = oneapi::mkl::lapack::potrf_batch(
+        queue, &(matrix_info->uplo_info), &(matrix_info->n_info), (Ty **)a,
+        &(matrix_info->lda_info), 1, &(matrix_info->group_size_info),
+        scratchpad, scratchpad_size);
+  } catch (oneapi::mkl::lapack::batch_error const &be) {
+    std::cerr << "Unexpected exception caught during call to LAPACK API: "
+                 "potrf_batch_scratchpad_size/potrf_batch"
+              << std::endl
+              << "reason: " << be.what() << std::endl
+              << "number: " << be.info() << std::endl;
+    int i = 0;
+    auto &ids = be.ids();
+    std::vector<int> info_vec(group_size);
+    for (auto const &e : be.exceptions()) {
+      try {
+        std::rethrow_exception(e);
+      } catch (oneapi::mkl::lapack::exception &e) {
+        std::cerr << "Exception " << ids[i] << std::endl
+                  << "reason: " << e.what() << std::endl
+                  << "info: " << e.info() << std::endl;
+        info_vec[i] = e.info();
+        i++;
+      }
+    }
+    queue.memcpy(info, info_vec.data(), group_size * sizeof(int)).wait();
+    std::free(matrix_info);
+    if (scratchpad)
+      sycl::free(scratchpad, queue);
+    return 1;
+  } catch (sycl::exception const &e) {
+    std::cerr << "Caught synchronous SYCL exception:" << std::endl
+              << "reason: " << e.what() << std::endl;
+    queue.memset(info, 0, group_size * sizeof(int)).wait();
+    std::free(matrix_info);
+    if (scratchpad)
+      sycl::free(scratchpad, queue);
+    return 1;
+  }
+  queue.submit([&](sycl::handler &cgh) {
+    cgh.depends_on(e);
+    cgh.host_task([=] {
+      std::free(matrix_info);
+      sycl::free(scratchpad, queue);
+    });
+  });
+  queue.memset(info, 0, group_size * sizeof(int));
+  return 0;
+#endif
+}
+/// Solves a batch of systems of linear equations with a Cholesky-factored
+/// symmetric (Hermitian) positive-definite coefficient matrices.
+/// \return Returns 0 if no synchronous exception, otherwise returns 1.
+/// \param [in] queue Device queue where calculations will be performed. It must
+/// have the in_order property when using the USM mode (DPCT_USM_LEVEL_NONE is
+/// not defined).
+/// \param [in] uplo Must be uplo::upper or uplo::lower.
+/// \param [in] n The order of the matrix A.
+/// \param [in] nrhs The number of right-hand sides.
+/// \param [in,out] a Array of pointers to matrix A.
+/// \param [in] lda The leading dimension of matrix A.
+/// \param [in,out] b Array of pointers to matrix B.
+/// \param [in] ldb The leading dimension of matrix B.
+/// \param [out] info If lapack synchronous exception is caught, the value
+/// returned from info() method of the exception is set to \p info.
+/// \param [in] group_size The batch size.
+template <typename T>
+inline int potrs_batch(sycl::queue &queue, oneapi::mkl::uplo uplo, int n,
+                       int nrhs, T *a[], int lda, T *b[], int ldb, int *info,
+                       int group_size) {
+#ifdef DPCT_USM_LEVEL_NONE
+  throw std::runtime_error("this API is unsupported when USM level is none");
+#else
+  using Ty = typename DataType<T>::T2;
+  struct matrix_info_t {
+    oneapi::mkl::uplo uplo_info;
+    std::int64_t n_info;
+    std::int64_t nrhs_info;
+    std::int64_t lda_info;
+    std::int64_t ldb_info;
+    std::int64_t group_size_info;
+  };
+  matrix_info_t *matrix_info =
+      (matrix_info_t *)std::malloc(sizeof(matrix_info_t));
+  matrix_info->uplo_info = uplo;
+  matrix_info->n_info = n;
+  matrix_info->nrhs_info = nrhs;
+  matrix_info->lda_info = lda;
+  matrix_info->ldb_info = ldb;
+  matrix_info->group_size_info = group_size;
+  std::int64_t scratchpad_size = 0;
+  sycl::event e;
+  Ty *scratchpad = nullptr;
+  try {
+    scratchpad_size = oneapi::mkl::lapack::potrs_batch_scratchpad_size<Ty>(
+        queue, &(matrix_info->uplo_info), &(matrix_info->n_info),
+        &(matrix_info->nrhs_info), &(matrix_info->lda_info),
+        &(matrix_info->ldb_info), 1, &(matrix_info->group_size_info));
+    scratchpad = sycl::malloc_device<Ty>(scratchpad_size, queue);
+    e = oneapi::mkl::lapack::potrs_batch(
+        queue, &(matrix_info->uplo_info), &(matrix_info->n_info),
+        &(matrix_info->nrhs_info), (Ty **)a, &(matrix_info->lda_info), (Ty **)b,
+        &(matrix_info->ldb_info), 1, &(matrix_info->group_size_info),
+        scratchpad, scratchpad_size);
+  } catch (oneapi::mkl::lapack::batch_error const &be) {
+    std::cerr << "Unexpected exception caught during call to LAPACK API: "
+                 "potrs_batch_scratchpad_size/potrs_batch"
+              << std::endl
+              << "reason: " << be.what() << std::endl
+              << "number: " << be.info() << std::endl;
+    int i = 0;
+    auto &ids = be.ids();
+    std::vector<int> info_vec(group_size);
+    for (auto const &e : be.exceptions()) {
+      try {
+        std::rethrow_exception(e);
+      } catch (oneapi::mkl::lapack::exception &e) {
+        std::cerr << "Exception " << ids[i] << std::endl
+                  << "reason: " << e.what() << std::endl
+                  << "info: " << e.info() << std::endl;
+        info_vec[i] = e.info();
+        i++;
+      }
+    }
+    queue.memcpy(info, info_vec.data(), group_size * sizeof(int)).wait();
+    std::free(matrix_info);
+    if (scratchpad)
+      sycl::free(scratchpad, queue);
+    return 1;
+  } catch (sycl::exception const &e) {
+    std::cerr << "Caught synchronous SYCL exception:" << std::endl
+              << "reason: " << e.what() << std::endl;
+    queue.memset(info, 0, group_size * sizeof(int)).wait();
+    std::free(matrix_info);
+    if (scratchpad)
+      sycl::free(scratchpad, queue);
+    return 1;
+  }
+  queue.submit([&](sycl::handler &cgh) {
+    cgh.depends_on(e);
+    cgh.host_task([=] {
+      std::free(matrix_info);
+      sycl::free(scratchpad, queue);
+    });
+  });
+  queue.memset(info, 0, group_size * sizeof(int));
+  return 0;
+#endif
+}
+
+namespace detail {
+template <template <typename> typename functor_t, typename... args_t>
+inline int lapack_shim(sycl::queue &q, library_data_t a_type, int *info,
+                       std::string const &lapack_api_name, args_t &&...args) {
+  auto handle_lapack_exception = [&](const oneapi::mkl::lapack::exception &e) {
+    std::cerr << "Unexpected exception caught during call to LAPACK API: "
+              << lapack_api_name << std::endl
+              << "reason: " << e.what() << std::endl
+              << "info: " << e.info() << std::endl
+              << "detail: " << e.detail() << std::endl;
+    if (e.info() < std::numeric_limits<int>::min() ||
+        e.info() > std::numeric_limits<int>::max()) {
+      throw std::runtime_error("e.info() exceeds the limit of int type");
+    }
+    int info_val = static_cast<int>(e.info());
+    if (info)
+      dpct::detail::dpct_memcpy(q, info, &info_val, sizeof(int),
+                                memcpy_direction::host_to_device)
+          .wait();
+    return 1;
+  };
+  try {
+    switch (a_type) {
+    case library_data_t::real_float: {
+      functor_t<float>()(std::forward<args_t>(args)...);
+      break;
+    }
+    case library_data_t::real_double: {
+      functor_t<double>()(std::forward<args_t>(args)...);
+      break;
+    }
+    case library_data_t::complex_float: {
+      functor_t<std::complex<float>>()(std::forward<args_t>(args)...);
+      break;
+    }
+    case library_data_t::complex_double: {
+      functor_t<std::complex<double>>()(std::forward<args_t>(args)...);
+      break;
+    }
+    default:
+      throw std::runtime_error("the data type is unsupported");
+    }
+  } catch (oneapi::mkl::lapack::batch_error const &be) {
+    try {
+      std::rethrow_exception(be.exceptions()[0]);
+    } catch (oneapi::mkl::lapack::exception &e) {
+      return handle_lapack_exception(e);
+    }
+  } catch (oneapi::mkl::lapack::exception const &e) {
+    return handle_lapack_exception(e);
+  } catch (sycl::exception const &e) {
+    std::cerr << "Caught synchronous SYCL exception:" << std::endl
+              << "reason: " << e.what() << std::endl;
+    if (info)
+      dpct::detail::dpct_memset<unsigned char>(q, info, 0, sizeof(int)).wait();
+    return 1;
+  }
+  return 0;
+}
+
+template <typename T> class working_memory {
+public:
+  working_memory(std::size_t element_number, const sycl::queue &q) : _q(q) {
+    _ptr = dpct::detail::dpct_malloc(element_number * sizeof(T), _q);
+  }
+  auto get_memory() {
+    return dpct::detail::get_memory<T>(_ptr);
+  }
+  auto get_ptr() {
+    return _ptr;
+  }
+  void set_event(sycl::event e) { _e = e; }
+  ~working_memory() {
+    if (_ptr) {
+      dpct::async_dpct_free({_ptr}, {_e}, _q);
+    }
+  }
+
+private:
+  void *_ptr = nullptr;
+  sycl::event _e;
+  sycl::queue _q;
+};
+
+std::size_t byte_to_element_number(std::size_t size_in_byte,
+                                   dpct::library_data_t element_type) {
+  auto dv = std::lldiv(
+      size_in_byte,
+      dpct::detail::library_data_size[static_cast<unsigned int>(element_type)] /
+          8);
+  if (dv.rem) {
+    throw std::runtime_error(
+        "size_in_byte is not divisible by the size of element (in bytes)");
+  }
+  return dv.quot;
+}
+std::size_t element_number_to_byte(std::size_t size_in_element,
+                                   dpct::library_data_t element_type) {
+  auto dv = std::lldiv(
+      dpct::detail::library_data_size[static_cast<unsigned int>(element_type)],
+      8);
+  if (dv.rem) {
+    throw std::runtime_error(
+        "the size of element (in bits) is not divisible by 8");
+  }
+  return size_in_element * dv.quot;
+}
+
+inline oneapi::mkl::jobsvd char2jobsvd(signed char job) {
+  switch (job) {
+  case 'A':
+    return oneapi::mkl::jobsvd::vectors;
+  case 'S':
+    return oneapi::mkl::jobsvd::somevec;
+  case 'O':
+    return oneapi::mkl::jobsvd::vectorsina;
+  case 'N':
+    return oneapi::mkl::jobsvd::novec;
+  default:
+    throw std::runtime_error("the job type is unsupported");
+  }
+}
+
+template <typename T> struct getrf_scratchpad_size_impl {
+  void operator()(sycl::queue &q, std::int64_t m, std::int64_t n,
+                  library_data_t a_type, std::int64_t lda,
+                  std::size_t &device_ws_size) {
+    device_ws_size =
+        oneapi::mkl::lapack::getrf_scratchpad_size<T>(q, m, n, lda);
+  }
+};
+
+template <typename T> struct getrf_impl {
+  void operator()(sycl::queue &q, std::int64_t m, std::int64_t n,
+                  library_data_t a_type, void *a, std::int64_t lda,
+                  std::int64_t *ipiv, void *device_ws,
+                  std::size_t device_ws_size, int *info) {
+    auto ipiv_data = dpct::detail::get_memory<std::int64_t>(ipiv);
+    auto a_data = dpct::detail::get_memory<T>(a);
+    auto device_ws_data = dpct::detail::get_memory<T>(device_ws);
+    oneapi::mkl::lapack::getrf(q, m, n, a_data, lda, ipiv_data, device_ws_data,
+                               device_ws_size);
+    dpct::detail::dpct_memset<unsigned char>(q, info, 0, sizeof(int));
+  }
+};
+
+template <typename T> struct getrs_impl {
+  void operator()(sycl::queue &q, oneapi::mkl::transpose trans, std::int64_t n,
+                  std::int64_t nrhs, library_data_t a_type, void *a,
+                  std::int64_t lda, std::int64_t *ipiv, library_data_t b_type,
+                  void *b, std::int64_t ldb, int *info) {
+    auto ipiv_data = dpct::detail::get_memory<std::int64_t>(ipiv);
+    std::int64_t device_ws_size = oneapi::mkl::lapack::getrs_scratchpad_size<T>(
+        q, trans, n, nrhs, lda, ldb);
+    working_memory<T> device_ws(device_ws_size, q);
+    auto device_ws_data = device_ws.get_memory();
+    auto a_data = dpct::detail::get_memory<T>(a);
+    auto b_data = dpct::detail::get_memory<T>(b);
+    oneapi::mkl::lapack::getrs(q, trans, n, nrhs, a_data, lda, ipiv_data,
+                               b_data, ldb, device_ws_data, device_ws_size);
+    sycl::event e = dpct::detail::dpct_memset<unsigned char>(q, info, 0, sizeof(int));
+    device_ws.set_event(e);
+  }
+};
+
+template <typename T> struct geqrf_scratchpad_size_impl {
+  void operator()(sycl::queue &q, std::int64_t m, std::int64_t n,
+                  library_data_t a_type, std::int64_t lda,
+                  std::size_t &device_ws_size) {
+    device_ws_size =
+        oneapi::mkl::lapack::geqrf_scratchpad_size<T>(q, m, n, lda);
+  }
+};
+
+template <typename T> struct geqrf_impl {
+  void operator()(sycl::queue &q, std::int64_t m, std::int64_t n,
+                  library_data_t a_type, void *a, std::int64_t lda,
+                  library_data_t tau_type, void *tau, void *device_ws,
+                  std::size_t device_ws_size, int *info) {
+    auto a_data = dpct::detail::get_memory<T>(a);
+    auto tau_data = dpct::detail::get_memory<T>(tau);
+    auto device_ws_data = dpct::detail::get_memory<T>(device_ws);
+    oneapi::mkl::lapack::geqrf(q, m, n, a_data, lda, tau_data, device_ws_data,
+                               device_ws_size);
+    dpct::detail::dpct_memset<unsigned char>(q, info, 0, sizeof(int));
+  }
+};
+
+template <typename T> struct getrfnp_impl {
+  void operator()(sycl::queue &q, std::int64_t m, std::int64_t n,
+                  library_data_t a_type, void *a, std::int64_t lda,
+                  std::int64_t *ipiv, void *device_ws,
+                  std::size_t device_ws_size, int *info) {
+#ifndef __INTEL_MKL__
+    throw std::runtime_error(
+        "The oneAPI Math Kernel Library (oneMKL) Interfaces "
+        "Project does not support this API.");
+#else
+    std::int64_t a_stride = m * lda;
+    auto a_data = dpct::detail::get_memory<T>(a);
+    auto device_ws_data = dpct::detail::get_memory<T>(device_ws);
+    oneapi::mkl::lapack::getrfnp_batch(q, m, n, a_data, lda, a_stride, 1,
+                                       device_ws_data, device_ws_size);
+    dpct::detail::dpct_memset<unsigned char>(q, info, 0, sizeof(int));
+#endif
+  }
+};
+
+template <typename T> struct gesvd_scratchpad_size_impl {
+  void operator()(sycl::queue &q, oneapi::mkl::jobsvd jobu,
+                  oneapi::mkl::jobsvd jobvt, std::int64_t m, std::int64_t n,
+                  library_data_t a_type, std::int64_t lda,
+                  library_data_t u_type, std::int64_t ldu,
+                  library_data_t vt_type, std::int64_t ldvt,
+                  std::size_t &device_ws_size) {
+    device_ws_size = oneapi::mkl::lapack::gesvd_scratchpad_size<T>(
+        q, jobu, jobvt, m, n, lda, ldu, ldvt);
+  }
+};
+
+template <typename T> struct ElementType {
+  using value_tpye = T;
+};
+template <typename T> struct ElementType<std::complex<T>> {
+  using value_tpye = T;
+};
+template <typename T> struct gesvd_impl {
+  void operator()(sycl::queue &q, oneapi::mkl::jobsvd jobu,
+                  oneapi::mkl::jobsvd jobvt, std::int64_t m, std::int64_t n,
+                  library_data_t a_type, void *a, std::int64_t lda,
+                  library_data_t s_type, void *s, library_data_t u_type,
+                  void *u, std::int64_t ldu, library_data_t vt_type, void *vt,
+                  std::int64_t ldvt, void *device_ws,
+                  std::size_t device_ws_size, int *info) {
+    auto a_data = dpct::detail::get_memory<T>(a);
+    auto s_data =
+        dpct::detail::get_memory<typename ElementType<T>::value_tpye>(s);
+    auto u_data = dpct::detail::get_memory<T>(u);
+    auto vt_data = dpct::detail::get_memory<T>(vt);
+    auto device_ws_data = dpct::detail::get_memory<T>(device_ws);
+    oneapi::mkl::lapack::gesvd(q, jobu, jobvt, m, n, a_data, lda, s_data,
+                               u_data, ldu, vt_data, ldvt, device_ws_data,
+                               device_ws_size);
+    dpct::detail::dpct_memset<unsigned char>(q, info, 0, sizeof(int));
+  }
+};
+template <typename T> struct gesvd_conj_impl : public gesvd_impl<T> {
+  void operator()(sycl::queue &q, oneapi::mkl::jobsvd jobu,
+                  oneapi::mkl::jobsvd jobvt, std::int64_t m, std::int64_t n,
+                  library_data_t a_type, void *a, std::int64_t lda,
+                  library_data_t s_type, void *s, library_data_t u_type,
+                  void *u, std::int64_t ldu, library_data_t vt_type, void *vt,
+                  std::int64_t ldvt, void *device_ws,
+                  std::size_t device_ws_size, int *info) {
+#ifndef __INTEL_MKL__
+    throw std::runtime_error(
+        "The oneAPI Math Kernel Library (oneMKL) Interfaces "
+        "Project does not support this API.");
+#else
+    using base = gesvd_impl<T>;
+    base::operator()(q, jobu, jobvt, m, n, a_type, a, lda, s_type, s, u_type, u,
+                     ldu, vt_type, vt, ldvt, device_ws, device_ws_size, info);
+    auto vt_data = dpct::detail::get_memory<T>(vt);
+    oneapi::mkl::blas::row_major::imatcopy(q, oneapi::mkl::transpose::conjtrans,
+                                           n, n, T(1.0f), vt_data, ldvt, ldvt);
+    dpct::detail::dpct_memset<unsigned char>(q, info, 0, sizeof(int));
+#endif
+  }
+};
+
+template <typename T> struct potrf_scratchpad_size_impl {
+  void operator()(sycl::queue &q, oneapi::mkl::uplo uplo, std::int64_t n,
+                  library_data_t a_type, std::int64_t lda,
+                  std::size_t &device_ws_size) {
+    device_ws_size =
+        oneapi::mkl::lapack::potrf_scratchpad_size<T>(q, uplo, n, lda);
+  }
+};
+
+template <typename T> struct potrf_impl {
+  void operator()(sycl::queue &q, oneapi::mkl::uplo uplo, std::int64_t n,
+                  library_data_t a_type, void *a, std::int64_t lda,
+                  void *device_ws, std::size_t device_ws_size, int *info) {
+    auto a_data = dpct::detail::get_memory<T>(a);
+    auto device_ws_data = dpct::detail::get_memory<T>(device_ws);
+    oneapi::mkl::lapack::potrf(q, uplo, n, a_data, lda, device_ws_data,
+                               device_ws_size);
+    dpct::detail::dpct_memset<unsigned char>(q, info, 0, sizeof(int));
+  }
+};
+
+template <typename T> struct potrs_impl {
+  void operator()(sycl::queue &q, oneapi::mkl::uplo uplo, std::int64_t n,
+                  std::int64_t nrhs, library_data_t a_type, void *a,
+                  std::int64_t lda, library_data_t b_type, void *b,
+                  std::int64_t ldb, int *info) {
+    std::int64_t device_ws_size = oneapi::mkl::lapack::potrs_scratchpad_size<T>(
+        q, uplo, n, nrhs, lda, ldb);
+    working_memory<T> device_ws(device_ws_size, q);
+    auto device_ws_data = device_ws.get_memory();
+    auto a_data = dpct::detail::get_memory<T>(a);
+    auto b_data = dpct::detail::get_memory<T>(b);
+    oneapi::mkl::lapack::potrs(q, uplo, n, nrhs, a_data, lda, b_data, ldb,
+                               device_ws_data, device_ws_size);
+    sycl::event e = dpct::detail::dpct_memset<unsigned char>(q, info, 0, sizeof(int));
+    device_ws.set_event(e);
+  }
+};
+
+template <typename T> struct value_type_trait {
+  using value_type = T;
+};
+template <typename T> struct value_type_trait<std::complex<T>> {
+  using value_type = T;
+};
+
+template <typename T> auto lamch_s() {
+#ifndef __INTEL_MKL__
+  throw std::runtime_error("The oneAPI Math Kernel Library (oneMKL) Interfaces "
+                           "Project does not support this API.");
+#else
+  if constexpr (std::is_same_v<T, float>) {
+    return slamch("S");
+  } else if constexpr (std::is_same_v<T, double>) {
+    return dlamch("S");
+  }
+  throw std::runtime_error("the type is unsupported");
+#endif
+}
+
+#define DISPATCH_FLOAT_FOR_SCRATCHPAD_SIZE(FUNC, ...)                          \
+  do {                                                                         \
+    if constexpr (std::is_floating_point_v<T>) {                               \
+      device_ws_size = oneapi::mkl::lapack::sy##FUNC(__VA_ARGS__);             \
+    } else {                                                                   \
+      device_ws_size = oneapi::mkl::lapack::he##FUNC(__VA_ARGS__);             \
+    }                                                                          \
+  } while (0)
+
+#define DISPATCH_FLOAT_FOR_CALCULATION(FUNC, ...)                              \
+  do {                                                                         \
+    if constexpr (std::is_floating_point_v<T>) {                               \
+      oneapi::mkl::lapack::sy##FUNC(__VA_ARGS__);                              \
+    } else {                                                                   \
+      oneapi::mkl::lapack::he##FUNC(__VA_ARGS__);                              \
+    }                                                                          \
+  } while (0)
+
+template <typename T> struct syheevx_scratchpad_size_impl {
+  void operator()(sycl::queue &q, oneapi::mkl::compz jobz,
+                  oneapi::mkl::rangev range, oneapi::mkl::uplo uplo,
+                  std::int64_t n, std::int64_t lda, void *vl, void *vu,
+                  std::int64_t il, std::int64_t iu,
+                  std::size_t &device_ws_size) {
+#ifndef __INTEL_MKL__
+    throw std::runtime_error(
+        "The oneAPI Math Kernel Library (oneMKL) Interfaces "
+        "Project does not support this API.");
+#else
+    using value_t = typename value_type_trait<T>::value_type;
+    auto vl_value = *reinterpret_cast<value_t *>(vl);
+    auto vu_value = *reinterpret_cast<value_t *>(vu);
+    auto abstol = 2 * lamch_s<value_t>();
+    DISPATCH_FLOAT_FOR_SCRATCHPAD_SIZE(evx_scratchpad_size<T>, q, jobz, range,
+                                       uplo, n, lda, vl_value, vu_value, il, iu,
+                                       abstol, lda);
+#endif
+  }
+};
+
+template <typename T> constexpr library_data_t get_library_data_t_from_type() {
+  if constexpr (std::is_same_v<T, float>) {
+    return library_data_t::real_float;
+  } else if constexpr (std::is_same_v<T, double>) {
+    return library_data_t::real_double;
+  } else if constexpr (std::is_same_v<T, sycl::float2> ||
+                       std::is_same_v<T, std::complex<float>>) {
+    return library_data_t::complex_float;
+  } else if constexpr (std::is_same_v<T, sycl::double2> ||
+                       std::is_same_v<T, std::complex<double>>) {
+    return library_data_t::complex_double;
+  }
+  throw std::runtime_error("the type is unsupported");
+}
+
+template <typename T> struct syheevx_impl {
+  void operator()(sycl::queue &q, oneapi::mkl::compz jobz,
+                  oneapi::mkl::rangev range, oneapi::mkl::uplo uplo,
+                  std::int64_t n, library_data_t a_type, void *a,
+                  std::int64_t lda, void *vl, void *vu, std::int64_t il,
+                  std::int64_t iu, std::int64_t *m, library_data_t w_type,
+                  void *w, void *device_ws, std::size_t device_ws_size,
+                  int *info) {
+#ifndef __INTEL_MKL__
+    throw std::runtime_error(
+        "The oneAPI Math Kernel Library (oneMKL) Interfaces "
+        "Project does not support this API.");
+#else
+    using value_t = typename value_type_trait<T>::value_type;
+    working_memory<T> z(n * lda, q);
+    working_memory<std::int64_t> m_device(1, q);
+    auto z_data = z.get_memory();
+    auto m_device_data = m_device.get_memory();
+    auto a_data = dpct::detail::get_memory<T>(a);
+    auto device_ws_data = dpct::detail::get_memory<T>(device_ws);
+    auto vl_value = *reinterpret_cast<value_t *>(vl);
+    auto vu_value = *reinterpret_cast<value_t *>(vu);
+    auto w_data = dpct::detail::get_memory<value_t>(w);
+    auto abstol = 2 * lamch_s<value_t>();
+    DISPATCH_FLOAT_FOR_CALCULATION(evx, q, jobz, range, uplo, n, a_data, lda,
+                                   vl_value, vu_value, il, iu, abstol,
+                                   m_device_data, w_data, z_data, lda,
+                                   device_ws_data, device_ws_size);
+    dpct::async_dpct_memcpy(a, z.get_ptr(), n * lda * sizeof(T),
+                            memcpy_direction::device_to_device, q);
+    dpct::async_dpct_memcpy(m, m_device.get_ptr(), sizeof(std::int64_t),
+                            memcpy_direction::device_to_host, q);
+    sycl::event e = dpct::detail::dpct_memset<unsigned char>(q, info, 0, sizeof(int));
+    z.set_event(e);
+    m_device.set_event(e);
+#endif
+  }
+};
+
+template <typename T> struct syhegvx_scratchpad_size_impl {
+  void operator()(sycl::queue &q, std::int64_t itype, oneapi::mkl::compz jobz,
+                  oneapi::mkl::rangev range, oneapi::mkl::uplo uplo,
+                  std::int64_t n, std::int64_t lda, std::int64_t ldb, void *vl,
+                  void *vu, std::int64_t il, std::int64_t iu,
+                  std::size_t &device_ws_size) {
+#ifndef __INTEL_MKL__
+    throw std::runtime_error(
+        "The oneAPI Math Kernel Library (oneMKL) Interfaces "
+        "Project does not support this API.");
+#else
+    using value_t = typename value_type_trait<T>::value_type;
+    auto vl_value = *reinterpret_cast<value_t *>(vl);
+    auto vu_value = *reinterpret_cast<value_t *>(vu);
+    auto abstol = 2 * lamch_s<value_t>();
+    DISPATCH_FLOAT_FOR_SCRATCHPAD_SIZE(gvx_scratchpad_size<T>, q, itype, jobz,
+                                       range, uplo, n, lda, ldb, vl_value,
+                                       vu_value, il, iu, abstol, lda);
+#endif
+  }
+};
+
+template <typename T> struct syhegvx_impl {
+  void operator()(sycl::queue &q, std::int64_t itype, oneapi::mkl::compz jobz,
+                  oneapi::mkl::rangev range, oneapi::mkl::uplo uplo,
+                  std::int64_t n, void *a, std::int64_t lda, void *b,
+                  std::int64_t ldb, void *vl, void *vu, std::int64_t il,
+                  std::int64_t iu, std::int64_t *m, void *w, void *device_ws,
+                  std::size_t device_ws_size, int *info) {
+#ifndef __INTEL_MKL__
+    throw std::runtime_error(
+        "The oneAPI Math Kernel Library (oneMKL) Interfaces "
+        "Project does not support this API.");
+#else
+    using value_t = typename value_type_trait<T>::value_type;
+    working_memory<T> z(n * lda, q);
+    working_memory<std::int64_t> m_device(1, q);
+    auto z_data = z.get_memory();
+    auto m_device_data = m_device.get_memory();
+    auto a_data = dpct::detail::get_memory<T>(a);
+    auto b_data = dpct::detail::get_memory<T>(b);
+    auto device_ws_data = dpct::detail::get_memory<T>(device_ws);
+    auto vl_value = *reinterpret_cast<value_t *>(vl);
+    auto vu_value = *reinterpret_cast<value_t *>(vu);
+    auto w_data = dpct::detail::get_memory<value_t>(w);
+    auto abstol = 2 * lamch_s<value_t>();
+    DISPATCH_FLOAT_FOR_CALCULATION(gvx, q, itype, jobz, range, uplo, n, a_data,
+                                   lda, b_data, ldb, vl_value, vu_value, il, iu,
+                                   abstol, m_device_data, w_data, z_data, lda,
+                                   device_ws_data, device_ws_size);
+    dpct::async_dpct_memcpy(a, z.get_ptr(), n * lda * sizeof(T),
+                            memcpy_direction::device_to_device, q);
+    dpct::async_dpct_memcpy(m, m_device.get_ptr(), sizeof(std::int64_t),
+                            memcpy_direction::device_to_host, q);
+    sycl::event e = dpct::detail::dpct_memset<unsigned char>(q, info, 0, sizeof(int));
+    z.set_event(e);
+    m_device.set_event(e);
+#endif
+  }
+};
+
+template <typename T> struct syhegvd_scratchpad_size_impl {
+  void operator()(sycl::queue &q, std::int64_t itype, oneapi::mkl::job jobz,
+                  oneapi::mkl::uplo uplo, std::int64_t n, std::int64_t lda,
+                  std::int64_t ldb, std::size_t &device_ws_size) {
+    DISPATCH_FLOAT_FOR_SCRATCHPAD_SIZE(gvd_scratchpad_size<T>, q, itype, jobz,
+                                       uplo, n, lda, ldb);
+  }
+};
+
+template <typename T> struct syhegvd_impl {
+  void operator()(sycl::queue &q, std::int64_t itype, oneapi::mkl::job jobz,
+                  oneapi::mkl::uplo uplo, std::int64_t n, void *a,
+                  std::int64_t lda, void *b, std::int64_t ldb, void *w,
+                  void *device_ws, std::size_t device_ws_size,
+                  int *info) {
+    using value_t = typename value_type_trait<T>::value_type;
+    auto a_data = dpct::detail::get_memory<T>(a);
+    auto b_data = dpct::detail::get_memory<T>(b);
+    auto device_ws_data = dpct::detail::get_memory<T>(device_ws);
+    auto w_data = dpct::detail::get_memory<value_t>(w);
+    DISPATCH_FLOAT_FOR_CALCULATION(gvd, q, itype, jobz, uplo, n, a_data, lda,
+                                   b_data, ldb, w_data, device_ws_data,
+                                   device_ws_size);
+    dpct::detail::dpct_memset<unsigned char>(q, info, 0, sizeof(int));
+  }
+};
+
+oneapi::mkl::compz job2compz(const oneapi::mkl::job &job) {
+  oneapi::mkl::compz ret;
+  if (job == oneapi::mkl::job::novec) {
+    ret = oneapi::mkl::compz::novectors;
+  } else if (job == oneapi::mkl::job::vec) {
+    ret = oneapi::mkl::compz::vectors;
+  } else {
+    throw std::runtime_error("the job type is unsupported");
+  }
+  return ret;
+}
+
+template <typename T> struct syheev_scratchpad_size_impl {
+  void operator()(sycl::queue &q, oneapi::mkl::compz jobz,
+                  oneapi::mkl::uplo uplo, std::int64_t n, std::int64_t lda,
+                  std::size_t &device_ws_size) {
+#ifndef __INTEL_MKL__
+    throw std::runtime_error(
+        "The oneAPI Math Kernel Library (oneMKL) Interfaces "
+        "Project does not support this API.");
+#else
+    DISPATCH_FLOAT_FOR_SCRATCHPAD_SIZE(ev_scratchpad_size<T>, q, jobz, uplo, n,
+                                       lda);
+#endif
+  }
+};
+
+template <typename T> struct syheev_impl {
+  void operator()(sycl::queue &q, oneapi::mkl::compz jobz,
+                  oneapi::mkl::uplo uplo, std::int64_t n, void *a,
+                  std::int64_t lda, void *w, void *device_ws,
+                  std::size_t device_ws_size, int *info) {
+#ifndef __INTEL_MKL__
+    throw std::runtime_error(
+        "The oneAPI Math Kernel Library (oneMKL) Interfaces "
+        "Project does not support this API.");
+#else
+    using value_t = typename value_type_trait<T>::value_type;
+    auto a_data = dpct::detail::get_memory<T>(a);
+    auto device_ws_data = dpct::detail::get_memory<T>(device_ws);
+    auto w_data = dpct::detail::get_memory<value_t>(w);
+    DISPATCH_FLOAT_FOR_CALCULATION(ev, q, jobz, uplo, n, a_data, lda, w_data,
+                                   device_ws_data, device_ws_size);
+    dpct::detail::dpct_memset<unsigned char>(q, info, 0, sizeof(int));
+#endif
+  }
+};
+
+template <typename T> struct syheevd_scratchpad_size_impl {
+  void operator()(sycl::queue &q, oneapi::mkl::job jobz, oneapi::mkl::uplo uplo,
+                  std::int64_t n, library_data_t a_type, std::int64_t lda,
+                  std::size_t &device_ws_size) {
+    DISPATCH_FLOAT_FOR_SCRATCHPAD_SIZE(evd_scratchpad_size<T>, q, jobz, uplo, n,
+                                       lda);
+  }
+};
+
+template <typename T> struct syheevd_impl {
+  void operator()(sycl::queue &q, oneapi::mkl::job jobz, oneapi::mkl::uplo uplo,
+                  std::int64_t n, library_data_t a_type, void *a,
+                  std::int64_t lda, void *w, void *device_ws,
+                  std::size_t device_ws_size, int *info) {
+    using value_t = typename value_type_trait<T>::value_type;
+    auto a_data = dpct::detail::get_memory<T>(a);
+    auto device_ws_data = dpct::detail::get_memory<T>(device_ws);
+    auto w_data = dpct::detail::get_memory<value_t>(w);
+    DISPATCH_FLOAT_FOR_CALCULATION(evd, q, jobz, uplo, n, a_data, lda, w_data,
+                                   device_ws_data, device_ws_size);
+    dpct::detail::dpct_memset<unsigned char>(q, info, 0, sizeof(int));
+  }
+};
+
+#undef DISPATCH_FLOAT_FOR_SCRATCHPAD_SIZE
+#undef DISPATCH_FLOAT_FOR_CALCULATION
+
+template <typename T> struct trtri_scratchpad_size_impl {
+  void operator()(sycl::queue &q, oneapi::mkl::uplo uplo,
+                  oneapi::mkl::diag diag, std::int64_t n, library_data_t a_type,
+                  std::int64_t lda, std::size_t &device_ws_size) {
+#ifndef __INTEL_MKL__
+    throw std::runtime_error(
+        "The oneAPI Math Kernel Library (oneMKL) Interfaces "
+        "Project does not support this API.");
+#else
+    device_ws_size =
+        oneapi::mkl::lapack::trtri_scratchpad_size<T>(q, uplo, diag, n, lda);
+#endif
+  }
+};
+
+template <typename T> struct trtri_impl {
+  void operator()(sycl::queue &q, oneapi::mkl::uplo uplo,
+                  oneapi::mkl::diag diag, std::int64_t n, library_data_t a_type,
+                  void *a, std::int64_t lda, void *device_ws,
+                  std::size_t device_ws_size, int *info) {
+#ifndef __INTEL_MKL__
+    throw std::runtime_error(
+        "The oneAPI Math Kernel Library (oneMKL) Interfaces "
+        "Project does not support this API.");
+#else
+    auto a_data = dpct::detail::get_memory<T>(a);
+    auto device_ws_data = dpct::detail::get_memory<T>(device_ws);
+    oneapi::mkl::lapack::trtri(q, uplo, diag, n, a_data, lda, device_ws_data,
+                               device_ws_size);
+    dpct::detail::dpct_memset<unsigned char>(q, info, 0, sizeof(int));
+#endif
+  }
+};
+} // namespace detail
+
+/// Computes the size of workspace memory of getrf function.
+/// \return Returns 0 if no synchronous exception, otherwise returns 1.
+/// \param [in] q Device queue where computation will be performed. It must
+/// have the in_order property when using the USM mode (DPCT_USM_LEVEL_NONE is
+/// not defined).
+/// \param [in] m The number of rows in the matrix A.
+/// \param [in] n The number of columns in the matrix A.
+/// \param [in] a_type The data type of the matrix A.
+/// \param [in] lda The leading dimension of the matrix A.
+/// \param [out] device_ws_size The workspace size in bytes.
+/// \param [out] host_ws_size The host workspace size in bytes. Currently the
+/// value is always zero.
+inline int getrf_scratchpad_size(sycl::queue &q, std::int64_t m, std::int64_t n,
+                                 library_data_t a_type, std::int64_t lda,
+                                 std::size_t *device_ws_size,
+                                 std::size_t *host_ws_size = nullptr) {
+  if (host_ws_size)
+    *host_ws_size = 0;
+  std::size_t device_ws_size_tmp;
+  int ret = detail::lapack_shim<detail::getrf_scratchpad_size_impl>(
+      q, a_type, nullptr, "getrf_scratchpad_size", q, m, n, a_type, lda,
+      device_ws_size_tmp);
+  *device_ws_size = detail::element_number_to_byte(device_ws_size_tmp, a_type);
+  return ret;
+}
+
+/// Computes the LU factorization of a general m-by-n matrix.
+/// \return Returns 0 if no synchronous exception, otherwise returns 1.
+/// \param [in] q The queue where the routine should be executed. It must
+/// have the in_order property when using the USM mode (DPCT_USM_LEVEL_NONE is
+/// not defined).
+/// \param [in] m The number of rows in the matrix A.
+/// \param [in] n The number of columns in the matrix A.
+/// \param [in] a_type The data type of the matrix A.
+/// \param [in, out] a The input matrix A. Overwritten by L and U. The unit
+/// diagonal elements of L are not stored.
+/// \param [in] lda The leading dimension of the matrix A.
+/// \param [out] ipiv The pivot indices. If \p ipiv is nullptr, non-pivoting
+/// LU factorization is computed.
+/// \param [in] device_ws The workspace.
+/// \param [in] device_ws_size The workspace size in bytes.
+/// \param [out] info If lapack synchronous exception is caught, the value
+/// returned from info() method of the exception is set to \p info.
+inline int getrf(sycl::queue &q, std::int64_t m, std::int64_t n,
+                 library_data_t a_type, void *a, std::int64_t lda,
+                 std::int64_t *ipiv, void *device_ws,
+                 std::size_t device_ws_size, int *info) {
+#ifndef __INTEL_MKL__
+  throw std::runtime_error("The oneAPI Math Kernel Library (oneMKL) Interfaces "
+                           "Project does not support this API.");
+#else
+  std::size_t device_ws_size_in_element_number =
+      detail::byte_to_element_number(device_ws_size, a_type);
+  if (ipiv == nullptr) {
+    return detail::lapack_shim<detail::getrfnp_impl>(
+        q, a_type, info, "getrfnp_batch", q, m, n, a_type, a, lda, ipiv,
+        device_ws, device_ws_size_in_element_number, info);
+  }
+  return detail::lapack_shim<detail::getrf_impl>(
+      q, a_type, info, "getrf", q, m, n, a_type, a, lda, ipiv, device_ws,
+      device_ws_size_in_element_number, info);
+#endif
+}
+
+/// Solves a system of linear equations with a LU-factored square coefficient
+/// matrix, with multiple right-hand sides.
+/// \return Returns 0 if no synchronous exception, otherwise returns 1.
+/// \param [in] q The queue where the routine should be executed. It must
+/// have the in_order property when using the USM mode (DPCT_USM_LEVEL_NONE is
+/// not defined).
+/// \param [in] trans Indicates the form of the linear equation.
+/// \param [in] n The order of the matrix A and the number of rows in matrix B.
+/// \param [in] nrhs The number of right hand sides.
+/// \param [in] a_type The data type of the matrix A.
+/// \param [in] a The input matrix A.
+/// \param [in] lda The leading dimension of the matrix A.
+/// \param [in] ipiv The pivot indices.
+/// \param [in] b_type The data type of the matrix B.
+/// \param [in, out] b The matrix B, whose columns are the right-hand sides
+/// for the systems of equations.
+/// \param [in] ldb The leading dimension of the matrix B.
+/// \param [out] info If lapack synchronous exception is caught, the value
+/// returned from info() method of the exception is set to \p info.
+inline int getrs(sycl::queue &q, oneapi::mkl::transpose trans, std::int64_t n,
+                 std::int64_t nrhs, library_data_t a_type, void *a,
+                 std::int64_t lda, std::int64_t *ipiv, library_data_t b_type,
+                 void *b, std::int64_t ldb, int *info) {
+  return detail::lapack_shim<detail::getrs_impl>(
+      q, a_type, info, "getrs_scratchpad_size/getrs", q, trans, n, nrhs, a_type,
+      a, lda, ipiv, b_type, b, ldb, info);
+}
+
+/// Computes the size of workspace memory of geqrf function.
+/// \return Returns 0 if no synchronous exception, otherwise returns 1.
+/// \param [in] q Device queue where computation will be performed. It must
+/// have the in_order property when using the USM mode (DPCT_USM_LEVEL_NONE is
+/// not defined).
+/// \param [in] m The number of rows in the matrix A.
+/// \param [in] n The number of columns in the matrix A.
+/// \param [in] a_type The data type of the matrix A.
+/// \param [in] lda The leading dimension of the matrix A.
+/// \param [out] device_ws_size The device workspace size in bytes.
+/// \param [out] host_ws_size The host workspace size in bytes. Currently the
+/// value is always zero.
+inline int geqrf_scratchpad_size(sycl::queue &q, std::int64_t m, std::int64_t n,
+                                 library_data_t a_type, std::int64_t lda,
+                                 std::size_t *device_ws_size,
+                                 std::size_t *host_ws_size = nullptr) {
+  if (host_ws_size)
+    *host_ws_size = 0;
+  std::size_t device_ws_size_tmp;
+  int ret = detail::lapack_shim<detail::geqrf_scratchpad_size_impl>(
+      q, a_type, nullptr, "geqrf_scratchpad_size", q, m, n, a_type, lda,
+      device_ws_size_tmp);
+  *device_ws_size = detail::element_number_to_byte(device_ws_size_tmp, a_type);
+  return ret;
+}
+
+/// Computes the QR factorization of a general m-by-n matrix.
+/// \return Returns 0 if no synchronous exception, otherwise returns 1.
+/// \param [in] q The queue where the routine should be executed. It must
+/// have the in_order property when using the USM mode (DPCT_USM_LEVEL_NONE is
+/// not defined).
+/// \param [in] m The number of rows in the matrix A.
+/// \param [in] n The number of columns in the matrix A.
+/// \param [in] a_type The data type of the matrix A.
+/// \param [in, out] a The input matrix A. Overwritten by the factorization data.
+/// \param [in] lda The leading dimension of the matrix A.
+/// \param [in] tau_type The data type of the array tau.
+/// \param [in] tau The array contains scalars that define elementary reflectors
+/// for the matrix Q in its decomposition in a product of elementary reflectors.
+/// \param [in] device_ws The workspace.
+/// \param [in] device_ws_size The workspace size in bytes.
+/// \param [out] info If lapack synchronous exception is caught, the value
+/// returned from info() method of the exception is set to \p info.
+inline int geqrf(sycl::queue &q, std::int64_t m, std::int64_t n,
+                 library_data_t a_type, void *a, std::int64_t lda,
+                 library_data_t tau_type, void *tau, void *device_ws,
+                 std::size_t device_ws_size, int *info) {
+  std::size_t device_ws_size_in_element_number =
+      detail::byte_to_element_number(device_ws_size, a_type);
+  return detail::lapack_shim<detail::geqrf_impl>(
+      q, a_type, info, "geqrf", q, m, n, a_type, a, lda, tau_type, tau,
+      device_ws, device_ws_size_in_element_number, info);
+}
+
+/// Computes the size of workspace memory of gesvd function.
+/// \return Returns 0 if no synchronous exception, otherwise returns 1.
+/// \param [in] q Device queue where computation will be performed. It must
+/// have the in_order property when using the USM mode (DPCT_USM_LEVEL_NONE is
+/// not defined).
+/// \param [in] jobu Must be 'A' (representing jobsvd::vectors), 'S'
+/// (representing jobsvd::somevec), 'O' (representing jobsvd::vectorsina) or 'N'
+/// (representing jobsvd::novec).
+/// \param [in] jobvt Must be 'A' (representing jobsvd::vectors), 'S'
+/// (representing jobsvd::somevec), 'O' (representing jobsvd::vectorsina) or 'N'
+/// (representing jobsvd::novec).
+/// \param [in] m The number of rows in the matrix A.
+/// \param [in] n The number of columns in the matrix A.
+/// \param [in] a_type The data type of the matrix A.
+/// \param [in] lda The leading dimension of the matrix A.
+/// \param [in] u_type The data type of the matrix U.
+/// \param [in] ldu The leading dimension of the matrix U.
+/// \param [in] vt_type The data type of the matrix VT.
+/// \param [in] ldvt The leading dimension of the matrix VT.
+/// \param [out] device_ws_size The device workspace size in bytes.
+/// \param [out] host_ws_size The host workspace size in bytes. Currently the
+/// value is always zero.
+inline int gesvd_scratchpad_size(sycl::queue &q, signed char jobu,
+                                 signed char jobvt, std::int64_t m,
+                                 std::int64_t n, library_data_t a_type,
+                                 std::int64_t lda, library_data_t u_type,
+                                 std::int64_t ldu, library_data_t vt_type,
+                                 std::int64_t ldvt, std::size_t *device_ws_size,
+                                 std::size_t *host_ws_size = nullptr) {
+  oneapi::mkl::jobsvd jobu_enum = detail::char2jobsvd(jobu);
+  oneapi::mkl::jobsvd jobvt_enum = detail::char2jobsvd(jobvt);
+  if (host_ws_size)
+    *host_ws_size = 0;
+  std::size_t device_ws_size_tmp;
+  int ret = detail::lapack_shim<detail::gesvd_scratchpad_size_impl>(
+      q, a_type, nullptr, "gesvd_scratchpad_size", q, jobu_enum, jobvt_enum, m,
+      n, a_type, lda, u_type, ldu, vt_type, ldvt, device_ws_size_tmp);
+  *device_ws_size = detail::element_number_to_byte(device_ws_size_tmp, a_type);
+  return ret;
+}
+
+/// Computes the size of workspace memory of gesvd function.
+/// \return Returns 0 if no synchronous exception, otherwise returns 1.
+/// \param [in] q Device queue where computation will be performed. It must
+/// have the in_order property when using the USM mode (DPCT_USM_LEVEL_NONE is
+/// not defined).
+/// \param [in] jobz Must be job::vec or job::novec
+/// \param [in] all_vec Only have effects when \param jobz is job::vec.If the
+/// value is zero, all m columns of U are returned in the matrix U, otherwise
+/// the first min( \param m, \param n ) columns of U (the left singular vectors)
+/// are returned in the matrix U.
+/// \param [in] m The number of rows in the matrix A.
+/// \param [in] n The number of columns in the matrix A.
+/// \param [in] a_type The data type of the matrix A.
+/// \param [in] lda The leading dimension of the matrix A.
+/// \param [in] u_type The data type of the matrix U.
+/// \param [in] ldu The leading dimension of the matrix U.
+/// \param [in] vt_type The data type of the matrix VT.
+/// \param [in] ldvt The leading dimension of the matrix VT.
+/// \param [out] device_ws_size The device workspace size as a number of
+/// elements of type \param a_type.
+/// \param [out] host_ws_size The host workspace size as a number of elements
+/// of type \param a_type. Currently the value is always zero.
+inline int gesvd_scratchpad_size(sycl::queue &q, oneapi::mkl::job jobz,
+                                 std::int64_t all_vec, std::int64_t m,
+                                 std::int64_t n, library_data_t a_type,
+                                 std::int64_t lda, library_data_t u_type,
+                                 std::int64_t ldu, library_data_t vt_type,
+                                 std::int64_t ldvt, int *device_ws_size,
+                                 std::size_t *host_ws_size = nullptr) {
+  if (host_ws_size)
+    *host_ws_size = 0;
+  oneapi::mkl::jobsvd jobu;
+  oneapi::mkl::jobsvd jobvt;
+  if (jobz == oneapi::mkl::job::vec) {
+    if (all_vec) {
+      jobu = jobvt = oneapi::mkl::jobsvd::somevec;
+    } else {
+      jobu = jobvt = oneapi::mkl::jobsvd::vectors;
+    }
+  } else if (jobz == oneapi::mkl::job::novec) {
+    jobu = jobvt = oneapi::mkl::jobsvd::novec;
+  } else {
+    throw std::runtime_error("the job type is unsupported");
+  }
+  std::size_t device_ws_size_64;
+  int ret = detail::lapack_shim<detail::gesvd_scratchpad_size_impl>(
+      q, a_type, nullptr, "gesvd_scratchpad_size", q, jobu, jobvt, m, n, a_type,
+      lda, u_type, ldu, vt_type, ldvt, device_ws_size_64);
+  *device_ws_size = device_ws_size_64;
+  return ret;
+}
+
+/// Computes the size of workspace memory of gesvd function.
+/// \return Returns 0 if no synchronous exception, otherwise returns 1.
+/// \param [in] q Device queue where computation will be performed. It must
+/// have the in_order property when using the USM mode (DPCT_USM_LEVEL_NONE is
+/// not defined).
+/// \param [in] jobu Must be 'A' (representing jobsvd::vectors), 'S'
+/// (representing jobsvd::somevec), 'O' (representing jobsvd::vectorsina) or 'N'
+/// (representing jobsvd::novec).
+/// \param [in] jobvt Must be 'A' (representing jobsvd::vectors), 'S'
+/// (representing jobsvd::somevec), 'O' (representing jobsvd::vectorsina) or 'N'
+/// (representing jobsvd::novec).
+/// \param [in] m The number of rows in the matrix A.
+/// \param [in] n The number of columns in the matrix A.
+/// \param [in] a_type The data type of the matrix A.
+/// \param [in, out] a The input matrix A and it will be overwritten according
+/// to \p jobu and \p jobvt.
+/// \param [in] lda The leading dimension of the matrix A.
+/// \param [in] s_type The data type of the matrix S.
+/// \param [out] s The output matrix S.
+/// \param [in] u_type The data type of the matrix U.
+/// \param [out] u The output matrix U.
+/// \param [in] ldu The leading dimension of the matrix U.
+/// \param [in] vt_type The data type of the matrix VT.
+/// \param [out] vt The output matrix VT.
+/// \param [in] ldvt The leading dimension of the matrix VT.
+/// \param [in] device_ws The workspace.
+/// \param [in] device_ws_size The workspace size in bytes.
+/// \param [out] info If lapack synchronous exception is caught, the value
+/// returned from info() method of the exception is set to \p info.
+inline int gesvd(sycl::queue &q, signed char jobu, signed char jobvt,
+                 std::int64_t m, std::int64_t n, library_data_t a_type, void *a,
+                 std::int64_t lda, library_data_t s_type, void *s,
+                 library_data_t u_type, void *u, std::int64_t ldu,
+                 library_data_t vt_type, void *vt, std::int64_t ldvt,
+                 void *device_ws, std::size_t device_ws_size, int *info) {
+  oneapi::mkl::jobsvd jobu_enum = detail::char2jobsvd(jobu);
+  oneapi::mkl::jobsvd jobvt_enum = detail::char2jobsvd(jobvt);
+  std::size_t device_ws_size_in_element_number =
+      detail::byte_to_element_number(device_ws_size, a_type);
+  return detail::lapack_shim<detail::gesvd_impl>(
+      q, a_type, info, "gesvd", q, jobu_enum, jobvt_enum, m, n, a_type, a, lda,
+      s_type, s, u_type, u, ldu, vt_type, vt, ldvt, device_ws,
+      device_ws_size_in_element_number, info);
+}
+
+/// Computes the size of workspace memory of gesvd function.
+/// \return Returns 0 if no synchronous exception, otherwise returns 1.
+/// \param [in] q Device queue where computation will be performed. It must
+/// have the in_order property when using the USM mode (DPCT_USM_LEVEL_NONE is
+/// not defined).
+/// \param [in] jobz Must be job::vec or job::novec.
+/// \param [in] all_vec Only have effects when \param jobz is job::vec.If the
+/// value is zero, all m columns of U are returned in the matrix U, otherwise
+/// the first min( \param m, \param n ) columns of U (the left singular vectors)
+/// are returned in the matrix U.
+/// \param [in] m The number of rows in the matrix A.
+/// \param [in] n The number of columns in the matrix A.
+/// \param [in] a_type The data type of the matrix A.
+/// \param [in, out] a The input matrix A and it will be overwritten according
+/// to \p jobu and \p jobvt.
+/// \param [in] lda The leading dimension of the matrix A.
+/// \param [in] s_type The data type of the matrix S.
+/// \param [out] s The output matrix S.
+/// \param [in] u_type The data type of the matrix U.
+/// \param [out] u The output matrix U.
+/// \param [in] ldu The leading dimension of the matrix U.
+/// \param [in] vt_type The data type of the matrix VT.
+/// \param [out] vt The output matrix VT.
+/// \param [in] ldvt The leading dimension of the matrix VT.
+/// \param [in] device_ws The workspace.
+/// \param [in] device_ws_size The device workspace size as a number of
+/// elements of type \param a_type.
+/// \param [out] info If lapack synchronous exception is caught, the value
+/// returned from info() method of the exception is set to \p info.
+inline int gesvd(sycl::queue &q, oneapi::mkl::job jobz, std::int64_t all_vec,
+                 std::int64_t m, std::int64_t n, library_data_t a_type, void *a,
+                 std::int64_t lda, library_data_t s_type, void *s,
+                 library_data_t u_type, void *u, std::int64_t ldu,
+                 library_data_t vt_type, void *vt, std::int64_t ldvt,
+                 void *device_ws, std::size_t device_ws_size, int *info) {
+  oneapi::mkl::jobsvd jobu;
+  oneapi::mkl::jobsvd jobvt;
+  if (jobz == oneapi::mkl::job::vec) {
+    if (all_vec) {
+      jobu = jobvt = oneapi::mkl::jobsvd::somevec;
+    } else {
+      jobu = jobvt = oneapi::mkl::jobsvd::vectors;
+    }
+  } else if (jobz == oneapi::mkl::job::novec) {
+    jobu = jobvt = oneapi::mkl::jobsvd::novec;
+  } else {
+    throw std::runtime_error("the job type is unsupported");
+  }
+
+  detail::lapack_shim<detail::gesvd_conj_impl>(
+      q, a_type, info, "gesvd", q, jobu, jobvt, m, n, a_type, a, lda, s_type, s,
+      u_type, u, ldu, vt_type, vt, ldvt, device_ws, device_ws_size, info);
+  return 0;
+}
+
+/// Computes the size of workspace memory of potrf function.
+/// \return Returns 0 if no synchronous exception, otherwise returns 1.
+/// \param [in] q Device queue where computation will be performed. It must
+/// have the in_order property when using the USM mode (DPCT_USM_LEVEL_NONE is
+/// not defined).
+/// \param [in] uplo Must be uplo::upper or uplo::lower.
+/// \param [in] n The number of columns in the matrix A.
+/// \param [in] a_type The data type of the matrix A.
+/// \param [in] lda The leading dimension of the matrix A.
+/// \param [out] device_ws_size The device workspace size in bytes.
+/// \param [out] host_ws_size The host workspace size in bytes. Currently the
+/// value is always zero.
+inline int potrf_scratchpad_size(sycl::queue &q, oneapi::mkl::uplo uplo,
+                                 std::int64_t n, library_data_t a_type,
+                                 std::int64_t lda, std::size_t *device_ws_size,
+                                 std::size_t *host_ws_size = nullptr) {
+  if (host_ws_size)
+    *host_ws_size = 0;
+  std::size_t device_ws_size_tmp;
+  int ret = detail::lapack_shim<detail::potrf_scratchpad_size_impl>(
+      q, a_type, nullptr, "potrf_scratchpad_size", q, uplo, n, a_type, lda,
+      device_ws_size_tmp);
+  *device_ws_size = detail::element_number_to_byte(device_ws_size_tmp, a_type);
+  return ret;
+}
+
+/// Computes the Cholesky factorization of a symmetric (Hermitian)
+/// positive-definite matrix.
+/// \return Returns 0 if no synchronous exception, otherwise returns 1.
+/// \param [in] q The queue where the routine should be executed. It must
+/// have the in_order property when using the USM mode (DPCT_USM_LEVEL_NONE is
+/// not defined).
+/// \param [in] uplo Must be uplo::upper or uplo::lower.
+/// \param [in] n The number of columns in the matrix A.
+/// \param [in] a_type The data type of the matrix A.
+/// \param [in, out] a The input matrix A. Overwritten by the Cholesky factor U
+/// or L, as specified by \p uplo.
+/// \param [in] lda The leading dimension of the matrix A.
+/// \param [in] device_ws The workspace.
+/// \param [in] device_ws_size The workspace size in bytes.
+/// \param [out] info If lapack synchronous exception is caught, the value
+/// returned from info() method of the exception is set to \p info.
+inline int potrf(sycl::queue &q, oneapi::mkl::uplo uplo, std::int64_t n,
+                 library_data_t a_type, void *a, std::int64_t lda,
+                 void *device_ws, std::size_t device_ws_size, int *info) {
+  std::size_t device_ws_size_in_element_number =
+      detail::byte_to_element_number(device_ws_size, a_type);
+  return detail::lapack_shim<detail::potrf_impl>(
+      q, a_type, info, "potrf", q, uplo, n, a_type, a, lda, device_ws,
+      device_ws_size_in_element_number, info);
+}
+
+/// Solves a system of linear equations with a Cholesky-factored symmetric
+/// (Hermitian) positive-definite coefficient matrix.
+/// \return Returns 0 if no synchronous exception, otherwise returns 1.
+/// \param [in] q The queue where the routine should be executed. It must
+/// have the in_order property when using the USM mode (DPCT_USM_LEVEL_NONE is
+/// not defined).
+/// \param [in] uplo Must be uplo::upper or uplo::lower.
+/// \param [in] n The order of the matrix A and the number of rows in matrix B.
+/// \param [in] nrhs The number of right hand sides.
+/// \param [in] a_type The data type of the matrix A.
+/// \param [in, out] a The input matrix A. Overwritten by the Cholesky factor U
+/// or L, as specified by \p uplo.
+/// \param [in] lda The leading dimension of the matrix A.
+/// \param [in] b_type The data type of the matrix B.
+/// \param [in, out] b The matrix B, whose columns are the right-hand sides
+/// for the systems of equations.
+/// \param [in] ldb The leading dimension of the matrix B.
+/// \param [out] info If lapack synchronous exception is caught, the value
+/// returned from info() method of the exception is set to \p info.
+inline int potrs(sycl::queue &q, oneapi::mkl::uplo uplo, std::int64_t n,
+                 std::int64_t nrhs, library_data_t a_type, void *a,
+                 std::int64_t lda, library_data_t b_type, void *b,
+                 std::int64_t ldb, int *info) {
+  return detail::lapack_shim<detail::potrs_impl>(
+      q, a_type, info, "potrs_scratchpad_size/potrs", q, uplo, n, nrhs, a_type,
+      a, lda, b_type, b, ldb, info);
+}
+
+/// Computes the size of workspace memory of syevx/heevx function.
+/// \return Returns 0 if no synchronous exception, otherwise returns 1.
+/// \param [in] q Device queue where computation will be performed. It must
+/// have the in_order property when using the USM mode (DPCT_USM_LEVEL_NONE is
+/// not defined).
+/// \param [in] jobz Must be job::novec or job::vec.
+/// \param [in] range Must be rangev::all, rangev::values or uplo::indices.
+/// \param [in] uplo Must be uplo::upper or uplo::lower.
+/// \param [in] n The order of the matrix A.
+/// \param [in] a_type The data type of the matrix A.
+/// \param [in] lda The leading dimension of the matrix A.
+/// \param [in] vl If range == rangev::values, the lower bound of the interval
+/// to be searched for eigenvalues
+/// \param [in] vu If range == rangev::values, the upper bound of the interval
+/// to be searched for eigenvalues
+/// \param [in] il If range == rangev::indices, the indices of the smallest
+/// eigenvalue to be returned.
+/// \param [in] iu If range == rangev::indices, the indices of the largest
+/// eigenvalue to be returned.
+/// \param [in] w_type The data type of the eigenvalues.
+/// \param [out] device_ws_size The device workspace size in bytes.
+/// \param [out] host_ws_size The host workspace size in bytes. Currently the
+/// value is always zero.
+inline int syheevx_scratchpad_size(sycl::queue &q, oneapi::mkl::job jobz,
+                                   oneapi::mkl::rangev range,
+                                   oneapi::mkl::uplo uplo, std::int64_t n,
+                                   library_data_t a_type, std::int64_t lda,
+                                   void *vl, void *vu, std::int64_t il,
+                                   std::int64_t iu, library_data_t w_type,
+                                   std::size_t *device_ws_size,
+                                   std::size_t *host_ws_size = nullptr) {
+  if (host_ws_size)
+    *host_ws_size = 0;
+  oneapi::mkl::compz compz_jobz = detail::job2compz(jobz);
+  std::size_t device_ws_size_tmp;
+  int ret = detail::lapack_shim<detail::syheevx_scratchpad_size_impl>(
+      q, a_type, nullptr, "syevx_scratchpad_size/heevx_scratchpad_size", q,
+      compz_jobz, range, uplo, n, lda, vl, vu, il, iu,
+      device_ws_size_tmp);
+  *device_ws_size = detail::element_number_to_byte(device_ws_size_tmp, a_type);
+  return ret;
+}
+
+/// Computes selected eigenvalues and, optionally, eigenvectors of a
+/// symmetric/Hermitian matrix.
+/// \return Returns 0 if no synchronous exception, otherwise returns 1.
+/// \param [in] q Device queue where computation will be performed. It must
+/// have the in_order property when using the USM mode (DPCT_USM_LEVEL_NONE is
+/// not defined).
+/// \param [in] jobz Must be job::novec or job::vec.
+/// \param [in] range Must be rangev::all, rangev::values or uplo::indices.
+/// \param [in] uplo Must be uplo::upper or uplo::lower.
+/// \param [in] n The order of the matrix A.
+/// \param [in] a_type The data type of the matrix A.
+/// \param [in, out] a The input matrix A. On exit, the lower or upper triangle is
+/// overwritten.
+/// \param [in] lda The leading dimension of the matrix A.
+/// \param [in] vl If range == rangev::values, the lower bound of the interval
+/// to be searched for eigenvalues
+/// \param [in] vu If range == rangev::values, the upper bound of the interval
+/// to be searched for eigenvalues
+/// \param [in] il If range == rangev::indices, the indices of the smallest
+/// eigenvalue to be returned.
+/// \param [in] iu If range == rangev::indices, the indices of the largest
+/// eigenvalue to be returned.
+/// \param [out] m The total number of eigenvalues found.
+/// \param [in] w_type The data type of the eigenvalues.
+/// \param [out] w The eigenvalues of the matrix A in ascending order.
+/// \param [in] device_ws The workspace.
+/// \param [in] device_ws_size The workspace size in bytes.
+/// \param [out] info If lapack synchronous exception is caught, the value
+/// returned from info() method of the exception is set to \p info.
+inline int syheevx(sycl::queue &q, oneapi::mkl::job jobz,
+                   oneapi::mkl::rangev range, oneapi::mkl::uplo uplo,
+                   std::int64_t n, library_data_t a_type, void *a,
+                   std::int64_t lda, void *vl, void *vu, std::int64_t il,
+                   std::int64_t iu, std::int64_t *m, library_data_t w_type,
+                   void *w, void *device_ws, std::size_t device_ws_size,
+                   int *info) {
+  std::size_t device_ws_size_in_element_number =
+      detail::byte_to_element_number(device_ws_size, a_type);
+  oneapi::mkl::compz compz_jobz = detail::job2compz(jobz);
+  int ret = detail::lapack_shim<detail::syheevx_impl>(
+      q, a_type, info, "syevx/heevx", q, compz_jobz, range, uplo, n, a_type, a,
+      lda, vl, vu, il, iu, m, w_type, w, device_ws,
+      device_ws_size_in_element_number, info);
+  q.wait();
+  return ret;
+}
+
+/// Computes the size of workspace memory of syevx/heevx function.
+/// \return Returns 0 if no synchronous exception, otherwise returns 1.
+/// \param [in] q Device queue where computation will be performed. It must
+/// have the in_order property when using the USM mode (DPCT_USM_LEVEL_NONE is
+/// not defined).
+/// \param [in] jobz Must be job::novec or job::vec.
+/// \param [in] range Must be rangev::all, rangev::values or uplo::indices.
+/// \param [in] uplo Must be uplo::upper or uplo::lower.
+/// \param [in] n The order of the matrix A.
+/// \param [in] lda The leading dimension of the matrix A.
+/// \param [in] vl If range == rangev::values, the lower bound of the interval
+/// to be searched for eigenvalues
+/// \param [in] vu If range == rangev::values, the upper bound of the interval
+/// to be searched for eigenvalues
+/// \param [in] il If range == rangev::indices, the indices of the smallest
+/// eigenvalue to be returned.
+/// \param [in] iu If range == rangev::indices, the indices of the largest
+/// eigenvalue to be returned.
+/// \param [out] device_ws_size The device workspace size as a number of
+/// elements of type \tparam T.
+template <typename T, typename ValueT>
+inline int syheevx_scratchpad_size(sycl::queue &q, oneapi::mkl::job jobz,
+                                   oneapi::mkl::rangev range,
+                                   oneapi::mkl::uplo uplo, int n, int lda,
+                                   ValueT vl, ValueT vu, int il, int iu,
+                                   int *device_ws_size) {
+  oneapi::mkl::compz compz_jobz = detail::job2compz(jobz);
+  std::size_t device_ws_size_tmp;
+  int ret = detail::lapack_shim<detail::syheevx_scratchpad_size_impl>(
+      q, detail::get_library_data_t_from_type<T>(), nullptr,
+      "syevx_scratchpad_size/heevx_scratchpad_size", q, compz_jobz, range, uplo,
+      n, lda, &vl, &vu, il, iu, device_ws_size_tmp);
+  *device_ws_size = (int)device_ws_size_tmp;
+  return ret;
+}
+
+/// Computes selected eigenvalues and, optionally, eigenvectors of a
+/// symmetric/Hermitian matrix.
+/// \return Returns 0 if no synchronous exception, otherwise returns 1.
+/// \param [in] q Device queue where computation will be performed. It must
+/// have the in_order property when using the USM mode (DPCT_USM_LEVEL_NONE is
+/// not defined).
+/// \param [in] jobz Must be job::novec or job::vec.
+/// \param [in] range Must be rangev::all, rangev::values or uplo::indices.
+/// \param [in] uplo Must be uplo::upper or uplo::lower.
+/// \param [in] n The order of the matrix A.
+/// \param [in, out] a The input matrix A. On exit, the lower or upper triangle is
+/// overwritten.
+/// \param [in] lda The leading dimension of the matrix A.
+/// \param [in] vl If range == rangev::values, the lower bound of the interval
+/// to be searched for eigenvalues
+/// \param [in] vu If range == rangev::values, the upper bound of the interval
+/// to be searched for eigenvalues
+/// \param [in] il If range == rangev::indices, the indices of the smallest
+/// eigenvalue to be returned.
+/// \param [in] iu If range == rangev::indices, the indices of the largest
+/// eigenvalue to be returned.
+/// \param [out] m The total number of eigenvalues found.
+/// \param [out] w The eigenvalues of the matrix A in ascending order.
+/// \param [in] device_ws The workspace.
+/// \param [in] device_ws_size The device workspace size as a number of
+/// elements of type \tparam T.
+/// \param [out] info If lapack synchronous exception is caught, the value
+/// returned from info() method of the exception is set to \p info.
+template <typename T, typename ValueT>
+inline int syheevx(sycl::queue &q, oneapi::mkl::job jobz,
+                   oneapi::mkl::rangev range, oneapi::mkl::uplo uplo, int n,
+                   T *a, int lda, ValueT vl, ValueT vu, int il, int iu, int *m,
+                   ValueT *w, T *device_ws, int device_ws_size, int *info) {
+  oneapi::mkl::compz compz_jobz = detail::job2compz(jobz);
+  std::int64_t m64;
+  int ret = detail::lapack_shim<detail::syheevx_impl>(
+      q, detail::get_library_data_t_from_type<T>(), info, "syevx/heevx", q,
+      compz_jobz, range, uplo, n, detail::get_library_data_t_from_type<T>(), a,
+      lda, &vl, &vu, il, iu, &m64,
+      detail::get_library_data_t_from_type<ValueT>(), w, device_ws,
+      device_ws_size, info);
+  q.wait();
+  *m = (int)m64;
+  return ret;
+}
+
+/// Computes the size of workspace memory of sygvx/hegvx function.
+/// \return Returns 0 if no synchronous exception, otherwise returns 1.
+/// \param [in] q Device queue where computation will be performed. It must
+/// have the in_order property when using the USM mode (DPCT_USM_LEVEL_NONE is
+/// not defined).
+/// \param [in] itype Must be 1, 2 or 3.
+/// \param [in] jobz Must be job::novec or job::vec.
+/// \param [in] range Must be rangev::all, rangev::values or uplo::indices.
+/// \param [in] uplo Must be uplo::upper or uplo::lower.
+/// \param [in] n The order of the matrix A.
+/// \param [in] lda The leading dimension of the matrix A.
+/// \param [in] ldb The leading dimension of the matrix B.
+/// \param [in] vl If range == rangev::values, the lower bound of the interval
+/// to be searched for eigenvalues
+/// \param [in] vu If range == rangev::values, the upper bound of the interval
+/// to be searched for eigenvalues
+/// \param [in] il If range == rangev::indices, the indices of the smallest
+/// eigenvalue to be returned.
+/// \param [in] iu If range == rangev::indices, the indices of the largest
+/// eigenvalue to be returned.
+/// \param [in] device_ws_size The device workspace size as a number of
+/// elements of type \tparam T.
+template <typename T, typename ValueT>
+inline int
+syhegvx_scratchpad_size(sycl::queue &q, int itype, oneapi::mkl::job jobz,
+                        oneapi::mkl::rangev range, oneapi::mkl::uplo uplo,
+                        int n, int lda, int ldb, ValueT vl, ValueT vu, int il,
+                        int iu, int *device_ws_size) {
+  oneapi::mkl::compz compz_jobz = detail::job2compz(jobz);
+  std::size_t device_ws_size_tmp;
+  int ret = detail::lapack_shim<detail::syhegvx_scratchpad_size_impl>(
+      q, detail::get_library_data_t_from_type<T>(), nullptr,
+      "sygvx_scratchpad_size/hegvx_scratchpad_size", q, itype, compz_jobz,
+      range, uplo, n, lda, ldb, &vl, &vu, il, iu, device_ws_size_tmp);
+  *device_ws_size = (int)device_ws_size_tmp;
+  return ret;
+}
+
+/// Computes selected eigenvalues and, optionally, eigenvectors of a real
+/// generalized symmetric/Hermitian definite eigenproblem.
+/// \return Returns 0 if no synchronous exception, otherwise returns 1.
+/// \param [in] q Device queue where computation will be performed. It must
+/// have the in_order property when using the USM mode (DPCT_USM_LEVEL_NONE is
+/// not defined).
+/// \param [in] itype Must be 1, 2 or 3.
+/// \param [in] jobz Must be job::novec or job::vec.
+/// \param [in] range Must be rangev::all, rangev::values or uplo::indices.
+/// \param [in] uplo Must be uplo::upper or uplo::lower.
+/// \param [in] n The order of the matrix A.
+/// \param [in, out] a The input matrix A. On exit, the lower or upper triangle is
+/// overwritten.
+/// \param [in] lda The leading dimension of the matrix A.
+/// \param [in, out] b The input matrix B.
+/// \param [in] ldb The leading dimension of the matrix B.
+/// \param [in] vl If range == rangev::values, the lower bound of the interval
+/// to be searched for eigenvalues
+/// \param [in] vu If range == rangev::values, the upper bound of the interval
+/// to be searched for eigenvalues
+/// \param [in] il If range == rangev::indices, the indices of the smallest
+/// eigenvalue to be returned.
+/// \param [in] iu If range == rangev::indices, the indices of the largest
+/// eigenvalue to be returned.
+/// \param [out] m The total number of eigenvalues found.
+/// \param [out] w The eigenvalues of the matrix A in ascending order.
+/// \param [in] device_ws The workspace.
+/// \param [in] device_ws_size The device workspace size as a number of
+/// elements of type \tparam T.
+/// \param [out] info If lapack synchronous exception is caught, the value
+/// returned from info() method of the exception is set to \p info.
+template <typename T, typename ValueT>
+inline int syhegvx(sycl::queue &q, int itype, oneapi::mkl::job jobz,
+                   oneapi::mkl::rangev range, oneapi::mkl::uplo uplo, int n,
+                   T *a, int lda, T *b, int ldb, ValueT vl, ValueT vu, int il,
+                   int iu, int *m, ValueT *w, T *device_ws, int device_ws_size,
+                   int *info) {
+  oneapi::mkl::compz compz_jobz = detail::job2compz(jobz);
+  std::int64_t m64;
+  int ret = detail::lapack_shim<detail::syhegvx_impl>(
+      q, detail::get_library_data_t_from_type<T>(), info, "sygvx/hegvx", q,
+      itype, compz_jobz, range, uplo, n, a, lda, b, ldb, &vl, &vu, il, iu, &m64,
+      w, device_ws, device_ws_size, info);
+  q.wait();
+  *m = (int)m64;
+  return ret;
+}
+
+/// Computes the size of workspace memory of sygvd/hegvd function.
+/// \return Returns 0 if no synchronous exception, otherwise returns 1.
+/// \param [in] q Device queue where computation will be performed. It must
+/// have the in_order property when using the USM mode (DPCT_USM_LEVEL_NONE is
+/// not defined).
+/// \param [in] itype Must be 1, 2 or 3.
+/// \param [in] jobz Must be job::novec or job::vec.
+/// \param [in] uplo Must be uplo::upper or uplo::lower.
+/// \param [in] n The order of the matrix A.
+/// \param [in] lda The leading dimension of the matrix A.
+/// \param [in] ldb The leading dimension of the matrix B.
+/// \param [in] device_ws_size The device workspace size as a number of
+/// elements of type \tparam T.
+template <typename T>
+inline int syhegvd_scratchpad_size(sycl::queue &q, int itype,
+                                   oneapi::mkl::job jobz,
+                                   oneapi::mkl::uplo uplo, int n, int lda,
+                                   int ldb, int *device_ws_size) {
+  std::size_t device_ws_size_tmp;
+  int ret = detail::lapack_shim<detail::syhegvd_scratchpad_size_impl>(
+      q, detail::get_library_data_t_from_type<T>(), nullptr,
+      "sygvd_scratchpad_size/hegvd_scratchpad_size", q, itype, jobz, uplo, n,
+      lda, ldb, device_ws_size_tmp);
+  *device_ws_size = (int)device_ws_size_tmp;
+  return ret;
+}
+
+/// Computes all eigenvalues and, optionally, eigenvectors of a real generalized
+/// symmetric/Hermitian definite eigenproblem using a divide and conquer method.
+/// \return Returns 0 if no synchronous exception, otherwise returns 1.
+/// \param [in] q Device queue where computation will be performed. It must
+/// have the in_order property when using the USM mode (DPCT_USM_LEVEL_NONE is
+/// not defined).
+/// \param [in] itype Must be 1, 2 or 3.
+/// \param [in] jobz Must be job::novec or job::vec.
+/// \param [in] uplo Must be uplo::upper or uplo::lower.
+/// \param [in] n The order of the matrix A.
+/// \param [in, out] a The input matrix A. On exit, it is overwritten by eigenvectors.
+/// \param [in] lda The leading dimension of the matrix A.
+/// \param [in, out] b The input matrix B.
+/// \param [in] ldb The leading dimension of the matrix B.
+/// \param [out] w The eigenvalues of the matrix A in ascending order.
+/// \param [in] device_ws The workspace.
+/// \param [in] device_ws_size The device workspace size as a number of
+/// elements of type \tparam T.
+/// \param [out] info If lapack synchronous exception is caught, the value
+/// returned from info() method of the exception is set to \p info.
+template <typename T, typename ValueT>
+inline int syhegvd(sycl::queue &q, int itype, oneapi::mkl::job jobz,
+                   oneapi::mkl::uplo uplo, int n, T *a, int lda, T *b, int ldb,
+                   ValueT *w, T *device_ws, int device_ws_size, int *info) {
+  return detail::lapack_shim<detail::syhegvd_impl>(
+      q, detail::get_library_data_t_from_type<T>(), info, "sygvd/hegvd", q,
+      itype, jobz, uplo, n, a, lda, b, ldb, w, device_ws, device_ws_size, info);
+}
+
+/// Computes the size of workspace memory of syev/heev function.
+/// \return Returns 0 if no synchronous exception, otherwise returns 1.
+/// \param [in] q Device queue where computation will be performed. It must
+/// have the in_order property when using the USM mode (DPCT_USM_LEVEL_NONE is
+/// not defined).
+/// \param [in] jobz Must be job::novec or job::vec.
+/// \param [in] uplo Must be uplo::upper or uplo::lower.
+/// \param [in] n The order of the matrix A.
+/// \param [in] lda The leading dimension of the matrix A.
+/// \param [out] device_ws_size The device workspace size as a number of
+/// elements of type \tparam T.
+template <typename T>
+inline int syheev_scratchpad_size(sycl::queue &q, oneapi::mkl::job jobz,
+                                  oneapi::mkl::uplo uplo, int n, int lda,
+                                  int *device_ws_size) {
+  std::size_t device_ws_size_tmp;
+  oneapi::mkl::compz compz_jobz = detail::job2compz(jobz);
+  int ret = detail::lapack_shim<detail::syheev_scratchpad_size_impl>(
+      q, detail::get_library_data_t_from_type<T>(), nullptr,
+      "syev_scratchpad_size/heev_scratchpad_size", q, compz_jobz, uplo, n, lda,
+      device_ws_size_tmp);
+  *device_ws_size = (int)device_ws_size_tmp;
+  return ret;
+}
+
+/// Computes all eigenvalues and, optionally, eigenvectors of a real symmetric
+/// or Hermitian matrix.
+/// \return Returns 0 if no synchronous exception, otherwise returns 1.
+/// \param [in] q Device queue where computation will be performed. It must
+/// have the in_order property when using the USM mode (DPCT_USM_LEVEL_NONE is
+/// not defined).
+/// \param [in] jobz Must be job::novec or job::vec.
+/// \param [in] uplo Must be uplo::upper or uplo::lower.
+/// \param [in] n The order of the matrix A.
+/// \param [in, out] a The input matrix A. On exit, it is overwritten by
+/// eigenvectors.
+/// \param [in] lda The leading dimension of the matrix A.
+/// \param [out] w The eigenvalues of the matrix A in ascending order.
+/// \param [in] device_ws The workspace.
+/// \param [in] device_ws_size The device workspace size as a number of
+/// elements of type \tparam T.
+/// \param [out] info If lapack synchronous exception is caught, the value
+/// returned from info() method of the exception is set to \p info.
+template <typename T, typename ValueT>
+inline int syheev(sycl::queue &q, oneapi::mkl::job jobz, oneapi::mkl::uplo uplo,
+                  int n, T *a, int lda, ValueT *w, T *device_ws,
+                  int device_ws_size, int *info) {
+  oneapi::mkl::compz compz_jobz = detail::job2compz(jobz);
+  return detail::lapack_shim<detail::syheev_impl>(
+      q, detail::get_library_data_t_from_type<T>(), info, "syev/heev", q,
+      compz_jobz, uplo, n, a, lda, w, device_ws, device_ws_size, info);
+}
+
+/// Computes the size of workspace memory of syevd/heevd function.
+/// \return Returns 0 if no synchronous exception, otherwise returns 1.
+/// \param [in] q Device queue where computation will be performed. It must
+/// have the in_order property when using the USM mode (DPCT_USM_LEVEL_NONE is
+/// not defined).
+/// \param [in] jobz Must be job::novec or job::vec.
+/// \param [in] uplo Must be uplo::upper or uplo::lower.
+/// \param [in] n The order of the matrix A.
+/// \param [in] a_type The data type of the matrix A.
+/// \param [in] lda The leading dimension of the matrix A.
+/// \param [in] w_type The data type of the eigenvalues.
+/// \param [out] device_ws_size The device workspace in bytes.
+/// \param [out] host_ws_size The host workspace size in bytes. Currently the
+/// value is always zero.
+inline int syheevd_scratchpad_size(sycl::queue &q, oneapi::mkl::job jobz,
+                                   oneapi::mkl::uplo uplo, std::int64_t n,
+                                   library_data_t a_type, std::int64_t lda,
+                                   library_data_t w_type,
+                                   std::size_t *device_ws_size,
+                                   std::size_t *host_ws_size = nullptr) {
+  if (host_ws_size)
+    *host_ws_size = 0;
+  std::size_t device_ws_size_tmp;
+  int ret = detail::lapack_shim<detail::syheevd_scratchpad_size_impl>(
+      q, a_type, nullptr, "syevd_scratchpad_size/heevd_scratchpad_size", q,
+      jobz, uplo, n, a_type, lda, device_ws_size_tmp);
+  *device_ws_size = detail::element_number_to_byte(device_ws_size_tmp, a_type);
+  return ret;
+}
+
+/// Computes all eigenvalues and, optionally, all eigenvectors of a real
+/// symmetric or Hermitian matrix using divide and conquer algorithm.
+/// \return Returns 0 if no synchronous exception, otherwise returns 1.
+/// \param [in] q Device queue where computation will be performed. It must
+/// have the in_order property when using the USM mode (DPCT_USM_LEVEL_NONE is
+/// not defined).
+/// \param [in] jobz Must be job::novec or job::vec.
+/// \param [in] uplo Must be uplo::upper or uplo::lower.
+/// \param [in] n The order of the matrix A.
+/// \param [in] a_type The data type of the matrix A.
+/// \param [in, out] a The input matrix A. On exit, it is overwritten by
+/// eigenvectors.
+/// \param [in] lda The leading dimension of the matrix A.
+/// \param [in] w_type The data type of the eigenvalues.
+/// \param [out] w The eigenvalues of the matrix A in ascending order.
+/// \param [in] device_ws The workspace.
+/// \param [in] device_ws_size The workspace size in bytes.
+/// \param [out] info If lapack synchronous exception is caught, the value
+/// returned from info() method of the exception is set to \p info.
+inline int syheevd(sycl::queue &q, oneapi::mkl::job jobz,
+                   oneapi::mkl::uplo uplo, std::int64_t n,
+                   library_data_t a_type, void *a, std::int64_t lda,
+                   library_data_t w_type, void *w, void *device_ws,
+                   std::size_t device_ws_size, int *info) {
+  std::size_t device_ws_size_in_element_number =
+      detail::byte_to_element_number(device_ws_size, a_type);
+  return detail::lapack_shim<detail::syheevd_impl>(
+      q, a_type, info, "syevd/heevd", q, jobz, uplo, n, a_type, a, lda, w,
+      device_ws, device_ws_size_in_element_number, info);
+}
+
+/// Computes the size of workspace memory of syevd/heevd function.
+/// \return Returns 0 if no synchronous exception, otherwise returns 1.
+/// \param [in] q Device queue where computation will be performed. It must
+/// have the in_order property when using the USM mode (DPCT_USM_LEVEL_NONE is
+/// not defined).
+/// \param [in] jobz Must be job::novec or job::vec.
+/// \param [in] uplo Must be uplo::upper or uplo::lower.
+/// \param [in] n The order of the matrix A.
+/// \param [in] a_type The data type of the matrix A.
+/// \param [in] lda The leading dimension of the matrix A.
+/// \param [in] w_type The data type of the eigenvalues.
+/// \param [out] device_ws_size The device workspace size as a number of
+/// elements of type \tparam T.
+template <typename T>
+inline int syheevd_scratchpad_size(sycl::queue &q, oneapi::mkl::job jobz,
+                                   oneapi::mkl::uplo uplo, std::int64_t n,
+                                   std::int64_t lda, int *device_ws_size) {
+  std::size_t device_ws_size_tmp;
+  int ret = detail::lapack_shim<detail::syheevd_scratchpad_size_impl>(
+      q, detail::get_library_data_t_from_type<T>(), nullptr,
+      "syevd_scratchpad_size/heevd_scratchpad_size", q, jobz, uplo, n,
+      detail::get_library_data_t_from_type<T>(), lda, device_ws_size_tmp);
+  *device_ws_size = (int)device_ws_size_tmp;
+  return ret;
+}
+
+/// Computes all eigenvalues and, optionally, all eigenvectors of a real
+/// symmetric or Hermitian matrix using divide and conquer algorithm.
+/// \return Returns 0 if no synchronous exception, otherwise returns 1.
+/// \param [in] q Device queue where computation will be performed. It must
+/// have the in_order property when using the USM mode (DPCT_USM_LEVEL_NONE is
+/// not defined).
+/// \param [in] jobz Must be job::novec or job::vec.
+/// \param [in] uplo Must be uplo::upper or uplo::lower.
+/// \param [in] n The order of the matrix A.
+/// \param [in] a_type The data type of the matrix A.
+/// \param [in, out] a The input matrix A. On exit, it is overwritten by
+/// eigenvectors.
+/// \param [in] lda The leading dimension of the matrix A.
+/// \param [in] w_type The data type of the eigenvalues.
+/// \param [out] w The eigenvalues of the matrix A in ascending order.
+/// \param [in] device_ws The workspace.
+/// \param [in] device_ws_size The workspace size as a number of
+/// elements of type \tparam T.
+/// \param [out] info If lapack synchronous exception is caught, the value
+/// returned from info() method of the exception is set to \p info.
+template <typename T, typename ValueT>
+inline int syheevd(sycl::queue &q, oneapi::mkl::job jobz,
+                   oneapi::mkl::uplo uplo, std::int64_t n, T *a,
+                   std::int64_t lda, ValueT *w, T *device_ws,
+                   int device_ws_size, int *info) {
+  return detail::lapack_shim<detail::syheevd_impl>(
+      q, detail::get_library_data_t_from_type<T>(), info, "syevd/heevd", q,
+      jobz, uplo, n, detail::get_library_data_t_from_type<T>(), a, lda, w,
+      device_ws, device_ws_size, info);
+}
+
+/// Computes the size of workspace memory of trtri function.
+/// \return Returns 0 if no synchronous exception, otherwise returns 1.
+/// \param [in] q Device queue where computation will be performed. It must
+/// have the in_order property when using the USM mode (DPCT_USM_LEVEL_NONE is
+/// not defined).
+/// \param [in] uplo Must be uplo::upper or uplo::lower.
+/// \param [in] diag Must be diag::nonunit or diag::unit.
+/// \param [in] n The order of the matrix A.
+/// \param [in] a_type The data type of the matrix A.
+/// \param [in] lda The leading dimension of the matrix A.
+/// \param [out] device_ws_size The device workspace in bytes.
+/// \param [out] host_ws_size The host workspace size in bytes. Currently the
+/// value is always zero.
+inline int trtri_scratchpad_size(sycl::queue &q, oneapi::mkl::uplo uplo,
+                                 oneapi::mkl::diag diag, std::int64_t n,
+                                 library_data_t a_type, std::int64_t lda,
+                                 std::size_t *device_ws_size,
+                                 std::size_t *host_ws_size = nullptr) {
+  if (host_ws_size)
+    *host_ws_size = 0;
+  std::size_t device_ws_size_tmp;
+  int ret = detail::lapack_shim<detail::trtri_scratchpad_size_impl>(
+      q, a_type, nullptr, "trtri_scratchpad_size", q, uplo, diag, n, a_type,
+      lda, device_ws_size_tmp);
+  *device_ws_size = detail::element_number_to_byte(device_ws_size_tmp, a_type);
+  return ret;
+}
+
+/// Computes the inverse of a triangular matrix.
+/// \return Returns 0 if no synchronous exception, otherwise returns 1.
+/// \param [in] q Device queue where computation will be performed. It must
+/// have the in_order property when using the USM mode (DPCT_USM_LEVEL_NONE is
+/// not defined).
+/// \param [in] uplo Must be uplo::upper or uplo::lower.
+/// \param [in] diag Must be diag::nonunit or diag::unit.
+/// \param [in] n The order of the matrix A.
+/// \param [in] a_type The data type of the matrix A.
+/// \param [in, out] a The input matrix A. On exit, it is overwritten by
+/// the inverse matrix of A.
+/// \param [in] lda The leading dimension of the matrix A.
+/// \param [in] device_ws The workspace.
+/// \param [in] device_ws_size The workspace size in bytes.
+/// \param [out] info If lapack synchronous exception is caught, the value
+/// returned from info() method of the exception is set to \p info.
+inline int trtri(sycl::queue &q, oneapi::mkl::uplo uplo, oneapi::mkl::diag diag,
+                 std::int64_t n, library_data_t a_type, void *a,
+                 std::int64_t lda, void *device_ws, std::size_t device_ws_size,
+                 int *info) {
+#ifdef DPCT_USM_LEVEL_NONE
+  throw std::runtime_error("this API is unsupported when USM level is none");
+#else
+  std::size_t device_ws_size_in_element_number =
+      detail::byte_to_element_number(device_ws_size, a_type);
+  return detail::lapack_shim<detail::trtri_impl>(
+      q, a_type, info, "trtri", q, uplo, diag, n, a_type, a, lda, device_ws,
+      device_ws_size_in_element_number, info);
+#endif
+}
+} // namespace lapack
+} // namespace dpct
+
+#endif // __DPCT_LAPACK_UTILS_HPP__
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/lib_common_utils.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/lib_common_utils.hpp
new file mode 100644
index 0000000..c96a65f
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/lib_common_utils.hpp
@@ -0,0 +1,159 @@
+//==---- lib_common_utils.hpp ---------------------*- C++ -*----------------==//
+//
+// Copyright (C) Intel Corporation
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+// See https://llvm.org/LICENSE.txt for license information.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef __DPCT_LIB_COMMON_UTILS_HPP__
+#define __DPCT_LIB_COMMON_UTILS_HPP__
+
+#include <sycl/sycl.hpp>
+#include <oneapi/mkl.hpp>
+#include "memory.hpp"
+#include "util.hpp"
+
+namespace dpct {
+namespace detail {
+template <typename T> inline auto get_memory(const void *x) {
+  T *new_x = reinterpret_cast<T *>(const_cast<void *>(x));
+#ifdef DPCT_USM_LEVEL_NONE
+  return dpct::get_buffer<std::remove_cv_t<T>>(new_x);
+#else
+  return new_x;
+#endif
+}
+
+template <typename T>
+inline typename DataType<T>::T2 get_value(const T *s, sycl::queue &q) {
+  using Ty = typename DataType<T>::T2;
+  Ty s_h;
+  if (get_pointer_attribute(q, s) == pointer_access_attribute::device_only)
+    detail::dpct_memcpy(q, (void *)&s_h, (void *)s, sizeof(T), device_to_host)
+        .wait();
+  else
+    s_h = *reinterpret_cast<const Ty *>(s);
+  return s_h;
+}
+} // namespace detail
+
+enum class version_field : int { major, minor, update, patch };
+
+/// Returns the requested field of Intel(R) oneAPI Math Kernel Library version.
+/// \param field The version information field (major, minor, update or patch).
+/// \param result The result value.
+inline void mkl_get_version(version_field field, int *result) {
+#ifndef __INTEL_MKL__
+  throw std::runtime_error("The oneAPI Math Kernel Library (oneMKL) Interfaces "
+                           "Project does not support this API.");
+#else
+  MKLVersion version;
+  mkl_get_version(&version);
+  if (version_field::major == field) {
+    *result = version.MajorVersion;
+  } else if (version_field::minor == field) {
+    *result = version.MinorVersion;
+  } else if (version_field::update == field) {
+    *result = version.UpdateVersion;
+  } else if (version_field::patch == field) {
+    *result = 0;
+  } else {
+    throw std::runtime_error("unknown field");
+  }
+#endif
+}
+
+enum class library_data_t : unsigned char {
+  real_float = 0,
+  complex_float,
+  real_double,
+  complex_double,
+  real_half,
+  complex_half,
+  real_bfloat16,
+  complex_bfloat16,
+  real_int4,
+  complex_int4,
+  real_uint4,
+  complex_uint4,
+  real_int8,
+  complex_int8,
+  real_uint8,
+  complex_uint8,
+  real_int16,
+  complex_int16,
+  real_uint16,
+  complex_uint16,
+  real_int32,
+  complex_int32,
+  real_uint32,
+  complex_uint32,
+  real_int64,
+  complex_int64,
+  real_uint64,
+  complex_uint64,
+  real_int8_4,
+  real_int8_32,
+  real_uint8_4,
+  library_data_t_size
+};
+
+namespace detail {
+template <typename ArgT>
+inline constexpr std::uint64_t get_type_combination_id(ArgT Val) {
+  static_assert((unsigned char)library_data_t::library_data_t_size <=
+                    std::numeric_limits<unsigned char>::max() &&
+                "library_data_t size exceeds limit.");
+  static_assert(std::is_same_v<ArgT, library_data_t>, "Unsupported ArgT");
+  return (std::uint64_t)Val;
+}
+
+template <typename FirstT, typename... RestT>
+inline constexpr std::uint64_t get_type_combination_id(FirstT FirstVal,
+                                                       RestT... RestVal) {
+  static_assert((std::uint8_t)library_data_t::library_data_t_size <=
+                    std::numeric_limits<unsigned char>::max() &&
+                "library_data_t size exceeds limit.");
+  static_assert(sizeof...(RestT) <= 8 && "Too many parameters");
+  static_assert(std::is_same_v<FirstT, library_data_t>, "Unsupported FirstT");
+  return get_type_combination_id(RestVal...) << 8 | ((std::uint64_t)FirstVal);
+}
+
+inline constexpr std::size_t library_data_size[] = {
+    8 * sizeof(float),                    // real_float
+    8 * sizeof(std::complex<float>),      // complex_float
+    8 * sizeof(double),                   // real_double
+    8 * sizeof(std::complex<double>),     // complex_double
+    8 * sizeof(sycl::half),               // real_half
+    8 * sizeof(std::complex<sycl::half>), // complex_half
+    16,                                   // real_bfloat16
+    16 * 2,                               // complex_bfloat16
+    4,                                    // real_int4
+    4 * 2,                                // complex_int4
+    4,                                    // real_uint4
+    4 * 2,                                // complex_uint4
+    8,                                    // real_int8
+    8 * 2,                                // complex_int8
+    8,                                    // real_uint8
+    8 * 2,                                // complex_uint8
+    16,                                   // real_int16
+    16 * 2,                               // complex_int16
+    16,                                   // real_uint16
+    16 * 2,                               // complex_uint16
+    32,                                   // real_int32
+    32 * 2,                               // complex_int32
+    32,                                   // real_uint32
+    32 * 2,                               // complex_uint32
+    64,                                   // real_int64
+    64 * 2,                               // complex_int64
+    64,                                   // real_uint64
+    64 * 2,                               // complex_uint64
+    8,                                    // real_int8_4
+    8,                                    // real_int8_32
+    8                                     // real_uint8_4
+};
+} // namespace detail
+} // namespace dpct
+
+#endif // __DPCT_LIB_COMMON_UTILS_HPP__
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/math.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/math.hpp
new file mode 100644
index 0000000..c569a28
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/math.hpp
@@ -0,0 +1,1011 @@
+//==---- math.hpp ---------------------------------*- C++ -*----------------==//
+//
+// Copyright (C) Intel Corporation
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+// See https://llvm.org/LICENSE.txt for license information.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef __DPCT_MATH_HPP__
+#define __DPCT_MATH_HPP__
+
+#include <limits>
+#include <sycl/sycl.hpp>
+#include <type_traits>
+
+namespace dpct {
+namespace detail {
+template <typename VecT, class BinaryOperation, class = void>
+class vectorized_binary {
+public:
+  inline VecT operator()(VecT a, VecT b, const BinaryOperation binary_op) {
+    VecT v4;
+    for (size_t i = 0; i < v4.size(); ++i) {
+      v4[i] = binary_op(a[i], b[i]);
+    }
+    return v4;
+  }
+};
+template <typename VecT, class BinaryOperation>
+class vectorized_binary<
+    VecT, BinaryOperation,
+    std::void_t<std::invoke_result_t<BinaryOperation, VecT, VecT>>> {
+public:
+  inline VecT operator()(VecT a, VecT b, const BinaryOperation binary_op) {
+    return binary_op(a, b).template as<VecT>();
+  }
+};
+
+template <typename T> inline bool isnan(const T a) { return sycl::isnan(a); }
+#ifdef SYCL_EXT_ONEAPI_BFLOAT16_MATH_FUNCTIONS
+inline bool isnan(const sycl::ext::oneapi::bfloat16 a) {
+  return sycl::ext::oneapi::experimental::isnan(a);
+}
+#endif
+} // namespace detail
+
+/// Compute fast_length for variable-length array
+/// \param [in] a The array
+/// \param [in] len Length of the array
+/// \returns The computed fast_length
+inline float fast_length(const float *a, int len) {
+  switch (len) {
+  case 1:
+    return a[0];
+  case 2:
+    return sycl::fast_length(sycl::float2(a[0], a[1]));
+  case 3:
+    return sycl::fast_length(sycl::float3(a[0], a[1], a[2]));
+  case 4:
+    return sycl::fast_length(sycl::float4(a[0], a[1], a[2], a[3]));
+  case 0:
+    return 0;
+  default:
+    float f = 0;
+    for (int i = 0; i < len; ++i)
+      f += a[i] * a[i];
+    return sycl::sqrt(f);
+  }
+}
+
+/// Calculate the square root of the input array.
+/// \param [in] a The array pointer
+/// \param [in] len Length of the array
+/// \returns The square root
+template <typename T> inline T length(const T *a, const int len) {
+  switch (len) {
+  case 1:
+    return a[0];
+  case 2:
+    return sycl::length(sycl::vec<T, 2>(a[0], a[1]));
+  case 3:
+    return sycl::length(sycl::vec<T, 3>(a[0], a[1], a[2]));
+  case 4:
+    return sycl::length(sycl::vec<T, 4>(a[0], a[1], a[2], a[3]));
+  default:
+    T ret = 0;
+    for (int i = 0; i < len; ++i)
+      ret += a[i] * a[i];
+    return sycl::sqrt(ret);
+  }
+}
+
+/// Returns min(max(val, min_val), max_val)
+/// \param [in] val The input value
+/// \param [in] min_val The minimum value
+/// \param [in] max_val The maximum value
+/// \returns the value between min_val and max_val
+template <typename T> inline T clamp(T val, T min_val, T max_val) {
+  return sycl::clamp(val, min_val, max_val);
+}
+#ifdef SYCL_EXT_ONEAPI_BFLOAT16_MATH_FUNCTIONS
+template <>
+inline sycl::ext::oneapi::bfloat16 clamp(sycl::ext::oneapi::bfloat16 val,
+                                         sycl::ext::oneapi::bfloat16 min_val,
+                                         sycl::ext::oneapi::bfloat16 max_val) {
+  if (val < min_val)
+    return min_val;
+  if (val > max_val)
+    return max_val;
+  return val;
+}
+#endif
+template <typename T>
+inline sycl::marray<T, 2> clamp(sycl::marray<T, 2> val,
+                                sycl::marray<T, 2> min_val,
+                                sycl::marray<T, 2> max_val) {
+  return {clamp(val[0], min_val[0], max_val[0]),
+          clamp(val[1], min_val[1], max_val[1])};
+}
+
+/// Performs comparison.
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \param [in] binary_op functor that implements the binary operation
+/// \returns the comparison result
+template <typename T, class BinaryOperation>
+inline std::enable_if_t<
+    std::is_same_v<std::invoke_result_t<BinaryOperation, T, T>, bool>, bool>
+compare(const T a, const T b, const BinaryOperation binary_op) {
+  return binary_op(a, b);
+}
+template <typename T>
+inline std::enable_if_t<
+    std::is_same_v<std::invoke_result_t<std::not_equal_to<>, T, T>, bool>, bool>
+compare(const T a, const T b, const std::not_equal_to<> binary_op) {
+  return !detail::isnan(a) && !detail::isnan(b) && binary_op(a, b);
+}
+
+/// Performs unordered comparison.
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \param [in] binary_op functor that implements the binary operation
+/// \returns the comparison result
+template <typename T, class BinaryOperation>
+inline std::enable_if_t<
+    std::is_same_v<std::invoke_result_t<BinaryOperation, T, T>, bool>, bool>
+unordered_compare(const T a, const T b, const BinaryOperation binary_op) {
+  return detail::isnan(a) || detail::isnan(b) || binary_op(a, b);
+}
+
+/// Performs 2 element comparison and return true if both results are true.
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \param [in] binary_op functor that implements the binary operation
+/// \returns the comparison result
+template <typename T, class BinaryOperation>
+inline std::enable_if_t<T::size() == 2, bool>
+compare_both(const T a, const T b, const BinaryOperation binary_op) {
+  return compare(a[0], b[0], binary_op) && compare(a[1], b[1], binary_op);
+}
+
+/// Performs 2 element unordered comparison and return true if both results are
+/// true.
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \param [in] binary_op functor that implements the binary operation
+/// \returns the comparison result
+template <typename T, class BinaryOperation>
+inline std::enable_if_t<T::size() == 2, bool>
+unordered_compare_both(const T a, const T b, const BinaryOperation binary_op) {
+  return unordered_compare(a[0], b[0], binary_op) &&
+         unordered_compare(a[1], b[1], binary_op);
+}
+
+/// Performs 2 element comparison.
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \param [in] binary_op functor that implements the binary operation
+/// \returns the comparison result
+template <typename T, class BinaryOperation>
+inline std::enable_if_t<T::size() == 2, T>
+compare(const T a, const T b, const BinaryOperation binary_op) {
+  return {compare(a[0], b[0], binary_op), compare(a[1], b[1], binary_op)};
+}
+
+/// Performs 2 elements comparison, compare result of each element is 0 (false)
+/// or 0xffff (true), returns an unsigned int by composing compare result of two
+/// elements.
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \param [in] binary_op functor that implements the binary operation
+/// \returns the comparison result
+template <typename T, class BinaryOperation>
+inline unsigned compare_mask(const sycl::vec<T, 2> a, const sycl::vec<T, 2> b,
+                             const BinaryOperation binary_op) {
+  return sycl::vec<short, 2>(-compare(a[0], b[0], binary_op),
+                             -compare(a[1], b[1], binary_op))
+      .as<sycl::vec<unsigned, 1>>();
+}
+template <typename T, class BinaryOperation>
+inline unsigned compare_mask(const sycl::marray<T, 2> a,
+                             const sycl::marray<T, 2> b,
+                             const BinaryOperation binary_op) {
+  return sycl::vec<short, 2>(-compare(a[0], b[0], binary_op),
+                             -compare(a[1], b[1], binary_op))
+      .as<sycl::vec<unsigned, 1>>();
+}
+
+/// Performs 2 element unordered comparison.
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \param [in] binary_op functor that implements the binary operation
+/// \returns the comparison result
+template <typename T, class BinaryOperation>
+inline std::enable_if_t<T::size() == 2, T>
+unordered_compare(const T a, const T b, const BinaryOperation binary_op) {
+  return {unordered_compare(a[0], b[0], binary_op),
+          unordered_compare(a[1], b[1], binary_op)};
+}
+
+/// Performs 2 elements unordered comparison, compare result of each element is
+/// 0 (false) or 0xffff (true), returns an unsigned int by composing compare
+/// result of two elements.
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \param [in] binary_op functor that implements the binary operation
+/// \returns the comparison result
+template <typename T, class BinaryOperation>
+inline unsigned unordered_compare_mask(const sycl::vec<T, 2> a,
+                                       const sycl::vec<T, 2> b,
+                                       const BinaryOperation binary_op) {
+  return sycl::vec<short, 2>(-unordered_compare(a[0], b[0], binary_op),
+                             -unordered_compare(a[1], b[1], binary_op))
+      .as<sycl::vec<unsigned, 1>>();
+}
+template <typename T, class BinaryOperation>
+inline unsigned unordered_compare_mask(const sycl::marray<T, 2> a,
+                                       const sycl::marray<T, 2> b,
+                                       const BinaryOperation binary_op) {
+  return sycl::vec<short, 2>(-unordered_compare(a[0], b[0], binary_op),
+                             -unordered_compare(a[1], b[1], binary_op))
+      .as<sycl::vec<unsigned, 1>>();
+}
+
+/// Determine whether 2 element value is NaN.
+/// \param [in] a The input value
+/// \returns the comparison result
+template <typename T>
+inline std::enable_if_t<T::size() == 2, T> isnan(const T a) {
+  return {detail::isnan(a[0]), detail::isnan(a[1])};
+}
+
+/// Emulated function for __funnelshift_l
+inline unsigned int funnelshift_l(unsigned int low, unsigned int high,
+                                  unsigned int shift) {
+  return (sycl::upsample(high, low) << (shift & 31U)) >> 32;
+}
+
+/// Emulated function for __funnelshift_lc
+inline unsigned int funnelshift_lc(unsigned int low, unsigned int high,
+                                   unsigned int shift) {
+  return (sycl::upsample(high, low) << sycl::min(shift, 32U)) >> 32;
+}
+
+/// Emulated function for __funnelshift_r
+inline unsigned int funnelshift_r(unsigned int low, unsigned int high,
+                                  unsigned int shift) {
+  return (sycl::upsample(high, low) >> (shift & 31U)) & 0xFFFFFFFF;
+}
+
+/// Emulated function for __funnelshift_rc
+inline unsigned int funnelshift_rc(unsigned int low, unsigned int high,
+                                   unsigned int shift) {
+  return (sycl::upsample(high, low) >> sycl::min(shift, 32U)) & 0xFFFFFFFF;
+}
+
+/// cbrt function wrapper.
+template <typename T> inline T cbrt(T val) { return sycl::cbrt((T)val); }
+
+// min function overloads.
+// For floating-point types, `float` or `double` arguments are acceptable.
+// For integer types, `std::uint32_t`, `std::int32_t`, `std::uint64_t` or
+// `std::int64_t` type arguments are acceptable.
+inline double min(const double a, const float b) {
+  return sycl::fmin(a, static_cast<double>(b));
+}
+inline double min(const float a, const double b) {
+  return sycl::fmin(static_cast<double>(a), b);
+}
+inline float min(const float a, const float b) { return sycl::fmin(a, b); }
+inline double min(const double a, const double b) { return sycl::fmin(a, b); }
+inline std::uint32_t min(const std::uint32_t a, const std::int32_t b) {
+  return sycl::min(a, static_cast<std::uint32_t>(b));
+}
+inline std::uint32_t min(const std::int32_t a, const std::uint32_t b) {
+  return sycl::min(static_cast<std::uint32_t>(a), b);
+}
+inline std::int32_t min(const std::int32_t a, const std::int32_t b) {
+  return sycl::min(a, b);
+}
+inline std::uint32_t min(const std::uint32_t a, const std::uint32_t b) {
+  return sycl::min(a, b);
+}
+inline std::uint64_t min(const std::uint64_t a, const std::int64_t b) {
+  return sycl::min(a, static_cast<std::uint64_t>(b));
+}
+inline std::uint64_t min(const std::int64_t a, const std::uint64_t b) {
+  return sycl::min(static_cast<std::uint64_t>(a), b);
+}
+inline std::int64_t min(const std::int64_t a, const std::int64_t b) {
+  return sycl::min(a, b);
+}
+inline std::uint64_t min(const std::uint64_t a, const std::uint64_t b) {
+  return sycl::min(a, b);
+}
+inline std::uint64_t min(const std::uint64_t a, const std::int32_t b) {
+  return sycl::min(a, static_cast<std::uint64_t>(b));
+}
+inline std::uint64_t min(const std::int32_t a, const std::uint64_t b) {
+  return sycl::min(static_cast<std::uint64_t>(a), b);
+}
+inline std::uint64_t min(const std::uint64_t a, const std::uint32_t b) {
+  return sycl::min(a, static_cast<std::uint64_t>(b));
+}
+inline std::uint64_t min(const std::uint32_t a, const std::uint64_t b) {
+  return sycl::min(static_cast<std::uint64_t>(a), b);
+}
+// max function overloads.
+// For floating-point types, `float` or `double` arguments are acceptable.
+// For integer types, `std::uint32_t`, `std::int32_t`, `std::uint64_t` or
+// `std::int64_t` type arguments are acceptable.
+inline double max(const double a, const float b) {
+  return sycl::fmax(a, static_cast<double>(b));
+}
+inline double max(const float a, const double b) {
+  return sycl::fmax(static_cast<double>(a), b);
+}
+inline float max(const float a, const float b) { return sycl::fmax(a, b); }
+inline double max(const double a, const double b) { return sycl::fmax(a, b); }
+inline std::uint32_t max(const std::uint32_t a, const std::int32_t b) {
+  return sycl::max(a, static_cast<std::uint32_t>(b));
+}
+inline std::uint32_t max(const std::int32_t a, const std::uint32_t b) {
+  return sycl::max(static_cast<std::uint32_t>(a), b);
+}
+inline std::int32_t max(const std::int32_t a, const std::int32_t b) {
+  return sycl::max(a, b);
+}
+inline std::uint32_t max(const std::uint32_t a, const std::uint32_t b) {
+  return sycl::max(a, b);
+}
+inline std::uint64_t max(const std::uint64_t a, const std::int64_t b) {
+  return sycl::max(a, static_cast<std::uint64_t>(b));
+}
+inline std::uint64_t max(const std::int64_t a, const std::uint64_t b) {
+  return sycl::max(static_cast<std::uint64_t>(a), b);
+}
+inline std::int64_t max(const std::int64_t a, const std::int64_t b) {
+  return sycl::max(a, b);
+}
+inline std::uint64_t max(const std::uint64_t a, const std::uint64_t b) {
+  return sycl::max(a, b);
+}
+inline std::uint64_t max(const std::uint64_t a, const std::int32_t b) {
+  return sycl::max(a, static_cast<std::uint64_t>(b));
+}
+inline std::uint64_t max(const std::int32_t a, const std::uint64_t b) {
+  return sycl::max(static_cast<std::uint64_t>(a), b);
+}
+inline std::uint64_t max(const std::uint64_t a, const std::uint32_t b) {
+  return sycl::max(a, static_cast<std::uint64_t>(b));
+}
+inline std::uint64_t max(const std::uint32_t a, const std::uint64_t b) {
+  return sycl::max(static_cast<std::uint64_t>(a), b);
+}
+
+// pow functions overload.
+inline float pow(const float a, const int b) { return sycl::pown(a, b); }
+inline double pow(const double a, const int b) { return sycl::pown(a, b); }
+inline float pow(const float a, const float b) { return sycl::pow(a, b); }
+inline double pow(const double a, const double b) { return sycl::pow(a, b); }
+template <typename T, typename U>
+inline typename std::enable_if_t<std::is_floating_point_v<T>, T>
+pow(const T a, const U b) {
+  return sycl::pow(a, static_cast<T>(b));
+}
+template <typename T, typename U>
+inline typename std::enable_if_t<!std::is_floating_point_v<T>, double>
+pow(const T a, const U b) {
+  return sycl::pow(static_cast<double>(a), static_cast<double>(b));
+}
+
+namespace detail {
+template <typename T>
+constexpr bool is_floating_point =
+    std::disjunction_v<std::is_floating_point<T>, std::is_same<T, sycl::half>
+#ifdef SYCL_EXT_ONEAPI_BFLOAT16_MATH_FUNCTIONS
+                       ,
+                       std::is_same<T, sycl::ext::oneapi::bfloat16>
+#endif
+                       >;
+} // namespace detail
+
+/// Performs relu saturation.
+/// \param [in] a The input value
+/// \returns the relu saturation result
+template <typename T> inline T relu(T a) {
+  T zero{};
+  if constexpr (detail::is_floating_point<T>)
+    return !detail::isnan(a) && a < zero ? zero : a;
+  else
+    return a < zero ? zero : a;
+}
+template <class T> inline sycl::vec<T, 2> relu(const sycl::vec<T, 2> a) {
+  return {relu(a[0]), relu(a[1])};
+}
+template <class T> inline sycl::marray<T, 2> relu(const sycl::marray<T, 2> a) {
+  return {relu(a[0]), relu(a[1])};
+}
+
+/// Performs complex number multiply addition.
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \param [in] c The third value
+/// \returns the operation result
+template <typename T>
+inline sycl::vec<T, 2> complex_mul_add(const sycl::vec<T, 2> a,
+                                       const sycl::vec<T, 2> b,
+                                       const sycl::vec<T, 2> c) {
+  return sycl::vec<T, 2>{a[0] * b[0] - a[1] * b[1] + c[0],
+                         a[0] * b[1] + a[1] * b[0] + c[1]};
+}
+template <typename T>
+inline sycl::marray<T, 2> complex_mul_add(const sycl::marray<T, 2> a,
+                                          const sycl::marray<T, 2> b,
+                                          const sycl::marray<T, 2> c) {
+  return sycl::marray<T, 2>{a[0] * b[0] - a[1] * b[1] + c[0],
+                            a[0] * b[1] + a[1] * b[0] + c[1]};
+}
+
+/// Performs 2 elements comparison and returns the bigger one. If either of
+/// inputs is NaN, then return NaN.
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \returns the bigger value
+template <typename T> inline T fmax_nan(const T a, const T b) {
+  if (detail::isnan(a) || detail::isnan(b))
+    return NAN;
+  return sycl::fmax(a, b);
+}
+#ifdef SYCL_EXT_ONEAPI_BFLOAT16_MATH_FUNCTIONS
+template <>
+inline sycl::ext::oneapi::bfloat16
+fmax_nan(const sycl::ext::oneapi::bfloat16 a,
+         const sycl::ext::oneapi::bfloat16 b) {
+  if (detail::isnan(a) || detail::isnan(b))
+    return NAN;
+  return sycl::fmax(float(a), float(b));
+}
+#endif
+template <typename T>
+inline sycl::vec<T, 2> fmax_nan(const sycl::vec<T, 2> a,
+                                const sycl::vec<T, 2> b) {
+  return {fmax_nan(a[0], b[0]), fmax_nan(a[1], b[1])};
+}
+template <typename T>
+inline sycl::marray<T, 2> fmax_nan(const sycl::marray<T, 2> a,
+                                   const sycl::marray<T, 2> b) {
+  return {fmax_nan(a[0], b[0]), fmax_nan(a[1], b[1])};
+}
+
+/// Performs 2 elements comparison and returns the smaller one. If either of
+/// inputs is NaN, then return NaN.
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \returns the smaller value
+template <typename T> inline T fmin_nan(const T a, const T b) {
+  if (detail::isnan(a) || detail::isnan(b))
+    return NAN;
+  return sycl::fmin(a, b);
+}
+#ifdef SYCL_EXT_ONEAPI_BFLOAT16_MATH_FUNCTIONS
+template <>
+inline sycl::ext::oneapi::bfloat16
+fmin_nan(const sycl::ext::oneapi::bfloat16 a,
+         const sycl::ext::oneapi::bfloat16 b) {
+  if (detail::isnan(a) || detail::isnan(b))
+    return NAN;
+  return sycl::fmin(float(a), float(b));
+}
+#endif
+template <typename T>
+inline sycl::vec<T, 2> fmin_nan(const sycl::vec<T, 2> a,
+                                const sycl::vec<T, 2> b) {
+  return {fmin_nan(a[0], b[0]), fmin_nan(a[1], b[1])};
+}
+template <typename T>
+inline sycl::marray<T, 2> fmin_nan(const sycl::marray<T, 2> a,
+                                   const sycl::marray<T, 2> b) {
+  return {fmin_nan(a[0], b[0]), fmin_nan(a[1], b[1])};
+}
+
+/// A sycl::abs wrapper functors.
+struct abs {
+  template <typename T> auto operator()(const T x) const {
+    return sycl::abs(x);
+  }
+};
+
+/// A sycl::abs_diff wrapper functors.
+struct abs_diff {
+  template <typename T> auto operator()(const T x, const T y) const {
+    return sycl::abs_diff(x, y);
+  }
+};
+
+/// A sycl::add_sat wrapper functors.
+struct add_sat {
+  template <typename T> auto operator()(const T x, const T y) const {
+    return sycl::add_sat(x, y);
+  }
+};
+
+/// A sycl::rhadd wrapper functors.
+struct rhadd {
+  template <typename T> auto operator()(const T x, const T y) const {
+    return sycl::rhadd(x, y);
+  }
+};
+
+/// A sycl::hadd wrapper functors.
+struct hadd {
+  template <typename T> auto operator()(const T x, const T y) const {
+    return sycl::hadd(x, y);
+  }
+};
+
+/// A sycl::max wrapper functors.
+struct maximum {
+  template <typename T> auto operator()(const T x, const T y) const {
+    return sycl::max(x, y);
+  }
+};
+
+/// A sycl::min wrapper functors.
+struct minimum {
+  template <typename T> auto operator()(const T x, const T y) const {
+    return sycl::min(x, y);
+  }
+};
+
+/// A sycl::sub_sat wrapper functors.
+struct sub_sat {
+  template <typename T> auto operator()(const T x, const T y) const {
+    return sycl::sub_sat(x, y);
+  }
+};
+
+/// Compute vectorized binary operation value for two values, with each value
+/// treated as a vector type \p VecT.
+/// \tparam [in] VecT The type of the vector
+/// \tparam [in] BinaryOperation The binary operation class
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \returns The vectorized binary operation value of the two values
+template <typename VecT, class BinaryOperation>
+inline unsigned vectorized_binary(unsigned a, unsigned b,
+                                  const BinaryOperation binary_op) {
+  sycl::vec<unsigned, 1> v0{a}, v1{b};
+  auto v2 = v0.as<VecT>();
+  auto v3 = v1.as<VecT>();
+  auto v4 =
+      detail::vectorized_binary<VecT, BinaryOperation>()(v2, v3, binary_op);
+  v0 = v4.template as<sycl::vec<unsigned, 1>>();
+  return v0;
+}
+
+/// Compute vectorized isgreater for two values, with each value treated as a
+/// vector type \p S.
+/// \tparam [in] S The type of the vector
+/// \tparam [in] T The type of the original values
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \returns The vectorized greater than of the two values
+template <typename S, typename T> inline T vectorized_isgreater(T a, T b) {
+  sycl::vec<T, 1> v0{a}, v1{b};
+  auto v2 = v0.template as<S>();
+  auto v3 = v1.template as<S>();
+  auto v4 = v2 > v3;
+  v0 = v4.template as<sycl::vec<T, 1>>();
+  return v0;
+}
+
+/// Compute vectorized max for two values, with each value treated as a vector
+/// type \p S.
+/// \tparam [in] S The type of the vector
+/// \tparam [in] T The type of the original values
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \returns The vectorized max of the two values
+template <typename S, typename T> inline T vectorized_max(T a, T b) {
+  sycl::vec<T, 1> v0{a}, v1{b};
+  auto v2 = v0.template as<S>();
+  auto v3 = v1.template as<S>();
+  auto v4 = sycl::max(v2, v3);
+  v0 = v4.template as<sycl::vec<T, 1>>();
+  return v0;
+}
+
+/// Compute vectorized min for two values, with each value treated as a vector
+/// type \p S.
+/// \tparam [in] S The type of the vector
+/// \tparam [in] T The type of the original values
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \returns The vectorized min of the two values
+template <typename S, typename T> inline T vectorized_min(T a, T b) {
+  sycl::vec<T, 1> v0{a}, v1{b};
+  auto v2 = v0.template as<S>();
+  auto v3 = v1.template as<S>();
+  auto v4 = sycl::min(v2, v3);
+  v0 = v4.template as<sycl::vec<T, 1>>();
+  return v0;
+}
+
+/// Compute vectorized unary operation for a value, with the value treated as a
+/// vector type \p VecT.
+/// \tparam [in] VecT The type of the vector
+/// \tparam [in] UnaryOperation The unary operation class
+/// \param [in] a The input value
+/// \returns The vectorized unary operation value of the input value
+template <typename VecT, class UnaryOperation>
+inline unsigned vectorized_unary(unsigned a, const UnaryOperation unary_op) {
+  sycl::vec<unsigned, 1> v0{a};
+  auto v1 = v0.as<VecT>();
+  auto v2 = unary_op(v1);
+  v0 = v2.template as<sycl::vec<unsigned, 1>>();
+  return v0;
+}
+
+/// Compute vectorized absolute difference for two values without modulo
+/// overflow, with each value treated as a vector type \p VecT.
+/// \tparam [in] VecT The type of the vector
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \returns The vectorized absolute difference of the two values
+template <typename VecT>
+inline unsigned vectorized_sum_abs_diff(unsigned a, unsigned b) {
+  sycl::vec<unsigned, 1> v0{a}, v1{b};
+  auto v2 = v0.as<VecT>();
+  auto v3 = v1.as<VecT>();
+  auto v4 = sycl::abs_diff(v2, v3);
+  unsigned sum = 0;
+  for (size_t i = 0; i < v4.size(); ++i) {
+    sum += v4[i];
+  }
+  return sum;
+}
+
+namespace detail {
+/// Extend the 'val' to 'bit' size, zero extend for unsigned int and signed
+/// extend for signed int.
+template <typename T>
+inline int64_t zero_or_signed_extent(T val, unsigned bit) {
+  if constexpr (std::is_signed_v<T>) {
+    return int64_t(val) << (64 - bit) >> (64 - bit);
+  }
+  return val;
+}
+
+template <typename RetT, bool NeedSat, typename AT, typename BT,
+          typename BinaryOperation>
+inline constexpr RetT extend_binary(AT a, BT b, BinaryOperation binary_op) {
+  int64_t extend_a = zero_or_signed_extent(a, 33);
+  int64_t extend_b = zero_or_signed_extent(b, 33);
+  int64_t ret = binary_op(extend_a, extend_b);
+  if constexpr (NeedSat)
+    return dpct::clamp<int64_t>(ret, std::numeric_limits<RetT>::min(),
+                                std::numeric_limits<RetT>::max());
+  return ret;
+}
+
+template <typename RetT, bool NeedSat, typename AT, typename BT, typename CT,
+          typename BinaryOperation1, typename BinaryOperation2>
+inline constexpr RetT extend_binary(AT a, BT b, CT c,
+                                    BinaryOperation1 binary_op,
+                                    BinaryOperation2 second_op) {
+  int64_t extend_a = zero_or_signed_extent(a, 33);
+  int64_t extend_b = zero_or_signed_extent(b, 33);
+  int64_t extend_temp =
+      zero_or_signed_extent(binary_op(extend_a, extend_b), 34);
+  if constexpr (NeedSat)
+    extend_temp =
+        dpct::clamp<int64_t>(extend_temp, std::numeric_limits<RetT>::min(),
+                             std::numeric_limits<RetT>::max());
+  int64_t extend_c = zero_or_signed_extent(c, 33);
+  return second_op(extend_temp, extend_c);
+}
+} // namespace detail
+
+/// Extend \p a and \p b to 33 bit and add them.
+/// \tparam [in] RetT The type of the return value
+/// \tparam [in] AT The type of the first value
+/// \tparam [in] BT The type of the second value
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \returns The extend addition of the two values
+template <typename RetT, typename AT, typename BT>
+inline constexpr RetT extend_add(AT a, BT b) {
+  return detail::extend_binary<RetT, false>(a, b, std::plus());
+}
+
+/// Extend Inputs to 33 bit, add \p a, \p b, then do \p second_op with \p c.
+/// \tparam [in] RetT The type of the return value
+/// \tparam [in] AT The type of the first value
+/// \tparam [in] BT The type of the second value
+/// \tparam [in] CT The type of the third value
+/// \tparam [in] BinaryOperation The type of the second operation
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \param [in] c The third value
+/// \param [in] second_op The operation to do with the third value
+/// \returns The extend addition of \p a, \p b and \p second_op with \p c
+template <typename RetT, typename AT, typename BT, typename CT,
+          typename BinaryOperation>
+inline constexpr RetT extend_add(AT a, BT b, CT c, BinaryOperation second_op) {
+  return detail::extend_binary<RetT, false>(a, b, c, std::plus(), second_op);
+}
+
+/// Extend \p a and \p b to 33 bit and add them with saturation.
+/// \tparam [in] RetT The type of the return value
+/// \tparam [in] AT The type of the first value
+/// \tparam [in] BT The type of the second value
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \returns The extend addition of the two values with saturation
+template <typename RetT, typename AT, typename BT>
+inline constexpr RetT extend_add_sat(AT a, BT b) {
+  return detail::extend_binary<RetT, true>(a, b, std::plus());
+}
+
+/// Extend Inputs to 33 bit, add \p a, \p b with saturation, then do \p
+/// second_op with \p c.
+/// \tparam [in] RetT The type of the return value
+/// \tparam [in] AT The type of the first value
+/// \tparam [in] BT The type of the second value
+/// \tparam [in] CT The type of the third value
+/// \tparam [in] BinaryOperation The type of the second operation
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \param [in] c The third value
+/// \param [in] second_op The operation to do with the third value
+/// \returns The extend addition of \p a, \p b with saturation and \p second_op
+/// with \p c
+template <typename RetT, typename AT, typename BT, typename CT,
+          typename BinaryOperation>
+inline constexpr RetT extend_add_sat(AT a, BT b, CT c,
+                                     BinaryOperation second_op) {
+  return detail::extend_binary<RetT, true>(a, b, c, std::plus(), second_op);
+}
+
+/// Extend \p a and \p b to 33 bit and minus them.
+/// \tparam [in] RetT The type of the return value
+/// \tparam [in] AT The type of the first value
+/// \tparam [in] BT The type of the second value
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \returns The extend subtraction of the two values
+template <typename RetT, typename AT, typename BT>
+inline constexpr RetT extend_sub(AT a, BT b) {
+  return detail::extend_binary<RetT, false>(a, b, std::minus());
+}
+
+/// Extend Inputs to 33 bit, minus \p a, \p b, then do \p second_op with \p c.
+/// \tparam [in] RetT The type of the return value
+/// \tparam [in] AT The type of the first value
+/// \tparam [in] BT The type of the second value
+/// \tparam [in] CT The type of the third value
+/// \tparam [in] BinaryOperation The type of the second operation
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \param [in] c The third value
+/// \param [in] second_op The operation to do with the third value
+/// \returns The extend subtraction of \p a, \p b and \p second_op with \p c
+template <typename RetT, typename AT, typename BT, typename CT,
+          typename BinaryOperation>
+inline constexpr RetT extend_sub(AT a, BT b, CT c, BinaryOperation second_op) {
+  return detail::extend_binary<RetT, false>(a, b, c, std::minus(), second_op);
+}
+
+/// Extend \p a and \p b to 33 bit and minus them with saturation.
+/// \tparam [in] RetT The type of the return value
+/// \tparam [in] AT The type of the first value
+/// \tparam [in] BT The type of the second value
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \returns The extend subtraction of the two values with saturation
+template <typename RetT, typename AT, typename BT>
+inline constexpr RetT extend_sub_sat(AT a, BT b) {
+  return detail::extend_binary<RetT, true>(a, b, std::minus());
+}
+
+/// Extend Inputs to 33 bit, minus \p a, \p b with saturation, then do \p
+/// second_op with \p c.
+/// \tparam [in] RetT The type of the return value
+/// \tparam [in] AT The type of the first value
+/// \tparam [in] BT The type of the second value
+/// \tparam [in] CT The type of the third value
+/// \tparam [in] BinaryOperation The type of the second operation
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \param [in] c The third value
+/// \param [in] second_op The operation to do with the third value
+/// \returns The extend subtraction of \p a, \p b with saturation and \p
+/// second_op with \p c
+template <typename RetT, typename AT, typename BT, typename CT,
+          typename BinaryOperation>
+inline constexpr RetT extend_sub_sat(AT a, BT b, CT c,
+                                     BinaryOperation second_op) {
+  return detail::extend_binary<RetT, true>(a, b, c, std::minus(), second_op);
+}
+
+/// Extend \p a and \p b to 33 bit and do abs_diff.
+/// \tparam [in] RetT The type of the return value
+/// \tparam [in] AT The type of the first value
+/// \tparam [in] BT The type of the second value
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \returns The extend abs_diff of the two values
+template <typename RetT, typename AT, typename BT>
+inline constexpr RetT extend_absdiff(AT a, BT b) {
+  return detail::extend_binary<RetT, false>(a, b, abs_diff());
+}
+
+/// Extend Inputs to 33 bit, abs_diff \p a, \p b, then do \p second_op with \p
+/// c.
+/// \tparam [in] RetT The type of the return value
+/// \tparam [in] AT The type of the first value
+/// \tparam [in] BT The type of the second value
+/// \tparam [in] CT The type of the third value
+/// \tparam [in] BinaryOperation The type of the second operation
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \param [in] c The third value
+/// \param [in] second_op The operation to do with the third value
+/// \returns The extend abs_diff of \p a, \p b and \p second_op with \p c
+template <typename RetT, typename AT, typename BT, typename CT,
+          typename BinaryOperation>
+inline constexpr RetT extend_absdiff(AT a, BT b, CT c,
+                                     BinaryOperation second_op) {
+  return detail::extend_binary<RetT, false>(a, b, c, abs_diff(), second_op);
+}
+
+/// Extend \p a and \p b to 33 bit and do abs_diff with saturation.
+/// \tparam [in] RetT The type of the return value
+/// \tparam [in] AT The type of the first value
+/// \tparam [in] BT The type of the second value
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \returns The extend abs_diff of the two values with saturation
+template <typename RetT, typename AT, typename BT>
+inline constexpr RetT extend_absdiff_sat(AT a, BT b) {
+  return detail::extend_binary<RetT, true>(a, b, abs_diff());
+}
+
+/// Extend Inputs to 33 bit, abs_diff \p a, \p b with saturation, then do \p
+/// second_op with \p c.
+/// \tparam [in] RetT The type of the return value
+/// \tparam [in] AT The type of the first value
+/// \tparam [in] BT The type of the second value
+/// \tparam [in] CT The type of the third value
+/// \tparam [in] BinaryOperation The type of the second operation
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \param [in] c The third value
+/// \param [in] second_op The operation to do with the third value
+/// \returns The extend abs_diff of \p a, \p b with saturation and \p
+/// second_op with \p c
+template <typename RetT, typename AT, typename BT, typename CT,
+          typename BinaryOperation>
+inline constexpr RetT extend_absdiff_sat(AT a, BT b, CT c,
+                                         BinaryOperation second_op) {
+  return detail::extend_binary<RetT, true>(a, b, c, abs_diff(), second_op);
+}
+
+/// Extend \p a and \p b to 33 bit and return smaller one.
+/// \tparam [in] RetT The type of the return value
+/// \tparam [in] AT The type of the first value
+/// \tparam [in] BT The type of the second value
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \returns The smaller one of the two extended values
+template <typename RetT, typename AT, typename BT>
+inline constexpr RetT extend_min(AT a, BT b) {
+  return detail::extend_binary<RetT, false>(a, b, minimum());
+}
+
+/// Extend Inputs to 33 bit, find the smaller one in \p a, \p b, then do \p
+/// second_op with \p c.
+/// \tparam [in] RetT The type of the return value
+/// \tparam [in] AT The type of the first value
+/// \tparam [in] BT The type of the second value
+/// \tparam [in] CT The type of the third value
+/// \tparam [in] BinaryOperation The type of the second operation
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \param [in] c The third value
+/// \param [in] second_op The operation to do with the third value
+/// \returns The smaller one of \p a, \p b and \p second_op with \p c
+template <typename RetT, typename AT, typename BT, typename CT,
+          typename BinaryOperation>
+inline constexpr RetT extend_min(AT a, BT b, CT c, BinaryOperation second_op) {
+  return detail::extend_binary<RetT, false>(a, b, c, minimum(), second_op);
+}
+
+/// Extend \p a and \p b to 33 bit and return smaller one with saturation.
+/// \tparam [in] RetT The type of the return value
+/// \tparam [in] AT The type of the first value
+/// \tparam [in] BT The type of the second value
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \returns The smaller one of the two extended values with saturation
+template <typename RetT, typename AT, typename BT>
+inline constexpr RetT extend_min_sat(AT a, BT b) {
+  return detail::extend_binary<RetT, true>(a, b, minimum());
+}
+
+/// Extend Inputs to 33 bit, find the smaller one in \p a, \p b with saturation,
+/// then do \p second_op with \p c.
+/// \tparam [in] RetT The type of the return value
+/// \tparam [in] AT The type of the first value
+/// \tparam [in] BT The type of the second value
+/// \tparam [in] CT The type of the third value
+/// \tparam [in] BinaryOperation The type of the second operation
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \param [in] c The third value
+/// \param [in] second_op The operation to do with the third value
+/// \returns The smaller one of \p a, \p b with saturation and \p
+/// second_op with \p c
+template <typename RetT, typename AT, typename BT, typename CT,
+          typename BinaryOperation>
+inline constexpr RetT extend_min_sat(AT a, BT b, CT c,
+                                     BinaryOperation second_op) {
+  return detail::extend_binary<RetT, true>(a, b, c, minimum(), second_op);
+}
+
+/// Extend \p a and \p b to 33 bit and return bigger one.
+/// \tparam [in] RetT The type of the return value
+/// \tparam [in] AT The type of the first value
+/// \tparam [in] BT The type of the second value
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \returns The bigger one of the two extended values
+template <typename RetT, typename AT, typename BT>
+inline constexpr RetT extend_max(AT a, BT b) {
+  return detail::extend_binary<RetT, false>(a, b, maximum());
+}
+
+/// Extend Inputs to 33 bit, find the bigger one in \p a, \p b, then do \p
+/// second_op with \p c.
+/// \tparam [in] RetT The type of the return value
+/// \tparam [in] AT The type of the first value
+/// \tparam [in] BT The type of the second value
+/// \tparam [in] CT The type of the third value
+/// \tparam [in] BinaryOperation The type of the second operation
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \param [in] c The third value
+/// \param [in] second_op The operation to do with the third value
+/// \returns The bigger one of \p a, \p b and \p second_op with \p c
+template <typename RetT, typename AT, typename BT, typename CT,
+          typename BinaryOperation>
+inline constexpr RetT extend_max(AT a, BT b, CT c, BinaryOperation second_op) {
+  return detail::extend_binary<RetT, false>(a, b, c, maximum(), second_op);
+}
+
+/// Extend \p a and \p b to 33 bit and return bigger one with saturation.
+/// \tparam [in] RetT The type of the return value
+/// \tparam [in] AT The type of the first value
+/// \tparam [in] BT The type of the second value
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \returns The bigger one of the two extended values with saturation
+template <typename RetT, typename AT, typename BT>
+inline constexpr RetT extend_max_sat(AT a, BT b) {
+  return detail::extend_binary<RetT, true>(a, b, maximum());
+}
+
+/// Extend Inputs to 33 bit, find the bigger one in \p a, \p b with saturation,
+/// then do \p second_op with \p c.
+/// \tparam [in] RetT The type of the return value
+/// \tparam [in] AT The type of the first value
+/// \tparam [in] BT The type of the second value
+/// \tparam [in] CT The type of the third value
+/// \tparam [in] BinaryOperation The type of the second operation
+/// \param [in] a The first value
+/// \param [in] b The second value
+/// \param [in] c The third value
+/// \param [in] second_op The operation to do with the third value
+/// \returns The bigger one of \p a, \p b with saturation and \p
+/// second_op with \p c
+template <typename RetT, typename AT, typename BT, typename CT,
+          typename BinaryOperation>
+inline constexpr RetT extend_max_sat(AT a, BT b, CT c,
+                                     BinaryOperation second_op) {
+  return detail::extend_binary<RetT, true>(a, b, c, maximum(), second_op);
+}
+} // namespace dpct
+
+#endif // __DPCT_MATH_HPP__
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/memory.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/memory.hpp
new file mode 100644
index 0000000..4b30478
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/memory.hpp
@@ -0,0 +1,1493 @@
+//==---- memory.hpp -------------------------------*- C++ -*----------------==//
+//
+// Copyright (C) Intel Corporation
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+// See https://llvm.org/LICENSE.txt for license information.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef __DPCT_MEMORY_HPP__
+#define __DPCT_MEMORY_HPP__
+
+#include "device.hpp"
+#include <sycl/sycl.hpp>
+#include <cassert>
+#include <cstdint>
+#include <cstring>
+#include <mutex>
+#include <unordered_map>
+#include <map>
+#include <utility>
+#include <thread>
+#include <type_traits>
+
+#if defined(__linux__)
+#include <sys/mman.h>
+#elif defined(_WIN64)
+#ifndef NOMINMAX
+#define NOMINMAX
+#endif
+#include <windows.h>
+#else
+#error "Only support Windows and Linux."
+#endif
+
+namespace dpct {
+
+enum memcpy_direction {
+  host_to_host,
+  host_to_device,
+  device_to_host,
+  device_to_device,
+  automatic
+};
+enum memory_region {
+  global = 0,  // device global memory
+  constant,    // device constant memory
+  local,       // device local memory
+  shared,      // memory which can be accessed by host and device
+};
+
+typedef uint8_t byte_t;
+
+/// Buffer type to be used in Memory Management runtime.
+typedef sycl::buffer<byte_t> buffer_t;
+
+/// Pitched 2D/3D memory data.
+class pitched_data {
+public:
+  pitched_data() : pitched_data(nullptr, 0, 0, 0) {}
+  pitched_data(void *data, size_t pitch, size_t x, size_t y)
+      : _data(data), _pitch(pitch), _x(x), _y(y) {}
+
+  void *get_data_ptr() { return _data; }
+  void set_data_ptr(void *data) { _data = data; }
+
+  size_t get_pitch() { return _pitch; }
+  void set_pitch(size_t pitch) { _pitch = pitch; }
+
+  size_t get_x() { return _x; }
+  void set_x(size_t x) { _x = x; };
+
+  size_t get_y() { return _y; }
+  void set_y(size_t y) { _y = y; }
+
+private:
+  void *_data;
+  size_t _pitch, _x, _y;
+};
+
+namespace detail {
+class mem_mgr {
+  mem_mgr() {
+    // Reserved address space, no real memory allocation happens here.
+#if defined(__linux__)
+    mapped_address_space =
+        (byte_t *)mmap(nullptr, mapped_region_size, PROT_NONE,
+                       MAP_PRIVATE | MAP_ANONYMOUS, -1, 0);
+#elif defined(_WIN64)
+    mapped_address_space = (byte_t *)VirtualAlloc(
+        NULL,               // NULL specified as the base address parameter
+        mapped_region_size, // Size of allocation
+        MEM_RESERVE,        // Allocate reserved pages
+        PAGE_NOACCESS);     // Protection = no access
+#else
+#error "Only support Windows and Linux."
+#endif
+    next_free = mapped_address_space;
+  };
+
+public:
+  using buffer_id_t = int;
+
+  struct allocation {
+    buffer_t buffer;
+    byte_t *alloc_ptr;
+    size_t size;
+  };
+
+  ~mem_mgr() {
+#if defined(__linux__)
+    munmap(mapped_address_space, mapped_region_size);
+#elif defined(_WIN64)
+    VirtualFree(mapped_address_space, 0, MEM_RELEASE);
+#else
+#error "Only support Windows and Linux."
+#endif
+  };
+
+  mem_mgr(const mem_mgr &) = delete;
+  mem_mgr &operator=(const mem_mgr &) = delete;
+  mem_mgr(mem_mgr &&) = delete;
+  mem_mgr &operator=(mem_mgr &&) = delete;
+
+  /// Allocate
+  void *mem_alloc(size_t size) {
+    if (!size)
+      return nullptr;
+    std::lock_guard<std::mutex> lock(m_mutex);
+    if (next_free + size > mapped_address_space + mapped_region_size) {
+      throw std::runtime_error("dpct_malloc: out of memory for virtual memory pool");
+    }
+    // Allocation
+    sycl::range<1> r(size);
+    buffer_t buf(r);
+    allocation A{buf, next_free, size};
+    // Map allocation to device pointer
+    void *result = next_free;
+    m_map.emplace(next_free + size, A);
+    // Update pointer to the next free space.
+    next_free += (size + extra_padding + alignment - 1) & ~(alignment - 1);
+
+    return result;
+  }
+
+  /// Deallocate
+  void mem_free(const void *ptr) {
+    if (!ptr)
+      return;
+    std::lock_guard<std::mutex> lock(m_mutex);
+    auto it = get_map_iterator(ptr);
+    m_map.erase(it);
+  }
+
+  /// map: device pointer -> allocation(buffer, alloc_ptr, size)
+  allocation translate_ptr(const void *ptr) {
+    std::lock_guard<std::mutex> lock(m_mutex);
+    auto it = get_map_iterator(ptr);
+    return it->second;
+  }
+
+  /// Check if the pointer represents device pointer or not.
+  bool is_device_ptr(const void *ptr) const {
+    std::lock_guard<std::mutex> lock(m_mutex);
+    return (mapped_address_space <= ptr) &&
+           (ptr < mapped_address_space + mapped_region_size);
+  }
+
+  /// Returns the instance of memory manager singleton.
+  static mem_mgr &instance() {
+    static mem_mgr m;
+    return m;
+  }
+
+private:
+  std::map<byte_t *, allocation> m_map;
+  mutable std::mutex m_mutex;
+  byte_t *mapped_address_space;
+  byte_t *next_free;
+  const size_t mapped_region_size = 128ull * 1024 * 1024 * 1024;
+  const size_t alignment = 256;
+  /// This padding may be defined to some positive value to debug
+  /// out of bound accesses.
+  const size_t extra_padding = 0;
+
+  std::map<byte_t *, allocation>::iterator get_map_iterator(const void *ptr) {
+    auto it = m_map.upper_bound((byte_t *)ptr);
+    if (it == m_map.end()) {
+      // Not a virtual pointer.
+      throw std::runtime_error("can not get buffer from non-virtual pointer");
+    }
+    const allocation &alloc = it->second;
+    if (ptr < alloc.alloc_ptr) {
+      // Out of bound.
+      // This may happen if there's a gap between allocations due to alignment
+      // or extra padding and pointer points to this gap.
+      throw std::runtime_error("invalid virtual pointer");
+    }
+    return it;
+  }
+};
+
+template <class T, memory_region Memory, size_t Dimension> class accessor;
+template <memory_region Memory, class T = byte_t> class memory_traits {
+public:
+  static constexpr sycl::access::target target =
+      sycl::access::target::device;
+  static constexpr sycl::access_mode mode =
+      (Memory == constant) ? sycl::access_mode::read
+                           : sycl::access_mode::read_write;
+  static constexpr size_t type_size = sizeof(T);
+  using element_t =
+      typename std::conditional<Memory == constant, const T, T>::type;
+  using value_t = typename std::remove_cv<T>::type;
+  template <size_t Dimension = 1>
+  using accessor_t = typename std::conditional<
+      Memory == local, sycl::local_accessor<value_t, Dimension>,
+      sycl::accessor<T, Dimension, mode, target>>::type;
+  using pointer_t = T *;
+};
+
+static inline void *dpct_malloc(size_t size, sycl::queue &q) {
+#ifdef DPCT_USM_LEVEL_NONE
+  return mem_mgr::instance().mem_alloc(size * sizeof(byte_t));
+#else
+  return sycl::malloc_device(size, q.get_device(), q.get_context());
+#endif // DPCT_USM_LEVEL_NONE
+}
+
+#define PITCH_DEFAULT_ALIGN(x) (((x) + 31) & ~(0x1F))
+static inline void *dpct_malloc(size_t &pitch, size_t x, size_t y, size_t z,
+                                sycl::queue &q) {
+  pitch = PITCH_DEFAULT_ALIGN(x);
+  return dpct_malloc(pitch * y * z, q);
+}
+
+/**
+ * @brief Sets \p value to the first \p size elements starting from \p dev_ptr in \p q.
+ * @tparam valueT The type of the element to be set.
+ * @param [in] q The queue in which the operation is done.
+ * @param [in] dev_ptr Pointer to the virtual device memory address.
+ * @param [in] value The value to be set.
+ * @param [in] size Number of elements to be set to the value.
+ * @return An event representing the memset operation.
+ */
+template <typename valueT>
+static inline sycl::event dpct_memset(sycl::queue &q, void *dev_ptr,
+                                      valueT value, size_t size) {
+#ifdef DPCT_USM_LEVEL_NONE
+  auto &mm = mem_mgr::instance();
+  assert(mm.is_device_ptr(dev_ptr));
+  auto alloc = mm.translate_ptr(dev_ptr);
+  size_t offset = (valueT *)dev_ptr - (valueT *)alloc.alloc_ptr;
+
+  return q.submit([&](sycl::handler &cgh) {
+    auto r = sycl::range<1>(size);
+    auto o = sycl::id<1>(offset);
+    auto new_buffer = alloc.buffer.reinterpret<valueT>(
+        sycl::range<1>(alloc.size / sizeof(valueT)));
+    sycl::accessor<valueT, 1, sycl::access_mode::write,
+                   sycl::access::target::device>
+        acc(new_buffer, cgh, r, o);
+    cgh.fill(acc, value);
+  });
+#else
+  return q.fill(dev_ptr, value, size);
+#endif // DPCT_USM_LEVEL_NONE
+}
+
+/**
+ * @brief Sets \p value to the 3D memory region pointed by \p data in \p q.
+ * @tparam valueT The type of the element to be set.
+ * @param [in] q The queue in which the operation is done.
+ * @param [in] data Pointer to the pitched device memory region.
+ * @param [in] value The value to be set.
+ * @param [in] size 3D memory region by number of elements.
+ * @return An event list representing the memset operations.
+ */
+template<typename valueT>
+static inline std::vector<sycl::event>
+dpct_memset(sycl::queue &q, pitched_data data, valueT value,
+            sycl::range<3> size) {
+  std::vector<sycl::event> event_list;
+  size_t slice = data.get_pitch() * data.get_y();
+  unsigned char *data_surface = (unsigned char *)data.get_data_ptr();
+  for (size_t z = 0; z < size.get(2); ++z) {
+    unsigned char *data_ptr = data_surface;
+    for (size_t y = 0; y < size.get(1); ++y) {
+      event_list.push_back(dpct_memset(q, data_ptr, value, size.get(0)));
+      data_ptr += data.get_pitch();
+    }
+    data_surface += slice;
+  }
+  return event_list;
+}
+
+/**
+ * @brief Sets \p val to the pitched 2D memory region pointed by \p ptr in \p q.
+ * @tparam valueT The type of the element to be set.
+ * @param [in] q The queue in which the operation is done.
+ * @param [in] ptr Pointer to the virtual device memory.
+ * @param [in] pitch The pitch size by number of elements, including padding.
+ * @param [in] val The value to be set.
+ * @param [in] x The width of memory region by number of elements.
+ * @param [in] y The height of memory region by number of elements.
+ * @return An event list representing the memset operations.
+ */
+template<typename valueT>
+static inline std::vector<sycl::event>
+dpct_memset(sycl::queue &q, void *ptr, size_t pitch, valueT val, size_t x,
+            size_t y) {
+  return dpct_memset(q, pitched_data(ptr, pitch, x, 1), val,
+                     sycl::range<3>(x, y, 1));
+}
+
+enum class pointer_access_attribute {
+  host_only = 0,
+  device_only,
+  host_device,
+  end
+};
+
+static pointer_access_attribute get_pointer_attribute(sycl::queue &q,
+                                                      const void *ptr) {
+#ifdef DPCT_USM_LEVEL_NONE
+  return mem_mgr::instance().is_device_ptr(ptr)
+             ? pointer_access_attribute::device_only
+             : pointer_access_attribute::host_only;
+#else
+  switch (sycl::get_pointer_type(ptr, q.get_context())) {
+  case sycl::usm::alloc::unknown:
+    return pointer_access_attribute::host_only;
+  case sycl::usm::alloc::device:
+    return pointer_access_attribute::device_only;
+  case sycl::usm::alloc::shared:
+  case sycl::usm::alloc::host:
+    return pointer_access_attribute::host_device;
+  }
+#endif
+}
+
+static memcpy_direction deduce_memcpy_direction(sycl::queue &q, void *to_ptr,
+                                             const void *from_ptr,
+                                             memcpy_direction dir) {
+  switch (dir) {
+  case memcpy_direction::host_to_host:
+  case memcpy_direction::host_to_device:
+  case memcpy_direction::device_to_host:
+  case memcpy_direction::device_to_device:
+    return dir;
+  case memcpy_direction::automatic: {
+    // table[to_attribute][from_attribute]
+    static const memcpy_direction
+        direction_table[static_cast<unsigned>(pointer_access_attribute::end)]
+                       [static_cast<unsigned>(pointer_access_attribute::end)] =
+                           {{memcpy_direction::host_to_host,
+                             memcpy_direction::device_to_host,
+                             memcpy_direction::host_to_host},
+                            {memcpy_direction::host_to_device,
+                             memcpy_direction::device_to_device,
+                             memcpy_direction::device_to_device},
+                            {memcpy_direction::host_to_host,
+                             memcpy_direction::device_to_device,
+                             memcpy_direction::device_to_device}};
+    return direction_table[static_cast<unsigned>(get_pointer_attribute(
+        q, to_ptr))][static_cast<unsigned>(get_pointer_attribute(q, from_ptr))];
+  }
+  default:
+    throw std::runtime_error("dpct_memcpy: invalid direction value");
+  }
+}
+
+static sycl::event
+dpct_memcpy(sycl::queue &q, void *to_ptr, const void *from_ptr, size_t size,
+            memcpy_direction direction,
+            const std::vector<sycl::event> &dep_events = {}) {
+  if (!size)
+    return sycl::event{};
+#ifdef DPCT_USM_LEVEL_NONE
+  auto &mm = mem_mgr::instance();
+  auto real_direction = deduce_memcpy_direction(q, to_ptr, from_ptr, direction);
+
+  switch (real_direction) {
+  case host_to_host:
+    return q.submit([&](sycl::handler &cgh) {
+      cgh.depends_on(dep_events);
+      cgh.host_task([=] { std::memcpy(to_ptr, from_ptr, size); });
+    });
+  case host_to_device: {
+    auto alloc = mm.translate_ptr(to_ptr);
+    size_t offset = (byte_t *)to_ptr - alloc.alloc_ptr;
+    return q.submit([&](sycl::handler &cgh) {
+      cgh.depends_on(dep_events);
+      auto r = sycl::range<1>(size);
+      auto o = sycl::id<1>(offset);
+      sycl::accessor<byte_t, 1, sycl::access_mode::write,
+                          sycl::access::target::device>
+          acc(alloc.buffer, cgh, r, o);
+      cgh.copy(from_ptr, acc);
+    });
+  }
+  case device_to_host: {
+    auto alloc = mm.translate_ptr(from_ptr);
+    size_t offset = (byte_t *)from_ptr - alloc.alloc_ptr;
+    return q.submit([&](sycl::handler &cgh) {
+      cgh.depends_on(dep_events);
+      auto r = sycl::range<1>(size);
+      auto o = sycl::id<1>(offset);
+      sycl::accessor<byte_t, 1, sycl::access_mode::read,
+                          sycl::access::target::device>
+          acc(alloc.buffer, cgh, r, o);
+      cgh.copy(acc, to_ptr);
+    });
+  }
+  case device_to_device: {
+    auto to_alloc = mm.translate_ptr(to_ptr);
+    auto from_alloc = mm.translate_ptr(from_ptr);
+    size_t to_offset = (byte_t *)to_ptr - to_alloc.alloc_ptr;
+    size_t from_offset = (byte_t *)from_ptr - from_alloc.alloc_ptr;
+    return q.submit([&](sycl::handler &cgh) {
+      cgh.depends_on(dep_events);
+      auto r = sycl::range<1>(size);
+      auto to_o = sycl::id<1>(to_offset);
+      auto from_o = sycl::id<1>(from_offset);
+      sycl::accessor<byte_t, 1, sycl::access_mode::write,
+                          sycl::access::target::device>
+          to_acc(to_alloc.buffer, cgh, r, to_o);
+      sycl::accessor<byte_t, 1, sycl::access_mode::read,
+                          sycl::access::target::device>
+          from_acc(from_alloc.buffer, cgh, r, from_o);
+      cgh.copy(from_acc, to_acc);
+    });
+  }
+  default:
+    throw std::runtime_error("dpct_memcpy: invalid direction value");
+  }
+#else
+  return q.memcpy(to_ptr, from_ptr, size, dep_events);
+#endif // DPCT_USM_LEVEL_NONE
+}
+
+// Get actual copy range and make sure it will not exceed range.
+static inline size_t get_copy_range(sycl::range<3> size, size_t slice,
+                                    size_t pitch) {
+  return slice * (size.get(2) - 1) + pitch * (size.get(1) - 1) + size.get(0);
+}
+
+static inline size_t get_offset(sycl::id<3> id, size_t slice,
+                                    size_t pitch) {
+  return slice * id.get(2) + pitch * id.get(1) + id.get(0);
+}
+
+/// copy 3D matrix specified by \p size from 3D matrix specified by \p from_ptr
+/// and \p from_range to another specified by \p to_ptr and \p to_range.
+static inline std::vector<sycl::event>
+dpct_memcpy(sycl::queue &q, void *to_ptr, const void *from_ptr,
+            sycl::range<3> to_range, sycl::range<3> from_range,
+            sycl::id<3> to_id, sycl::id<3> from_id,
+            sycl::range<3> size, memcpy_direction direction,
+            const std::vector<sycl::event> &dep_events = {}) {
+  // RAII for host pointer
+  class host_buffer {
+    void *_buf;
+    size_t _size;
+    sycl::queue &_q;
+    const std::vector<sycl::event> &_deps; // free operation depends
+
+  public:
+    host_buffer(size_t size, sycl::queue &q,
+                const std::vector<sycl::event> &deps)
+        : _buf(std::malloc(size)), _size(size), _q(q), _deps(deps) {}
+    void *get_ptr() const { return _buf; }
+    size_t get_size() const { return _size; }
+    ~host_buffer() {
+      if (_buf) {
+        _q.submit([&](sycl::handler &cgh) {
+          cgh.depends_on(_deps);
+          cgh.host_task([buf = _buf] { std::free(buf); });
+        });
+      }
+    }
+  };
+  std::vector<sycl::event> event_list;
+
+  size_t to_slice = to_range.get(1) * to_range.get(0),
+         from_slice = from_range.get(1) * from_range.get(0);
+  unsigned char *to_surface =
+      (unsigned char *)to_ptr + get_offset(to_id, to_slice, to_range.get(0));
+  const unsigned char *from_surface =
+      (const unsigned char *)from_ptr +
+      get_offset(from_id, from_slice, from_range.get(0));
+
+  if (to_slice == from_slice && to_slice == size.get(1) * size.get(0)) {
+    return {dpct_memcpy(q, to_surface, from_surface, to_slice * size.get(2),
+                        direction, dep_events)};
+  }
+  direction = deduce_memcpy_direction(q, to_ptr, from_ptr, direction);
+  size_t size_slice = size.get(1) * size.get(0);
+  switch (direction) {
+  case host_to_host:
+    for (size_t z = 0; z < size.get(2); ++z) {
+      unsigned char *to_ptr = to_surface;
+      const unsigned char *from_ptr = from_surface;
+      if (to_range.get(0) == from_range.get(0) &&
+          to_range.get(0) == size.get(0)) {
+        event_list.push_back(dpct_memcpy(q, to_ptr, from_ptr, size_slice,
+                                         direction, dep_events));
+      } else {
+        for (size_t y = 0; y < size.get(1); ++y) {
+          event_list.push_back(dpct_memcpy(q, to_ptr, from_ptr, size.get(0),
+                                           direction, dep_events));
+          to_ptr += to_range.get(0);
+          from_ptr += from_range.get(0);
+        }
+      }
+      to_surface += to_slice;
+      from_surface += from_slice;
+    }
+    break;
+  case host_to_device: {
+    host_buffer buf(get_copy_range(size, to_slice, to_range.get(0)), q,
+                    event_list);
+    std::vector<sycl::event> host_events;
+    if (to_slice == size_slice) {
+      // Copy host data to a temp host buffer with the shape of target.
+      host_events =
+          dpct_memcpy(q, buf.get_ptr(), from_surface, to_range, from_range,
+                      sycl::id<3>(0, 0, 0), sycl::id<3>(0, 0, 0), size,
+                      host_to_host, dep_events);
+    } else {
+      // Copy host data to a temp host buffer with the shape of target.
+      host_events = dpct_memcpy(
+          q, buf.get_ptr(), from_surface, to_range, from_range,
+          sycl::id<3>(0, 0, 0), sycl::id<3>(0, 0, 0), size, host_to_host,
+          // If has padding data, not sure whether it is useless. So fill temp
+          // buffer with it.
+          std::vector<sycl::event>{
+              dpct_memcpy(q, buf.get_ptr(), to_surface, buf.get_size(),
+                          device_to_host, dep_events)});
+    }
+    // Copy from temp host buffer to device with only one submit.
+    event_list.push_back(dpct_memcpy(q, to_surface, buf.get_ptr(),
+                                     buf.get_size(), host_to_device,
+                                     host_events));
+    break;
+  }
+  case device_to_host: {
+    host_buffer buf(get_copy_range(size, from_slice, from_range.get(0)), q,
+                    event_list);
+    // Copy from host temp buffer to host target with reshaping.
+    event_list = dpct_memcpy(
+        q, to_surface, buf.get_ptr(), to_range, from_range, sycl::id<3>(0, 0, 0),
+        sycl::id<3>(0, 0, 0), size, host_to_host,
+        // Copy from device to temp host buffer with only one submit.
+        std::vector<sycl::event>{dpct_memcpy(q, buf.get_ptr(), from_surface,
+                                                 buf.get_size(),
+                                                 device_to_host, dep_events)});
+    break;
+  }
+  case device_to_device:
+#ifdef DPCT_USM_LEVEL_NONE
+  {
+    auto &mm = mem_mgr::instance();
+    auto to_alloc = mm.translate_ptr(to_surface);
+    auto from_alloc = mm.translate_ptr(from_surface);
+    size_t to_offset = (byte_t *)to_surface - to_alloc.alloc_ptr;
+    size_t from_offset = (byte_t *)from_surface - from_alloc.alloc_ptr;
+    event_list.push_back(q.submit([&](sycl::handler &cgh) {
+      cgh.depends_on(dep_events);
+      auto to_o = sycl::id<1>(to_offset);
+      auto from_o = sycl::id<1>(from_offset);
+      sycl::accessor<byte_t, 1, sycl::access_mode::write,
+                         sycl::access::target::device>
+          to_acc(to_alloc.buffer, cgh,
+                 get_copy_range(size, to_slice, to_range.get(0)), to_o);
+      sycl::accessor<byte_t, 1, sycl::access_mode::read,
+                         sycl::access::target::device>
+          from_acc(from_alloc.buffer, cgh,
+                   get_copy_range(size, from_slice, from_range.get(0)), from_o);
+      cgh.parallel_for<class dpct_memcpy_3d_detail_usmnone>(
+          size,
+          [=](sycl::id<3> id) {
+            to_acc[get_offset(id, to_slice, to_range.get(0))] =
+                from_acc[get_offset(id, from_slice, from_range.get(0))];
+          });
+    }));
+  }
+#else
+    event_list.push_back(q.submit([&](sycl::handler &cgh) {
+      cgh.depends_on(dep_events);
+      cgh.parallel_for<class dpct_memcpy_3d_detail>(
+          size,
+          [=](sycl::id<3> id) {
+            to_surface[get_offset(id, to_slice, to_range.get(0))] =
+                from_surface[get_offset(id, from_slice, from_range.get(0))];
+          });
+    }));
+#endif
+  break;
+  default:
+    throw std::runtime_error("dpct_memcpy: invalid direction value");
+  }
+  return event_list;
+}
+
+/// memcpy 2D/3D matrix specified by pitched_data.
+static inline std::vector<sycl::event>
+dpct_memcpy(sycl::queue &q, pitched_data to, sycl::id<3> to_id,
+            pitched_data from, sycl::id<3> from_id, sycl::range<3> size,
+            memcpy_direction direction = automatic) {
+  return dpct_memcpy(q, to.get_data_ptr(), from.get_data_ptr(),
+                     sycl::range<3>(to.get_pitch(), to.get_y(), 1),
+                     sycl::range<3>(from.get_pitch(), from.get_y(), 1), to_id, from_id,
+                     size, direction);
+}
+
+/// memcpy 2D matrix with pitch.
+static inline std::vector<sycl::event>
+dpct_memcpy(sycl::queue &q, void *to_ptr, const void *from_ptr,
+            size_t to_pitch, size_t from_pitch, size_t x, size_t y,
+            memcpy_direction direction = automatic) {
+  return dpct_memcpy(q, to_ptr, from_ptr, sycl::range<3>(to_pitch, y, 1),
+                     sycl::range<3>(from_pitch, y, 1),
+                     sycl::id<3>(0, 0, 0), sycl::id<3>(0, 0, 0),
+                     sycl::range<3>(x, y, 1), direction);
+}
+
+namespace deprecated {
+
+template <typename T, sycl::usm::alloc AllocKind>
+class usm_allocator {
+private:
+  using Alloc = sycl::usm_allocator<T, AllocKind>;
+  Alloc _impl;
+
+public:
+  using value_type = typename std::allocator_traits<Alloc>::value_type;
+  using pointer = typename std::allocator_traits<Alloc>::pointer;
+  using const_pointer = typename std::allocator_traits<Alloc>::const_pointer;
+  using void_pointer = typename std::allocator_traits<Alloc>::void_pointer;
+  using const_void_pointer =
+      typename std::allocator_traits<Alloc>::const_void_pointer;
+  using reference = typename std::allocator_traits<Alloc>::value_type &;
+  using const_reference =
+      const typename std::allocator_traits<Alloc>::value_type &;
+  using difference_type =
+      typename std::allocator_traits<Alloc>::difference_type;
+  using size_type = typename std::allocator_traits<Alloc>::size_type;
+  using propagate_on_container_copy_assignment = typename std::allocator_traits<
+      Alloc>::propagate_on_container_copy_assignment;
+  using propagate_on_container_move_assignment = typename std::allocator_traits<
+      Alloc>::propagate_on_container_move_assignment;
+  using propagate_on_container_swap =
+      typename std::allocator_traits<Alloc>::propagate_on_container_swap;
+  using is_always_equal =
+      typename std::allocator_traits<Alloc>::is_always_equal;
+
+  template <typename U> struct rebind {
+    typedef usm_allocator<U, AllocKind> other;
+  };
+
+  usm_allocator() : _impl(dpct::get_default_queue()) {}
+  ~usm_allocator() {}
+  usm_allocator(const usm_allocator &other) : _impl(other._impl) {}
+  usm_allocator(usm_allocator &&other) : _impl(std::move(other._impl)) {}
+  pointer address(reference r) { return &r; }
+  const_pointer address(const_reference r) { return &r; }
+  pointer allocate(size_type cnt, const_void_pointer hint = nullptr) {
+    return std::allocator_traits<Alloc>::allocate(_impl, cnt, hint);
+  }
+  void deallocate(pointer p, size_type cnt) {
+    std::allocator_traits<Alloc>::deallocate(_impl, p, cnt);
+  }
+  size_type max_size() const {
+    return std::allocator_traits<Alloc>::max_size(_impl);
+  }
+  bool operator==(const usm_allocator &other) const { return _impl == other._impl; }
+  bool operator!=(const usm_allocator &other) const { return _impl != other._impl; }
+};
+
+} // namespace deprecated
+
+inline void dpct_free(void *ptr,
+                      const sycl::queue &q) {
+  if (ptr) {
+#ifdef DPCT_USM_LEVEL_NONE
+    detail::mem_mgr::instance().mem_free(ptr);
+#else
+    sycl::free(ptr, q.get_context());
+#endif // DPCT_USM_LEVEL_NONE
+  }
+}
+} // namespace detail
+
+#ifdef DPCT_USM_LEVEL_NONE
+/// Check if the pointer \p ptr represents device pointer or not.
+///
+/// \param ptr The pointer to be checked.
+/// \returns true if \p ptr is a device pointer.
+template<class T>
+static inline bool is_device_ptr(T ptr) {
+  if constexpr (std::is_pointer<T>::value) {
+    return detail::mem_mgr::instance().is_device_ptr(ptr);
+  }
+  return false;
+}
+#endif
+
+/// Get the buffer and the offset of a piece of memory pointed to by \p ptr.
+///
+/// \param ptr Pointer to a piece of memory.
+/// If NULL is passed as an argument, an exception will be thrown.
+/// \returns a pair containing both the buffer and the offset.
+static std::pair<buffer_t, size_t> get_buffer_and_offset(const void *ptr) {
+  if (ptr) {
+    auto alloc = detail::mem_mgr::instance().translate_ptr(ptr);
+    size_t offset = (byte_t *)ptr - alloc.alloc_ptr;
+    return std::make_pair(alloc.buffer, offset);
+  } else {
+    throw std::runtime_error(
+        "NULL pointer argument in get_buffer_and_offset function is invalid");
+  }
+}
+
+/// Get the data pointed from \p ptr as a 1D buffer reinterpreted as type T.
+template <typename T> static sycl::buffer<T> get_buffer(const void *ptr) {
+  if (!ptr)
+    return sycl::buffer<T>(sycl::range<1>(0));
+  auto alloc = detail::mem_mgr::instance().translate_ptr(ptr);
+  return alloc.buffer.reinterpret<T>(
+      sycl::range<1>(alloc.size / sizeof(T)));
+}
+
+/// Get the buffer of a piece of memory pointed to by \p ptr.
+///
+/// \param ptr Pointer to a piece of memory.
+/// \returns the buffer.
+static buffer_t get_buffer(const void *ptr) {
+  return detail::mem_mgr::instance().translate_ptr(ptr).buffer;
+}
+
+/// A wrapper class contains an accessor and an offset.
+template <typename dataT,
+          sycl::access_mode accessMode = sycl::access_mode::read_write>
+class access_wrapper {
+  sycl::accessor<byte_t, 1, accessMode> accessor;
+  size_t offset;
+
+public:
+  /// Construct the accessor wrapper for memory pointed by \p ptr.
+  ///
+  /// \param ptr Pointer to memory.
+  /// \param cgh The command group handler.
+  access_wrapper(const void *ptr, sycl::handler &cgh)
+      : accessor(get_buffer(ptr).get_access<accessMode>(cgh)), offset(0) {
+    auto alloc = detail::mem_mgr::instance().translate_ptr(ptr);
+    offset = (byte_t *)ptr - alloc.alloc_ptr;
+  }
+
+  /// Get the device pointer.
+  ///
+  /// \returns a device pointer with offset.
+  dataT get_raw_pointer() const { return (dataT)(&accessor[0] + offset); }
+};
+
+/// Get the accessor for memory pointed by \p ptr.
+///
+/// \param ptr Pointer to memory.
+/// If NULL is passed as an argument, an exception will be thrown.
+/// \param cgh The command group handler.
+/// \returns an accessor.
+template <sycl::access_mode accessMode = sycl::access_mode::read_write>
+static sycl::accessor<byte_t, 1, accessMode>
+get_access(const void *ptr, sycl::handler &cgh) {
+  if (ptr) {
+    auto alloc = detail::mem_mgr::instance().translate_ptr(ptr);
+    return alloc.buffer.get_access<accessMode>(cgh);
+  } else {
+    throw std::runtime_error(
+        "NULL pointer argument in get_access function is invalid");
+  }
+}
+
+/// Allocate memory block on the device.
+/// \param num_bytes Number of bytes to allocate.
+/// \param q Queue to execute the allocate task.
+/// \returns A pointer to the newly allocated memory.
+template <typename T>
+static inline void *dpct_malloc(T num_bytes,
+                                sycl::queue &q = get_default_queue()) {
+  return detail::dpct_malloc(static_cast<size_t>(num_bytes), q);
+}
+
+/// Get the host pointer from a buffer that is mapped to virtual pointer ptr.
+/// \param ptr Virtual Pointer mapped to device buffer
+/// \returns A host pointer
+template <typename T> static inline T *get_host_ptr(const void *ptr) {
+  auto BufferOffset = get_buffer_and_offset(ptr);
+  auto host_ptr =
+      BufferOffset.first.get_host_access()
+          .get_pointer();
+  return (T *)(host_ptr + BufferOffset.second);
+}
+
+/// Allocate memory block for 3D array on the device.
+/// \param size Size of the memory block, in bytes.
+/// \param q Queue to execute the allocate task.
+/// \returns A pitched_data object which stores the memory info.
+static inline pitched_data
+dpct_malloc(sycl::range<3> size, sycl::queue &q = get_default_queue()) {
+  pitched_data pitch(nullptr, 0, size.get(0), size.get(1));
+  size_t pitch_size;
+  pitch.set_data_ptr(detail::dpct_malloc(pitch_size, size.get(0), size.get(1),
+                                         size.get(2), q));
+  pitch.set_pitch(pitch_size);
+  return pitch;
+}
+
+/// Allocate memory block for 2D array on the device.
+/// \param [out] pitch Aligned size of x in bytes.
+/// \param x Range in dim x.
+/// \param y Range in dim y.
+/// \param q Queue to execute the allocate task.
+/// \returns A pointer to the newly allocated memory.
+static inline void *dpct_malloc(size_t &pitch, size_t x, size_t y,
+                                sycl::queue &q = get_default_queue()) {
+  return detail::dpct_malloc(pitch, x, y, 1, q);
+}
+
+/// free
+/// \param ptr Point to free.
+/// \param q Queue to execute the free task.
+/// \returns no return value.
+static inline void dpct_free(void *ptr,
+                             sycl::queue &q = get_default_queue()) {
+  detail::dpct_free(ptr, q);
+}
+
+/// Free the device memory pointed by a batch of pointers in \p pointers which
+/// are related to \p q after \p events completed.
+///
+/// \param pointers The pointers point to the device memory requested to be freed.
+/// \param events The events to be waited.
+/// \param q The sycl::queue the memory relates to.
+inline void async_dpct_free(const std::vector<void *> &pointers,
+                            const std::vector<sycl::event> &events,
+                            sycl::queue &q = get_default_queue()) {
+  q.submit([&](sycl::handler &cgh) {
+    cgh.depends_on(events);
+    cgh.host_task([=] {
+      for (auto p : pointers)
+        if (p) {
+          detail::dpct_free(p, q);
+        }
+    });
+  });
+}
+
+/// Synchronously copies \p size bytes from the address specified by \p from_ptr
+/// to the address specified by \p to_ptr. The value of \p direction is used to
+/// set the copy direction, it can be \a host_to_host, \a host_to_device,
+/// \a device_to_host, \a device_to_device or \a automatic. The function will
+/// return after the copy is completed.
+///
+/// \param to_ptr Pointer to destination memory address.
+/// \param from_ptr Pointer to source memory address.
+/// \param size Number of bytes to be copied.
+/// \param direction Direction of the copy.
+/// \param q Queue to execute the copy task.
+/// \returns no return value.
+static void dpct_memcpy(void *to_ptr, const void *from_ptr, size_t size,
+                        memcpy_direction direction = automatic,
+                        sycl::queue &q = get_default_queue()) {
+  detail::dpct_memcpy(q, to_ptr, from_ptr, size, direction).wait();
+}
+
+/// Asynchronously copies \p size bytes from the address specified by \p
+/// from_ptr to the address specified by \p to_ptr. The value of \p direction is
+/// used to set the copy direction, it can be \a host_to_host, \a
+/// host_to_device, \a device_to_host, \a device_to_device or \a automatic. The
+/// return of the function does NOT guarantee the copy is completed.
+///
+/// \param to_ptr Pointer to destination memory address.
+/// \param from_ptr Pointer to source memory address.
+/// \param size Number of bytes to be copied.
+/// \param direction Direction of the copy.
+/// \param q Queue to execute the copy task.
+/// \returns no return value.
+static void async_dpct_memcpy(void *to_ptr, const void *from_ptr, size_t size,
+                              memcpy_direction direction = automatic,
+                              sycl::queue &q = dpct::get_default_queue()) {
+  detail::dpct_memcpy(q, to_ptr, from_ptr, size, direction);
+}
+
+/// Synchronously copies 2D matrix specified by \p x and \p y from the address
+/// specified by \p from_ptr to the address specified by \p to_ptr, while \p
+/// from_pitch and \p to_pitch are the range of dim x in bytes of the matrix
+/// specified by \p from_ptr and \p to_ptr. The value of \p direction is used to
+/// set the copy direction, it can be \a host_to_host, \a host_to_device, \a
+/// device_to_host, \a device_to_device or \a automatic. The function will
+/// return after the copy is completed.
+///
+/// \param to_ptr Pointer to destination memory address.
+/// \param to_pitch Range of dim x in bytes of destination matrix.
+/// \param from_ptr Pointer to source memory address.
+/// \param from_pitch Range of dim x in bytes of source matrix.
+/// \param x Range of dim x of matrix to be copied.
+/// \param y Range of dim y of matrix to be copied.
+/// \param direction Direction of the copy.
+/// \param q Queue to execute the copy task.
+/// \returns no return value.
+static inline void dpct_memcpy(void *to_ptr, size_t to_pitch,
+                               const void *from_ptr, size_t from_pitch,
+                               size_t x, size_t y,
+                               memcpy_direction direction = automatic,
+                               sycl::queue &q = dpct::get_default_queue()) {
+  sycl::event::wait(detail::dpct_memcpy(q, to_ptr, from_ptr, to_pitch,
+                                            from_pitch, x, y, direction));
+}
+
+/// Asynchronously copies 2D matrix specified by \p x and \p y from the address
+/// specified by \p from_ptr to the address specified by \p to_ptr, while \p
+/// \p from_pitch and \p to_pitch are the range of dim x in bytes of the matrix
+/// specified by \p from_ptr and \p to_ptr. The value of \p direction is used to
+/// set the copy direction, it can be \a host_to_host, \a host_to_device, \a
+/// device_to_host, \a device_to_device or \a automatic. The return of the
+/// function does NOT guarantee the copy is completed.
+///
+/// \param to_ptr Pointer to destination memory address.
+/// \param to_pitch Range of dim x in bytes of destination matrix.
+/// \param from_ptr Pointer to source memory address.
+/// \param from_pitch Range of dim x in bytes of source matrix.
+/// \param x Range of dim x of matrix to be copied.
+/// \param y Range of dim y of matrix to be copied.
+/// \param direction Direction of the copy.
+/// \param q Queue to execute the copy task.
+/// \returns no return value.
+static inline void
+async_dpct_memcpy(void *to_ptr, size_t to_pitch, const void *from_ptr,
+                  size_t from_pitch, size_t x, size_t y,
+                  memcpy_direction direction = automatic,
+                  sycl::queue &q = get_default_queue()) {
+  detail::dpct_memcpy(q, to_ptr, from_ptr, to_pitch, from_pitch, x, y,
+                      direction);
+}
+
+/// Synchronously copies a subset of a 3D matrix specified by \p to to another
+/// 3D matrix specified by \p from. The from and to position info are specified
+/// by \p from_pos and \p to_pos The copied matrix size is specified by \p size.
+/// The value of \p direction is used to set the copy direction, it can be \a
+/// host_to_host, \a host_to_device, \a device_to_host, \a device_to_device or
+/// \a automatic. The function will return after the copy is completed.
+///
+/// \param to Destination matrix info.
+/// \param to_pos Position of destination.
+/// \param from Source matrix info.
+/// \param from_pos Position of destination.
+/// \param size Range of the submatrix to be copied.
+/// \param direction Direction of the copy.
+/// \param q Queue to execute the copy task.
+/// \returns no return value.
+static inline void dpct_memcpy(pitched_data to, sycl::id<3> to_pos,
+                               pitched_data from, sycl::id<3> from_pos,
+                               sycl::range<3> size,
+                               memcpy_direction direction = automatic,
+                               sycl::queue &q = dpct::get_default_queue()) {
+  sycl::event::wait(
+      detail::dpct_memcpy(q, to, to_pos, from, from_pos, size, direction));
+}
+
+/// Asynchronously copies a subset of a 3D matrix specified by \p to to another
+/// 3D matrix specified by \p from. The from and to position info are specified
+/// by \p from_pos and \p to_pos The copied matrix size is specified by \p size.
+/// The value of \p direction is used to set the copy direction, it can be \a
+/// host_to_host, \a host_to_device, \a device_to_host, \a device_to_device or
+/// \a automatic. The return of the function does NOT guarantee the copy is
+/// completed.
+///
+/// \param to Destination matrix info.
+/// \param to_pos Position of destination.
+/// \param from Source matrix info.
+/// \param from_pos Position of destination.
+/// \param size Range of the submatrix to be copied.
+/// \param direction Direction of the copy.
+/// \param q Queue to execute the copy task.
+/// \returns no return value.
+static inline void
+async_dpct_memcpy(pitched_data to, sycl::id<3> to_pos, pitched_data from,
+                  sycl::id<3> from_pos, sycl::range<3> size,
+                  memcpy_direction direction = automatic,
+                  sycl::queue &q = get_default_queue()) {
+  detail::dpct_memcpy(q, to, to_pos, from, from_pos, size, direction);
+}
+/**
+ * @brief Sets 1 byte data \p value to the first \p size elements starting from
+ * \p dev_ptr in \p q synchronously.
+ * @param [in] dev_ptr Pointer to the virtual device memory address.
+ * @param [in] value The value to be set.
+ * @param [in] size Number of elements to be set to the value.
+ * @param [in] q The queue in which the operation is done.
+ */
+static void dpct_memset(void *dev_ptr, int value, size_t size,
+                        sycl::queue &q = get_default_queue()) {
+  detail::dpct_memset<unsigned char>(q, dev_ptr, value, size).wait();
+}
+
+/**
+ * @brief Sets 2 bytes data \p value to the first \p size elements starting from
+ * \p dev_ptr in \p q synchronously.
+ * @param [in] dev_ptr Pointer to the virtual device memory address.
+ * @param [in] value The value to be set.
+ * @param [in] size Number of elements to be set to the value.
+ * @param [in] q The queue in which the operation is done.
+ */
+static void dpct_memset_d16(void *dev_ptr, unsigned short value, size_t size,
+                        sycl::queue &q = get_default_queue()) {
+  detail::dpct_memset(q, dev_ptr, value, size).wait();
+}
+/**
+ * @brief Sets 4 bytes data \p value to the first \p size elements starting from
+ * \p dev_ptr in \p q synchronously.
+ * @param [in] dev_ptr Pointer to the virtual device memory address.
+ * @param [in] value The value to be set.
+ * @param [in] size Number of elements to be set to the value.
+ * @param [in] q The queue in which the operation is done.
+ */
+static void dpct_memset_d32(void *dev_ptr, unsigned int value, size_t size,
+                        sycl::queue &q = get_default_queue()) {
+  detail::dpct_memset(q, dev_ptr, value, size).wait();
+}
+
+/**
+ * @brief Sets 1 byte data \p value to the first \p size elements starting from
+ * \p dev_ptr in \p q asynchronously.
+ * @param [in] dev_ptr Pointer to the virtual device memory address.
+ * @param [in] value The value to be set.
+ * @param [in] size Number of elements to be set to the value.
+ * @param [in] q The queue in which the operation is done.
+ */
+static void async_dpct_memset(void *dev_ptr, int value, size_t size,
+                              sycl::queue &q = dpct::get_default_queue()) {
+  detail::dpct_memset<unsigned char>(q, dev_ptr, value, size);
+}
+/**
+ * @brief Sets 2 bytes data \p value to the first \p size elements starting from
+ * \p dev_ptr in \p q asynchronously.
+ * @param [in] dev_ptr Pointer to the virtual device memory address.
+ * @param [in] value The value to be set.
+ * @param [in] size Number of elements to be set to the value.
+ * @param [in] q The queue in which the operation is done.
+ */
+static void async_dpct_memset_d16(void *dev_ptr, unsigned short value, size_t size,
+                              sycl::queue &q = dpct::get_default_queue()) {
+  detail::dpct_memset(q, dev_ptr, value, size);
+}
+/**
+ * @brief Sets 4 bytes data \p value to the first \p size elements starting from
+ * \p dev_ptr in \p q asynchronously.
+ * @param [in] dev_ptr Pointer to the virtual device memory address.
+ * @param [in] value The value to be set.
+ * @param [in] size Number of elements to be set to the value.
+ * @param [in] q The queue in which the operation is done.
+ */
+static void async_dpct_memset_d32(void *dev_ptr, unsigned int value, size_t size,
+                              sycl::queue &q = dpct::get_default_queue()) {
+  detail::dpct_memset(q, dev_ptr, value, size);
+}
+
+/**
+ * @brief Sets 1 byte data \p val to the pitched 2D memory region pointed by \p ptr in \p q
+ * synchronously.
+ * @param [in] ptr Pointer to the virtual device memory.
+ * @param [in] pitch The pitch size by number of elements, including padding.
+ * @param [in] val The value to be set.
+ * @param [in] x The width of memory region by number of elements.
+ * @param [in] y The height of memory region by number of elements.
+ * @param [in] q The queue in which the operation is done.
+ */
+static inline void dpct_memset(void *ptr, size_t pitch, int val, size_t x,
+                               size_t y,
+                               sycl::queue &q = get_default_queue()) {
+  sycl::event::wait(detail::dpct_memset<unsigned char>(q, ptr, pitch, val, x, y));
+}
+/**
+ * @brief Sets 2 bytes data \p val to the pitched 2D memory region pointed by \p ptr in \p q
+ * synchronously.
+ * @param [in] ptr Pointer to the virtual device memory.
+ * @param [in] pitch The pitch size by number of elements, including padding.
+ * @param [in] val The value to be set.
+ * @param [in] x The width of memory region by number of elements.
+ * @param [in] y The height of memory region by number of elements.
+ * @param [in] q The queue in which the operation is done.
+ */
+static inline void dpct_memset_d16(void *ptr, size_t pitch, unsigned short val, size_t x,
+                               size_t y,
+                               sycl::queue &q = get_default_queue()) {
+  sycl::event::wait(detail::dpct_memset(q, ptr, pitch, val, x, y));
+}
+/**
+ * @brief Sets 4 bytes data \p val to the pitched 2D memory region pointed by \p ptr in \p q
+ * synchronously.
+ * @param [in] ptr Pointer to the virtual device memory.
+ * @param [in] pitch The pitch size by number of elements, including padding.
+ * @param [in] val The value to be set.
+ * @param [in] x The width of memory region by number of elements.
+ * @param [in] y The height of memory region by number of elements.
+ * @param [in] q The queue in which the operation is done.
+ */
+static inline void dpct_memset_d32(void *ptr, size_t pitch, unsigned int val, size_t x,
+                               size_t y,
+                               sycl::queue &q = get_default_queue()) {
+  sycl::event::wait(detail::dpct_memset(q, ptr, pitch, val, x, y));
+}
+
+/**
+ * @brief Sets 1 byte data \p val to the pitched 2D memory region pointed by \p ptr in \p q
+ * asynchronously.
+ * @param [in] ptr Pointer to the virtual device memory.
+ * @param [in] pitch The pitch size by number of elements, including padding.
+ * @param [in] val The value to be set.
+ * @param [in] x The width of memory region by number of elements.
+ * @param [in] y The height of memory region by number of elements.
+ * @param [in] q The queue in which the operation is done.
+ */
+static inline void async_dpct_memset(void *ptr, size_t pitch, int val, size_t x,
+                                     size_t y,
+                                     sycl::queue &q = get_default_queue()) {
+  detail::dpct_memset<unsigned char>(q, ptr, pitch, val, x, y);
+}
+
+/**
+ * @brief Sets 2 bytes data \p val to the pitched 2D memory region pointed by \p ptr in \p q
+ * asynchronously.
+ * @param [in] ptr Pointer to the virtual device memory.
+ * @param [in] pitch The pitch size by number of elements, including padding.
+ * @param [in] val The value to be set.
+ * @param [in] x The width of memory region by number of elements.
+ * @param [in] y The height of memory region by number of elements.
+ * @param [in] q The queue in which the operation is done.
+ */
+static inline void async_dpct_memset_d16(void *ptr, size_t pitch,
+                                         unsigned short val, size_t x, size_t y,
+                                         sycl::queue &q = get_default_queue()) {
+  detail::dpct_memset(q, ptr, pitch, val, x, y);
+}
+
+/**
+ * @brief Sets 4 bytes data \p val to the pitched 2D memory region pointed by \p ptr in \p q
+ * asynchronously.
+ * @param [in] ptr Pointer to the virtual device memory.
+ * @param [in] pitch The pitch size by number of elements, including padding.
+ * @param [in] val The value to be set.
+ * @param [in] x The width of memory region by number of elements.
+ * @param [in] y The height of memory region by number of elements.
+ * @param [in] q The queue in which the operation is done.
+ */
+static inline void async_dpct_memset_d32(void *ptr, size_t pitch,
+                                         unsigned int val, size_t x, size_t y,
+                                         sycl::queue &q = get_default_queue()) {
+  detail::dpct_memset(q, ptr, pitch, val, x, y);
+}
+
+/**
+ * @brief Sets 1 byte data \p value to the 3D memory region pointed by \p data in \p q
+ * synchronously.
+ * @param [in] data Pointer to the pitched device memory region.
+ * @param [in] value The value to be set.
+ * @param [in] size 3D memory region by number of elements.
+ * @param [in] q The queue in which the operation is done.
+ */
+static inline void dpct_memset(pitched_data pitch, int val,
+                               sycl::range<3> size,
+                               sycl::queue &q = get_default_queue()) {
+  sycl::event::wait(detail::dpct_memset<unsigned char>(q, pitch, val, size));
+}
+
+/**
+ * @brief Sets 1 byte data \p value to the 3D memory region pointed by \p data in \p q
+ * asynchronously.
+ * @param [in] data Pointer to the pitched device memory region.
+ * @param [in] value The value to be set.
+ * @param [in] size 3D memory region by number of elements.
+ * @param [in] q The queue in which the operation is done.
+ */
+static inline void async_dpct_memset(pitched_data pitch, int val,
+                                     sycl::range<3> size,
+                                     sycl::queue &q = get_default_queue()) {
+  detail::dpct_memset<unsigned char>(q, pitch, val, size);
+}
+
+/// dpct accessor used as device function parameter.
+template <class T, memory_region Memory, size_t Dimension> class accessor;
+template <class T, memory_region Memory> class accessor<T, Memory, 3> {
+public:
+  using memory_t = detail::memory_traits<Memory, T>;
+  using element_t = typename memory_t::element_t;
+  using pointer_t = typename memory_t::pointer_t;
+  using accessor_t = typename memory_t::template accessor_t<3>;
+  accessor(pointer_t data, const sycl::range<3> &in_range)
+      : _data(data), _range(in_range) {}
+  template <memory_region M = Memory>
+  accessor(typename std::enable_if<M != local, const accessor_t>::type &acc)
+      : accessor(acc, acc.get_range()) {}
+  accessor(const accessor_t &acc, const sycl::range<3> &in_range)
+      : accessor(acc.get_pointer(), in_range) {}
+  accessor<T, Memory, 2> operator[](size_t index) const {
+    sycl::range<2> sub(_range.get(1), _range.get(2));
+    return accessor<T, Memory, 2>(_data + index * sub.size(), sub);
+  }
+
+  pointer_t get_ptr() const { return _data; }
+
+private:
+  pointer_t _data;
+  sycl::range<3> _range;
+};
+template <class T, memory_region Memory> class accessor<T, Memory, 2> {
+public:
+  using memory_t = detail::memory_traits<Memory, T>;
+  using element_t = typename memory_t::element_t;
+  using pointer_t = typename memory_t::pointer_t;
+  using accessor_t = typename memory_t::template accessor_t<2>;
+  accessor(pointer_t data, const sycl::range<2> &in_range)
+      : _data(data), _range(in_range) {}
+  template <memory_region M = Memory>
+  accessor(typename std::enable_if<M != local, const accessor_t>::type &acc)
+      : accessor(acc, acc.get_range()) {}
+  accessor(const accessor_t &acc, const sycl::range<2> &in_range)
+      : accessor(acc.get_pointer(), in_range) {}
+
+  pointer_t operator[](size_t index) const {
+    return _data + _range.get(1) * index;
+  }
+
+  pointer_t get_ptr() const { return _data; }
+
+private:
+  pointer_t _data;
+  sycl::range<2> _range;
+};
+
+namespace detail {
+/// Device variable with address space of shared, global or constant.
+template <class T, memory_region Memory, size_t Dimension>
+class device_memory {
+public:
+  using accessor_t =
+      typename detail::memory_traits<Memory, T>::template accessor_t<Dimension>;
+  using value_t = typename detail::memory_traits<Memory, T>::value_t;
+  using dpct_accessor_t = dpct::accessor<T, Memory, Dimension>;
+
+  device_memory() : device_memory(sycl::range<Dimension>(1)) {}
+
+  /// Constructor of 1-D array with initializer list
+  device_memory(
+      const sycl::range<Dimension> &in_range,
+      std::initializer_list<value_t> &&init_list)
+      : device_memory(in_range) {
+    assert(init_list.size() <= in_range.size());
+    _host_ptr = (value_t *)std::malloc(_size);
+    std::memset(_host_ptr, 0, _size);
+    std::memcpy(_host_ptr, init_list.begin(), init_list.size() * sizeof(T));
+  }
+
+  /// Constructor of 2-D array with initializer list
+  template <size_t D = Dimension>
+  device_memory(
+      const typename std::enable_if<D == 2, sycl::range<2>>::type &in_range,
+      std::initializer_list<std::initializer_list<value_t>> &&init_list)
+      : device_memory(in_range) {
+    assert(init_list.size() <= in_range[0]);
+    _host_ptr = (value_t *)std::malloc(_size);
+    std::memset(_host_ptr, 0, _size);
+    auto tmp_data = _host_ptr;
+    for (auto sub_list : init_list) {
+      assert(sub_list.size() <= in_range[1]);
+      std::memcpy(tmp_data, sub_list.begin(), sub_list.size() * sizeof(T));
+      tmp_data += in_range[1];
+    }
+  }
+
+  /// Constructor with range
+  device_memory(const sycl::range<Dimension> &range_in)
+      : _size(range_in.size() * sizeof(T)), _range(range_in), _reference(false),
+        _host_ptr(nullptr), _device_ptr(nullptr) {
+    static_assert(
+        (Memory == global) || (Memory == constant) || (Memory == shared),
+        "device memory region should be global, constant or shared");
+    // Make sure that singleton class mem_mgr and dev_mgr will destruct later
+    // than this.
+    detail::mem_mgr::instance();
+    dev_mgr::instance();
+  }
+
+  /// Constructor with range
+  template <class... Args>
+  device_memory(Args... Arguments)
+      : device_memory(sycl::range<Dimension>(Arguments...)) {}
+
+  ~device_memory() {
+    if (_device_ptr && !_reference)
+      dpct::dpct_free(_device_ptr);
+    if (_host_ptr)
+      std::free(_host_ptr);
+  }
+
+  /// Allocate memory with default queue, and init memory if has initial value.
+  void init() {
+    init(dpct::get_default_queue());
+  }
+  /// Allocate memory with specified queue, and init memory if has initial value.
+  void init(sycl::queue &q) {
+    if (_device_ptr)
+      return;
+    if (!_size)
+      return;
+    allocate_device(q);
+    if (_host_ptr)
+      detail::dpct_memcpy(q, _device_ptr, _host_ptr, _size, host_to_device);
+  }
+
+  /// The variable is assigned to a device pointer.
+  void assign(value_t *src, size_t size) {
+    this->~device_memory();
+    new (this) device_memory(src, size);
+  }
+
+  /// Get memory pointer of the memory object, which is virtual pointer when
+  /// usm is not used, and device pointer when usm is used.
+  value_t *get_ptr() {
+    return get_ptr(get_default_queue());
+  }
+  /// Get memory pointer of the memory object, which is virtual pointer when
+  /// usm is not used, and device pointer when usm is used.
+  value_t *get_ptr(sycl::queue &q) {
+    init(q);
+    return _device_ptr;
+  }
+
+  /// Get the device memory object size in bytes.
+  size_t get_size() { return _size; }
+
+  template <size_t D = Dimension>
+  typename std::enable_if<D == 1, T>::type &operator[](size_t index) {
+    init();
+#ifdef DPCT_USM_LEVEL_NONE
+    return dpct::get_buffer<typename std::enable_if<D == 1, T>::type>(
+               _device_ptr)
+        .template get_access<sycl::access_mode::read_write>()[index];
+#else
+    return _device_ptr[index];
+#endif // DPCT_USM_LEVEL_NONE
+  }
+
+#ifdef DPCT_USM_LEVEL_NONE
+  /// Get sycl::accessor for the device memory object when usm is not used.
+  accessor_t get_access(sycl::handler &cgh) {
+    return get_buffer(_device_ptr)
+        .template reinterpret<T, Dimension>(_range)
+        .template get_access<detail::memory_traits<Memory, T>::mode,
+                             detail::memory_traits<Memory, T>::target>(cgh);
+  }
+#else
+  /// Get dpct::accessor with dimension info for the device memory object
+  /// when usm is used and dimension is greater than 1.
+  template <size_t D = Dimension>
+  typename std::enable_if<D != 1, dpct_accessor_t>::type
+  get_access(sycl::handler &cgh) {
+    return dpct_accessor_t((T *)_device_ptr, _range);
+  }
+#endif // DPCT_USM_LEVEL_NONE
+
+private:
+  device_memory(value_t *memory_ptr, size_t size)
+      : _size(size), _range(size / sizeof(T)), _reference(true),
+        _device_ptr(memory_ptr) {}
+
+  void allocate_device(sycl::queue &q) {
+#ifndef DPCT_USM_LEVEL_NONE
+    if (Memory == shared) {
+      _device_ptr = (value_t *)sycl::malloc_shared(
+          _size, q.get_device(), q.get_context());
+      return;
+    }
+#ifdef SYCL_EXT_ONEAPI_USM_DEVICE_READ_ONLY
+    if (Memory == constant) {
+      _device_ptr = (value_t *)sycl::malloc_device(
+          _size, q.get_device(), q.get_context(),
+          sycl::ext::oneapi::property::usm::device_read_only());
+      return;
+    }
+#endif
+#endif
+    _device_ptr = (value_t *)detail::dpct_malloc(_size, q);
+  }
+
+  size_t _size;
+  sycl::range<Dimension> _range;
+  bool _reference;
+  value_t *_host_ptr;
+  value_t *_device_ptr;
+};
+template <class T, memory_region Memory>
+class device_memory<T, Memory, 0> : public device_memory<T, Memory, 1> {
+public:
+  using base = device_memory<T, Memory, 1>;
+  using value_t = typename base::value_t;
+  using accessor_t =
+      typename detail::memory_traits<Memory, T>::template accessor_t<0>;
+
+  /// Constructor with initial value.
+  device_memory(const value_t &val) : base(sycl::range<1>(1), {val}) {}
+
+  /// Default constructor
+  device_memory() : base(1) {}
+
+#ifdef DPCT_USM_LEVEL_NONE
+  /// Get sycl::accessor for the device memory object when usm is not used.
+  accessor_t get_access(sycl::handler &cgh) {
+    auto buf = get_buffer(base::get_ptr())
+                   .template reinterpret<T, 1>(sycl::range<1>(1));
+    return accessor_t(buf, cgh);
+  }
+#endif // DPCT_USM_LEVEL_NONE
+};
+}
+
+template <class T, size_t Dimension>
+using global_memory = detail::device_memory<T, global, Dimension>;
+template <class T, size_t Dimension>
+using constant_memory = detail::device_memory<T, constant, Dimension>;
+template <class T, size_t Dimension>
+using shared_memory = detail::device_memory<T, shared, Dimension>;
+
+// dpct::deprecated:: is for functionality that was introduced for compatibility
+// purpose, but relies on deprecated C++ features, which are either removed or
+// will be removed in the future standards.
+// Direct use of deprecated functionality in this namespace should be avoided.
+namespace deprecated {
+
+template <typename T>
+using usm_host_allocator = detail::deprecated::usm_allocator<T, sycl::usm::alloc::host>;
+
+template <typename T>
+using usm_device_allocator = detail::deprecated::usm_allocator<T, sycl::usm::alloc::shared>;
+} // namespace deprecated
+
+class pointer_attributes {
+public:
+  void init(const void *ptr,
+              sycl::queue &q = dpct::get_default_queue()) {
+#ifdef DPCT_USM_LEVEL_NONE
+    throw std::runtime_error(
+          "dpct::pointer_attributes: only works for USM pointer.");
+#else
+    memory_type = sycl::get_pointer_type(ptr, q.get_context());
+    device_pointer = (memory_type !=
+                        sycl::usm::alloc::unknown) ? ptr : nullptr;
+    host_pointer = (memory_type !=
+                        sycl::usm::alloc::unknown) &&
+                   (memory_type != sycl::usm::alloc::device) ? ptr : nullptr;
+    sycl::device device_obj = sycl::get_pointer_device(ptr, q.get_context());
+    device_id = dpct::dev_mgr::instance().get_device_id(device_obj);
+#endif
+  }
+
+  sycl::usm::alloc get_memory_type() {
+    return memory_type;
+  }
+
+  const void *get_device_pointer() {
+    return device_pointer;
+  }
+
+  const void *get_host_pointer() {
+    return host_pointer;
+  }
+
+  bool is_memory_shared() {
+    return memory_type == sycl::usm::alloc::shared;
+  }
+
+  unsigned int get_device_id() {
+    return device_id;
+  }
+
+private:
+  sycl::usm::alloc memory_type = sycl::usm::alloc::unknown;
+  const void *device_pointer = nullptr;
+  const void *host_pointer = nullptr;
+  unsigned int device_id = 0;
+};
+} // namespace dpct
+#endif // __DPCT_MEMORY_HPP__
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/rng_utils.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/rng_utils.hpp
new file mode 100644
index 0000000..6c79ca5
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/rng_utils.hpp
@@ -0,0 +1,535 @@
+//==---- rng_utils.hpp ----------------------------*- C++ -*----------------==//
+//
+// Copyright (C) Intel Corporation
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+// See https://llvm.org/LICENSE.txt for license information.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef __DPCT_RNG_UTILS_HPP__
+#define __DPCT_RNG_UTILS_HPP__
+
+#include <sycl/sycl.hpp>
+#include <oneapi/mkl.hpp>
+#ifdef __INTEL_MKL__ // The oneMKL Interfaces Project does not support this.
+#include <oneapi/mkl/rng/device.hpp>
+#endif
+#include "device.hpp"
+#include "lib_common_utils.hpp"
+
+namespace dpct {
+namespace rng {
+#ifdef __INTEL_MKL__ // The oneMKL Interfaces Project does not support this.
+namespace device {
+/// The random number generator on device.
+/// \tparam engine_t The device random number generator engine. It can only be
+/// oneapi::mkl::rng::device::mrg32k3a<1> or
+/// oneapi::mkl::rng::device::mrg32k3a<4> or
+/// oneapi::mkl::rng::device::philox4x32x10<1> or
+/// oneapi::mkl::rng::device::philox4x32x10<4>.
+template <typename engine_t> class rng_generator {
+  static_assert(
+      std::disjunction_v<
+          std::is_same<engine_t, oneapi::mkl::rng::device::mrg32k3a<1>>,
+          std::is_same<engine_t, oneapi::mkl::rng::device::mrg32k3a<4>>,
+          std::is_same<engine_t, oneapi::mkl::rng::device::philox4x32x10<1>>,
+          std::is_same<engine_t, oneapi::mkl::rng::device::philox4x32x10<4>>,
+          std::is_same<engine_t, oneapi::mkl::rng::device::mcg59<1>>>,
+      "engine_t can only be oneapi::mkl::rng::device::mrg32k3a<1> or "
+      "oneapi::mkl::rng::device::mrg32k3a<4> or "
+      "oneapi::mkl::rng::device::philox4x32x10<1> or "
+      "oneapi::mkl::rng::device::philox4x32x10<4> or "
+      "oneapi::mkl::rng::device::mcg59<1>.");
+  static constexpr bool _is_engine_vec_size_one = std::disjunction_v<
+      std::is_same<engine_t, oneapi::mkl::rng::device::mrg32k3a<1>>,
+      std::is_same<engine_t, oneapi::mkl::rng::device::philox4x32x10<1>>,
+      std::is_same<engine_t, oneapi::mkl::rng::device::mcg59<1>>>;
+  static constexpr std::uint64_t default_seed = 0;
+  oneapi::mkl::rng::device::bits<std::uint32_t> _distr_bits;
+  oneapi::mkl::rng::device::uniform_bits<std::uint32_t> _distr_uniform_bits;
+  oneapi::mkl::rng::device::gaussian<float> _distr_gaussian_float;
+  oneapi::mkl::rng::device::gaussian<double> _distr_gaussian_double;
+  oneapi::mkl::rng::device::lognormal<float> _distr_lognormal_float;
+  oneapi::mkl::rng::device::lognormal<double> _distr_lognormal_double;
+  oneapi::mkl::rng::device::poisson<std::uint32_t> _distr_poisson;
+  oneapi::mkl::rng::device::uniform<float> _distr_uniform_float;
+  oneapi::mkl::rng::device::uniform<double> _distr_uniform_double;
+  engine_t _engine;
+
+public:
+  /// Default constructor of rng_generator
+  rng_generator() { _engine = engine_t(default_seed); }
+  /// Constructor of rng_generator if engine type is not mcg59
+  /// \param [in] seed The seed to initialize the engine state.
+  /// \param [in] num_to_skip Set the number of elements need to be skipped.
+  /// The number is calculated as: num_to_skip[0] + num_to_skip[1] * 2^64 +
+  /// num_to_skip[2] * 2^128 + ... + num_to_skip[n-1] * 2^(64*(n-1))
+  template <typename T = engine_t,
+            typename std::enable_if<!std::is_same_v<
+                T, oneapi::mkl::rng::device::mcg59<1>>>::type * = nullptr>
+  rng_generator(std::uint64_t seed,
+                std::initializer_list<std::uint64_t> num_to_skip) {
+    _engine = engine_t(seed, num_to_skip);
+  }
+  /// Constructor of rng_generator if engine type is mcg59
+  /// \param [in] seed The seed to initialize the engine state.
+  /// \param [in] num_to_skip Set the number of elements need to be skipped.
+  template <typename T = engine_t,
+            typename std::enable_if<std::is_same_v<
+                T, oneapi::mkl::rng::device::mcg59<1>>>::type * = nullptr>
+  rng_generator(std::uint64_t seed, std::uint64_t num_to_skip) {
+    _engine = engine_t(seed, num_to_skip);
+  }
+
+  /// Generate random number(s) obeys distribution \tparam distr_t.
+  /// \tparam T The distribution of the random number. It can only be
+  /// oneapi::mkl::rng::device::bits<std::uint32_t>,
+  /// oneapi::mkl::rng::device::uniform_bits<std::uint32_t>,
+  /// oneapi::mkl::rng::device::gaussian<float>,
+  /// oneapi::mkl::rng::device::gaussian<double>,
+  /// oneapi::mkl::rng::device::lognormal<float>,
+  /// oneapi::mkl::rng::device::lognormal<double>,
+  /// oneapi::mkl::rng::device::poisson<std::uint32_t>,
+  /// oneapi::mkl::rng::device::uniform<float> or
+  /// oneapi::mkl::rng::device::uniform<double>
+  /// \tparam vec_size The length of the return vector. It can only be 1, 2
+  /// or 4.
+  /// \param distr_params The parameter(s) for lognormal or poisson
+  /// distribution.
+  /// \return The vector of the random number(s).
+  template <typename distr_t, int vec_size, class... distr_params_t>
+  auto generate(distr_params_t... distr_params) {
+    static_assert(vec_size == 1 || vec_size == 2 || vec_size == 4,
+                  "vec_size is not supported.");
+    static_assert(
+        std::disjunction_v<
+            std::is_same<distr_t,
+                         oneapi::mkl::rng::device::bits<std::uint32_t>>,
+            std::is_same<distr_t,
+                         oneapi::mkl::rng::device::uniform_bits<std::uint32_t>>,
+            std::is_same<distr_t, oneapi::mkl::rng::device::gaussian<float>>,
+            std::is_same<distr_t, oneapi::mkl::rng::device::gaussian<double>>,
+            std::is_same<distr_t, oneapi::mkl::rng::device::lognormal<float>>,
+            std::is_same<distr_t, oneapi::mkl::rng::device::lognormal<double>>,
+            std::is_same<distr_t,
+                         oneapi::mkl::rng::device::poisson<std::uint32_t>>,
+            std::is_same<distr_t, oneapi::mkl::rng::device::uniform<float>>,
+            std::is_same<distr_t, oneapi::mkl::rng::device::uniform<double>>>,
+        "distribution is not supported.");
+
+    if constexpr (std::is_same_v<
+                      distr_t, oneapi::mkl::rng::device::bits<std::uint32_t>>) {
+      return generate_vec<vec_size>(_distr_bits);
+    }
+    if constexpr (std::is_same_v<
+                      distr_t,
+                      oneapi::mkl::rng::device::uniform_bits<std::uint32_t>>) {
+      return generate_vec<vec_size>(_distr_uniform_bits);
+    }
+    if constexpr (std::is_same_v<distr_t,
+                                 oneapi::mkl::rng::device::gaussian<float>>) {
+      return generate_vec<vec_size>(_distr_gaussian_float);
+    }
+    if constexpr (std::is_same_v<distr_t,
+                                 oneapi::mkl::rng::device::gaussian<double>>) {
+      return generate_vec<vec_size>(_distr_gaussian_double);
+    }
+    if constexpr (std::is_same_v<distr_t,
+                                 oneapi::mkl::rng::device::lognormal<float>>) {
+      return generate_vec<vec_size>(_distr_lognormal_float, distr_params...,
+                                    0.0f, 1.0f);
+    }
+    if constexpr (std::is_same_v<distr_t,
+                                 oneapi::mkl::rng::device::lognormal<double>>) {
+      return generate_vec<vec_size>(_distr_lognormal_double, distr_params...,
+                                    0.0, 1.0);
+    }
+    if constexpr (std::is_same_v<distr_t, oneapi::mkl::rng::device::poisson<
+                                              std::uint32_t>>) {
+      return generate_vec<vec_size>(_distr_poisson, distr_params...);
+    }
+    if constexpr (std::is_same_v<distr_t,
+                                 oneapi::mkl::rng::device::uniform<float>>) {
+      return generate_vec<vec_size>(_distr_uniform_float);
+    }
+    if constexpr (std::is_same_v<distr_t,
+                                 oneapi::mkl::rng::device::uniform<double>>) {
+      return generate_vec<vec_size>(_distr_uniform_double);
+    }
+  }
+
+  /// Get the random number generator engine.
+  /// \return The reference of the internal random number generator engine.
+  engine_t &get_engine() { return _engine; }
+
+private:
+  template <int vec_size, typename distr_t, class... distr_params_t>
+  auto generate_vec(distr_t &distr, distr_params_t... distr_params) {
+    if constexpr (sizeof...(distr_params_t)) {
+      typename distr_t::param_type pt(distr_params...);
+      distr.param(pt);
+    }
+    if constexpr (vec_size == 4) {
+      if constexpr (_is_engine_vec_size_one) {
+        sycl::vec<typename distr_t::result_type, 4> res;
+        res.x() = oneapi::mkl::rng::device::generate(distr, _engine);
+        res.y() = oneapi::mkl::rng::device::generate(distr, _engine);
+        res.z() = oneapi::mkl::rng::device::generate(distr, _engine);
+        res.w() = oneapi::mkl::rng::device::generate(distr, _engine);
+        return res;
+      } else {
+        return oneapi::mkl::rng::device::generate(distr, _engine);
+      }
+    } else if constexpr (vec_size == 1) {
+      if constexpr (_is_engine_vec_size_one) {
+        return oneapi::mkl::rng::device::generate(distr, _engine);
+      } else {
+        return oneapi::mkl::rng::device::generate_single(distr, _engine);
+      }
+    } else if constexpr (vec_size == 2) {
+      if constexpr (_is_engine_vec_size_one) {
+        sycl::vec<typename distr_t::result_type, 2> res;
+        res.x() = oneapi::mkl::rng::device::generate(distr, _engine);
+        res.y() = oneapi::mkl::rng::device::generate(distr, _engine);
+        return res;
+      } else {
+        sycl::vec<typename distr_t::result_type, 2> res;
+        res.x() = oneapi::mkl::rng::device::generate_single(distr, _engine);
+        res.y() = oneapi::mkl::rng::device::generate_single(distr, _engine);
+        return res;
+      }
+    }
+  }
+};
+
+} // namespace device
+#endif
+
+namespace host {
+namespace detail {
+class rng_generator_base {
+public:
+  /// Set the seed of host rng_generator.
+  /// \param seed The engine seed.
+  virtual void set_seed(const std::uint64_t seed) = 0;
+
+  /// Set the dimensions of host rng_generator.
+  /// \param dimensions The engine dimensions.
+  virtual void set_dimensions(const std::uint32_t dimensions) = 0;
+
+  /// Set the queue of host rng_generator.
+  /// \param queue The engine queue.
+  virtual void set_queue(sycl::queue *queue) = 0;
+
+  /// Generate unsigned int random number(s) with 'uniform_bits' distribution.
+  /// \param output The pointer of the first random number.
+  /// \param n The number of random numbers.
+  virtual inline void generate_uniform_bits(unsigned int *output,
+                                            std::int64_t n) = 0;
+
+  /// Generate unsigned long long random number(s) with 'uniform_bits'
+  /// distribution.
+  /// \param output The pointer of the first random number.
+  /// \param n The number of random numbers.
+  virtual inline void generate_uniform_bits(unsigned long long *output,
+                                            std::int64_t n) = 0;
+
+  /// Generate float random number(s) with 'lognormal' distribution.
+  /// \param output The pointer of the first random number.
+  /// \param n The number of random numbers.
+  /// \param m Mean of associated normal distribution
+  /// \param s Standard deviation of associated normal distribution.
+  virtual inline void generate_lognormal(float *output, std::int64_t n, float m,
+                                         float s) = 0;
+
+  /// Generate double random number(s) with 'lognormal' distribution.
+  /// \param output The pointer of the first random number.
+  /// \param n The number of random numbers.
+  /// \param m Mean of associated normal distribution
+  /// \param s Standard deviation of associated normal distribution.
+  virtual inline void generate_lognormal(double *output, std::int64_t n,
+                                         double m, double s) = 0;
+
+  /// Generate float random number(s) with 'gaussian' distribution.
+  /// \param output The pointer of the first random number.
+  /// \param n The number of random numbers.
+  /// \param mean Mean of normal distribution
+  /// \param stddev Standard deviation of normal distribution.
+  virtual inline void generate_gaussian(float *output, std::int64_t n,
+                                        float mean, float stddev) = 0;
+
+  /// Generate double random number(s) with 'gaussian' distribution.
+  /// \param output The pointer of the first random number.
+  /// \param n The number of random numbers.
+  /// \param mean Mean of normal distribution
+  /// \param stddev Standard deviation of normal distribution.
+  virtual inline void generate_gaussian(double *output, std::int64_t n,
+                                        double mean, double stddev) = 0;
+
+  /// Generate unsigned int random number(s) with 'poisson' distribution.
+  /// \param output The pointer of the first random number.
+  /// \param n The number of random numbers.
+  /// \param lambda Lambda for the Poisson distribution.
+  virtual inline void generate_poisson(unsigned int *output, std::int64_t n,
+                                       double lambda) = 0;
+
+  /// Generate float random number(s) with 'uniform' distribution.
+  /// \param output The pointer of the first random number.
+  /// \param n The number of random numbers.
+  virtual inline void generate_uniform(float *output, std::int64_t n) = 0;
+
+  /// Generate double random number(s) with 'uniform' distribution.
+  /// \param output The pointer of the first random number.
+  /// \param n The number of random numbers.
+  virtual inline void generate_uniform(double *output, std::int64_t n) = 0;
+
+  /// Skip ahead several random number(s).
+  /// \param num_to_skip The number of random numbers to be skipped.
+  virtual void skip_ahead(const std::uint64_t num_to_skip) = 0;
+
+  /// Set the direction numbers of host rng_generator. Only Sobol engine
+  /// supports this method.
+  /// \param direction_numbers The engine direction numbers.
+  virtual void set_direction_numbers(
+      const std::vector<std::uint32_t> &direction_numbers) = 0;
+
+protected:
+  sycl::queue *_queue{&dpct::get_default_queue()};
+  std::uint64_t _seed{0};
+  std::uint32_t _dimensions{1};
+  std::vector<std::uint32_t> _direction_numbers;
+};
+
+/// The random number generator on host.
+template <typename engine_t = oneapi::mkl::rng::philox4x32x10>
+class rng_generator : public rng_generator_base {
+public:
+  /// Constructor of rng_generator.
+  rng_generator() : _engine(create_engine(_queue, _seed, _dimensions)) {}
+
+  /// Set the seed of host rng_generator.
+  /// \param seed The engine seed.
+  void set_seed(const std::uint64_t seed) {
+    if (seed == _seed) {
+      return;
+    }
+    _seed = seed;
+    _engine = create_engine(_queue, _seed, _dimensions);
+  }
+
+  /// Set the dimensions of host rng_generator.
+  /// \param dimensions The engine dimensions.
+  void set_dimensions(const std::uint32_t dimensions) {
+    if (dimensions == _dimensions) {
+      return;
+    }
+    _dimensions = dimensions;
+    _engine = create_engine(_queue, _seed, _dimensions);
+  }
+
+  /// Set the queue of host rng_generator.
+  /// \param queue The engine queue.
+  void set_queue(sycl::queue *queue) {
+    if (queue == _queue) {
+      return;
+    }
+    _queue = queue;
+    _engine = create_engine(_queue, _seed, _dimensions);
+  }
+
+  /// Set the direction numbers of Sobol host rng_generator.
+  /// \param direction_numbers The user-defined direction numbers.
+  void
+  set_direction_numbers(const std::vector<std::uint32_t> &direction_numbers) {
+#ifndef __INTEL_MKL__
+    throw std::runtime_error("The oneAPI Math Kernel Library (oneMKL) "
+                             "Interfaces Project does not support this API.");
+#else
+    if constexpr (std::is_same_v<engine_t, oneapi::mkl::rng::sobol>) {
+      if (direction_numbers == _direction_numbers) {
+        return;
+      }
+      _direction_numbers = direction_numbers;
+      _engine = oneapi::mkl::rng::sobol(*_queue, _direction_numbers);
+    } else {
+      throw std::runtime_error("Only Sobol engine supports this method.");
+    }
+#endif
+  }
+
+  /// Generate unsigned int random number(s) with 'uniform_bits' distribution.
+  /// \param output The pointer of the first random number.
+  /// \param n The number of random numbers.
+  inline void generate_uniform_bits(unsigned int *output, std::int64_t n) {
+#ifndef __INTEL_MKL__
+    throw std::runtime_error("The oneAPI Math Kernel Library (oneMKL) "
+                             "Interfaces Project does not support this API.");
+#else
+    static_assert(sizeof(unsigned int) == sizeof(std::uint32_t));
+    generate<oneapi::mkl::rng::uniform_bits<std::uint32_t>>(
+        (std::uint32_t *)output, n);
+#endif
+  }
+
+  /// Generate unsigned long long random number(s) with 'uniform_bits'
+  /// distribution.
+  /// \param output The pointer of the first random number.
+  /// \param n The number of random numbers.
+  inline void generate_uniform_bits(unsigned long long *output,
+                                    std::int64_t n) {
+#ifndef __INTEL_MKL__
+    throw std::runtime_error("The oneAPI Math Kernel Library (oneMKL) "
+                             "Interfaces Project does not support this API.");
+#else
+    static_assert(sizeof(unsigned long long) == sizeof(std::uint64_t));
+    generate<oneapi::mkl::rng::uniform_bits<std::uint64_t>>(
+        (std::uint64_t *)output, n);
+#endif
+  }
+
+  /// Generate float random number(s) with 'lognormal' distribution.
+  /// \param output The pointer of the first random number.
+  /// \param n The number of random numbers.
+  /// \param m Mean of associated normal distribution
+  /// \param s Standard deviation of associated normal distribution.
+  inline void generate_lognormal(float *output, std::int64_t n, float m,
+                                 float s) {
+    generate<oneapi::mkl::rng::lognormal<float>>(output, n, m, s);
+  }
+
+  /// Generate double random number(s) with 'lognormal' distribution.
+  /// \param output The pointer of the first random number.
+  /// \param n The number of random numbers.
+  /// \param m Mean of associated normal distribution
+  /// \param s Standard deviation of associated normal distribution.
+  inline void generate_lognormal(double *output, std::int64_t n, double m,
+                                 double s) {
+    generate<oneapi::mkl::rng::lognormal<double>>(output, n, m, s);
+  }
+
+  /// Generate float random number(s) with 'gaussian' distribution.
+  /// \param output The pointer of the first random number.
+  /// \param n The number of random numbers.
+  /// \param mean Mean of normal distribution
+  /// \param stddev Standard deviation of normal distribution.
+  inline void generate_gaussian(float *output, std::int64_t n, float mean,
+                                float stddev) {
+    generate<oneapi::mkl::rng::gaussian<float>>(output, n, mean, stddev);
+  }
+
+  /// Generate double random number(s) with 'gaussian' distribution.
+  /// \param output The pointer of the first random number.
+  /// \param n The number of random numbers.
+  /// \param mean Mean of normal distribution
+  /// \param stddev Standard deviation of normal distribution.
+  inline void generate_gaussian(double *output, std::int64_t n, double mean,
+                                double stddev) {
+    generate<oneapi::mkl::rng::gaussian<double>>(output, n, mean, stddev);
+  }
+
+  /// Generate unsigned int random number(s) with 'poisson' distribution.
+  /// \param output The pointer of the first random number.
+  /// \param n The number of random numbers.
+  /// \param lambda Lambda for the Poisson distribution.
+  inline void generate_poisson(unsigned int *output, std::int64_t n,
+                               double lambda) {
+    generate<oneapi::mkl::rng::poisson<unsigned int>>(output, n, lambda);
+  }
+
+  /// Generate float random number(s) with 'uniform' distribution.
+  /// \param output The pointer of the first random number.
+  /// \param n The number of random numbers.
+  inline void generate_uniform(float *output, std::int64_t n) {
+    generate<oneapi::mkl::rng::uniform<float>>(output, n);
+  }
+
+  /// Generate double random number(s) with 'uniform' distribution.
+  /// \param output The pointer of the first random number.
+  /// \param n The number of random numbers.
+  inline void generate_uniform(double *output, std::int64_t n) {
+    generate<oneapi::mkl::rng::uniform<double>>(output, n);
+  }
+
+  /// Skip ahead several random number(s).
+  /// \param num_to_skip The number of random numbers to be skipped.
+  void skip_ahead(const std::uint64_t num_to_skip) {
+#ifndef __INTEL_MKL__
+    oneapi::mkl::rng::skip_ahead(_engine, num_to_skip);
+#else
+    if constexpr (std::is_same_v<engine_t, oneapi::mkl::rng::mt2203>)
+      throw std::runtime_error("no skip_ahead method of mt2203 engine.");
+    else
+      oneapi::mkl::rng::skip_ahead(_engine, num_to_skip);
+#endif
+  }
+
+private:
+  static inline engine_t create_engine(sycl::queue *queue,
+                                       const std::uint64_t seed,
+                                       const std::uint32_t dimensions) {
+#ifdef __INTEL_MKL__
+    return std::is_same_v<engine_t, oneapi::mkl::rng::sobol>
+               ? engine_t(*queue, dimensions)
+               : engine_t(*queue, seed);
+#else
+    return engine_t(*queue, seed);
+#endif
+  }
+
+  template <typename distr_t, typename buffer_t, class... distr_params_t>
+  void generate(buffer_t *output, const std::int64_t n,
+                const distr_params_t... distr_params) {
+    auto output_buf = dpct::detail::get_memory<buffer_t>(output);
+    oneapi::mkl::rng::generate(distr_t(distr_params...), _engine, n,
+                               output_buf);
+  }
+  engine_t _engine{};
+};
+} // namespace detail
+} // namespace host
+
+enum class random_engine_type {
+  philox4x32x10,
+  mrg32k3a,
+  mt2203,
+  mt19937,
+  sobol,
+  mcg59
+};
+
+typedef std::shared_ptr<rng::host::detail::rng_generator_base> host_rng_ptr;
+
+/// Create a host random number generator.
+/// \param type The random engine type.
+/// \return The pointer of random number generator.
+inline host_rng_ptr create_host_rng(const random_engine_type type) {
+  switch (type) {
+  case random_engine_type::philox4x32x10:
+    return std::make_shared<
+        rng::host::detail::rng_generator<oneapi::mkl::rng::philox4x32x10>>();
+  case random_engine_type::mrg32k3a:
+    return std::make_shared<
+        rng::host::detail::rng_generator<oneapi::mkl::rng::mrg32k3a>>();
+#ifndef __INTEL_MKL__
+    throw std::runtime_error("The oneAPI Math Kernel Library (oneMKL) "
+                             "Interfaces Project does not support this API.");
+#else
+  case random_engine_type::mt2203:
+    return std::make_shared<
+        rng::host::detail::rng_generator<oneapi::mkl::rng::mt2203>>();
+  case random_engine_type::mt19937:
+    return std::make_shared<
+        rng::host::detail::rng_generator<oneapi::mkl::rng::mt19937>>();
+  case random_engine_type::sobol:
+    return std::make_shared<
+        rng::host::detail::rng_generator<oneapi::mkl::rng::sobol>>();
+  case random_engine_type::mcg59:
+    return std::make_shared<
+        rng::host::detail::rng_generator<oneapi::mkl::rng::mcg59>>();
+#endif
+  }
+}
+} // namespace rng
+} // namespace dpct
+
+#endif // __DPCT_RNG_UTILS_HPP__
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/sparse_utils.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/sparse_utils.hpp
new file mode 100644
index 0000000..d65526e
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/sparse_utils.hpp
@@ -0,0 +1,1171 @@
+//==---- sparse_utils.hpp -------------------------*- C++ -*----------------==//
+//
+// Copyright (C) Intel Corporation
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+// See https://llvm.org/LICENSE.txt for license information.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef __DPCT_SPARSE_UTILS_HPP__
+#define __DPCT_SPARSE_UTILS_HPP__
+
+#include "lib_common_utils.hpp"
+#include <oneapi/mkl.hpp>
+#include <sycl/sycl.hpp>
+
+namespace dpct {
+namespace sparse {
+/// Describes properties of a sparse matrix.
+/// The properties are matrix type, diag, uplo and index base.
+class matrix_info {
+public:
+  /// Matrix types are:
+  /// ge: General matrix
+  /// sy: Symmetric matrix
+  /// he: Hermitian matrix
+  /// tr: Triangular matrix
+  enum class matrix_type : int { ge = 0, sy, he, tr };
+
+  auto get_matrix_type() const { return _matrix_type; }
+  auto get_diag() const { return _diag; }
+  auto get_uplo() const { return _uplo; }
+  auto get_index_base() const { return _index_base; }
+  void set_matrix_type(matrix_type mt) { _matrix_type = mt; }
+  void set_diag(oneapi::mkl::diag d) { _diag = d; }
+  void set_uplo(oneapi::mkl::uplo u) { _uplo = u; }
+  void set_index_base(oneapi::mkl::index_base ib) { _index_base = ib; }
+
+private:
+  matrix_type _matrix_type = matrix_type::ge;
+  oneapi::mkl::diag _diag = oneapi::mkl::diag::nonunit;
+  oneapi::mkl::uplo _uplo = oneapi::mkl::uplo::upper;
+  oneapi::mkl::index_base _index_base = oneapi::mkl::index_base::zero;
+};
+
+namespace detail {
+template <template <typename> typename functor_t, typename... args_t>
+inline void spblas_shim(library_data_t type, args_t &&...args) {
+  switch (type) {
+  case library_data_t::real_float: {
+    functor_t<float>()(std::forward<args_t>(args)...);
+    break;
+  }
+  case library_data_t::real_double: {
+    functor_t<double>()(std::forward<args_t>(args)...);
+    break;
+  }
+  case library_data_t::complex_float: {
+    functor_t<std::complex<float>>()(std::forward<args_t>(args)...);
+    break;
+  }
+  case library_data_t::complex_double: {
+    functor_t<std::complex<double>>()(std::forward<args_t>(args)...);
+    break;
+  }
+  default:
+    throw std::runtime_error("The data type is not supported.");
+  }
+}
+
+template <typename T> struct csrmv_impl {
+  void operator()(sycl::queue &queue, oneapi::mkl::transpose trans,
+                  int num_rows, int num_cols, const void *alpha,
+                  const std::shared_ptr<matrix_info> info, const void *val,
+                  const int *row_ptr, const int *col_ind, const void *x,
+                  const void *beta, void *y) {
+#ifndef __INTEL_MKL__
+    throw std::runtime_error(
+        "The oneAPI Math Kernel Library (oneMKL) Interfaces "
+        "Project does not support this API.");
+#else
+    using Ty = typename dpct::DataType<T>::T2;
+    auto alpha_value =
+        dpct::detail::get_value(reinterpret_cast<const Ty *>(alpha), queue);
+    auto beta_value =
+        dpct::detail::get_value(reinterpret_cast<const Ty *>(beta), queue);
+
+    oneapi::mkl::sparse::matrix_handle_t *sparse_matrix_handle =
+        new oneapi::mkl::sparse::matrix_handle_t;
+    oneapi::mkl::sparse::init_matrix_handle(sparse_matrix_handle);
+    auto data_row_ptr = dpct::detail::get_memory<int>(row_ptr);
+    auto data_col_ind = dpct::detail::get_memory<int>(col_ind);
+    auto data_val = dpct::detail::get_memory<Ty>(val);
+    oneapi::mkl::sparse::set_csr_data(queue, *sparse_matrix_handle, num_rows,
+                                      num_cols, info->get_index_base(),
+                                      data_row_ptr, data_col_ind, data_val);
+
+    auto data_x = dpct::detail::get_memory<Ty>(x);
+    auto data_y = dpct::detail::get_memory<Ty>(y);
+    switch (info->get_matrix_type()) {
+    case matrix_info::matrix_type::ge: {
+      oneapi::mkl::sparse::optimize_gemv(queue, trans, *sparse_matrix_handle);
+      oneapi::mkl::sparse::gemv(queue, trans, alpha_value,
+                                *sparse_matrix_handle, data_x, beta_value,
+                                data_y);
+      break;
+    }
+    case matrix_info::matrix_type::sy: {
+      oneapi::mkl::sparse::symv(queue, info->get_uplo(), alpha_value,
+                                *sparse_matrix_handle, data_x, beta_value,
+                                data_y);
+      break;
+    }
+    case matrix_info::matrix_type::tr: {
+      oneapi::mkl::sparse::optimize_trmv(queue, info->get_uplo(), trans,
+                                         info->get_diag(),
+                                         *sparse_matrix_handle);
+      oneapi::mkl::sparse::trmv(
+          queue, info->get_uplo(), trans, info->get_diag(), alpha_value,
+          *sparse_matrix_handle, data_x, beta_value, data_y);
+      break;
+    }
+    default:
+      throw std::runtime_error(
+          "the spmv does not support matrix_info::matrix_type::he");
+    }
+
+    sycl::event e =
+        oneapi::mkl::sparse::release_matrix_handle(queue, sparse_matrix_handle);
+    queue.submit([&](sycl::handler &cgh) {
+      cgh.depends_on(e);
+      cgh.host_task([=] { delete sparse_matrix_handle; });
+    });
+#endif
+  }
+};
+} // namespace detail
+
+/// Computes a CSR format sparse matrix-dense vector product.
+/// y = alpha * op(A) * x + beta * y
+/// \param [in] queue The queue where the routine should be executed. It must
+/// have the in_order property when using the USM mode.
+/// \param [in] trans The operation applied to the matrix A.
+/// \param [in] num_rows Number of rows of the matrix A.
+/// \param [in] num_cols Number of columns of the matrix A.
+/// \param [in] alpha Scaling factor for the matrix A.
+/// \param [in] info Matrix info of the matrix A.
+/// \param [in] val An array containing the non-zero elements of the matrix A.
+/// \param [in] row_ptr An array of length \p num_rows + 1.
+/// \param [in] col_ind An array containing the column indices in index-based
+/// numbering.
+/// \param [in] x Data of the vector x.
+/// \param [in] beta Scaling factor for the vector x.
+/// \param [in, out] y Data of the vector y.
+template <typename T>
+void csrmv(sycl::queue &queue, oneapi::mkl::transpose trans, int num_rows,
+           int num_cols, const T *alpha,
+           const std::shared_ptr<matrix_info> info, const T *val,
+           const int *row_ptr, const int *col_ind, const T *x, const T *beta,
+           T *y) {
+  detail::csrmv_impl<T>()(queue, trans, num_rows, num_cols, alpha, info, val,
+                          row_ptr, col_ind, x, beta, y);
+}
+
+/// Computes a CSR format sparse matrix-dense vector product.
+/// y = alpha * op(A) * x + beta * y
+/// \param [in] queue The queue where the routine should be executed. It must
+/// have the in_order property when using the USM mode.
+/// \param [in] trans The operation applied to the matrix A.
+/// \param [in] num_rows Number of rows of the matrix A.
+/// \param [in] num_cols Number of columns of the matrix A.
+/// \param [in] alpha Scaling factor for the matrix A.
+/// \param [in] alpha_type Data type of \p alpha .
+/// \param [in] info Matrix info of the matrix A.
+/// \param [in] val An array containing the non-zero elements of the matrix A.
+/// \param [in] val_type Data type of \p val .
+/// \param [in] row_ptr An array of length \p num_rows + 1.
+/// \param [in] col_ind An array containing the column indices in index-based
+/// numbering.
+/// \param [in] x Data of the vector x.
+/// \param [in] x_type Data type of \p x .
+/// \param [in] beta Scaling factor for the vector x.
+/// \param [in] beta_type Data type of \p beta .
+/// \param [in, out] y Data of the vector y.
+/// \param [in] y_type Data type of \p y .
+inline void csrmv(sycl::queue &queue, oneapi::mkl::transpose trans,
+                  int num_rows, int num_cols, const void *alpha,
+                  library_data_t alpha_type,
+                  const std::shared_ptr<matrix_info> info, const void *val,
+                  library_data_t val_type, const int *row_ptr,
+                  const int *col_ind, const void *x, library_data_t x_type,
+                  const void *beta, library_data_t beta_type, void *y,
+                  library_data_t y_type) {
+  detail::spblas_shim<detail::csrmv_impl>(val_type, queue, trans, num_rows,
+                                          num_cols, alpha, info, val, row_ptr,
+                                          col_ind, x, beta, y);
+}
+
+/// Computes a CSR format sparse matrix-dense matrix product.
+/// C = alpha * op(A) * B + beta * C
+/// \param [in] queue The queue where the routine should be executed. It must
+/// have the in_order property when using the USM mode.
+/// \param [in] trans The operation applied to the matrix A.
+/// \param [in] sparse_rows Number of rows of the matrix A.
+/// \param [in] dense_cols Number of columns of the matrix B or C.
+/// \param [in] sparse_cols Number of columns of the matrix A.
+/// \param [in] alpha Scaling factor for the matrix A.
+/// \param [in] info Matrix info of the matrix A.
+/// \param [in] val An array containing the non-zero elements of the matrix A.
+/// \param [in] row_ptr An array of length \p num_rows + 1.
+/// \param [in] col_ind An array containing the column indices in index-based
+/// numbering.
+/// \param [in] b Data of the matrix B.
+/// \param [in] ldb Leading dimension of the matrix B.
+/// \param [in] beta Scaling factor for the matrix B.
+/// \param [in, out] c Data of the matrix C.
+/// \param [in] ldc Leading dimension of the matrix C.
+template <typename T>
+void csrmm(sycl::queue &queue, oneapi::mkl::transpose trans, int sparse_rows,
+           int dense_cols, int sparse_cols, const T *alpha,
+           const std::shared_ptr<matrix_info> info, const T *val,
+           const int *row_ptr, const int *col_ind, const T *b, int ldb,
+           const T *beta, T *c, int ldc) {
+#ifndef __INTEL_MKL__
+  throw std::runtime_error("The oneAPI Math Kernel Library (oneMKL) Interfaces "
+                           "Project does not support this API.");
+#else
+  using Ty = typename dpct::DataType<T>::T2;
+  auto alpha_value =
+      dpct::detail::get_value(reinterpret_cast<const Ty *>(alpha), queue);
+  auto beta_value =
+      dpct::detail::get_value(reinterpret_cast<const Ty *>(beta), queue);
+
+  oneapi::mkl::sparse::matrix_handle_t *sparse_matrix_handle =
+      new oneapi::mkl::sparse::matrix_handle_t;
+  oneapi::mkl::sparse::init_matrix_handle(sparse_matrix_handle);
+  auto data_row_ptr = dpct::detail::get_memory<int>(row_ptr);
+  auto data_col_ind = dpct::detail::get_memory<int>(col_ind);
+  auto data_val = dpct::detail::get_memory<Ty>(val);
+  oneapi::mkl::sparse::set_csr_data(queue, *sparse_matrix_handle, sparse_rows,
+                                    sparse_cols, info->get_index_base(),
+                                    data_row_ptr, data_col_ind, data_val);
+
+  auto data_b = dpct::detail::get_memory<Ty>(b);
+  auto data_c = dpct::detail::get_memory<Ty>(c);
+  switch (info->get_matrix_type()) {
+  case matrix_info::matrix_type::ge: {
+    oneapi::mkl::sparse::gemm(queue, oneapi::mkl::layout::row_major, trans,
+                              oneapi::mkl::transpose::nontrans, alpha_value,
+                              *sparse_matrix_handle, data_b, dense_cols, ldb,
+                              beta_value, data_c, ldc);
+    break;
+  }
+  default:
+    throw std::runtime_error(
+        "the csrmm does not support matrix_info::matrix_type::sy, "
+        "matrix_info::matrix_type::tr and matrix_info::matrix_type::he");
+  }
+
+  sycl::event e =
+      oneapi::mkl::sparse::release_matrix_handle(queue, sparse_matrix_handle);
+  queue.submit([&](sycl::handler &cgh) {
+    cgh.depends_on(e);
+    cgh.host_task([=] { delete sparse_matrix_handle; });
+  });
+#endif
+}
+
+#ifdef __INTEL_MKL__ // The oneMKL Interfaces Project does not support this.
+/// Saving the optimization information for solving a system of linear
+/// equations.
+class optimize_info {
+public:
+  /// Constructor
+  optimize_info() { oneapi::mkl::sparse::init_matrix_handle(&_matrix_handle); }
+  /// Destructor
+  ~optimize_info() {
+    oneapi::mkl::sparse::release_matrix_handle(get_default_queue(),
+                                               &_matrix_handle, _deps)
+        .wait();
+  }
+  /// Add dependency for the destructor.
+  /// \param [in] e The event which the destructor depends on.
+  void add_dependency(sycl::event e) { _deps.push_back(e); }
+  /// Get the internal saved matrix handle.
+  /// \return Returns the matrix handle.
+  oneapi::mkl::sparse::matrix_handle_t get_matrix_handle() const noexcept {
+    return _matrix_handle;
+  }
+
+private:
+  oneapi::mkl::sparse::matrix_handle_t _matrix_handle = nullptr;
+  std::vector<sycl::event> _deps;
+};
+#endif
+
+#ifdef __INTEL_MKL__ // The oneMKL Interfaces Project does not support this.
+/// Performs internal optimizations for solving a system of linear equations for
+/// a CSR format sparse matrix.
+/// \param [in] queue The queue where the routine should be executed. It must
+/// have the in_order property when using the USM mode.
+/// \param [in] trans The operation applied to the sparse matrix.
+/// \param [in] row_col Number of rows of the sparse matrix.
+/// \param [in] info Matrix info of the sparse matrix.
+/// \param [in] val An array containing the non-zero elements of the sparse matrix.
+/// \param [in] row_ptr An array of length \p num_rows + 1.
+/// \param [in] col_ind An array containing the column indices in index-based
+/// numbering.
+/// \param [out] optimize_info The result of the optimizations.
+template <typename T>
+void optimize_csrsv(sycl::queue &queue, oneapi::mkl::transpose trans,
+                    int row_col, const std::shared_ptr<matrix_info> info,
+                    const T *val, const int *row_ptr, const int *col_ind,
+                    std::shared_ptr<optimize_info> optimize_info) {
+  using Ty = typename dpct::DataType<T>::T2;
+  auto data_row_ptr = dpct::detail::get_memory<int>(row_ptr);
+  auto data_col_ind = dpct::detail::get_memory<int>(col_ind);
+  auto data_val = dpct::detail::get_memory<Ty>(val);
+  oneapi::mkl::sparse::set_csr_data(queue, optimize_info->get_matrix_handle(),
+                                    row_col, row_col, info->get_index_base(),
+                                    data_row_ptr, data_col_ind, data_val);
+  if (info->get_matrix_type() != matrix_info::matrix_type::tr)
+    return;
+#ifndef DPCT_USM_LEVEL_NONE
+  sycl::event e;
+  e =
+#endif
+      oneapi::mkl::sparse::optimize_trsv(queue, info->get_uplo(), trans,
+                                         info->get_diag(),
+                                         optimize_info->get_matrix_handle());
+#ifndef DPCT_USM_LEVEL_NONE
+  optimize_info->add_dependency(e);
+#endif
+}
+#endif
+
+class sparse_matrix_desc;
+
+using sparse_matrix_desc_t = std::shared_ptr<sparse_matrix_desc>;
+
+/// Structure for describe a dense vector
+class dense_vector_desc {
+public:
+  dense_vector_desc(std::int64_t ele_num, void *value,
+                    library_data_t value_type)
+      : _ele_num(ele_num), _value(value), _value_type(value_type) {}
+  void get_desc(std::int64_t *ele_num, const void **value,
+                library_data_t *value_type) const noexcept {
+    *ele_num = _ele_num;
+    *value = _value;
+    *value_type = _value_type;
+  }
+  void get_desc(std::int64_t *ele_num, void **value,
+                library_data_t *value_type) const noexcept {
+    get_desc(ele_num, const_cast<const void **>(value), value_type);
+  }
+  void *get_value() const noexcept { return _value; }
+  void set_value(void *value) { _value = value; }
+  library_data_t get_value_type() const noexcept { return _value_type; }
+  std::int64_t get_ele_num() const noexcept { return _ele_num; }
+
+private:
+  std::int64_t _ele_num;
+  void *_value;
+  library_data_t _value_type;
+};
+
+/// Structure for describe a dense matrix
+class dense_matrix_desc {
+public:
+  dense_matrix_desc(std::int64_t row_num, std::int64_t col_num,
+                    std::int64_t leading_dim, void *value,
+                    library_data_t value_type, oneapi::mkl::layout layout)
+      : _row_num(row_num), _col_num(col_num), _leading_dim(leading_dim),
+        _value(value), _value_type(value_type), _layout(layout) {}
+  void get_desc(std::int64_t *row_num, std::int64_t *col_num,
+                std::int64_t *leading_dim, void **value,
+                library_data_t *value_type,
+                oneapi::mkl::layout *layout) const noexcept {
+    *row_num = _row_num;
+    *col_num = _col_num;
+    *leading_dim = _leading_dim;
+    *value = _value;
+    *value_type = _value_type;
+    *layout = _layout;
+  }
+  void *get_value() const noexcept { return _value; }
+  void set_value(void *value) { _value = value; }
+  std::int64_t get_col_num() const noexcept { return _col_num; }
+  std::int64_t get_leading_dim() const noexcept { return _leading_dim; }
+  oneapi::mkl::layout get_layout() const noexcept { return _layout; }
+
+private:
+  std::int64_t _row_num;
+  std::int64_t _col_num;
+  std::int64_t _leading_dim;
+  void *_value;
+  library_data_t _value_type;
+  oneapi::mkl::layout _layout;
+};
+
+/// Sparse matrix data format
+enum matrix_format : int {
+  csr = 1,
+};
+
+/// Sparse matrix attribute
+enum matrix_attribute : int { uplo = 0, diag };
+
+#ifdef __INTEL_MKL__ // The oneMKL Interfaces Project does not support this.
+/// Structure for describe a sparse matrix
+class sparse_matrix_desc {
+public:
+  /// Constructor
+  /// \param [out] desc The descriptor to be created
+  /// \param [in] row_num Number of rows of the sparse matrix.
+  /// \param [in] col_num Number of colums of the sparse matrix.
+  /// \param [in] nnz Non-zero elements in the sparse matrix.
+  /// \param [in] row_ptr An array of length \p row_num + 1. If the \p row_ptr is
+  /// NULL, the sparse_matrix_desc will allocate internal memory for it. This
+  /// internal memory can be gotten from get_shadow_row_ptr().
+  /// \param [in] col_ind An array containing the column indices in index-based
+  /// numbering.
+  /// \param [in] value An array containing the non-zero elements of the sparse matrix.
+  /// \param [in] row_ptr_type Data type of the \p row_ptr .
+  /// \param [in] col_ind_type Data type of the \p col_ind .
+  /// \param [in] base Indicates how input arrays are indexed.
+  /// \param [in] value_type Data type of the \p value .
+  /// \param [in] data_format The matrix data format.
+  sparse_matrix_desc(std::int64_t row_num, std::int64_t col_num,
+                     std::int64_t nnz, void *row_ptr, void *col_ind,
+                     void *value, library_data_t row_ptr_type,
+                     library_data_t col_ind_type, oneapi::mkl::index_base base,
+                     library_data_t value_type, matrix_format data_format)
+      : _row_num(row_num), _col_num(col_num), _nnz(nnz), _row_ptr(row_ptr),
+        _col_ind(col_ind), _value(value), _row_ptr_type(row_ptr_type),
+        _col_ind_type(col_ind_type), _base(base), _value_type(value_type),
+        _data_format(data_format) {
+    if (_data_format != matrix_format::csr) {
+      throw std::runtime_error("the sparse matrix data format is unsupported");
+    }
+    oneapi::mkl::sparse::init_matrix_handle(&_matrix_handle);
+    set_data();
+  }
+  /// Destructor
+  ~sparse_matrix_desc() {
+    oneapi::mkl::sparse::release_matrix_handle(get_default_queue(),
+                                               &_matrix_handle, _deps)
+        .wait();
+  }
+
+  /// Add dependency for the destroy method.
+  /// \param [in] e The event which the destroy method depends on.
+  void add_dependency(sycl::event e) { _deps.push_back(e); }
+  /// Get the internal saved matrix handle.
+  /// \return Returns the matrix handle.
+  oneapi::mkl::sparse::matrix_handle_t get_matrix_handle() const noexcept {
+    return _matrix_handle;
+  }
+  /// Get the values saved in the descriptor
+  /// \param [out] row_num Number of rows of the sparse matrix.
+  /// \param [out] col_num Number of colums of the sparse matrix.
+  /// \param [out] nnz Non-zero elements in the sparse matrix.
+  /// \param [out] row_ptr An array of length \p row_num + 1.
+  /// \param [out] col_ind An array containing the column indices in index-based
+  /// numbering.
+  /// \param [out] value An array containing the non-zero elements of the sparse matrix.
+  /// \param [out] row_ptr_type Data type of the \p row_ptr .
+  /// \param [out] col_ind_type Data type of the \p col_ind .
+  /// \param [out] base Indicates how input arrays are indexed.
+  /// \param [out] value_type Data type of the \p value .
+  void get_desc(int64_t *row_num, int64_t *col_num, int64_t *nnz,
+                void **row_ptr, void **col_ind, void **value,
+                library_data_t *row_ptr_type, library_data_t *col_ind_type,
+                oneapi::mkl::index_base *base,
+                library_data_t *value_type) const noexcept {
+    *row_num = _row_num;
+    *col_num = _col_num;
+    *nnz = _nnz;
+    *row_ptr = _row_ptr;
+    *col_ind = _col_ind;
+    *value = _value;
+    *row_ptr_type = _row_ptr_type;
+    *col_ind_type = _col_ind_type;
+    *base = _base;
+    *value_type = _value_type;
+  }
+  /// Get the sparse matrix data format of this descriptor
+  /// \param [out] format The matrix data format result
+  void get_format(matrix_format *data_format) const noexcept {
+    *data_format = _data_format;
+  }
+  /// Get the index base of this descriptor
+  /// \param [out] base The index base result
+  void get_base(oneapi::mkl::index_base *base) const noexcept { *base = _base; }
+  /// Get the value pointer of this descriptor
+  /// \param [out] value The value pointer result
+  void get_value(void **value) const noexcept { *value = _value; }
+  /// Set the value pointer of this descriptor
+  /// \param [in] value The input value pointer
+  void set_value(void *value) {
+    if (!value) {
+      throw std::runtime_error(
+          "dpct::sparse::sparse_matrix_desc::set_value(): The value "
+          "pointer is NULL.");
+    }
+    if (_value && (_value != value)) {
+      throw std::runtime_error(
+          "dpct::sparse::sparse_matrix_desc::set_value(): "
+          "The _value pointer is not NULL. It cannot be reset.");
+    }
+    _value = value;
+    set_data();
+  }
+  /// Get the size of the sparse matrix
+  /// \param [out] row_num Number of rows of the sparse matrix.
+  /// \param [out] col_num Number of colums of the sparse matrix.
+  /// \param [out] nnz Non-zero elements in the sparse matrix.
+  void get_size(int64_t *row_num, int64_t *col_num,
+                int64_t *nnz) const noexcept {
+    *row_num = _row_num;
+    *col_num = _col_num;
+    *nnz = _nnz;
+  }
+  /// Set the sparse matrix attribute
+  /// \param [in] attribute The attribute type
+  /// \param [in] data The attribute value
+  /// \param [in] data_size The data size of the attribute value
+  void set_attribute(matrix_attribute attribute, const void *data,
+                     size_t data_size) {
+    if (attribute == matrix_attribute::diag) {
+      const oneapi::mkl::diag *diag_ptr =
+          reinterpret_cast<const oneapi::mkl::diag *>(data);
+      if (*diag_ptr == oneapi::mkl::diag::unit) {
+        _diag = oneapi::mkl::diag::unit;
+      } else if (*diag_ptr == oneapi::mkl::diag::nonunit) {
+        _diag = oneapi::mkl::diag::nonunit;
+      } else {
+        throw std::runtime_error("unsupported diag value");
+      }
+    } else if (attribute == matrix_attribute::uplo) {
+      const oneapi::mkl::uplo *uplo_ptr =
+          reinterpret_cast<const oneapi::mkl::uplo *>(data);
+      if (*uplo_ptr == oneapi::mkl::uplo::upper) {
+        _uplo = oneapi::mkl::uplo::upper;
+      } else if (*uplo_ptr == oneapi::mkl::uplo::lower) {
+        _uplo = oneapi::mkl::uplo::lower;
+      } else {
+        throw std::runtime_error("unsupported uplo value");
+      }
+    } else {
+      throw std::runtime_error("unsupported attribute");
+    }
+  }
+  /// Get the sparse matrix attribute
+  /// \param [out] attribute The attribute type
+  /// \param [out] data The attribute value
+  /// \param [out] data_size The data size of the attribute value
+  void get_attribute(matrix_attribute attribute, void *data,
+                     size_t data_size) const {
+    if (attribute == matrix_attribute::diag) {
+      oneapi::mkl::diag *diag_ptr = reinterpret_cast<oneapi::mkl::diag *>(data);
+      if (_diag.has_value()) {
+        *diag_ptr = _diag.value();
+      } else {
+        *diag_ptr = oneapi::mkl::diag::nonunit;
+      }
+    } else if (attribute == matrix_attribute::uplo) {
+      oneapi::mkl::uplo *uplo_ptr = reinterpret_cast<oneapi::mkl::uplo *>(data);
+      if (_uplo.has_value()) {
+        *uplo_ptr = _uplo.value();
+      } else {
+        *uplo_ptr = oneapi::mkl::uplo::lower;
+      }
+    } else {
+      throw std::runtime_error("unsupported attribute");
+    }
+  }
+  /// Set the pointers for describing the sparse matrix
+  /// \param [in] row_ptr An array of length \p row_num + 1.
+  /// \param [in] col_ind An array containing the column indices in index-based
+  /// numbering.
+  /// \param [in] value An array containing the non-zero elements of the sparse matrix.
+  void set_pointers(void *row_ptr, void *col_ind, void *value) {
+    if (!row_ptr) {
+      throw std::runtime_error(
+          "dpct::sparse::sparse_matrix_desc::set_pointers(): The "
+          "row_ptr pointer is NULL.");
+    }
+    if (!col_ind) {
+      throw std::runtime_error(
+          "dpct::sparse::sparse_matrix_desc::set_pointers(): The "
+          "col_ind pointer is NULL.");
+    }
+    if (_row_ptr && (_row_ptr != row_ptr)) {
+      throw std::runtime_error("dpct::sparse::sparse_matrix_desc::set_pointers("
+                               "): The _row_ptr pointer is "
+                               "not NULL. It cannot be reset.");
+    }
+    if (_col_ind && (_col_ind != col_ind)) {
+      throw std::runtime_error("dpct::sparse::sparse_matrix_desc::set_pointers("
+                               "): The _col_ind pointer is "
+                               "not NULL. It cannot be reset.");
+    }
+    _row_ptr = row_ptr;
+    _col_ind = col_ind;
+
+    // The descriptor will be updated in the set_value function
+    set_value(value);
+  }
+
+  /// Get the diag attribute
+  /// \return diag value
+  std::optional<oneapi::mkl::diag> get_diag() const noexcept { return _diag; }
+  /// Get the uplo attribute
+  /// \return uplo value
+  std::optional<oneapi::mkl::uplo> get_uplo() const noexcept { return _uplo; }
+  /// Set the number of non-zero elements
+  /// \param nnz [in] The number of non-zero elements.
+  void set_nnz(std::int64_t nnz) noexcept { _nnz = nnz; }
+  /// Get the type of the value pointer.
+  /// \return The type of the value pointer.
+  library_data_t get_value_type() const noexcept { return _value_type; }
+  /// Get the row_ptr.
+  /// \return The row_ptr.
+  void *get_row_ptr() const noexcept { return _row_ptr; }
+  /// If the internal _row_ptr is NULL, the sparse_matrix_desc will allocate
+  /// internal memory for it in the constructor. The internal memory can be gotten
+  /// from this interface.
+  /// \return The shadow row_ptr.
+  void *get_shadow_row_ptr() const noexcept { return _shadow_row_ptr.get(); }
+  /// Get the type of the col_ind pointer.
+  /// \return The type of the col_ind pointer.
+  library_data_t get_col_ind_type() const noexcept { return _col_ind_type; }
+  /// Get the row_num.
+  /// \return The row_num.
+  std::int64_t get_row_num() const noexcept { return _row_num; }
+
+private:
+  inline static const std::function<void(void *)> _shadow_row_ptr_deleter =
+      [](void *ptr) { dpct::dpct_free(ptr); };
+  template <typename index_t, typename value_t> void set_data() {
+    void *row_ptr = nullptr;
+    if (_shadow_row_ptr) {
+      row_ptr = _shadow_row_ptr.get();
+    } else if (_row_ptr) {
+      row_ptr = _row_ptr;
+    } else {
+      row_ptr = dpct::dpct_malloc(sizeof(index_t) * (_row_num + 1),
+                                  get_default_queue());
+      _shadow_row_ptr.reset(row_ptr);
+    }
+#ifdef DPCT_USM_LEVEL_NONE
+    using data_index_t = sycl::buffer<index_t>;
+    using data_value_t = sycl::buffer<value_t>;
+#else
+    using data_index_t = index_t *;
+    using data_value_t = value_t *;
+#endif
+    _data_row_ptr = dpct::detail::get_memory<index_t>(row_ptr);
+    _data_col_ind = dpct::detail::get_memory<index_t>(_col_ind);
+    _data_value = dpct::detail::get_memory<value_t>(_value);
+    oneapi::mkl::sparse::set_csr_data(get_default_queue(), _matrix_handle,
+                                      _row_num, _col_num, _base,
+                                      std::get<data_index_t>(_data_row_ptr),
+                                      std::get<data_index_t>(_data_col_ind),
+                                      std::get<data_value_t>(_data_value));
+    get_default_queue().wait();
+  }
+
+  void set_data() {
+    std::uint64_t key = dpct::detail::get_type_combination_id(
+        _row_ptr_type, _col_ind_type, _value_type);
+    switch (key) {
+    case dpct::detail::get_type_combination_id(library_data_t::real_int32,
+                                               library_data_t::real_int32,
+                                               library_data_t::real_float): {
+      set_data<std::int32_t, float>();
+      break;
+    }
+    case dpct::detail::get_type_combination_id(library_data_t::real_int32,
+                                               library_data_t::real_int32,
+                                               library_data_t::real_double): {
+      set_data<std::int32_t, double>();
+      break;
+    }
+    case dpct::detail::get_type_combination_id(library_data_t::real_int32,
+                                               library_data_t::real_int32,
+                                               library_data_t::complex_float): {
+      set_data<std::int32_t, std::complex<float>>();
+      break;
+    }
+    case dpct::detail::get_type_combination_id(
+        library_data_t::real_int32, library_data_t::real_int32,
+        library_data_t::complex_double): {
+      set_data<std::int32_t, std::complex<double>>();
+      break;
+    }
+    case dpct::detail::get_type_combination_id(library_data_t::real_int64,
+                                               library_data_t::real_int64,
+                                               library_data_t::real_float): {
+      set_data<std::int64_t, float>();
+      break;
+    }
+    case dpct::detail::get_type_combination_id(library_data_t::real_int64,
+                                               library_data_t::real_int64,
+                                               library_data_t::real_double): {
+      set_data<std::int64_t, double>();
+      break;
+    }
+    case dpct::detail::get_type_combination_id(library_data_t::real_int64,
+                                               library_data_t::real_int64,
+                                               library_data_t::complex_float): {
+      set_data<std::int64_t, std::complex<float>>();
+      break;
+    }
+    case dpct::detail::get_type_combination_id(
+        library_data_t::real_int64, library_data_t::real_int64,
+        library_data_t::complex_double): {
+      set_data<std::int64_t, std::complex<double>>();
+      break;
+    }
+    default:
+      throw std::runtime_error("the combination of data type is unsupported");
+    }
+  }
+
+  std::int64_t _row_num;
+  std::int64_t _col_num;
+  std::int64_t _nnz;
+  void *_row_ptr;
+  void *_col_ind;
+  void *_value;
+  library_data_t _row_ptr_type;
+  library_data_t _col_ind_type;
+  oneapi::mkl::index_base _base;
+  library_data_t _value_type;
+  oneapi::mkl::sparse::matrix_handle_t _matrix_handle = nullptr;
+  std::vector<sycl::event> _deps;
+  matrix_format _data_format;
+  std::optional<oneapi::mkl::uplo> _uplo;
+  std::optional<oneapi::mkl::diag> _diag;
+  std::unique_ptr<void, std::function<void(void *)>> _shadow_row_ptr =
+      std::unique_ptr<void, std::function<void(void *)>>(
+          nullptr, _shadow_row_ptr_deleter);
+
+  static constexpr size_t _max_data_variable_size = std::max(
+      {sizeof(sycl::buffer<std::int32_t>), sizeof(sycl::buffer<std::int64_t>),
+       sizeof(sycl::buffer<float>), sizeof(sycl::buffer<double>),
+       sizeof(sycl::buffer<std::complex<float>>),
+       sizeof(sycl::buffer<std::complex<double>>), sizeof(void *)});
+  using index_variant_t =
+      std::variant<std::array<std::byte, _max_data_variable_size>,
+                   sycl::buffer<std::int32_t>, sycl::buffer<std::int64_t>,
+                   std::int32_t *, std::int64_t *>;
+  using value_variant_t =
+      std::variant<std::array<std::byte, _max_data_variable_size>,
+                   sycl::buffer<float>, sycl::buffer<double>,
+                   sycl::buffer<std::complex<float>>,
+                   sycl::buffer<std::complex<double>>, float *, double *,
+                   std::complex<float> *, std::complex<double> *>;
+  index_variant_t _data_row_ptr;
+  index_variant_t _data_col_ind;
+  value_variant_t _data_value;
+};
+
+namespace detail {
+#ifdef DPCT_USM_LEVEL_NONE
+#define SPARSE_CALL(X)                                                         \
+  do {                                                                         \
+    X;                                                                         \
+  } while (0)
+#else
+#define SPARSE_CALL(X)                                                         \
+  do {                                                                         \
+    sycl::event e = X;                                                         \
+    a->add_dependency(e);                                                      \
+  } while (0)
+#endif
+
+template <typename T> struct spmv_impl {
+  void operator()(sycl::queue queue, oneapi::mkl::transpose trans,
+                  const void *alpha, sparse_matrix_desc_t a,
+                  std::shared_ptr<dense_vector_desc> x, const void *beta,
+                  std::shared_ptr<dense_vector_desc> y) {
+    auto alpha_value =
+        dpct::detail::get_value(reinterpret_cast<const T *>(alpha), queue);
+    auto beta_value =
+        dpct::detail::get_value(reinterpret_cast<const T *>(beta), queue);
+    auto data_x = dpct::detail::get_memory<T>(x->get_value());
+    auto data_y = dpct::detail::get_memory<T>(y->get_value());
+    if (a->get_diag().has_value() && a->get_uplo().has_value()) {
+      oneapi::mkl::sparse::optimize_trmv(queue, a->get_uplo().value(), trans,
+                                         a->get_diag().value(),
+                                         a->get_matrix_handle());
+      SPARSE_CALL(oneapi::mkl::sparse::trmv(
+          queue, a->get_uplo().value(), trans, a->get_diag().value(),
+          alpha_value, a->get_matrix_handle(), data_x, beta_value, data_y));
+    } else {
+      oneapi::mkl::sparse::optimize_gemv(queue, trans, a->get_matrix_handle());
+      SPARSE_CALL(oneapi::mkl::sparse::gemv(queue, trans, alpha_value,
+                                            a->get_matrix_handle(), data_x,
+                                            beta_value, data_y));
+    }
+  }
+};
+
+template <typename T> struct spmm_impl {
+  void operator()(sycl::queue queue, oneapi::mkl::transpose trans_a,
+                  oneapi::mkl::transpose trans_b, const void *alpha,
+                  sparse_matrix_desc_t a, std::shared_ptr<dense_matrix_desc> b,
+                  const void *beta, std::shared_ptr<dense_matrix_desc> c) {
+    auto alpha_value =
+        dpct::detail::get_value(reinterpret_cast<const T *>(alpha), queue);
+    auto beta_value =
+        dpct::detail::get_value(reinterpret_cast<const T *>(beta), queue);
+    auto data_b = dpct::detail::get_memory<T>(b->get_value());
+    auto data_c = dpct::detail::get_memory<T>(c->get_value());
+    SPARSE_CALL(oneapi::mkl::sparse::gemm(
+        queue, b->get_layout(), trans_a, trans_b, alpha_value,
+        a->get_matrix_handle(), data_b, b->get_col_num(), b->get_leading_dim(),
+        beta_value, data_c, c->get_leading_dim()));
+  }
+};
+#undef SPARSE_CALL
+} // namespace detail
+
+/// Computes a sparse matrix-dense vector product: y = alpha * op(a) * x + beta * y.
+/// \param [in] queue The queue where the routine should be executed. It must
+/// have the in_order property when using the USM mode.
+/// \param [in] trans Specifies operation on input matrix.
+/// \param [in] alpha Specifies the scalar alpha.
+/// \param [in] a Specifies the sparse matrix a.
+/// \param [in] x Specifies the dense vector x.
+/// \param [in] beta Specifies the scalar beta.
+/// \param [in, out] y Specifies the dense vector y.
+/// \param [in] data_type Specifies the data type of \param a, \param x and \param y .
+inline void spmv(sycl::queue queue, oneapi::mkl::transpose trans,
+                 const void *alpha, sparse_matrix_desc_t a,
+                 std::shared_ptr<dense_vector_desc> x, const void *beta,
+                 std::shared_ptr<dense_vector_desc> y,
+                 library_data_t data_type) {
+  detail::spblas_shim<detail::spmv_impl>(data_type, queue, trans, alpha, a, x,
+                                         beta, y);
+}
+
+/// Computes a sparse matrix-dense matrix product: c = alpha * op(a) * op(b) + beta * c.
+/// \param [in] queue The queue where the routine should be executed. It must
+/// have the in_order property when using the USM mode.
+/// \param [in] trans_a Specifies operation on input matrix a.
+/// \param [in] trans_b Specifies operation on input matrix b.
+/// \param [in] alpha Specifies the scalar alpha.
+/// \param [in] a Specifies the sparse matrix a.
+/// \param [in] b Specifies the dense matrix b.
+/// \param [in] beta Specifies the scalar beta.
+/// \param [in, out] c Specifies the dense matrix c.
+/// \param [in] data_type Specifies the data type of \param a, \param b and \param c .
+inline void spmm(sycl::queue queue, oneapi::mkl::transpose trans_a,
+                 oneapi::mkl::transpose trans_b, const void *alpha,
+                 sparse_matrix_desc_t a, std::shared_ptr<dense_matrix_desc> b,
+                 const void *beta, std::shared_ptr<dense_matrix_desc> c,
+                 library_data_t data_type) {
+  if (b->get_layout() != c->get_layout())
+    throw std::runtime_error("the layout of b and c are different");
+  detail::spblas_shim<detail::spmm_impl>(data_type, queue, trans_a, trans_b,
+                                         alpha, a, b, beta, c);
+}
+
+namespace detail {
+template <typename T, bool is_host_memory, typename host_memory_t = void>
+struct temp_memory {
+  static_assert(!is_host_memory || !std::is_same_v<host_memory_t, void>,
+                "host_memory_t cannot be void when the input parameter ptr "
+                "points to host memory");
+  temp_memory(sycl::queue queue, void *ptr)
+      : _queue(queue)
+#ifdef DPCT_USM_LEVEL_NONE
+        ,
+        _buffer(is_host_memory ? sycl::buffer<T, 1>(sycl::range<1>(1))
+                               : sycl::buffer<T, 1>(dpct::get_buffer<T>(ptr)))
+#endif
+  {
+    if constexpr (is_host_memory) {
+      _original_host_ptr = static_cast<host_memory_t *>(ptr);
+#ifdef DPCT_USM_LEVEL_NONE
+      auto _buffer_acc = _buffer.get_host_access(sycl::write_only);
+      _buffer_acc[0] = static_cast<T>(*_original_host_ptr);
+#else
+      _memory_ptr = sycl::malloc_host<T>(1, _queue);
+      *_memory_ptr = static_cast<T>(*_original_host_ptr);
+#endif
+    } else {
+#ifndef DPCT_USM_LEVEL_NONE
+      _memory_ptr = static_cast<T *>(ptr);
+#endif
+    }
+  }
+
+  ~temp_memory() {
+    if constexpr (is_host_memory) {
+#ifdef DPCT_USM_LEVEL_NONE
+      auto _buffer_acc = _buffer.get_host_access(sycl::read_only);
+      *_original_host_ptr = static_cast<host_memory_t>(_buffer_acc[0]);
+#else
+      _queue.wait();
+      *_original_host_ptr = *_memory_ptr;
+      sycl::free(_memory_ptr, _queue);
+#endif
+    }
+  }
+  auto get_memory_ptr() {
+#ifdef DPCT_USM_LEVEL_NONE
+    return &_buffer;
+#else
+    return _memory_ptr;
+#endif
+  }
+
+private:
+  sycl::queue _queue;
+  host_memory_t *_original_host_ptr = nullptr;
+#ifdef DPCT_USM_LEVEL_NONE
+  sycl::buffer<T, 1> _buffer;
+#else
+  T *_memory_ptr;
+#endif
+};
+} // namespace detail
+
+/// Do initial estimation of work and load balancing of computing a sparse
+/// matrix-sparse matrix product.
+/// \param [in] queue The queue where the routine should be executed. It must
+/// have the in_order property when using the USM mode.
+/// \param [in] trans_a Specifies operation on input matrix a.
+/// \param [in] trans_b Specifies operation on input matrix b.
+/// \param [in] alpha Specifies the scalar alpha.
+/// \param [in] a Specifies the sparse matrix a.
+/// \param [in] b Specifies the sparse matrix b.
+/// \param [in] beta Specifies the scalar beta.
+/// \param [in, out] c Specifies the sparse matrix c.
+/// \param [in] matmat_descr Describes the sparse matrix-sparse matrix operation
+/// to be executed.
+/// \param [in, out] size_temp_buffer Specifies the size of workspace.
+/// \param [in] temp_buffer Specifies the memory of the workspace.
+inline void
+spgemm_work_estimation(sycl::queue queue, oneapi::mkl::transpose trans_a,
+                       oneapi::mkl::transpose trans_b, const void *alpha,
+                       sparse_matrix_desc_t a, sparse_matrix_desc_t b,
+                       const void *beta, sparse_matrix_desc_t c,
+                       oneapi::mkl::sparse::matmat_descr_t matmat_descr,
+                       size_t *size_temp_buffer, void *temp_buffer) {
+  if (temp_buffer) {
+    detail::temp_memory<std::int64_t, true, size_t> size_memory(
+        queue, size_temp_buffer);
+    detail::temp_memory<std::uint8_t, false> work_memory(queue, temp_buffer);
+    oneapi::mkl::sparse::matmat(
+        queue, a->get_matrix_handle(), b->get_matrix_handle(),
+        c->get_matrix_handle(),
+        oneapi::mkl::sparse::matmat_request::work_estimation, matmat_descr,
+        size_memory.get_memory_ptr(), work_memory.get_memory_ptr()
+#ifndef DPCT_USM_LEVEL_NONE
+        , {}
+#endif
+    );
+  } else {
+    oneapi::mkl::sparse::set_matmat_data(
+        matmat_descr, oneapi::mkl::sparse::matrix_view_descr::general, trans_a,
+        oneapi::mkl::sparse::matrix_view_descr::general, trans_b,
+        oneapi::mkl::sparse::matrix_view_descr::general);
+    detail::temp_memory<std::int64_t, true, size_t> size_memory(
+        queue, size_temp_buffer);
+    oneapi::mkl::sparse::matmat(
+        queue, a->get_matrix_handle(), b->get_matrix_handle(),
+        c->get_matrix_handle(),
+        oneapi::mkl::sparse::matmat_request::get_work_estimation_buf_size,
+        matmat_descr, size_memory.get_memory_ptr(), nullptr
+#ifndef DPCT_USM_LEVEL_NONE
+        , {}
+#endif
+    );
+  }
+}
+
+/// Do internal products for computing the C matrix of computing a sparse
+/// matrix-sparse matrix product.
+/// \param [in] queue The queue where the routine should be executed. It must
+/// have the in_order property when using the USM mode.
+/// \param [in] trans_a Specifies operation on input matrix a.
+/// \param [in] trans_b Specifies operation on input matrix b.
+/// \param [in] alpha Specifies the scalar alpha.
+/// \param [in] a Specifies the sparse matrix a.
+/// \param [in] b Specifies the sparse matrix b.
+/// \param [in] beta Specifies the scalar beta.
+/// \param [in, out] c Specifies the sparse matrix c.
+/// \param [in] matmat_descr Describes the sparse matrix-sparse matrix operation
+/// to be executed.
+/// \param [in, out] size_temp_buffer Specifies the size of workspace.
+/// \param [in] temp_buffer Specifies the memory of the workspace.
+inline void spgemm_compute(sycl::queue queue, oneapi::mkl::transpose trans_a,
+                           oneapi::mkl::transpose trans_b, const void *alpha,
+                           sparse_matrix_desc_t a, sparse_matrix_desc_t b,
+                           const void *beta, sparse_matrix_desc_t c,
+                           oneapi::mkl::sparse::matmat_descr_t matmat_descr,
+                           size_t *size_temp_buffer, void *temp_buffer) {
+  if (temp_buffer) {
+    std::int64_t nnz_value = 0;
+    {
+      detail::temp_memory<std::int64_t, true, size_t> size_memory(
+          queue, size_temp_buffer);
+      detail::temp_memory<std::uint8_t, false> work_memory(queue, temp_buffer);
+      detail::temp_memory<std::int64_t, true, std::int64_t> nnz_memory(
+          queue, &nnz_value);
+      oneapi::mkl::sparse::matmat(
+          queue, a->get_matrix_handle(), b->get_matrix_handle(),
+          c->get_matrix_handle(), oneapi::mkl::sparse::matmat_request::compute,
+          matmat_descr, size_memory.get_memory_ptr(),
+          work_memory.get_memory_ptr()
+#ifndef DPCT_USM_LEVEL_NONE
+          , {}
+#endif
+      );
+      oneapi::mkl::sparse::matmat(
+          queue, a->get_matrix_handle(), b->get_matrix_handle(),
+          c->get_matrix_handle(), oneapi::mkl::sparse::matmat_request::get_nnz,
+          matmat_descr, nnz_memory.get_memory_ptr(), nullptr
+#ifndef DPCT_USM_LEVEL_NONE
+          , {}
+#endif
+      );
+    }
+    c->set_nnz(nnz_value);
+  } else {
+    detail::temp_memory<std::int64_t, true, size_t> size_memory(
+        queue, size_temp_buffer);
+    oneapi::mkl::sparse::matmat(
+        queue, a->get_matrix_handle(), b->get_matrix_handle(),
+        c->get_matrix_handle(),
+        oneapi::mkl::sparse::matmat_request::get_compute_buf_size, matmat_descr,
+        size_memory.get_memory_ptr(), nullptr
+#ifndef DPCT_USM_LEVEL_NONE
+        , {}
+#endif
+    );
+  }
+}
+
+/// Do any remaining internal products and accumulation and transfer into final
+/// C matrix arrays of computing a sparse matrix-sparse matrix product.
+/// \param [in] queue The queue where the routine should be executed. It must
+/// have the in_order property when using the USM mode.
+/// \param [in] trans_a Specifies operation on input matrix a.
+/// \param [in] trans_b Specifies operation on input matrix b.
+/// \param [in] alpha Specifies the scalar alpha.
+/// \param [in] a Specifies the sparse matrix a.
+/// \param [in] b Specifies the sparse matrix b.
+/// \param [in] beta Specifies the scalar beta.
+/// \param [in, out] c Specifies the sparse matrix c.
+/// \param [in] matmat_descr Describes the sparse matrix-sparse matrix operation
+/// to be executed.
+inline void spgemm_finalize(sycl::queue queue, oneapi::mkl::transpose trans_a,
+                            oneapi::mkl::transpose trans_b, const void *alpha,
+                            sparse_matrix_desc_t a, sparse_matrix_desc_t b,
+                            const void *beta, sparse_matrix_desc_t c,
+                            oneapi::mkl::sparse::matmat_descr_t matmat_descr) {
+  oneapi::mkl::sparse::matmat(queue, a->get_matrix_handle(),
+                              b->get_matrix_handle(), c->get_matrix_handle(),
+                              oneapi::mkl::sparse::matmat_request::finalize,
+                              matmat_descr, nullptr, nullptr
+#ifdef DPCT_USM_LEVEL_NONE
+  );
+#else
+  , {}).wait();
+#endif
+  if (c->get_shadow_row_ptr()) {
+    switch (c->get_col_ind_type()) {
+    case library_data_t::real_int32: {
+      dpct::dpct_memcpy(c->get_row_ptr(), c->get_shadow_row_ptr(),
+                        sizeof(std::int32_t) * (c->get_row_num() + 1));
+      break;
+    }
+    case library_data_t::real_int64: {
+      dpct::dpct_memcpy(c->get_row_ptr(), c->get_shadow_row_ptr(),
+                        sizeof(std::int64_t) * (c->get_row_num() + 1));
+      break;
+    }
+    default:
+      throw std::runtime_error("dpct::sparse::spgemm_finalize(): The data type "
+                               "of the col_ind in matrix c is unsupported.");
+    }
+  }
+}
+
+namespace detail {
+template <typename T> struct spsv_impl {
+  void operator()(sycl::queue queue, oneapi::mkl::uplo uplo,
+                  oneapi::mkl::diag diag, oneapi::mkl::transpose trans_a,
+                  const void *alpha, sparse_matrix_desc_t a,
+                  std::shared_ptr<dense_vector_desc> x,
+                  std::shared_ptr<dense_vector_desc> y) {
+    auto alpha_value =
+        dpct::detail::get_value(reinterpret_cast<const T *>(alpha), queue);
+    T *new_x_ptr = nullptr;
+    if (alpha_value != T(1.0f)) {
+      new_x_ptr = (T *)dpct::dpct_malloc(x->get_ele_num() * sizeof(T));
+      dpct::dpct_memcpy(new_x_ptr, x->get_value(),
+                        x->get_ele_num() * sizeof(T));
+      auto data_new_x = dpct::detail::get_memory<T>(new_x_ptr);
+      oneapi::mkl::blas::column_major::scal(queue, x->get_ele_num(),
+                                            alpha_value, data_new_x, 1);
+    } else {
+      new_x_ptr = static_cast<T *>(x->get_value());
+    }
+    auto data_new_x = dpct::detail::get_memory<T>(new_x_ptr);
+    auto data_y = dpct::detail::get_memory<T>(y->get_value());
+    oneapi::mkl::sparse::trsv(queue, uplo, trans_a, diag,
+                              a->get_matrix_handle(), data_new_x, data_y);
+    if (alpha_value != T(1.0f)) {
+      queue.wait();
+      dpct::dpct_free(new_x_ptr);
+    }
+  }
+};
+} // namespace detail
+
+/// Performs internal optimizations for spsv by analyzing the provided matrix
+/// structure and operation parameters.
+/// \param [in] queue The queue where the routine should be executed. It must
+/// have the in_order property when using the USM mode.
+/// \param [in] trans_a Specifies operation on input matrix a.
+/// \param [in] a Specifies the sparse matrix a.
+inline void spsv_optimize(sycl::queue queue, oneapi::mkl::transpose trans_a,
+                          sparse_matrix_desc_t a) {
+  if (!a->get_uplo() || !a->get_diag()) {
+    throw std::runtime_error(
+        "dpct::sparse::spsv_optimize(): oneapi::mkl::sparse::optimize_trsv "
+        "needs uplo and diag attributes to be specified.");
+  }
+  oneapi::mkl::sparse::optimize_trsv(
+      queue, a->get_uplo().value(), oneapi::mkl::transpose::nontrans,
+      a->get_diag().value(), a->get_matrix_handle());
+}
+
+/// Solves a system of linear equations for a sparse matrix.
+/// \param [in] queue The queue where the routine should be executed. It must
+/// have the in_order property when using the USM mode.
+/// \param [in] trans_a Specifies operation on input matrix a.
+/// \param [in] alpha Specifies the scalar alpha.
+/// \param [in] a Specifies the sparse matrix a.
+/// \param [in] x Specifies the dense vector x.
+/// \param [in, out] y Specifies the dense vector y.
+/// \param [in] data_type Specifies the data type of \param a, \param x and
+/// \param y .
+inline void spsv(sycl::queue queue, oneapi::mkl::transpose trans_a,
+                 const void *alpha, sparse_matrix_desc_t a,
+                 std::shared_ptr<dense_vector_desc> x,
+                 std::shared_ptr<dense_vector_desc> y,
+                 library_data_t data_type) {
+  if (!a->get_uplo() || !a->get_diag()) {
+    throw std::runtime_error(
+        "dpct::sparse::spsv(): oneapi::mkl::sparse::trsv needs uplo and diag "
+        "attributes to be specified.");
+  }
+  oneapi::mkl::uplo uplo = a->get_uplo().value();
+  oneapi::mkl::diag diag = a->get_diag().value();
+  detail::spblas_shim<detail::spsv_impl>(a->get_value_type(), queue, uplo, diag,
+                                         trans_a, alpha, a, x, y);
+}
+#endif
+} // namespace sparse
+} // namespace dpct
+
+#endif // __DPCT_SPARSE_UTILS_HPP__
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/util.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/util.hpp
new file mode 100644
index 0000000..d916c59
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/includes/dpct/util.hpp
@@ -0,0 +1,1030 @@
+//==---- util.hpp ---------------------------------*- C++ -*----------------==//
+//
+// Copyright (C) Intel Corporation
+// SPDX-License-Identifier: Apache-2.0 WITH LLVM-exception
+// See https://llvm.org/LICENSE.txt for license information.
+//
+//===----------------------------------------------------------------------===//
+
+#ifndef __DPCT_UTIL_HPP__
+#define __DPCT_UTIL_HPP__
+
+#include <sycl/sycl.hpp>
+#include <complex>
+#include <type_traits>
+#include <cassert>
+#include <cstdint>
+
+// TODO: Remove these function definitions once they exist in the DPC++ compiler
+#if defined(__SYCL_DEVICE_ONLY__) && defined(__INTEL_LLVM_COMPILER)
+template <typename T>
+__SYCL_CONVERGENT__ extern SYCL_EXTERNAL __SYCL_EXPORT __attribute__((noduplicate))
+T __spirv_GroupNonUniformShuffle(__spv::Scope::Flag, T, unsigned) noexcept;
+
+template <typename T>
+__SYCL_CONVERGENT__ extern SYCL_EXTERNAL __SYCL_EXPORT __attribute__((noduplicate))
+T __spirv_GroupNonUniformShuffleDown(__spv::Scope::Flag, T, unsigned) noexcept;
+
+template <typename T>
+__SYCL_CONVERGENT__ extern SYCL_EXTERNAL __SYCL_EXPORT __attribute__((noduplicate))
+T __spirv_GroupNonUniformShuffleUp(__spv::Scope::Flag, T, unsigned) noexcept;
+#endif
+
+namespace dpct {
+
+namespace detail {
+
+template <typename tag, typename T> class generic_error_type {
+public:
+  generic_error_type() = default;
+  generic_error_type(T value) : value{value} {}
+  operator T() const { return value; }
+
+private:
+  T value;
+};
+
+} // namespace detail
+
+using err0 = detail::generic_error_type<struct err0_tag, int>;
+using err1 = detail::generic_error_type<struct err1_tag, int>;
+
+template <int... Ints> struct integer_sequence {};
+template <int Size, int... Ints>
+struct make_index_sequence
+    : public make_index_sequence<Size - 1, Size - 1, Ints...> {};
+template <int... Ints>
+struct make_index_sequence<0, Ints...> : public integer_sequence<Ints...> {};
+
+template <typename T> struct DataType { using T2 = T; };
+template <typename T> struct DataType<sycl::vec<T, 2>> {
+  using T2 = std::complex<T>;
+};
+
+inline void matrix_mem_copy(void *to_ptr, const void *from_ptr, int to_ld,
+                            int from_ld, int rows, int cols, int elem_size,
+                            memcpy_direction direction = automatic,
+                            sycl::queue &queue = dpct::get_default_queue(),
+                            bool async = false) {
+  if (to_ptr == from_ptr && to_ld == from_ld) {
+    return;
+  }
+
+  if (to_ld == from_ld) {
+    size_t copy_size = elem_size * ((cols - 1) * (size_t)to_ld + rows);
+    if (async)
+      detail::dpct_memcpy(queue, (void *)to_ptr, (void *)from_ptr,
+                          copy_size, direction);
+    else
+      detail::dpct_memcpy(queue, (void *)to_ptr, (void *)from_ptr,
+                          copy_size, direction).wait();
+  } else {
+    if (async)
+      detail::dpct_memcpy(queue, to_ptr, from_ptr, elem_size * to_ld,
+                          elem_size * from_ld, elem_size * rows, cols,
+                          direction);
+    else
+      sycl::event::wait(detail::dpct_memcpy(
+          queue, to_ptr, from_ptr, elem_size * to_ld, elem_size * from_ld,
+          elem_size * rows, cols, direction));
+  }
+}
+
+/// Copy matrix data. The default leading dimension is column.
+/// \param [out] to_ptr A pointer points to the destination location.
+/// \param [in] from_ptr A pointer points to the source location.
+/// \param [in] to_ld The leading dimension the destination matrix.
+/// \param [in] from_ld The leading dimension the source matrix.
+/// \param [in] rows The number of rows of the source matrix.
+/// \param [in] cols The number of columns of the source matrix.
+/// \param [in] direction The direction of the data copy.
+/// \param [in] queue The queue where the routine should be executed.
+/// \param [in] async If this argument is true, the return of the function
+/// does NOT guarantee the copy is completed.
+template <typename T>
+inline void matrix_mem_copy(T *to_ptr, const T *from_ptr, int to_ld,
+                            int from_ld, int rows, int cols,
+                            memcpy_direction direction = automatic,
+                            sycl::queue &queue = dpct::get_default_queue(),
+                            bool async = false) {
+  using Ty = typename DataType<T>::T2;
+  matrix_mem_copy((void *)to_ptr, (void *)from_ptr, to_ld, from_ld, rows, cols,
+                  sizeof(Ty), direction, queue, async);
+}
+
+/// Cast the high or low 32 bits of a double to an integer.
+/// \param [in] d The double value.
+/// \param [in] use_high32 Cast the high 32 bits of the double if true;
+/// otherwise cast the low 32 bits.
+inline int cast_double_to_int(double d, bool use_high32 = true) {
+  sycl::vec<double, 1> v0{d};
+  auto v1 = v0.as<sycl::int2>();
+  if (use_high32)
+    return v1[1];
+  return v1[0];
+}
+
+/// Combine two integers, the first as the high 32 bits and the second
+/// as the low 32 bits, into a double.
+/// \param [in] high32 The integer as the high 32 bits
+/// \param [in] low32 The integer as the low 32 bits
+inline double cast_ints_to_double(int high32, int low32) {
+  sycl::int2 v0{low32, high32};
+  auto v1 = v0.as<sycl::vec<double, 1>>();
+  return v1;
+}
+
+/// Reverse the bit order of an unsigned integer
+/// \param [in] a Input unsigned integer value
+/// \returns Value of a with the bit order reversed
+template <typename T> inline T reverse_bits(T a) {
+  static_assert(std::is_unsigned<T>::value && std::is_integral<T>::value,
+                "unsigned integer required");
+  if (!a)
+    return 0;
+  T mask = 0;
+  size_t count = 4 * sizeof(T);
+  mask = ~mask >> count;
+  while (count) {
+    a = ((a & mask) << count) | ((a & ~mask) >> count);
+    count = count >> 1;
+    mask = mask ^ (mask << count);
+  }
+  return a;
+}
+
+/// \param [in] a The first value contains 4 bytes
+/// \param [in] b The second value contains 4 bytes
+/// \param [in] s The selector value, only lower 16bit used
+/// \returns the permutation result of 4 bytes selected in the way
+/// specified by \p s from \p a and \p b
+inline unsigned int byte_level_permute(unsigned int a, unsigned int b,
+                                       unsigned int s) {
+  unsigned int ret;
+  ret =
+      ((((std::uint64_t)b << 32 | a) >> (s & 0x7) * 8) & 0xff) |
+      (((((std::uint64_t)b << 32 | a) >> ((s >> 4) & 0x7) * 8) & 0xff) << 8) |
+      (((((std::uint64_t)b << 32 | a) >> ((s >> 8) & 0x7) * 8) & 0xff) << 16) |
+      (((((std::uint64_t)b << 32 | a) >> ((s >> 12) & 0x7) * 8) & 0xff) << 24);
+  return ret;
+}
+
+/// Find position of first least significant set bit in an integer.
+/// ffs(0) returns 0.
+///
+/// \param [in] a Input integer value
+/// \returns The position
+template <typename T> inline int ffs(T a) {
+  static_assert(std::is_integral<T>::value, "integer required");
+  return (sycl::ctz(a) + 1) % (sizeof(T) * 8 + 1);
+}
+
+/// select_from_sub_group allows work-items to obtain a copy of a value held by
+/// any other work-item in the sub_group. The input sub_group will be divided
+/// into several logical sub_groups with id range [0, \p logical_sub_group_size
+/// - 1]. Each work-item in logical sub_group gets value from another work-item
+/// whose id is \p remote_local_id. If \p remote_local_id is outside the
+/// logical sub_group id range, \p remote_local_id will modulo with \p
+/// logical_sub_group_size. The \p logical_sub_group_size must be a power of 2
+/// and not exceed input sub_group size.
+/// \tparam T Input value type
+/// \param [in] g Input sub_group
+/// \param [in] x Input value
+/// \param [in] remote_local_id Input source work item id
+/// \param [in] logical_sub_group_size Input logical sub_group size
+/// \returns The result
+template <typename T>
+T select_from_sub_group(sycl::sub_group g, T x, int remote_local_id,
+                        int logical_sub_group_size = 32) {
+  unsigned int start_index =
+      g.get_local_linear_id() / logical_sub_group_size * logical_sub_group_size;
+  return sycl::select_from_group(
+      g, x, start_index + remote_local_id % logical_sub_group_size);
+}
+
+/// shift_sub_group_left move values held by the work-items in a sub_group
+/// directly to another work-item in the sub_group, by shifting values a fixed
+/// number of work-items to the left. The input sub_group will be divided into
+/// several logical sub_groups with id range [0, \p logical_sub_group_size - 1].
+/// Each work-item in logical sub_group gets value from another work-item whose
+/// id is caller's id adds \p delta. If calculated id is outside the logical
+/// sub_group id range, the work-item will get value from itself. The \p
+/// logical_sub_group_size must be a power of 2 and not exceed input sub_group
+/// size.
+/// \tparam T Input value type
+/// \param [in] g Input sub_group
+/// \param [in] x Input value
+/// \param [in] delta Input delta
+/// \param [in] logical_sub_group_size Input logical sub_group size
+/// \returns The result
+template <typename T>
+T shift_sub_group_left(sycl::sub_group g, T x, unsigned int delta,
+                       int logical_sub_group_size = 32) {
+  unsigned int id = g.get_local_linear_id();
+  unsigned int end_index =
+      (id / logical_sub_group_size + 1) * logical_sub_group_size;
+  T result = sycl::shift_group_left(g, x, delta);
+  if ((id + delta) >= end_index) {
+    result = x;
+  }
+  return result;
+}
+
+/// shift_sub_group_right move values held by the work-items in a sub_group
+/// directly to another work-item in the sub_group, by shifting values a fixed
+/// number of work-items to the right. The input sub_group will be divided into
+/// several logical_sub_groups with id range [0, \p logical_sub_group_size - 1].
+/// Each work-item in logical_sub_group gets value from another work-item whose
+/// id is caller's id subtracts \p delta. If calculated id is outside the
+/// logical sub_group id range, the work-item will get value from itself. The \p
+/// logical_sub_group_size must be a power of 2 and not exceed input sub_group
+/// size.
+/// \tparam T Input value type
+/// \param [in] g Input sub_group
+/// \param [in] x Input value
+/// \param [in] delta Input delta
+/// \param [in] logical_sub_group_size Input logical sub_group size
+/// \returns The result
+template <typename T>
+T shift_sub_group_right(sycl::sub_group g, T x, unsigned int delta,
+                        int logical_sub_group_size = 32) {
+  unsigned int id = g.get_local_linear_id();
+  unsigned int start_index =
+      id / logical_sub_group_size * logical_sub_group_size;
+  T result = sycl::shift_group_right(g, x, delta);
+  if ((id - start_index) < delta) {
+    result = x;
+  }
+  return result;
+}
+
+/// permute_sub_group_by_xor permutes values by exchanging values held by pairs
+/// of work-items identified by computing the bitwise exclusive OR of the
+/// work-item id and some fixed mask. The input sub_group will be divided into
+/// several logical sub_groups with id range [0, \p logical_sub_group_size - 1].
+/// Each work-item in logical sub_group gets value from another work-item whose
+/// id is bitwise exclusive OR of the caller's id and \p mask. If calculated id
+/// is outside the logical sub_group id range, the work-item will get value from
+/// itself. The \p logical_sub_group_size must be a power of 2 and not exceed
+/// input sub_group size.
+/// \tparam T Input value type
+/// \param [in] g Input sub_group
+/// \param [in] x Input value
+/// \param [in] mask Input mask
+/// \param [in] logical_sub_group_size Input logical sub_group size
+/// \returns The result
+template <typename T>
+T permute_sub_group_by_xor(sycl::sub_group g, T x, unsigned int mask,
+                           int logical_sub_group_size = 32) {
+  unsigned int id = g.get_local_linear_id();
+  unsigned int start_index =
+      id / logical_sub_group_size * logical_sub_group_size;
+  unsigned int target_offset = (id % logical_sub_group_size) ^ mask;
+  return sycl::select_from_group(g, x,
+                                 target_offset < logical_sub_group_size
+                                     ? start_index + target_offset
+                                     : id);
+}
+
+/// The function match_any_over_sub_group conducts a comparison of values
+/// across work-items within a sub-group. match_any_over_sub_group return a mask
+/// in which some bits are set to 1, indicating that the \p value provided by
+/// the work-item represented by these bits are equal. The n-th bit of mask
+/// representing the work-item with id n. The parameter \p member_mask
+/// indicating the work-items participating the call.
+/// \tparam T Input value type
+/// \param [in] g Input sub_group
+/// \param [in] member_mask Input mask
+/// \param [in] value Input value
+/// \returns The result
+template <typename T>
+unsigned int match_any_over_sub_group(sycl::sub_group g, unsigned member_mask,
+                                      T value) {
+  static_assert(std::is_arithmetic_v<T>, "Value type must be arithmetic type.");                    
+  if (!member_mask) {
+    return 0;
+  }
+  unsigned int id = g.get_local_linear_id();
+  unsigned int flag = 0, result = 0, reduce_result = 0;
+  unsigned int bit_index = 0x1 << id;
+  bool is_participate = member_mask & bit_index;
+  T broadcast_value = 0;
+  bool matched = false;
+  while (flag != member_mask) {
+    broadcast_value =
+        sycl::select_from_group(g, value, sycl::ctz((~flag & member_mask)));
+    reduce_result = sycl::reduce_over_group(
+        g, is_participate ? (broadcast_value == value ? bit_index : 0) : 0,
+        sycl::plus<>());
+    flag |= reduce_result;
+    matched = reduce_result & bit_index;
+    result = matched * reduce_result + (1 - matched) * result;
+  }
+  return result;
+}
+
+/// The function match_all_over_sub_group conducts a comparison of values
+/// across work-items within a sub-group. match_all_over_sub_group return \p
+/// member_mask and predicate \p pred will be set to 1 if all \p value that
+/// provided by each work-item in \p member_mask are equal, otherwise return 0
+/// and the predicate \p pred will be set to 0. The n-th bit of \p member_mask
+/// representing the work-item with id n. The parameter \p member_mask
+/// indicating the work-items participating the call.
+/// \tparam T Input value type
+/// \param [in] g Input sub_group
+/// \param [in] member_mask Input mask
+/// \param [in] value Input value
+/// \param [out] pred Output predicate
+/// \returns The result
+template <typename T>
+unsigned int match_all_over_sub_group(sycl::sub_group g, unsigned member_mask,
+                                      T value, int *pred) {
+  static_assert(std::is_arithmetic_v<T>, "Value type must be arithmetic type."); 
+  if (!member_mask) {
+    return 0;
+  }
+  unsigned int id = g.get_local_linear_id();
+  unsigned int bit_index = 0x1 << id;
+  bool is_participate = member_mask & bit_index;
+  T broadcast_value = sycl::select_from_group(g, value, sycl::ctz(member_mask));
+  unsigned int reduce_result = sycl::reduce_over_group(
+      g,
+      (member_mask & bit_index) ? (broadcast_value == value ? bit_index : 0)
+                                : 0,
+      sycl::plus<>());
+  bool all_equal = (reduce_result == member_mask);
+  *pred = is_participate & all_equal;
+  return all_equal * member_mask;
+}
+
+namespace experimental {
+/// Masked version of select_from_sub_group, which execute masked sub-group
+/// operation. The parameter member_mask indicating the work-items participating
+/// the call. Whether the n-th bit is set to 1 representing whether the
+/// work-item with id n is participating the call. All work-items named in
+/// member_mask must be executed with the same member_mask, or the result is
+/// undefined.
+/// \tparam T Input value type
+/// \param [in] member_mask Input mask
+/// \param [in] g Input sub_group
+/// \param [in] x Input value
+/// \param [in] remote_local_id Input source work item id
+/// \param [in] logical_sub_group_size Input logical sub_group size
+/// \returns The result
+template <typename T>
+T select_from_sub_group(unsigned int member_mask,
+                        sycl::sub_group g, T x, int remote_local_id,
+                        int logical_sub_group_size = 32) {
+  unsigned int start_index =
+      g.get_local_linear_id() / logical_sub_group_size * logical_sub_group_size;
+  unsigned logical_remote_id =
+      start_index + remote_local_id % logical_sub_group_size;
+#if defined(__SYCL_DEVICE_ONLY__) && defined(__INTEL_LLVM_COMPILER)
+#if defined(__SPIR__)
+  return __spirv_GroupNonUniformShuffle(__spv::Scope::Subgroup, x, logical_remote_id);
+#else
+  throw sycl::exception(sycl::errc::runtime, "Masked version of select_from_sub_group "
+                        "only supports SPIR-V backends.");
+#endif // __SPIR__
+#else
+  (void)g;
+  (void)x;
+  (void)remote_local_id;
+  (void)logical_sub_group_size;
+  (void)member_mask;
+  throw sycl::exception(sycl::errc::runtime, "Masked version of select_from_sub_group not "
+                        "supported on host device and none intel compiler.");
+#endif // __SYCL_DEVICE_ONLY__ && __INTEL_LLVM_COMPILER
+}
+
+/// Masked version of shift_sub_group_left, which execute masked sub-group
+/// operation. The parameter member_mask indicating the work-items participating
+/// the call. Whether the n-th bit is set to 1 representing whether the
+/// work-item with id n is participating the call. All work-items named in
+/// member_mask must be executed with the same member_mask, or the result is
+/// undefined.
+/// \tparam T Input value type
+/// \param [in] member_mask Input mask
+/// \param [in] g Input sub_group
+/// \param [in] x Input value
+/// \param [in] delta Input delta
+/// \param [in] logical_sub_group_size Input logical sub_group size
+/// \returns The result
+template <typename T>
+T shift_sub_group_left(unsigned int member_mask,
+                       sycl::sub_group g, T x, unsigned int delta,
+                       int logical_sub_group_size = 32) {
+  unsigned int id = g.get_local_linear_id();
+  unsigned int end_index =
+      (id / logical_sub_group_size + 1) * logical_sub_group_size;
+#if defined(__SYCL_DEVICE_ONLY__) && defined(__INTEL_LLVM_COMPILER)
+#if defined(__SPIR__)
+  T result = __spirv_GroupNonUniformShuffleDown(__spv::Scope::Subgroup, x, delta);
+  if ((id + delta) >= end_index) {
+    result = x;
+  }
+  return result;
+#else
+  throw sycl::exception(sycl::errc::runtime, "Masked version of shift_sub_group_left "
+                        "only supports SPIR-V backends.");
+#endif // __SPIR__
+#else
+  (void)g;
+  (void)x;
+  (void)delta;
+  (void)logical_sub_group_size;
+  (void)member_mask;
+  throw sycl::exception(sycl::errc::runtime, "Masked version of select_from_sub_group not "
+                        "supported on host device and none intel compiler.");
+#endif // __SYCL_DEVICE_ONLY__ && __INTEL_LLVM_COMPILER
+}
+
+/// Masked version of shift_sub_group_right, which execute masked sub-group
+/// operation. The parameter member_mask indicating the work-items participating
+/// the call. Whether the n-th bit is set to 1 representing whether the
+/// work-item with id n is participating the call. All work-items named in
+/// member_mask must be executed with the same member_mask, or the result is
+/// undefined.
+/// \tparam T Input value type
+/// \param [in] member_mask Input mask
+/// \param [in] g Input sub_group
+/// \param [in] x Input value
+/// \param [in] delta Input delta
+/// \param [in] logical_sub_group_size Input logical sub_group size
+/// \returns The result
+template <typename T>
+T shift_sub_group_right(unsigned int member_mask,
+                        sycl::sub_group g, T x, unsigned int delta,
+                        int logical_sub_group_size = 32) {
+  unsigned int id = g.get_local_linear_id();
+  unsigned int start_index =
+      id / logical_sub_group_size * logical_sub_group_size;
+#if defined(__SYCL_DEVICE_ONLY__) && defined(__INTEL_LLVM_COMPILER)
+#if defined(__SPIR__)
+  T result = __spirv_GroupNonUniformShuffleUp(__spv::Scope::Subgroup, x, delta);
+  if ((id - start_index) < delta) {
+    result = x;
+  }
+  return result;
+#else
+  throw sycl::exception(sycl::errc::runtime, "Masked version of shift_sub_group_right "
+                        "only supports SPIR-V backends.");
+#endif // __SPIR__
+#else
+  (void)g;
+  (void)x;
+  (void)delta;
+  (void)logical_sub_group_size;
+  (void)member_mask;
+  throw sycl::exception(sycl::errc::runtime, "Masked version of select_from_sub_group not "
+                        "supported on host device and none intel compiler.");
+#endif // __SYCL_DEVICE_ONLY && __INTEL_LLVM_COMPILER
+}
+
+/// Masked version of permute_sub_group_by_xor, which execute masked sub-group
+/// operation. The parameter member_mask indicating the work-items participating
+/// the call. Whether the n-th bit is set to 1 representing whether the
+/// work-item with id n is participating the call. All work-items named in
+/// member_mask must be executed with the same member_mask, or the result is
+/// undefined.
+/// \tparam T Input value type
+/// \param [in] member_mask Input mask
+/// \param [in] g Input sub_group
+/// \param [in] x Input value
+/// \param [in] mask Input mask
+/// \param [in] logical_sub_group_size Input logical sub_group size
+/// \returns The result
+template <typename T>
+T permute_sub_group_by_xor(unsigned int member_mask,
+                           sycl::sub_group g, T x, unsigned int mask,
+                           int logical_sub_group_size = 32) {
+  unsigned int id = g.get_local_linear_id();
+  unsigned int start_index =
+      id / logical_sub_group_size * logical_sub_group_size;
+  unsigned int target_offset = (id % logical_sub_group_size) ^ mask;
+  unsigned logical_remote_id = (target_offset < logical_sub_group_size) ? start_index + target_offset : id;
+#if defined(__SYCL_DEVICE_ONLY__) && defined(__INTEL_LLVM_COMPILER)
+#if defined(__SPIR__)
+  return __spirv_GroupNonUniformShuffle(__spv::Scope::Subgroup, x, logical_remote_id);
+#else
+  throw sycl::exception(sycl::errc::runtime, "Masked version of permute_sub_group_by_xor "
+                        "only supports SPIR-V backends.");
+#endif // __SPIR__
+#else
+  (void)g;
+  (void)x;
+  (void)mask;
+  (void)logical_sub_group_size;
+  (void)member_mask;
+  throw sycl::exception(sycl::errc::runtime, "Masked version of select_from_sub_group not "
+                        "supported on host device and none intel compiler.");
+#endif // __SYCL_DEVICE_ONLY__ && __INTEL_LLVM_COMPILER
+}
+} // namespace experimental
+
+/// Computes the multiplication of two complex numbers.
+/// \tparam T Complex element type
+/// \param [in] x The first input complex number
+/// \param [in] y The second input complex number
+/// \returns The result
+template <typename T>
+sycl::vec<T, 2> cmul(sycl::vec<T, 2> x, sycl::vec<T, 2> y) {
+  std::complex<T> t1(x[0], x[1]), t2(y[0], y[1]);
+  t1 = t1 * t2;
+  return sycl::vec<T, 2>(t1.real(), t1.imag());
+}
+
+/// Computes the division of two complex numbers.
+/// \tparam T Complex element type
+/// \param [in] x The first input complex number
+/// \param [in] y The second input complex number
+/// \returns The result
+template <typename T>
+sycl::vec<T, 2> cdiv(sycl::vec<T, 2> x, sycl::vec<T, 2> y) {
+  std::complex<T> t1(x[0], x[1]), t2(y[0], y[1]);
+  t1 = t1 / t2;
+  return sycl::vec<T, 2>(t1.real(), t1.imag());
+}
+
+/// Computes the magnitude of a complex number.
+/// \tparam T Complex element type
+/// \param [in] x The input complex number
+/// \returns The result
+template <typename T>
+T cabs(sycl::vec<T, 2> x) {
+  std::complex<T> t(x[0], x[1]);
+  return std::abs(t);
+}
+
+/// Computes the complex conjugate of a complex number.
+/// \tparam T Complex element type
+/// \param [in] x The input complex number
+/// \returns The result
+template <typename T>
+sycl::vec<T, 2> conj(sycl::vec<T, 2> x) {
+  std::complex<T> t(x[0], x[1]);
+  t = std::conj(t);
+  return sycl::vec<T, 2>(t.real(), t.imag());
+}
+
+inline int get_sycl_language_version() {
+#ifdef SYCL_LANGUAGE_VERSION
+  return SYCL_LANGUAGE_VERSION;
+#else
+  return 202000;
+#endif
+}
+
+namespace experimental {
+/// Synchronize work items from all work groups within a SYCL kernel.
+/// \param [in] item:  Represents a work group.
+/// \param [in] counter: An atomic object defined on a device memory which can
+/// be accessed by work items in all work groups. The initial value of the
+/// counter should be zero.
+/// Note: Please make sure that all the work items of all work groups within
+/// a SYCL kernel can be scheduled actively at the same time on a device.
+template <int dimensions = 3>
+inline void
+nd_range_barrier(const sycl::nd_item<dimensions> &item,
+                 sycl::atomic_ref<
+                     unsigned int, sycl::memory_order::seq_cst,
+                     sycl::memory_scope::device,
+                     sycl::access::address_space::global_space> &counter) {
+
+  static_assert(dimensions == 3, "dimensions must be 3.");
+
+  unsigned int num_groups = item.get_group_range(2) * item.get_group_range(1) *
+                            item.get_group_range(0);
+
+  item.barrier();
+
+  if (item.get_local_linear_id() == 0) {
+    unsigned int inc = 1;
+    unsigned int old_arrive = 0;
+    bool is_group0 =
+        (item.get_group(2) + item.get_group(1) + item.get_group(0) == 0);
+    if (is_group0) {
+      inc = 0x80000000 - (num_groups - 1);
+    }
+
+    old_arrive = counter.fetch_add(inc);
+    // Synchronize all the work groups
+    while (((old_arrive ^ counter.load()) & 0x80000000) == 0)
+      ;
+  }
+
+  item.barrier();
+}
+
+/// Synchronize work items from all work groups within a SYCL kernel.
+/// \param [in] item:  Represents a work group.
+/// \param [in] counter: An atomic object defined on a device memory which can
+/// be accessed by work items in all work groups. The initial value of the
+/// counter should be zero.
+/// Note: Please make sure that all the work items of all work groups within
+/// a SYCL kernel can be scheduled actively at the same time on a device.
+template <>
+inline void
+nd_range_barrier(const sycl::nd_item<1> &item,
+                 sycl::atomic_ref<
+                     unsigned int, sycl::memory_order::seq_cst,
+                     sycl::memory_scope::device,
+                     sycl::access::address_space::global_space> &counter) {
+  unsigned int num_groups = item.get_group_range(0);
+
+  item.barrier();
+
+  if (item.get_local_linear_id() == 0) {
+    unsigned int inc = 1;
+    unsigned int old_arrive = 0;
+    bool is_group0 = (item.get_group(0) == 0);
+    if (is_group0) {
+      inc = 0x80000000 - (num_groups - 1);
+    }
+
+    old_arrive = counter.fetch_add(inc);
+    // Synchronize all the work groups
+    while (((old_arrive ^ counter.load()) & 0x80000000) == 0)
+      ;
+  }
+
+  item.barrier();
+}
+
+/// The logical-group is a logical collection of some work-items within a
+/// work-group.
+/// Note: Please make sure that the logical-group size is a power of 2 in the
+/// range [1, current_sub_group_size].
+template <int dimensions = 3> class logical_group {
+  sycl::nd_item<dimensions> _item;
+  sycl::group<dimensions> _g;
+  uint32_t _logical_group_size;
+  uint32_t _group_linear_range_in_parent;
+
+public:
+  /// Dividing \p parent_group into several logical-groups.
+  /// \param [in] item Current work-item.
+  /// \param [in] parent_group The group to be divided.
+  /// \param [in] size The logical-group size.
+  logical_group(sycl::nd_item<dimensions> item,
+                sycl::group<dimensions> parent_group, uint32_t size)
+      : _item(item), _g(parent_group), _logical_group_size(size) {
+    _group_linear_range_in_parent =
+        (_g.get_local_linear_range() - 1) / _logical_group_size + 1;
+  }
+  logical_group(sycl::nd_item<dimensions> item)
+      : _item(item), _g(item.get_group()) {}
+  /// Returns the index of the work-item within the logical-group.
+  uint32_t get_local_linear_id() const {
+    return _item.get_local_linear_id() % _logical_group_size;
+  }
+  /// Returns the index of the logical-group in the parent group.
+  uint32_t get_group_linear_id() const {
+    return _item.get_local_linear_id() / _logical_group_size;
+  }
+  /// Returns the number of work-items in the logical-group.
+  uint32_t get_local_linear_range() const {
+    if (_g.get_local_linear_range() % _logical_group_size == 0) {
+      return _logical_group_size;
+    }
+    uint32_t last_item_group_id =
+        _g.get_local_linear_range() / _logical_group_size;
+    uint32_t first_of_last_group = last_item_group_id * _logical_group_size;
+    if (_item.get_local_linear_id() >= first_of_last_group) {
+      return _g.get_local_linear_range() - first_of_last_group;
+    } else {
+      return _logical_group_size;
+    }
+  }
+  /// Returns the number of logical-group in the parent group.
+  uint32_t get_group_linear_range() const {
+    return _group_linear_range_in_parent;
+  }
+};
+
+// The original source of the functions calculate_max_active_wg_per_xecore and
+// calculate_max_potential_wg were under the license below:
+//
+// Copyright (C) Intel Corporation
+//
+// Permission is hereby granted, free of charge, to any person obtaining a copy
+// of this software and associated documentation files (the "Software"), to deal
+// in the Software without restriction, including without limitation the rights
+// to use, copy, modify, merge, publish, distribute, sublicense, and/or sell
+// copies of the Software, and to permit persons to whom the Software is
+// furnished to do so, subject to the following conditions:
+//
+// The above copyright notice and this permission notice shall be included in
+// all copies or substantial portions of the Software.
+//
+// THE SOFTWARE IS PROVIDED "AS IS", WITHOUT WARRANTY OF ANY KIND, EXPRESS OR
+// IMPLIED, INCLUDING BUT NOT LIMITED TO THE WARRANTIES OF MERCHANTABILITY,
+// FITNESS FOR A PARTICULAR PURPOSE AND NONINFRINGEMENT. IN NO EVENT SHALL THE
+// AUTHORS OR COPYRIGHT HOLDERS BE LIABLE FOR ANY CLAIM, DAMAGES OR OTHER
+// LIABILITY, WHETHER IN AN ACTION OF CONTRACT, TORT OR OTHERWISE, ARISING FROM,
+// OUT OF OR IN CONNECTION WITH THE SOFTWARE OR THE USE OR OTHER DEALINGS IN THE
+// SOFTWARE.
+//
+/// This function is used for occupancy calculation, it computes the max active
+/// work-group number per Xe-Core. Ref to
+/// https://github.com/oneapi-src/oneAPI-samples/tree/master/Tools/GPU-Occupancy-Calculator
+/// \param [out] num_wg Active work-group number.
+/// \param [in] wg_size Work-group size.
+/// \param [in] slm_size Share local memory size.
+/// \param [in] sg_size Sub-group size.
+/// \param [in] used_barrier Whether barrier is used.
+/// \param [in] used_large_grf Whether large General Register File is used.
+/// \return If no error, returns 0.
+/// If \p wg_size exceeds the max work-group size, the max work-group size will
+/// be used instead of \p wg_size and returns -1.
+inline int calculate_max_active_wg_per_xecore(int *num_wg, int wg_size,
+                                              int slm_size = 0,
+                                              int sg_size = 32,
+                                              bool used_barrier = false,
+                                              bool used_large_grf = false) {
+  int ret = 0;
+  const int slm_size_per_xe_core = 64 * 1024;
+  const int max_barrier_registers = 32;
+  dpct::device_ext &dev = dpct::get_current_device();
+
+  size_t max_wg_size = dev.get_info<sycl::info::device::max_work_group_size>();
+  if (wg_size > max_wg_size) {
+    wg_size = max_wg_size;
+    ret = -1;
+  }
+
+  int num_threads_ss = 56;
+  int max_num_wg = 56;
+  if (dev.has(sycl::aspect::ext_intel_gpu_eu_count_per_subslice) &&
+      dev.has(sycl::aspect::ext_intel_gpu_hw_threads_per_eu)) {
+    auto eu_count =
+        dev.get_info<sycl::info::device::ext_intel_gpu_eu_count_per_subslice>();
+    auto threads_count =
+        dev.get_info<sycl::ext::intel::info::device::gpu_hw_threads_per_eu>();
+    num_threads_ss = eu_count * threads_count;
+    max_num_wg = eu_count * threads_count;
+  }
+
+  if (used_barrier) {
+    max_num_wg = max_barrier_registers;
+  }
+
+  // Calculate num_wg_slm
+  int num_wg_slm = 0;
+  if (slm_size == 0) {
+    num_wg_slm = max_num_wg;
+  } else {
+    num_wg_slm = std::floor((float)slm_size_per_xe_core / slm_size);
+  }
+
+  // Calculate num_wg_threads
+  if (used_large_grf)
+    num_threads_ss = num_threads_ss / 2;
+  int num_threads = std::ceil((float)wg_size / sg_size);
+  int num_wg_threads = std::floor((float)num_threads_ss / num_threads);
+
+  // Calculate num_wg
+  *num_wg = std::min(num_wg_slm, num_wg_threads);
+  *num_wg = std::min(*num_wg, max_num_wg);
+  return ret;
+}
+
+/// This function is used for occupancy calculation, it computes the work-group
+/// number and the work-group size which achieves the maximum occupancy of the
+/// device potentially. Ref to
+/// https://github.com/oneapi-src/oneAPI-samples/tree/master/Tools/GPU-Occupancy-Calculator
+/// \param [out] num_wg Work-group number.
+/// \param [out] wg_size Work-group size.
+/// \param [in] max_ws_size_for_device_code The maximum working work-group size
+/// for current device code logic. Zero means no limitation.
+/// \param [in] slm_size Share local memory size.
+/// \param [in] sg_size Sub-group size.
+/// \param [in] used_barrier Whether barrier is used.
+/// \param [in] used_large_grf Whether large General Register File is used.
+/// \return Returns 0.
+inline int calculate_max_potential_wg(int *num_wg, int *wg_size,
+                                      int max_ws_size_for_device_code,
+                                      int slm_size = 0, int sg_size = 32,
+                                      bool used_barrier = false,
+                                      bool used_large_grf = false) {
+  sycl::device &dev = dpct::get_current_device();
+  size_t max_wg_size = dev.get_info<sycl::info::device::max_work_group_size>();
+  if (max_ws_size_for_device_code == 0 ||
+      max_ws_size_for_device_code >= max_wg_size)
+    *wg_size = (int)max_wg_size;
+  else
+    *wg_size = max_ws_size_for_device_code;
+  calculate_max_active_wg_per_xecore(num_wg, *wg_size, slm_size, sg_size,
+                                     used_barrier, used_large_grf);
+  std::uint32_t num_ss = 1;
+  if (dev.has(sycl::aspect::ext_intel_gpu_slices) &&
+      dev.has(sycl::aspect::ext_intel_gpu_subslices_per_slice)) {
+    num_ss =
+        dev.get_info<sycl::ext::intel::info::device::gpu_slices>() *
+        dev.get_info<sycl::ext::intel::info::device::gpu_subslices_per_slice>();
+  }
+  num_wg[0] = num_ss * num_wg[0];
+  return 0;
+}
+
+/// Supported group type during migration.
+enum class group_type { work_group, sub_group, logical_group, root_group };
+
+/// The group_base will dispatch the function call to the specific interface
+/// based on the group type.
+template <int dimensions = 3> class group_base {
+public:
+  group_base(sycl::nd_item<dimensions> item)
+      : nd_item(item), logical_group(item) {}
+  ~group_base() {}
+  /// Returns the number of work-items in the group.
+  size_t get_local_linear_range() {
+    switch (type) {
+    case group_type::work_group:
+      return nd_item.get_group().get_local_linear_range();
+    case group_type::sub_group:
+      return nd_item.get_sub_group().get_local_linear_range();
+    case group_type::logical_group:
+      return logical_group.get_local_linear_range();
+    default:
+      return -1; // Unkonwn group type
+    }
+  }
+  /// Returns the index of the work-item within the group.
+  size_t get_local_linear_id() {
+    switch (type) {
+    case group_type::work_group:
+      return nd_item.get_group().get_local_linear_id();
+    case group_type::sub_group:
+      return nd_item.get_sub_group().get_local_linear_id();
+    case group_type::logical_group:
+      return logical_group.get_local_linear_id();
+    default:
+      return -1; // Unkonwn group type
+    }
+  }
+  /// Wait for all the elements within the group to complete their execution
+  /// before proceeding.
+  void barrier() {
+    switch (type) {
+    case group_type::work_group:
+      sycl::group_barrier(nd_item.get_group());
+      break;
+    case group_type::sub_group:
+    case group_type::logical_group:
+      sycl::group_barrier(nd_item.get_sub_group());
+      break;
+    default:
+      break;
+    }
+  }
+
+protected:
+  logical_group<dimensions> logical_group;
+  sycl::nd_item<dimensions> nd_item;
+  group_type type;
+};
+
+/// The group class is a container type that can storage supported group_type.
+template <typename T, int dimensions = 3>
+class group : public group_base<dimensions> {
+  using group_base<dimensions>::type;
+  using group_base<dimensions>::logical_group;
+
+public:
+  group(T g, sycl::nd_item<dimensions> item) : group_base<dimensions>(item) {
+    if constexpr (std::is_same_v<T, sycl::sub_group>) {
+      type = group_type::sub_group;
+    } else if constexpr (std::is_same_v<T, sycl::group<dimensions>>) {
+      type = group_type::work_group;
+    } else if constexpr (std::is_same_v<T, dpct::experimental::logical_group<
+                                               dimensions>>) {
+      logical_group = g;
+      type = group_type::logical_group;
+    }
+  }
+};
+} // namespace experimental
+
+/// If x <= 2, then return a pointer to the deafult queue;
+/// otherwise, return x reinterpreted as a dpct::queue_ptr.
+inline queue_ptr int_as_queue_ptr(uintptr_t x) {
+  return x <= 2 ?
+  &get_default_queue()
+  : reinterpret_cast<queue_ptr>(x);
+}
+
+template <int n_nondefault_params, int n_default_params, typename T>
+class args_selector;
+
+/// args_selector is a helper class for extracting arguments from an
+/// array of pointers to arguments or buffer of arguments to pass to a
+/// kernel function.
+///
+/// \param R(Ts...) The type of the kernel
+/// \param n_nondefault_params The number of nondefault parameters of the kernel
+/// (excluding parameters that like sycl::nd_item, etc.)
+/// \param n_default_params The number of default parameters of the kernel
+///
+/// Example usage:
+/// With the following kernel:
+///   void foo(sycl::float2 *x, int n, sycl::nd_item<3> item_ct1, float f=.1) {}
+/// and with the declaration:
+///   args_selector<2, 1, decltype(foo)> selector(kernelParams, extra);
+/// we have:
+///   selector.get<0>() returns a reference to sycl::float*,
+///   selector.get<1>() returns a reference to int,
+///   selector.get<2>() returns a reference to float
+template <int n_nondefault_params, int n_default_params,
+   typename R, typename... Ts>
+class args_selector<n_nondefault_params, n_default_params, R(Ts...)> {
+private:
+  void **kernel_params;
+  char *args_buffer;
+
+  template <int i>
+  static constexpr int account_for_default_params() {
+    constexpr int n_total_params = sizeof...(Ts);
+    if constexpr (i >= n_nondefault_params) {
+      return n_total_params - n_default_params + (i - n_nondefault_params);
+    } else {
+      return i;
+    }
+  }    
+
+public:
+  /// Get the type of the ith argument of R(Ts...)
+  /// \param [in] i Index of parameter to get
+  /// \returns Type of ith parameter
+  template <int i>
+  using arg_type = std::tuple_element_t<account_for_default_params<i>(),
+					  std::tuple<Ts...>>;
+private:
+  template <int i>
+  static constexpr int get_offset() {
+    if constexpr (i == 0) {
+      // we can assume args_buffer is properly aligned to the
+      // first argument
+      return 0;
+    } else {
+      constexpr int prev_off = get_offset<i-1>();
+      constexpr int prev_past_end = prev_off + sizeof(arg_type<i-1>);
+      using T = arg_type<i>;
+      // is the past-the-end of the i-1st element properly aligned
+      // with the ith element's alignment?
+      if constexpr (prev_past_end % alignof(T) == 0) {
+	return prev_past_end;
+      }
+      // otherwise bump prev_past_end to match alignment
+      else {
+	return prev_past_end + (alignof(T) - (prev_past_end % alignof(T)));
+      }
+    }
+  }
+
+  static char *get_args_buffer(void **extra) {
+    if (!extra)
+      return nullptr;
+    for (; (std::size_t) *extra != 0; ++extra) {
+      if ((std::size_t) *extra == 1) {
+	return static_cast<char*>(*(extra+1));
+      }
+    }
+    return nullptr;
+  }
+    
+public:
+  /// If kernel_params is nonnull, then args_selector will
+  /// extract arguments from kernel_params. Otherwise, it
+  /// will extract them from extra.
+  /// \param [in] kernel_params Array of pointers to arguments
+  /// a or null pointer.
+  /// \param [in] extra Array containing pointer to argument buffer.
+  args_selector(void **kernel_params, void **extra)
+    : kernel_params(kernel_params),
+      args_buffer(get_args_buffer(extra))
+  {}
+
+  /// Get a reference to the ith argument extracted from kernel_params
+  /// or extra.
+  /// \param [in] i Index of argument to get
+  /// \returns Reference to the ith argument
+  template <int i>    
+  arg_type<i> &get() {
+    if (kernel_params) {
+      return *static_cast<arg_type<i>*>(kernel_params[i]);
+    } else {
+      return *reinterpret_cast<arg_type<i>*>(args_buffer + get_offset<i>());
+    }
+  }
+};
+
+#ifdef _WIN32
+#define DPCT_EXPORT __declspec(dllexport)
+#else
+#define DPCT_EXPORT
+#endif
+
+} // namespace dpct
+
+#endif // __DPCT_UTIL_HPP__
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/dropout.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/dropout.hpp
deleted file mode 100644
index 69cb6c0..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/includes/dropout.hpp
+++ /dev/null
@@ -1,82 +0,0 @@
-#pragma once
-
-#include <stdio.h>
-#if __has_include(<sycl/sycl.hpp>)
-#include <sycl/sycl.hpp>
-#elif __has_include(<CL/sycl.hpp>)
-#include <CL/sycl.hpp>
-#else
-#error "Unsupported compiler"
-#endif
-#include "custom_sycl_layers.hpp"
-
-template <typename T>
-class Dropout {
-public:
-    struct Config {
-        float ratio;
-        uint32_t dim;
-        bool training;
-
-        Config(float r, uint32_t d) : ratio(r), dim(d), training(true) {}
-
-        float RATIO() const { return training ? ratio : 0.0; }
-        inline void SetDim(uint32_t d) { dim = d; }
-    };
-
-    Dropout(const Config& config) : _config(config), _mask(nullptr) {}
-
-    virtual ~Dropout() {}
-
-    void Forward(int bsz, T* out, const T* vals, sycl::queue* stream, bool bwd = false)
-    {
-        launch_dropout<T>(
-            out, vals, _mask, bsz * _config.dim, _config.dim, _config.RATIO(), stream, bwd);
-    }
-
-    void ForwardWithBias(int bsz, T* vals, const T* bias, sycl::queue* stream)
-    {
-        launch_dropout<T>(vals, bias, _mask, bsz, _config.dim, _config.RATIO(), stream);
-    }
-
-    void ForwardWithBias(int bsz,
-                         T* out,
-                         const T* vals,
-                         const T* residual,
-                         const T* bias,
-                         sycl::queue* stream)
-    {
-        launch_dropout<T>(
-            out, vals, residual, bias, _mask, bsz, _config.dim, _config.RATIO(), stream);
-    }
-
-    void Backward(int bsz, T* d_vals, sycl::queue* stream)
-    {
-        launch_dropout_grad<T>(d_vals, _mask, bsz * _config.dim, _config.RATIO(), stream);
-    }
-
-    void Backward(int bsz, T* d_vals_out, const T* d_vals, sycl::queue* stream)
-    {
-        launch_dropout_grad<T>(
-            d_vals_out, d_vals, _mask, bsz * _config.dim, _config.RATIO(), stream);
-    }
-
-    bool HasDropout() const { return _config.RATIO() > 0.0; }
-
-    void SetTrainingMode(bool training) { _config.training = training; }
-
-    void SetMask(uint8_t* mask)
-    {
-        if (!mask) { throw std::runtime_error("Dropout mask is null."); }
-
-        _mask = mask;
-    }
-
-    Config GetConfig() const { return _config; }
-
-    inline void SetDimension(uint32_t dim) { _config.SetDim(dim); }
-
-private:
-    uint8_t* _mask;
-    Config _config;
-};
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/ds_kernel_utils.h b/intel_extension_for_deepspeed/op_builder/csrc/includes/ds_kernel_utils.h
new file mode 100644
index 0000000..b0a67c0
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/includes/ds_kernel_utils.h
@@ -0,0 +1,54 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+/*
+Centralized header file for preprocessor macros and constants
+used throughout the codebase.
+*/
+
+#pragma once
+
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+
+#ifdef BF16_AVAILABLE
+#endif
+
+#define DS_HD_INLINE __dpct_inline__
+#define DS_D_INLINE __dpct_inline__
+
+#ifdef __HIP_PLATFORM_AMD__
+
+// constexpr variant of warpSize for templating
+constexpr int hw_warp_size = 64;
+#define HALF_PRECISION_AVAILABLE = 1
+#include <hip/hip_cooperative_groups.h>
+#include <hip/hip_fp16.h>
+
+#else  // !__HIP_PLATFORM_AMD__
+
+// constexpr variant of warpSize for templating
+constexpr int hw_warp_size = 32;
+
+#if DPCT_COMPATIBILITY_TEMP >= 530
+#define HALF_PRECISION_AVAILABLE = 1
+// #define PTX_AVAILABLE
+#endif  // __CUDA_ARCH__ >= 530
+
+#if DPCT_COMPATIBILITY_TEMP >= 800
+#define ASYNC_COPY_AVAILABLE
+#endif  // __CUDA_ARCH__ >= 800
+
+#endif  //__HIP_PLATFORM_AMD__
+
+inline int next_pow2(const int val)
+{
+    int rounded_val = val - 1;
+    rounded_val |= rounded_val >> 1;
+    rounded_val |= rounded_val >> 2;
+    rounded_val |= rounded_val >> 4;
+    rounded_val |= rounded_val >> 8;
+    return rounded_val + 1;
+}
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/ds_transformer_sycl.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/ds_transformer_sycl.hpp
deleted file mode 100644
index cfee1fc..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/includes/ds_transformer_sycl.hpp
+++ /dev/null
@@ -1,190 +0,0 @@
-#pragma once
-
-#if __has_include(<sycl/sycl.hpp>)
-#include <sycl/sycl.hpp>
-#elif __has_include(<CL/sycl.hpp>)
-#include <CL/sycl.hpp>
-#else
-#error "Unsupported compiler"
-#endif
-#include <memory>
-#include <oneapi/mkl.hpp>
-#include <oneapi/mkl/rng/device.hpp>
-#include <vector>
-
-#include "dropout.hpp"
-#include "feed_forward.hpp"
-#include "gelu.hpp"
-#include "general_kernels.hpp"
-#include "normalize_layer.hpp"
-#include "softmax.hpp"
-#include "strided_batch_gemm.hpp"
-
-struct BertGemmAlgos {
-    int m_gemm_qkv_algo;
-    int m_gemm_inter_algo;
-    int m_gemm_output_algo;
-    int m_gemm_batch1_algo;
-    int m_gemm_batch2_algo;
-
-    BertGemmAlgos()
-        : m_gemm_qkv_algo(-1),
-          m_gemm_inter_algo(-1),
-          m_gemm_output_algo(-1),
-          m_gemm_batch1_algo(-1),
-          m_gemm_batch2_algo(-1)
-    {
-    }
-};
-
-template <typename T>
-class BertTransformerLayer {
-public:
-    BertTransformerLayer(int layer_id,
-                         int batch_size,
-                         int hidden_size,
-                         int num_heads,
-                         int intermediate_size,
-                         int seq_length,
-                         float attn_dropout_ratio,
-                         float hidden_output_dropout_ratio,
-                         float layer_norm_eps,
-                         bool pre_or_postLayerNorm,
-                         const std::vector<std::array<int, 3>>& gemm_algos,
-                         bool attn_dropout_checkpoint,
-                         bool normalize_invertible,
-                         bool gelu_checkpoint,
-                         bool stochastic_mode);
-
-    virtual ~BertTransformerLayer();
-
-    void Forward(int bsz,
-                 const T* input_ptr,
-                 const T* input_mask_ptr,
-                 const T* attn_qkvw_ptr,
-                 const T* attn_qkvb_ptr,
-                 const T* attn_ow_ptr,
-                 const T* attn_ob_ptr,
-                 const T* attn_nw_ptr,
-                 const T* attn_nb_ptr,
-                 const T* inter_w_ptr,
-                 const T* inter_b_ptr,
-                 const T* output_w_ptr,
-                 const T* output_b_ptr,
-                 const T* norm_w_ptr,
-                 const T* norm_b_ptr,
-                 T* out_ptr,
-                 T* inp_norm_ptr,
-                 T* q_tf_ptr,
-                 T* k_tf_ptr,
-                 T* v_tf_ptr,
-                 T* softmax_output_ptr,
-                 T* ctx_bufB_ptr,
-                 T* attn_o_inp_ptr,
-                 T* add_res_ptr,
-                 T* ff1_inp_ptr,
-                 T* gelu_inp_ptr,
-                 T* ff2_inp_ptr);
-
-    void Backward(int bsz,
-                  const T* grad_output_ptr,
-                  const T* input_ptr,
-                  const T* output_ptr,
-                  const T* inp_norm_ptr,
-                  const T* q_tf_ptr,
-                  const T* k_tf_ptr,
-                  const T* v_tf_ptr,
-                  const T* softmax_output_ptr,
-                  const T* ctx_bufB_ptr,
-                  const T* attn_o_inp_ptr,
-                  const T* add_res_ptr,
-                  const T* ff1_inp_ptr,
-                  const T* gelu_inp_ptr,
-                  const T* ff2_inp_ptr,
-                  const T* input_mask_ptr,
-                  const T* attn_qkvw_ptr,
-                  const T* attn_ow_ptr,
-                  const T* attn_nw_ptr,
-                  const T* attn_nb_ptr,
-                  const T* inter_w_ptr,
-                  const T* inter_b_ptr,
-                  const T* output_w_ptr,
-                  const T* norm_w_ptr,
-                  const T* norm_b_ptr,
-
-                  T* grad_input_ptr,
-                  T* grad_attn_qkvw_ptr,
-                  T* grad_attn_qkvb_ptr,
-                  T* grad_attn_ow_ptr,
-                  T* grad_attn_ob_ptr,
-                  T* grad_attn_nw_ptr,
-                  T* grad_attn_nb_ptr,
-                  T* grad_inter_w_ptr,
-                  T* grad_inter_b_ptr,
-                  T* grad_output_w_ptr,
-                  T* grad_output_b_ptr,
-                  T* grad_norm_w_ptr,
-                  T* grad_norm_b_ptr);
-
-    void SetIntermediateBuffers(uint8_t* attn_prob_dropout_mask_ptr,
-                                uint8_t* attn_output_dropout_mask_ptr,
-                                uint8_t* layer_output_dropout_mask_ptr,
-                                T* layer_norm_var,
-                                T* layer_norm_mean,
-                                T* attn_layer_norm_var,
-                                T* attn_layer_norm_mean);
-
-    inline int GetBatchSize() const { return _batch_size; }
-    inline int GetNumHeads() const { return _heads; }
-    inline int GetSeqLength() const { return _seq_length; }
-    inline int GetIntermediateSize() const { return _intermediate_size; }
-
-    void SetSeqLength(int seq_len);
-    inline int GetHiddenSize() const { return _hidden_size; }
-    void SetTrainingMode(bool training);
-    inline bool IsTrainingMode() const { return _training; }
-    inline bool GeluCheckpoint() const { return _gelu_checkpoint; }
-
-private:
-    void Initialize();
-    size_t getWorkspaceSize(int maxBatchSize) const;
-
-    // Params
-    int _layer_id;
-    int _batch_size;
-    int _hidden_size;
-    int _heads;
-    int _size_per_head;
-    int _intermediate_size;
-    int _seq_length;
-
-    bool _pre_or_postLayerNorm;
-
-    sycl::queue* _onemklQ;
-    sycl::queue* _stream;
-
-    // layers
-    FeedForward<T> _qkv_linear;
-    FeedForward<T> _attn_out_linear;
-    Normalize_Layer<T> _attn_layer_norm;
-    Normalize_Layer<T> _layer_norm;
-    Normalize_Layer<T>* _last_normalize;
-    FeedForward<T> _ff1, _ff2;
-    Softmax<T> _softmax;
-    Gelu<T> _gelu;
-    Dropout<T> _attn_prob_dropout;
-    Dropout<T> _attn_output_dropout;
-    Dropout<T> _layer_output_dropout;
-    StridedBatchGemm<T> _attn_scores;
-    StridedBatchGemm<T> _attn_context;
-
-    bool _training;
-
-    // Memory saving flags
-    bool _attn_dropout_checkpoint;
-    bool _normalize_invertible;
-    bool _gelu_checkpoint;
-
-    // High Performace flags
-    bool _stochastic_mode;
-};
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/feed_forward.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/feed_forward.hpp
deleted file mode 100644
index 6a24aae..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/includes/feed_forward.hpp
+++ /dev/null
@@ -1,131 +0,0 @@
-#pragma once
-
-#include <stdio.h>
-#if __has_include(<sycl/sycl.hpp>)
-#include <sycl/sycl.hpp>
-#elif __has_include(<CL/sycl.hpp>)
-#include <CL/sycl.hpp>
-#else
-#error "Unsupported compiler"
-#endif
-#include "custom_sycl_layers.hpp"
-
-template <typename T>
-class FeedForward {
-public:
-    struct Config {
-        int batchSize, outputSize;
-        int inputSize;
-        Config(int batch, int outputs, int inputs)
-            : batchSize(batch), outputSize(outputs), inputSize(inputs)
-        {
-        }
-    };
-
-    FeedForward(Config config) : config_(config) {}
-
-    ~FeedForward() {}
-
-    void Forward(int bsz, const T* input_ptr, const T* weights, T* out, sycl::queue* _Q)
-    {
-        if constexpr (std::is_same_v<bf16, T>) {
-            float alpha = 1.0f;
-            float beta = 0.0f;
-            onednn_matmul_ex(_Q,
-                             false,
-                             true,
-                             bsz,
-                             config_.outputSize,
-                             config_.inputSize,
-                             alpha,
-                             beta,
-                             input_ptr,
-                             weights,
-                             out);
-        } else {
-            T alpha = T(1.);
-            T beta = T(0.);
-            onemkl_gemm_ex(_Q,
-                           oneapi::mkl::transpose::trans,
-                           oneapi::mkl::transpose::nontrans,
-                           config_.outputSize,
-                           bsz,
-                           config_.inputSize,
-                           alpha,
-                           beta,
-                           weights,
-                           input_ptr,
-                           out);
-        }
-    }
-    void Backward(int bsz,
-                  const T* out_grad,
-                  const T* input_ptr,
-                  const T* weights,
-                  T* weights_grad,
-                  T* bias_grad,
-                  sycl::queue* _Q,
-                  sycl::queue* stream,
-                  T* inp_grad_out = nullptr,
-                  T* out_grad_trans_out = nullptr)
-    {
-        if constexpr (std::is_same_v<bf16, T>) {
-            float alpha = 1.0f;
-            float beta = 0.0f;
-            onednn_matmul_ex(stream,
-                             true,
-                             false,
-                             config_.outputSize,
-                             config_.inputSize,
-                             bsz,
-                             alpha,
-                             beta,
-                             out_grad,
-                             input_ptr,
-                             weights_grad);
-            onednn_matmul_ex(stream,
-                             false,
-                             false,
-                             bsz,
-                             config_.inputSize,
-                             config_.outputSize,
-                             alpha,
-                             beta,
-                             out_grad,
-                             weights,
-                             inp_grad_out);
-            launch_fuse_transpose_bias_kernel<T>(
-                out_grad, bias_grad, bsz, config_.outputSize, stream);
-        } else {
-            T alpha = (T)1.0;
-            T beta = (T)0.0;
-            onemkl_gemm_ex(_Q,
-                           oneapi::mkl::transpose::nontrans,
-                           oneapi::mkl::transpose::trans,
-                           config_.inputSize,
-                           config_.outputSize,
-                           bsz,
-                           alpha,
-                           beta,
-                           input_ptr,
-                           out_grad,
-                           weights_grad);
-            onemkl_gemm_ex(_Q,
-                           oneapi::mkl::transpose::nontrans,
-                           oneapi::mkl::transpose::nontrans,
-                           config_.inputSize,
-                           bsz,
-                           config_.outputSize,
-                           alpha,
-                           beta,
-                           weights,
-                           out_grad,
-                           inp_grad_out);
-            launch_fuse_transpose_bias_kernel<T>(
-                out_grad, bias_grad, bsz, config_.outputSize, stream);
-        }
-    }
-
-private:
-    Config config_;
-};
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/gelu.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/gelu.hpp
deleted file mode 100644
index f773eee..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/includes/gelu.hpp
+++ /dev/null
@@ -1,41 +0,0 @@
-#pragma once
-
-#include <stdio.h>
-#if __has_include(<sycl/sycl.hpp>)
-#include <sycl/sycl.hpp>
-#elif __has_include(<CL/sycl.hpp>)
-#include <CL/sycl.hpp>
-#else
-#error "Unsupported compiler"
-#endif
-#include "custom_sycl_layers.hpp"
-
-template <typename T>
-class Gelu {
-public:
-    struct Config {
-        uint32_t intermediate_size;
-        Config(uint32_t inter_size) : intermediate_size(inter_size) {}
-    };
-
-    Gelu(const Config& config) : _config(config) {}
-
-    virtual ~Gelu() {}
-
-    void ForwardWithBiasAdd(int bsz,
-                            const T* input_buf,
-                            const T* bias,
-                            T* output,
-                            sycl::queue* stream)
-    {
-        launch_bias_gelu<T>(input_buf, bias, output, _config.intermediate_size, bsz, stream);
-    }
-
-    void Backward(int bsz, T* d_output, const T* input_buf, const T* bias, sycl::queue* stream)
-    {
-        launch_d_gelu<T>(d_output, input_buf, bias, _config.intermediate_size, bsz, stream);
-    }
-
-private:
-    Config _config;
-};
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/gemm_test.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/gemm_test.h
similarity index 65%
rename from intel_extension_for_deepspeed/op_builder/csrc/includes/gemm_test.hpp
rename to intel_extension_for_deepspeed/op_builder/csrc/includes/gemm_test.h
index 8a7f44d..f1e948e 100644
--- a/intel_extension_for_deepspeed/op_builder/csrc/includes/gemm_test.hpp
+++ b/intel_extension_for_deepspeed/op_builder/csrc/includes/gemm_test.h
@@ -1,11 +1,16 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
 #pragma once
 
-#if __has_include(<sycl/sycl.hpp>)
 #include <sycl/sycl.hpp>
-#elif __has_include(<CL/sycl.hpp>)
-#include <CL/sycl.hpp>
-#else
-#error "Unsupported compiler"
+#include <dpct/dpct.hpp>
+#ifndef __HIP_PLATFORM_AMD__
+#endif
+#ifdef __HIP_PLATFORM_AMD__
+#include <rocblas/rocblas.h>
 #endif
 #include <array>
 #include <cstdio>
@@ -14,7 +19,19 @@
 #include <limits>
 #include <memory>
 #include "StopWatch.h"
-#include "onemkl_wrappers.hpp"
+#include "cublas_wrappers.h"
+#include <cmath>
+
+template <typename T>
+void check(T result, char const* const func, const char* const file, int const line)
+{
+    if (result) {
+        std::cout << (std::string("CUDA runtime error: ") + +file + ":" + std::to_string(line) +
+                      " \n");
+    }
+}
+
+#define check_cuda_error(val) check((val), #val, __FILE__, __LINE__)
 
 template <typename T>
 class GemmTest {
@@ -24,23 +41,23 @@ public:
              int k,
              oneapi::mkl::transpose ta,
              oneapi::mkl::transpose tb,
-             sycl::queue* h)
+             dpct::queue_ptr h)
         : M(m), N(n), K(k), transa(ta), transb(tb), handle(h)
     {
-        dpct::device_ext& dev_ct1 = dpct::get_current_device();
-        sycl::queue& q_ct1 = dev_ct1.default_queue();
-        A = (T*)sycl::malloc_device(sizeof(T) * M * K, q_ct1);
-        B = (T*)sycl::malloc_device(sizeof(T) * K * N, q_ct1);
-        C = (T*)sycl::malloc_device(sizeof(T) * M * N, q_ct1);
+    dpct::device_ext& dev_ct1 = dpct::get_current_device();
+    sycl::queue& q_ct1 = dev_ct1.in_order_queue();
+        check_cuda_error(DPCT_CHECK_ERROR(A = (T*)sycl::malloc_device(sizeof(T) * M * K, q_ct1)));
+        check_cuda_error(DPCT_CHECK_ERROR(B = (T*)sycl::malloc_device(sizeof(T) * K * N, q_ct1)));
+        check_cuda_error(DPCT_CHECK_ERROR(C = (T*)sycl::malloc_device(sizeof(T) * M * N, q_ct1)));
     }
 
     ~GemmTest()
     {
-        dpct::device_ext& dev_ct1 = dpct::get_current_device();
-        sycl::queue& q_ct1 = dev_ct1.default_queue();
-        sycl::free(A, q_ct1);
-        sycl::free(B, q_ct1);
-        sycl::free(C, q_ct1);
+    dpct::device_ext& dev_ct1 = dpct::get_current_device();
+    sycl::queue& q_ct1 = dev_ct1.in_order_queue();
+        check_cuda_error(DPCT_CHECK_ERROR(sycl::free(A, q_ct1)));
+        check_cuda_error(DPCT_CHECK_ERROR(sycl::free(B, q_ct1)));
+        check_cuda_error(DPCT_CHECK_ERROR(sycl::free(C, q_ct1)));
     }
 
     std::array<int, 3> TestAlgo(int loops)
@@ -49,7 +66,7 @@ public:
         float beta = (T)0.0f;
 
         int algo_fw = Run(loops, [=](int algo) {
-            onemkl_gemm_ex(handle,
+            cublas_gemm_ex(handle,
                            oneapi::mkl::transpose::trans,
                            oneapi::mkl::transpose::nontrans,
                            N,
@@ -60,11 +77,15 @@ public:
                            B,
                            A,
                            C,
-                           static_cast<cublasGemmAlgo_t>(algo));
+#ifdef __HIP_PLATFORM_AMD__
+                           static_cast<rocblas_gemm_algo>(algo));
+#else
+                           static_cast<int>(algo));
+#endif
         });
 
         int algo_bw1 = Run(loops, [=](int algo) {
-            onemkl_gemm_ex(handle,
+            cublas_gemm_ex(handle,
                            oneapi::mkl::transpose::nontrans,
                            oneapi::mkl::transpose::trans,
                            K,
@@ -75,11 +96,15 @@ public:
                            A,
                            C,
                            B,
-                           static_cast<cublasGemmAlgo_t>(algo));
+#ifdef __HIP_PLATFORM_AMD__
+                           static_cast<rocblas_gemm_algo>(algo));
+#else
+                           static_cast<int>(algo));
+#endif
         });
 
         int algo_bw2 = Run(loops, [=](int algo) {
-            onemkl_gemm_ex(handle,
+            cublas_gemm_ex(handle,
                            oneapi::mkl::transpose::nontrans,
                            oneapi::mkl::transpose::nontrans,
                            K,
@@ -90,7 +115,11 @@ public:
                            B,
                            C,
                            A,
-                           static_cast<cublasGemmAlgo_t>(algo));
+#ifdef __HIP_PLATFORM_AMD__
+                           static_cast<rocblas_gemm_algo>(algo));
+#else
+                           static_cast<int>(algo));
+#endif
         });
 
         return std::array<int, 3>({algo_fw, algo_bw1, algo_bw2});
@@ -99,22 +128,26 @@ public:
     template <typename Func>
     int Run(int loops, Func f)
     {
+    dpct::device_ext& dev_ct1 = dpct::get_current_device();
         float fast_latency = (std::numeric_limits<float>::max)();
         int fast_algo = 0;
 
-        for (int algo = (int)CUBLAS_GEMM_DEFAULT_TENSOR_OP;
-             algo <= (int)CUBLAS_GEMM_ALGO15_TENSOR_OP;
+#ifdef __HIP_PLATFORM_AMD__
+        for (int algo = (int)rocblas_gemm_algo_standard; algo <= (int)rocblas_gemm_algo_standard;
+#else
+        for (int algo = (int)99; algo <= (int)115;
+#endif
              algo++) {
             int warm_up = 5;
             for (int i = 0; i < warm_up; ++i) f(algo);
 
-            cudaDeviceSynchronize();
+            dev_ct1.queues_wait_and_throw();
             Stopwatch timer;
             timer.Restart();
 
             for (int i = 0; i < loops; ++i) f(algo);
 
-            cudaDeviceSynchronize();
+            dev_ct1.queues_wait_and_throw();
             timer.Stop();
 
             float avg_latency = (float)timer.GetTimeInSeconds() * 1000 / loops;
@@ -134,7 +167,7 @@ public:
 
 private:
     int M, N, K;
-    sycl::queue* handle;
+    dpct::queue_ptr handle;
     oneapi::mkl::transpose transa, transb;
     T *A, *B, *C;
 };
@@ -148,23 +181,26 @@ public:
                     int k,
                     oneapi::mkl::transpose ta,
                     oneapi::mkl::transpose tb,
-                    sycl::queue* h)
+                    dpct::queue_ptr h)
         : bsz(b), M(m), N(n), K(k), transa(ta), transb(tb), handle(h)
     {
-        dpct::device_ext& dev_ct1 = dpct::get_current_device();
-        sycl::queue& q_ct1 = dev_ct1.default_queue();
-        A = (T*)sycl::malloc_device(sizeof(T) * M * K * bsz, q_ct1);
-        B = (T*)sycl::malloc_device(sizeof(T) * K * N * bsz, q_ct1);
-        C = (T*)sycl::malloc_device(sizeof(T) * M * N * bsz, q_ct1);
+    dpct::device_ext& dev_ct1 = dpct::get_current_device();
+    sycl::queue& q_ct1 = dev_ct1.in_order_queue();
+        check_cuda_error(
+            DPCT_CHECK_ERROR(A = (T*)sycl::malloc_device(sizeof(T) * M * K * bsz, q_ct1)));
+        check_cuda_error(
+            DPCT_CHECK_ERROR(B = (T*)sycl::malloc_device(sizeof(T) * K * N * bsz, q_ct1)));
+        check_cuda_error(
+            DPCT_CHECK_ERROR(C = (T*)sycl::malloc_device(sizeof(T) * M * N * bsz, q_ct1)));
     }
 
     ~StridedGemmTest()
     {
-        dpct::device_ext& dev_ct1 = dpct::get_current_device();
-        sycl::queue& q_ct1 = dev_ct1.default_queue();
-        sycl::free(A, q_ct1);
-        sycl::free(B, q_ct1);
-        sycl::free(C, q_ct1);
+    dpct::device_ext& dev_ct1 = dpct::get_current_device();
+    sycl::queue& q_ct1 = dev_ct1.in_order_queue();
+        check_cuda_error(DPCT_CHECK_ERROR(sycl::free(A, q_ct1)));
+        check_cuda_error(DPCT_CHECK_ERROR(sycl::free(B, q_ct1)));
+        check_cuda_error(DPCT_CHECK_ERROR(sycl::free(C, q_ct1)));
     }
 
     std::array<int, 3> TestAlgo(int loops)
@@ -192,7 +228,11 @@ public:
                                         stride_b,
                                         stride_c,
                                         bsz,
-                                        static_cast<cublasGemmAlgo_t>(algo));
+#ifdef __HIP_PLATFORM_AMD__
+                                        static_cast<rocblas_gemm_algo>(algo));
+#else
+                                        static_cast<int>(algo));
+#endif
         });
 
         int algo_bw1 = Run(loops, [=](int algo) {
@@ -204,7 +244,7 @@ public:
             int stride_c = M * K;
 
             // B need to transpose.
-            cublasOperation_t op_b =
+            oneapi::mkl::transpose op_b =
                 (transb == oneapi::mkl::transpose::trans ? oneapi::mkl::transpose::nontrans
                                                          : oneapi::mkl::transpose::trans);
 
@@ -224,12 +264,16 @@ public:
                                         stride_b,
                                         stride_c,
                                         bsz,
-                                        static_cast<cublasGemmAlgo_t>(algo));
+#ifdef __HIP_PLATFORM_AMD__
+                                        static_cast<rocblas_gemm_algo>(algo));
+#else
+                                        static_cast<int>(algo));
+#endif
         });
 
         int algo_bw2 = Run(loops, [=](int algo) {
             // A need to transpose.
-            cublasOperation_t op_a =
+            oneapi::mkl::transpose op_a =
                 (transa == oneapi::mkl::transpose::trans ? oneapi::mkl::transpose::nontrans
                                                          : oneapi::mkl::transpose::trans);
 
@@ -253,7 +297,11 @@ public:
                                         stride_b,
                                         stride_c,
                                         bsz,
-                                        static_cast<cublasGemmAlgo_t>(algo));
+#ifdef __HIP_PLATFORM_AMD__
+                                        static_cast<rocblas_gemm_algo>(algo));
+#else
+                                        static_cast<int>(algo));
+#endif
         });
 
         return std::array<int, 3>({algo_fw, algo_bw1, algo_bw2});
@@ -262,22 +310,26 @@ public:
     template <typename Func>
     int Run(int loops, Func f)
     {
+    dpct::device_ext& dev_ct1 = dpct::get_current_device();
         float fast_latency = (std::numeric_limits<float>::max)();
         int fast_algo = 0;
 
-        for (int algo = (int)CUBLAS_GEMM_DEFAULT_TENSOR_OP;
-             algo <= (int)CUBLAS_GEMM_ALGO15_TENSOR_OP;
+#ifdef __HIP_PLATFORM_AMD__
+        for (int algo = (int)rocblas_gemm_algo_standard; algo <= (int)rocblas_gemm_algo_standard;
+#else
+        for (int algo = (int)99; algo <= (int)115;
+#endif
              algo++) {
             int warm_up = 5;
             for (int i = 0; i < warm_up; ++i) f(algo);
 
-            cudaDeviceSynchronize();
+            dev_ct1.queues_wait_and_throw();
             Stopwatch timer;
             timer.Restart();
 
             for (int i = 0; i < loops; ++i) f(algo);
 
-            cudaDeviceSynchronize();
+            dev_ct1.queues_wait_and_throw();
             timer.Stop();
 
             float avg_latency = (float)timer.GetTimeInSeconds() * 1000 / loops;
@@ -297,7 +349,7 @@ public:
 
 private:
     int bsz, M, N, K;
-    sycl::queue* handle;
+    dpct::queue_ptr handle;
     oneapi::mkl::transpose transa, transb;
     T *A, *B, *C;
 };
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/general_kernels.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/general_kernels.hpp
deleted file mode 100644
index ea2135f..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/includes/general_kernels.hpp
+++ /dev/null
@@ -1,43 +0,0 @@
-#include <stdio.h>
-#include <stdlib.h>
-
-// #include <cooperative_groups.h>
-
-#include "context.hpp"
-#include "onemkl_wrappers.hpp"
-
-#define THREADS 256
-
-#define minus_infinity -1 * std::numeric_limits<float>::infinity()
-
-#define FINAL_MASK 0xffffffff
-
-template <typename T>
-void launch_fused_add2(T* out,
-                       const T* inp1,
-                       const T* inp2,
-                       int batch_size,
-                       int seq_length,
-                       int hidden_size,
-                       sycl::queue* stream);
-
-template <typename T>
-void launch_fused_add4(T* out,
-                       const T* inp1,
-                       const T* inp2,
-                       const T* inp3,
-                       const T* inp4,
-                       int batch_size,
-                       int seq_length,
-                       int hidden_size,
-                       sycl::queue* stream);
-
-template <typename T>
-void launch_fused_add3(T* out,
-                       const T* inp1,
-                       const T* inp2,
-                       const T* inp3,
-                       int batch_size,
-                       int seq_length,
-                       int hidden_size,
-                       sycl::queue* stream);
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/memory_access_utils.h b/intel_extension_for_deepspeed/op_builder/csrc/includes/memory_access_utils.h
new file mode 100644
index 0000000..0af12e3
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/includes/memory_access_utils.h
@@ -0,0 +1,1236 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+#pragma once
+
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include "ds_kernel_utils.h"
+
+/////////////////////////////// Memory Access Utils ///////////////////////////////
+namespace mem_access {
+
+enum class LoadPolicy {
+    CacheAll,       // Cache at all levels
+    CacheGlobal,    // Cache at L2 only
+    CacheStreaming  // Cache with evict first policy
+};
+
+enum class StorePolicy {
+    Writeback,      // Cache in L1, write-back on eviction
+    CacheGlobal,    // Bypass L1, write-back on eviction
+    CacheStreaming  // Allocate cache line with evict first policy
+};
+
+template <int AccessSize, LoadPolicy policy = LoadPolicy::CacheAll>
+__dpct_inline__ void load_global(void* dst, const void* src);
+
+template <int AccessSize, LoadPolicy policy = LoadPolicy::CacheAll>
+__dpct_inline__ void load_global(void* dst, const void* src, bool do_access);
+
+// Shared accesses have no cache policy
+template <int AccessSize>
+__dpct_inline__ void load_shared(void* dst, const void* src);
+
+template <int AccessSize>
+__dpct_inline__ void load_shared(void* dst, const void* src, bool do_access);
+
+template <int AccessSize, StorePolicy policy = StorePolicy::Writeback>
+__dpct_inline__ void store_global(void* dst, const void* src);
+
+// Shared accesses have no cache policy
+template <int AccessSize>
+__dpct_inline__ void store_shared(void* dst, const void* src);
+
+#ifdef ASYNC_COPY_AVAILABLE
+template <int AccessSize>
+__device__ __forceinline__ void memcpy_async(void* shr, const void* gbl);
+
+template <int AccessSize>
+__device__ __forceinline__ void memcpy_async_nop(void* shr, const void* gbl, bool predicate);
+
+template <int AccessSize>
+__device__ __forceinline__ void memcpy_async_zero(void* shr, const void* gbl, bool predicate);
+
+__device__ __forceinline__ void memcpy_async_fence();
+
+template <int stages>
+__device__ __forceinline__ void memcpy_async_wait();
+
+template <int stages>
+__device__ __forceinline__ void tail_complete_wait(int remaining_stages);
+#endif
+
+// Util for tracking pipeline buffers
+// TODO: Evaluate whether this should also be guarded by ASYNC_COPY_AVAILABLE
+template <int max>
+class BufferTracker {
+public:
+    int current_state;
+
+    __dpct_inline__ BufferTracker() : current_state(0) {}
+
+    __dpct_inline__ int get()
+    {
+        int return_val = current_state++;
+        current_state = (current_state == max ? 0 : current_state);
+        return return_val;
+    }
+};
+
+__dpct_inline__ uint32_t lane_id()
+{
+#ifdef PTX_AVAILABLE
+    unsigned int lane_id;
+    lane_id =
+        sycl::ext::oneapi::experimental::this_nd_item<3>().get_sub_group().get_local_linear_id();
+    return lane_id;
+#else
+    return sycl::ext::oneapi::experimental::this_nd_item<3>().get_local_id(2) &
+           (sycl::ext::oneapi::experimental::this_sub_group().get_local_range().get(0) -
+            1);  // Portable
+#endif
+}
+
+/////////// Load Global ///////////
+template <>
+__dpct_inline__ void load_global<16>(void* dst, const void* src)
+{
+    sycl::uint4* data = reinterpret_cast<sycl::uint4*>(dst);
+#ifdef PTX_AVAILABLE
+    /*
+    DPCT1053:1: Migration of device assembly code is not supported.
+    */
+    asm volatile("ld.global.ca.v4.u32 {%0, %1, %2, %3}, [%4];\n"
+                 : "=r"(data[0].x()), "=r"(data[0].y()), "=r"(data[0].z()), "=r"(data[0].w())
+                 : "l"(src));
+#else
+    const sycl::uint4* src_cast = reinterpret_cast<const sycl::uint4*>(src);
+    data[0] = src_cast[0];
+#endif
+}
+
+template <>
+__dpct_inline__ void load_global<16>(void* dst, const void* src, bool do_access)
+{
+    sycl::uint4* data = reinterpret_cast<sycl::uint4*>(dst);
+#ifdef PTX_AVAILABLE
+    /*
+    DPCT1053:2: Migration of device assembly code is not supported.
+    */
+    asm volatile(
+        "{\n"
+        "\t.reg .pred p;\n"
+        "\tsetp.ne.b32 p, %5, 0;\n"
+        "\tmov.b32 %0, 0;\n"
+        "\tmov.b32 %1, 0;\n"
+        "\tmov.b32 %2, 0;\n"
+        "\tmov.b32 %3, 0;\n"
+        "\t@p ld.global.v4.u32 {%0, %1, %2, %3}, [%4];\n"
+        "}\n"
+        : "=r"(data[0].x()), "=r"(data[0].y()), "=r"(data[0].z()), "=r"(data[0].w())
+        : "l"(src), "r"((int)do_access));
+#else
+    const sycl::uint4* src_cast = reinterpret_cast<const sycl::uint4*>(src);
+    if (do_access) {
+        data[0] = src_cast[0];
+    } else {
+        data[0].x() = 0;
+        data[0].y() = 0;
+        data[0].z() = 0;
+        data[0].w() = 0;
+    }
+#endif
+}
+
+template <>
+__dpct_inline__ void load_global<16, LoadPolicy::CacheGlobal>(void* dst, const void* src)
+{
+    sycl::uint4* data = reinterpret_cast<sycl::uint4*>(dst);
+#ifdef PTX_AVAILABLE
+    /*
+    DPCT1053:3: Migration of device assembly code is not supported.
+    */
+    asm volatile("ld.global.cg.v4.u32 {%0, %1, %2, %3}, [%4];\n"
+                 : "=r"(data[0].x()), "=r"(data[0].y()), "=r"(data[0].z()), "=r"(data[0].w())
+                 : "l"(src));
+#else
+    const sycl::uint4* src_cast = reinterpret_cast<const sycl::uint4*>(src);
+    data[0] = src_cast[0];
+#endif
+}
+
+template <>
+__dpct_inline__ void load_global<16, LoadPolicy::CacheGlobal>(void* dst,
+                                                              const void* src,
+                                                              bool do_access)
+{
+    sycl::uint4* data = reinterpret_cast<sycl::uint4*>(dst);
+#ifdef PTX_AVAILABLE
+    /*
+    DPCT1053:4: Migration of device assembly code is not supported.
+    */
+    asm volatile(
+        "{\n"
+        "\t.reg .pred p;\n"
+        "\tsetp.ne.b32 p, %5, 0;\n"
+        "\tmov.b32 %0, 0;\n"
+        "\tmov.b32 %1, 0;\n"
+        "\tmov.b32 %2, 0;\n"
+        "\tmov.b32 %3, 0;\n"
+        "\t@p ld.global.cg.v4.u32 {%0, %1, %2, %3}, [%4];\n"
+        "}\n"
+        : "=r"(data[0].x()), "=r"(data[0].y()), "=r"(data[0].z()), "=r"(data[0].w())
+        : "l"(src), "r"((int)do_access));
+#else
+    const sycl::uint4* src_cast = reinterpret_cast<const sycl::uint4*>(src);
+    if (do_access) {
+        data[0] = src_cast[0];
+    } else {
+        data[0].x() = 0;
+        data[0].y() = 0;
+        data[0].z() = 0;
+        data[0].w() = 0;
+    }
+#endif
+}
+
+template <>
+__dpct_inline__ void load_global<16, LoadPolicy::CacheStreaming>(void* dst, const void* src)
+{
+    sycl::uint4* data = reinterpret_cast<sycl::uint4*>(dst);
+#ifdef PTX_AVAILABLE
+    /*
+    DPCT1053:5: Migration of device assembly code is not supported.
+    */
+    asm volatile("ld.global.cs.v4.u32 {%0, %1, %2, %3}, [%4];\n"
+                 : "=r"(data[0].x()), "=r"(data[0].y()), "=r"(data[0].z()), "=r"(data[0].w())
+                 : "l"(src));
+#else
+    const sycl::uint4* src_cast = reinterpret_cast<const sycl::uint4*>(src);
+    data[0] = src_cast[0];
+#endif
+}
+
+template <>
+__dpct_inline__ void load_global<16, LoadPolicy::CacheStreaming>(void* dst,
+                                                                 const void* src,
+                                                                 bool do_access)
+{
+    sycl::uint4* data = reinterpret_cast<sycl::uint4*>(dst);
+#ifdef PTX_AVAILABLE
+    /*
+    DPCT1053:6: Migration of device assembly code is not supported.
+    */
+    asm volatile(
+        "{\n"
+        "\t.reg .pred p;\n"
+        "\tsetp.ne.b32 p, %5, 0;\n"
+        "\tmov.b32 %0, 0;\n"
+        "\tmov.b32 %1, 0;\n"
+        "\tmov.b32 %2, 0;\n"
+        "\tmov.b32 %3, 0;\n"
+        "\t@p ld.global.cg.v4.u32 {%0, %1, %2, %3}, [%4];\n"
+        "}\n"
+        : "=r"(data[0].x()), "=r"(data[0].y()), "=r"(data[0].z()), "=r"(data[0].w())
+        : "l"(src), "r"((int)do_access));
+#else
+    const sycl::uint4* src_cast = reinterpret_cast<const sycl::uint4*>(src);
+    if (do_access) {
+        data[0] = src_cast[0];
+    } else {
+        data[0].x() = 0;
+        data[0].y() = 0;
+        data[0].z() = 0;
+        data[0].w() = 0;
+    }
+#endif
+}
+
+template <>
+__dpct_inline__ void load_global<8>(void* dst, const void* src)
+{
+    sycl::uint2* data = reinterpret_cast<sycl::uint2*>(dst);
+#ifdef PTX_AVAILABLE
+    /*
+    DPCT1053:7: Migration of device assembly code is not supported.
+    */
+    asm volatile("ld.global.ca.v2.u32 {%0, %1}, [%2];\n"
+                 : "=r"(data[0].x()), "=r"(data[0].y())
+                 : "l"(src));
+#else
+    const sycl::uint2* src_cast = reinterpret_cast<const sycl::uint2*>(src);
+    data[0] = src_cast[0];
+#endif
+}
+
+template <>
+__dpct_inline__ void load_global<8>(void* dst, const void* src, bool do_access)
+{
+    sycl::uint2* data = reinterpret_cast<sycl::uint2*>(dst);
+#ifdef PTX_AVAILABLE
+    /*
+    DPCT1053:8: Migration of device assembly code is not supported.
+    */
+    asm volatile(
+        "{\n"
+        "\t.reg .pred p;\n"
+        "\tsetp.ne.b32 p, %3, 0;\n"
+        "\tmov.b32 %0, 0;\n"
+        "\tmov.b32 %1, 0;\n"
+        "\t@p ld.global.v2.u32 {%0, %1}, [%2];\n"
+        "}\n"
+        : "=r"(data[0].x()), "=r"(data[0].y())
+        : "l"(src), "r"((int)do_access));
+#else
+    const sycl::uint2* src_cast = reinterpret_cast<const sycl::uint2*>(src);
+    if (do_access) {
+        data[0] = src_cast[0];
+    } else {
+        data[0].x() = 0;
+        data[0].y() = 0;
+    }
+#endif
+}
+
+template <>
+__dpct_inline__ void load_global<8, LoadPolicy::CacheGlobal>(void* dst, const void* src)
+{
+    sycl::uint2* data = reinterpret_cast<sycl::uint2*>(dst);
+#ifdef PTX_AVAILABLE
+    /*
+    DPCT1053:9: Migration of device assembly code is not supported.
+    */
+    asm volatile("ld.global.cg.v2.u32 {%0, %1}, [%2];\n"
+                 : "=r"(data[0].x()), "=r"(data[0].y())
+                 : "l"(src));
+#else
+    const sycl::uint2* src_cast = reinterpret_cast<const sycl::uint2*>(src);
+    data[0] = src_cast[0];
+#endif
+}
+
+template <>
+__dpct_inline__ void load_global<8, LoadPolicy::CacheGlobal>(void* dst,
+                                                             const void* src,
+                                                             bool do_access)
+{
+    sycl::uint2* data = reinterpret_cast<sycl::uint2*>(dst);
+#ifdef PTX_AVAILABLE
+    /*
+    DPCT1053:10: Migration of device assembly code is not supported.
+    */
+    asm volatile(
+        "{\n"
+        "\t.reg .pred p;\n"
+        "\tsetp.ne.b32 p, %3, 0;\n"
+        "\tmov.b32 %0, 0;\n"
+        "\tmov.b32 %1, 0;\n"
+        "\t@p ld.global.cg.v2.u32 {%0, %1}, [%2];\n"
+        "}\n"
+        : "=r"(data[0].x()), "=r"(data[0].y())
+        : "l"(src), "r"((int)do_access));
+#else
+    const sycl::uint2* src_cast = reinterpret_cast<const sycl::uint2*>(src);
+    if (do_access) {
+        data[0] = src_cast[0];
+    } else {
+        data[0].x() = 0;
+        data[0].y() = 0;
+    }
+#endif
+}
+
+template <>
+__dpct_inline__ void load_global<8, LoadPolicy::CacheStreaming>(void* dst, const void* src)
+{
+    sycl::uint2* data = reinterpret_cast<sycl::uint2*>(dst);
+#ifdef PTX_AVAILABLE
+    /*
+    DPCT1053:11: Migration of device assembly code is not supported.
+    */
+    asm volatile("ld.global.cs.v2.u32 {%0, %1}, [%2];\n"
+                 : "=r"(data[0].x()), "=r"(data[0].y())
+                 : "l"(src));
+#else
+    const sycl::uint2* src_cast = reinterpret_cast<const sycl::uint2*>(src);
+    data[0] = src_cast[0];
+#endif
+}
+
+template <>
+__dpct_inline__ void load_global<8, LoadPolicy::CacheStreaming>(void* dst,
+                                                                const void* src,
+                                                                bool do_access)
+{
+    sycl::uint2* data = reinterpret_cast<sycl::uint2*>(dst);
+#ifdef PTX_AVAILABLE
+    /*
+    DPCT1053:12: Migration of device assembly code is not supported.
+    */
+    asm volatile(
+        "{\n"
+        "\t.reg .pred p;\n"
+        "\tsetp.ne.b32 p, %3, 0;\n"
+        "\tmov.b32 %0, 0;\n"
+        "\tmov.b32 %1, 0;\n"
+        "\t@p ld.global.cs.v2.u32 {%0, %1}, [%2];\n"
+        "}\n"
+        : "=r"(data[0].x()), "=r"(data[0].y())
+        : "l"(src), "r"((int)do_access));
+#else
+    const sycl::uint2* src_cast = reinterpret_cast<const sycl::uint2*>(src);
+    if (do_access) {
+        data[0] = src_cast[0];
+    } else {
+        data[0].x() = 0;
+        data[0].y() = 0;
+    }
+#endif
+}
+
+template <>
+__dpct_inline__ void load_global<4>(void* dst, const void* src)
+{
+    int32_t* data = reinterpret_cast<int32_t*>(dst);
+#ifdef PTX_AVAILABLE
+    /*
+    DPCT1053:13: Migration of device assembly code is not supported.
+    */
+    asm volatile("ld.global.ca.u32 {%0}, [%1];\n" : "=r"(*data) : "l"(src));
+#else
+    const int32_t* src_cast = reinterpret_cast<const int32_t*>(src);
+    data[0] = src_cast[0];
+#endif
+}
+
+template <>
+__dpct_inline__ void load_global<4>(void* dst, const void* src, bool do_access)
+{
+    int32_t* data = reinterpret_cast<int32_t*>(dst);
+#ifdef PTX_AVAILABLE
+    /*
+    DPCT1053:14: Migration of device assembly code is not supported.
+    */
+    asm volatile(
+        "{\n"
+        "\t.reg .pred p;\n"
+        "\tsetp.ne.b32 p, %2, 0;\n"
+        "\tmov.b32 %0, 0;\n"
+        "\t@p ld.global.u32 {%0}, [%1];\n"
+        "}\n"
+        : "=r"(data[0])
+        : "l"(src), "r"((int)do_access));
+#else
+    const int32_t* src_cast = reinterpret_cast<const int32_t*>(src);
+    if (do_access) {
+        data[0] = src_cast[0];
+    } else {
+        data[0] = 0;
+    }
+#endif
+}
+
+template <>
+__dpct_inline__ void load_global<4, LoadPolicy::CacheGlobal>(void* dst, const void* src)
+{
+    int32_t* data = reinterpret_cast<int32_t*>(dst);
+#ifdef PTX_AVAILABLE
+    /*
+    DPCT1053:15: Migration of device assembly code is not supported.
+    */
+    asm volatile("ld.global.cg.u32 {%0}, [%1];\n" : "=r"(*data) : "l"(src));
+#else
+    const int32_t* src_cast = reinterpret_cast<const int32_t*>(src);
+    data[0] = src_cast[0];
+#endif
+}
+
+template <>
+__dpct_inline__ void load_global<4, LoadPolicy::CacheGlobal>(void* dst,
+                                                             const void* src,
+                                                             bool do_access)
+{
+    int32_t* data = reinterpret_cast<int32_t*>(dst);
+#ifdef PTX_AVAILABLE
+    /*
+    DPCT1053:16: Migration of device assembly code is not supported.
+    */
+    asm volatile(
+        "{\n"
+        "\t.reg .pred p;\n"
+        "\tsetp.ne.b32 p, %2, 0;\n"
+        "\tmov.b32 %0, 0;\n"
+        "\t@p ld.global.cg.u32 {%0}, [%1];\n"
+        "}\n"
+        : "=r"(data[0])
+        : "l"(src), "r"((int)do_access));
+#else
+    const int32_t* src_cast = reinterpret_cast<const int32_t*>(src);
+    if (do_access) {
+        data[0] = src_cast[0];
+    } else {
+        data[0] = 0;
+    }
+#endif
+}
+
+template <>
+__dpct_inline__ void load_global<4, LoadPolicy::CacheStreaming>(void* dst, const void* src)
+{
+    int32_t* data = reinterpret_cast<int32_t*>(dst);
+#ifdef PTX_AVAILABLE
+    /*
+    DPCT1053:17: Migration of device assembly code is not supported.
+    */
+    asm volatile("ld.global.cs.u32 {%0}, [%1];\n" : "=r"(*data) : "l"(src));
+#else
+    const int32_t* src_cast = reinterpret_cast<const int32_t*>(src);
+    data[0] = src_cast[0];
+#endif
+}
+
+template <>
+__dpct_inline__ void load_global<4, LoadPolicy::CacheStreaming>(void* dst,
+                                                                const void* src,
+                                                                bool do_access)
+{
+    int32_t* data = reinterpret_cast<int32_t*>(dst);
+#ifdef PTX_AVAILABLE
+    /*
+    DPCT1053:18: Migration of device assembly code is not supported.
+    */
+    asm volatile(
+        "{\n"
+        "\t.reg .pred p;\n"
+        "\tsetp.ne.b32 p, %2, 0;\n"
+        "\tmov.b32 %0, 0;\n"
+        "\t@p ld.global.cs.u32 {%0}, [%1];\n"
+        "}\n"
+        : "=r"(data[0])
+        : "l"(src), "r"((int)do_access));
+#else
+    const int32_t* src_cast = reinterpret_cast<const int32_t*>(src);
+    if (do_access) {
+        data[0] = src_cast[0];
+    } else {
+        data[0] = 0;
+    }
+#endif
+}
+
+template <>
+__dpct_inline__ void load_global<2>(void* dst, const void* src)
+{
+    int16_t* data = reinterpret_cast<int16_t*>(dst);
+#ifdef PTX_AVAILABLE
+    /*
+    DPCT1053:19: Migration of device assembly code is not supported.
+    */
+    asm volatile("ld.global.ca.u16 {%0}, [%1];\n" : "=h"(*data) : "l"(src));
+#else
+    const int16_t* src_cast = reinterpret_cast<const int16_t*>(src);
+    data[0] = src_cast[0];
+#endif
+}
+
+template <>
+__dpct_inline__ void load_global<2>(void* dst, const void* src, bool do_access)
+{
+    int16_t* data = reinterpret_cast<int16_t*>(dst);
+#ifdef PTX_AVAILABLE
+    /*
+    DPCT1053:20: Migration of device assembly code is not supported.
+    */
+    asm volatile(
+        "{\n"
+        "\t.reg .pred p;\n"
+        "\tsetp.ne.b32 p, %2, 0;\n"
+        "\tmov.u16 %0, 0;\n"
+        "\t@p ld.global.u16 {%0}, [%1];\n"
+        "}\n"
+        : "=h"(*data)
+        : "l"(src), "r"((int)do_access));
+#else
+    const int16_t* src_cast = reinterpret_cast<const int16_t*>(src);
+    if (do_access) {
+        data[0] = src_cast[0];
+    } else {
+        data[0] = 0;
+    }
+#endif
+}
+
+template <>
+__dpct_inline__ void load_global<2, LoadPolicy::CacheGlobal>(void* dst, const void* src)
+{
+    int16_t* data = reinterpret_cast<int16_t*>(dst);
+#ifdef PTX_AVAILABLE
+    /*
+    DPCT1053:21: Migration of device assembly code is not supported.
+    */
+    asm volatile("ld.global.cg.u16 {%0}, [%1];\n" : "=h"(*data) : "l"(src));
+#else
+    const int16_t* src_cast = reinterpret_cast<const int16_t*>(src);
+    data[0] = src_cast[0];
+#endif
+}
+
+template <>
+__dpct_inline__ void load_global<2, LoadPolicy::CacheGlobal>(void* dst,
+                                                             const void* src,
+                                                             bool do_access)
+{
+    int16_t* data = reinterpret_cast<int16_t*>(dst);
+#ifdef PTX_AVAILABLE
+    /*
+    DPCT1053:22: Migration of device assembly code is not supported.
+    */
+    asm volatile(
+        "{\n"
+        "\t.reg .pred p;\n"
+        "\tsetp.ne.b32 p, %2, 0;\n"
+        "\tmov.u16 %0, 0;\n"
+        "\t@p ld.global.cg.u16 {%0}, [%1];\n"
+        "}\n"
+        : "=h"(*data)
+        : "l"(src), "r"((int)do_access));
+#else
+    const int16_t* src_cast = reinterpret_cast<const int16_t*>(src);
+    if (do_access) {
+        data[0] = src_cast[0];
+    } else {
+        data[0] = 0;
+    }
+#endif
+}
+
+template <>
+__dpct_inline__ void load_global<2, LoadPolicy::CacheStreaming>(void* dst, const void* src)
+{
+    int16_t* data = reinterpret_cast<int16_t*>(dst);
+#ifdef PTX_AVAILABLE
+    /*
+    DPCT1053:23: Migration of device assembly code is not supported.
+    */
+    asm volatile("ld.global.cs.u16 {%0}, [%1];\n" : "=h"(*data) : "l"(src));
+#else
+    const int16_t* src_cast = reinterpret_cast<const int16_t*>(src);
+    data[0] = src_cast[0];
+#endif
+}
+
+template <>
+__dpct_inline__ void load_global<2, LoadPolicy::CacheStreaming>(void* dst,
+                                                                const void* src,
+                                                                bool do_access)
+{
+    int16_t* data = reinterpret_cast<int16_t*>(dst);
+#ifdef PTX_AVAILABLE
+    /*
+    DPCT1053:24: Migration of device assembly code is not supported.
+    */
+    asm volatile(
+        "{\n"
+        "\t.reg .pred p;\n"
+        "\tsetp.ne.b32 p, %2, 0;\n"
+        "\tmov.u16 %0, 0;\n"
+        "\t@p ld.global.cs.u16 {%0}, [%1];\n"
+        "}\n"
+        : "=h"(*data)
+        : "l"(src), "r"((int)do_access));
+#else
+    const int16_t* src_cast = reinterpret_cast<const int16_t*>(src);
+    if (do_access) {
+        data[0] = src_cast[0];
+    } else {
+        data[0] = 0;
+    }
+#endif
+}
+
+/////////// Load Shared ///////////
+namespace internal {
+
+#ifdef PTX_AVAILABLE
+__dpct_inline__ unsigned convert_to_shared(const void* ptr)
+{
+#if __CUDACC_VER_MAJOR__ >= 11
+    // In CUDA 11 we have a builtin intrinsic
+    return __cvta_generic_to_shared(ptr);
+#else
+    unsigned ret_val;
+    asm volatile(
+        "{\n"
+        "\t.reg .u64 p1;\n"
+        "\tcvta.to.shared.u64 p1, %1\n"
+        "\tcvt.u32.u64 %0, p1;\n"
+        "}\n"
+        : "=r"(ret_val)
+        : "l"(ptr));
+    return ret_val;
+#endif
+}
+#endif
+
+}  // namespace internal
+
+template <>
+__dpct_inline__ void load_shared<16>(void* dst, const void* src)
+{
+    sycl::uint4* data = reinterpret_cast<sycl::uint4*>(dst);
+#ifdef PTX_AVAILABLE
+    unsigned src_shr = internal::convert_to_shared(src);
+
+    /*
+    DPCT1053:25: Migration of device assembly code is not supported.
+    */
+    asm volatile("ld.shared.v4.u32 {%0, %1, %2, %3}, [%4];\n"
+                 : "=r"(data[0].x()), "=r"(data[0].y()), "=r"(data[0].z()), "=r"(data[0].w())
+                 : "r"(src_shr));
+#else
+    const sycl::uint4* src_cast = reinterpret_cast<const sycl::uint4*>(src);
+    data[0] = src_cast[0];
+#endif
+}
+
+template <>
+__dpct_inline__ void load_shared<16>(void* dst, const void* src, bool do_access)
+{
+    sycl::uint4* data = reinterpret_cast<sycl::uint4*>(dst);
+#ifdef PTX_AVAILABLE
+    unsigned src_shr = internal::convert_to_shared(src);
+
+    /*
+    DPCT1053:26: Migration of device assembly code is not supported.
+    */
+    asm volatile(
+        "{\n"
+        "\t.reg .pred p;\n"
+        "\tsetp.ne.b32 p, %5, 0;\n"
+        "\tmov.b32 %0, 0;\n"
+        "\tmov.b32 %1, 0;\n"
+        "\tmov.b32 %2, 0;\n"
+        "\tmov.b32 %3, 0;\n"
+        "\t@p ld.shared.v4.u32 {%0, %1, %2, %3}, [%4];\n"
+        "}\n"
+        : "=r"(data[0].x()), "=r"(data[0].y()), "=r"(data[0].z()), "=r"(data[0].w())
+        : "r"(src_shr), "r"((int)do_access));
+#else
+    const sycl::uint4* src_cast = reinterpret_cast<const sycl::uint4*>(src);
+    if (do_access) {
+        data[0] = src_cast[0];
+    } else {
+        data[0].x() = 0;
+        data[0].y() = 0;
+        data[0].z() = 0;
+        data[0].w() = 0;
+    }
+#endif
+}
+
+template <>
+__dpct_inline__ void load_shared<8>(void* dst, const void* src)
+{
+    sycl::uint2* data = reinterpret_cast<sycl::uint2*>(dst);
+#ifdef PTX_AVAILABLE
+    unsigned src_shr = internal::convert_to_shared(src);
+
+    /*
+    DPCT1053:27: Migration of device assembly code is not supported.
+    */
+    asm volatile("ld.shared.v2.u32 {%0, %1}, [%2];\n"
+                 : "=r"(data[0].x()), "=r"(data[0].y())
+                 : "r"(src_shr));
+#else
+    const sycl::uint2* src_cast = reinterpret_cast<const sycl::uint2*>(src);
+    data[0] = src_cast[0];
+#endif
+}
+
+template <>
+__dpct_inline__ void load_shared<8>(void* dst, const void* src, bool do_access)
+{
+    sycl::uint2* data = reinterpret_cast<sycl::uint2*>(dst);
+#ifdef PTX_AVAILABLE
+    unsigned src_shr = internal::convert_to_shared(src);
+
+    /*
+    DPCT1053:28: Migration of device assembly code is not supported.
+    */
+    asm volatile(
+        "{\n"
+        "\t.reg .pred p;\n"
+        "\tsetp.ne.b32 p, %3, 0;\n"
+        "\tmov.b32 %0, 0;\n"
+        "\tmov.b32 %1, 0;\n"
+        "\t@p ld.shared.v2.u32 {%0, %1}, [%2];\n"
+        "}\n"
+        : "=r"(data[0].x()), "=r"(data[0].y())
+        : "r"(src_shr), "r"((int)do_access));
+#else
+    const sycl::uint2* src_cast = reinterpret_cast<const sycl::uint2*>(src);
+    if (do_access) {
+        data[0] = src_cast[0];
+    } else {
+        data[0].x() = 0;
+        data[0].y() = 0;
+    }
+#endif
+}
+
+template <>
+__dpct_inline__ void load_shared<4>(void* dst, const void* src)
+{
+    int32_t* data = reinterpret_cast<int32_t*>(dst);
+#ifdef PTX_AVAILABLE
+    unsigned src_shr = internal::convert_to_shared(src);
+
+    /*
+    DPCT1053:29: Migration of device assembly code is not supported.
+    */
+    asm volatile("ld.shared.u32 {%0}, [%1];\n" : "=r"(*data) : "r"(src_shr));
+#else
+    const int32_t* src_cast = reinterpret_cast<const int32_t*>(src);
+    data[0] = src_cast[0];
+#endif
+}
+
+template <>
+__dpct_inline__ void load_shared<4>(void* dst, const void* src, bool do_access)
+{
+    int32_t* data = reinterpret_cast<int32_t*>(dst);
+#ifdef PTX_AVAILABLE
+    unsigned src_shr = internal::convert_to_shared(src);
+
+    /*
+    DPCT1053:30: Migration of device assembly code is not supported.
+    */
+    asm volatile(
+        "{\n"
+        "\t.reg .pred p;\n"
+        "\tsetp.ne.b32 p, %2, 0;\n"
+        "\tmov.b32 %0, 0;\n"
+        "\t@p ld.shared.u32 %0, [%1];\n"
+        "}\n"
+        : "=r"(data[0])
+        : "r"(src_shr), "r"((int)do_access));
+#else
+    const int32_t* src_cast = reinterpret_cast<const int32_t*>(src);
+    if (do_access) {
+        data[0] = src_cast[0];
+    } else {
+        data[0] = 0;
+    }
+#endif
+}
+
+/////////// Store Global ///////////
+
+template <>
+__dpct_inline__ void store_global<16>(void* dst, const void* src)
+{
+    const sycl::uint4* data = reinterpret_cast<const sycl::uint4*>(src);
+#ifdef PTX_AVAILABLE
+    /*
+    DPCT1053:31: Migration of device assembly code is not supported.
+    */
+    asm volatile("st.global.wb.v4.u32 [%0], {%1, %2, %3, %4};\n"
+                 :
+                 : "l"(dst), "r"(data[0].x()), "r"(data[0].y()), "r"(data[0].z()), "r"(data[0].w())
+                 : "memory");
+#else
+    sycl::uint4* dst_cast = reinterpret_cast<sycl::uint4*>(dst);
+    dst_cast[0] = data[0];
+#endif
+}
+
+template <>
+__dpct_inline__ void store_global<16, StorePolicy::CacheGlobal>(void* dst, const void* src)
+{
+    const sycl::uint4* data = reinterpret_cast<const sycl::uint4*>(src);
+#ifdef PTX_AVAILABLE
+    /*
+    DPCT1053:32: Migration of device assembly code is not supported.
+    */
+    asm volatile("st.global.cg.v4.u32 [%0], {%1, %2, %3, %4};\n"
+                 :
+                 : "l"(dst), "r"(data[0].x()), "r"(data[0].y()), "r"(data[0].z()), "r"(data[0].w())
+                 : "memory");
+#else
+    sycl::uint4* dst_cast = reinterpret_cast<sycl::uint4*>(dst);
+    dst_cast[0] = data[0];
+#endif
+}
+
+template <>
+__dpct_inline__ void store_global<16, StorePolicy::CacheStreaming>(void* dst, const void* src)
+{
+    const sycl::uint4* data = reinterpret_cast<const sycl::uint4*>(src);
+#ifdef PTX_AVAILABLE
+    /*
+    DPCT1053:33: Migration of device assembly code is not supported.
+    */
+    asm volatile("st.global.cs.v4.u32 [%0], {%1, %2, %3, %4};\n"
+                 :
+                 : "l"(dst), "r"(data[0].x()), "r"(data[0].y()), "r"(data[0].z()), "r"(data[0].w())
+                 : "memory");
+#else
+    sycl::uint4* dst_cast = reinterpret_cast<sycl::uint4*>(dst);
+    dst_cast[0] = data[0];
+#endif
+}
+
+template <>
+__dpct_inline__ void store_global<8>(void* dst, const void* src)
+{
+    const sycl::uint2* data = reinterpret_cast<const sycl::uint2*>(src);
+#ifdef PTX_AVAILABLE
+    /*
+    DPCT1053:34: Migration of device assembly code is not supported.
+    */
+    asm volatile("st.global.wb.v2.u32 [%0], {%1, %2};\n"
+                 :
+                 : "l"(dst), "r"(data[0].x()), "r"(data[0].y()));
+#else
+    sycl::uint2* dst_cast = reinterpret_cast<sycl::uint2*>(dst);
+    dst_cast[0] = data[0];
+#endif
+}
+
+template <>
+__dpct_inline__ void store_global<8, StorePolicy::CacheGlobal>(void* dst, const void* src)
+{
+    const sycl::uint2* data = reinterpret_cast<const sycl::uint2*>(src);
+#ifdef PTX_AVAILABLE
+    /*
+    DPCT1053:35: Migration of device assembly code is not supported.
+    */
+    asm volatile("st.global.cg.v2.u32 [%0], {%1, %2};\n"
+                 :
+                 : "l"(dst), "r"(data[0].x()), "r"(data[0].y()));
+#else
+    sycl::uint2* dst_cast = reinterpret_cast<sycl::uint2*>(dst);
+    dst_cast[0] = data[0];
+#endif
+}
+
+template <>
+__dpct_inline__ void store_global<8, StorePolicy::CacheStreaming>(void* dst, const void* src)
+{
+    const sycl::uint2* data = reinterpret_cast<const sycl::uint2*>(src);
+#ifdef PTX_AVAILABLE
+    /*
+    DPCT1053:36: Migration of device assembly code is not supported.
+    */
+    asm volatile("st.global.cs.v2.u32 [%0], {%1, %2};\n"
+                 :
+                 : "l"(dst), "r"(data[0].x()), "r"(data[0].y()));
+#else
+    sycl::uint2* dst_cast = reinterpret_cast<sycl::uint2*>(dst);
+    dst_cast[0] = data[0];
+#endif
+}
+
+template <>
+__dpct_inline__ void store_global<4>(void* dst, const void* src)
+{
+    const int32_t* data = reinterpret_cast<const int32_t*>(src);
+#ifdef PTX_AVAILABLE
+    /*
+    DPCT1053:37: Migration of device assembly code is not supported.
+    */
+    asm volatile("st.global.wb.u32 [%0], %1;\n" : : "l"(dst), "r"(*data));
+#else
+    int32_t* dst_cast = reinterpret_cast<int32_t*>(dst);
+    dst_cast[0] = data[0];
+#endif
+}
+
+template <>
+__dpct_inline__ void store_global<4, StorePolicy::CacheGlobal>(void* dst, const void* src)
+{
+    const int32_t* data = reinterpret_cast<const int32_t*>(src);
+#ifdef PTX_AVAILABLE
+    /*
+    DPCT1053:38: Migration of device assembly code is not supported.
+    */
+    asm volatile("st.global.cg.u32 [%0], %1;\n" : : "l"(dst), "r"(*data));
+#else
+    int32_t* dst_cast = reinterpret_cast<int32_t*>(dst);
+    dst_cast[0] = data[0];
+#endif
+}
+
+template <>
+__dpct_inline__ void store_global<4, StorePolicy::CacheStreaming>(void* dst, const void* src)
+{
+    const int32_t* data = reinterpret_cast<const int32_t*>(src);
+#ifdef PTX_AVAILABLE
+    /*
+    DPCT1053:39: Migration of device assembly code is not supported.
+    */
+    asm volatile("st.global.cs.u32 [%0], %1;\n" : : "l"(dst), "r"(*data));
+#else
+    int32_t* dst_cast = reinterpret_cast<int32_t*>(dst);
+    dst_cast[0] = data[0];
+#endif
+}
+
+/////////// Store Shared ///////////
+
+template <>
+__dpct_inline__ void store_shared<16>(void* dst, const void* src)
+{
+    const sycl::uint4* data = reinterpret_cast<const sycl::uint4*>(src);
+#ifdef PTX_AVAILABLE
+    unsigned dst_int = internal::convert_to_shared(dst);
+
+    /*
+    DPCT1053:40: Migration of device assembly code is not supported.
+    */
+    asm volatile(
+        "st.shared.v4.u32 [%0], {%1, %2, %3, %4};\n"
+        :
+        : "r"(dst_int), "r"(data[0].x()), "r"(data[0].y()), "r"(data[0].z()), "r"(data[0].w()));
+#else
+    sycl::uint4* dst_cast = reinterpret_cast<sycl::uint4*>(dst);
+    dst_cast[0] = data[0];
+#endif
+}
+
+template <>
+__dpct_inline__ void store_shared<8>(void* dst, const void* src)
+{
+    const sycl::uint2* data = reinterpret_cast<const sycl::uint2*>(src);
+#ifdef PTX_AVAILABLE
+    unsigned dst_int = internal::convert_to_shared(dst);
+
+    /*
+    DPCT1053:41: Migration of device assembly code is not supported.
+    */
+    asm volatile("st.shared.v2.u32 [%0], {%1, %2};\n"
+                 :
+                 : "r"(dst_int), "r"(data[0].x()), "r"(data[0].y()));
+#else
+    sycl::uint2* dst_cast = reinterpret_cast<sycl::uint2*>(dst);
+    dst_cast[0] = data[0];
+#endif
+}
+
+template <>
+__dpct_inline__ void store_shared<4>(void* dst, const void* src)
+{
+    const int32_t* data = reinterpret_cast<const int32_t*>(src);
+#ifdef PTX_AVAILABLE
+    unsigned dst_int = internal::convert_to_shared(dst);
+
+    /*
+    DPCT1053:42: Migration of device assembly code is not supported.
+    */
+    asm volatile("st.shared.u32 [%0], %1;\n" : : "r"(dst_int), "r"(*data));
+#else
+    int32_t* dst_cast = reinterpret_cast<int32_t*>(dst);
+    dst_cast[0] = data[0];
+#endif
+}
+
+/////////// Asynchronous Memory Copy ///////////
+
+#ifdef ASYNC_COPY_AVAILABLE
+template <int AccessSize>
+__device__ __forceinline__ void memcpy_async(void* shr, const void* gbl)
+{
+    static_assert((AccessSize == 4 || AccessSize == 8 || AccessSize == 16));
+    unsigned shr_int = internal::convert_to_shared(shr);
+
+    asm volatile("cp.async.ca.shared.global [%0], [%1], %2;\n"
+                 :
+                 : "r"(shr_int), "l"(gbl), "n"(AccessSize));
+}
+
+template <int AccessSize>
+__device__ __forceinline__ void memcpy_async_nop(void* shr, const void* gbl, bool predicate)
+{
+    static_assert((AccessSize == 4 || AccessSize == 8 || AccessSize == 16));
+    unsigned shr_int = internal::convert_to_shared(shr);
+
+    asm volatile(
+        "{\n"
+        "   .reg .pred p;\n"
+        "   setp.ne.b32 p, %0, 0;\n"
+        "   @p cp.async.ca.shared.global [%1], [%2], %3;\n"
+        "}\n"
+        :
+        : "r"((int)predicate), "r"(shr_int), "l"(gbl), "n"(AccessSize));
+}
+
+template <int AccessSize>
+__device__ __forceinline__ void memcpy_async_zero(void* shr, const void* gbl, bool predicate)
+{
+    static_assert((AccessSize == 4 || AccessSize == 8 || AccessSize == 16));
+    unsigned shr_int = internal::convert_to_shared(shr);
+    int bytes_to_copy = (predicate ? AccessSize : 0);
+
+    asm volatile("cp.async.ca.shared.global [%0], [%1], %2, %3;\n"
+                 :
+                 : "r"(shr_int), "l"(gbl), "n"(AccessSize), "r"(bytes_to_copy));
+}
+
+template <int AccessSize>
+__device__ __forceinline__ void memcpy_async_zero_nop(void* shr,
+                                                      const void* gbl,
+                                                      bool zero_predicate,
+                                                      bool nop_predicate)
+{
+    static_assert((AccessSize == 4 || AccessSize == 8 || AccessSize == 16));
+    unsigned shr_int = internal::convert_to_shared(shr);
+    int bytes_to_copy = (zero_predicate ? AccessSize : 0);
+
+    asm volatile(
+        "{\n"
+        "   .reg .pred p;\n"
+        "   setp.ne.b32 p, %0, 0;\n"
+        "   @p cp.async.ca.shared.global [%1], [%2], %3, %4;\n"
+        "}\n"
+        :
+        : "r"((int)nop_predicate), "r"(shr_int), "l"(gbl), "n"(AccessSize), "r"(bytes_to_copy));
+}
+
+// Cache global variants. Separate interface to require deliberate use of them.
+__device__ __forceinline__ void memcpy_async_cg(void* shr, const void* gbl)
+{
+    unsigned shr_int = internal::convert_to_shared(shr);
+
+    asm volatile("cp.async.cg.shared.global [%0], [%1], 16;\n" : : "r"(shr_int), "l"(gbl));
+}
+
+__device__ __forceinline__ void memcpy_async_nop_cg(void* shr, const void* gbl, bool predicate)
+{
+    unsigned shr_int = internal::convert_to_shared(shr);
+
+    asm volatile(
+        "{\n"
+        "   .reg .pred p;\n"
+        "   setp.ne.b32 p, %0, 0;\n"
+        "   @p cp.async.cg.shared.global [%1], [%2], 16;\n"
+        "}\n"
+        :
+        : "r"((int)predicate), "r"(shr_int), "l"(gbl));
+}
+
+__device__ __forceinline__ void memcpy_async_zero_cg(void* shr, const void* gbl, bool predicate)
+{
+    unsigned shr_int = internal::convert_to_shared(shr);
+    int bytes_to_copy = (predicate ? 16 : 0);
+
+    asm volatile("cp.async.cg.shared.global [%0], [%1], 16, %2;\n"
+                 :
+                 : "r"(shr_int), "l"(gbl), "r"(bytes_to_copy));
+}
+
+__device__ __forceinline__ void memcpy_async_zero_nop_cg(void* shr,
+                                                         const void* gbl,
+                                                         bool zero_predicate,
+                                                         bool nop_predicate)
+{
+    unsigned shr_int = internal::convert_to_shared(shr);
+    int bytes_to_copy = (zero_predicate ? 16 : 0);
+
+    asm volatile(
+        "{\n"
+        "   .reg .pred p;\n"
+        "   setp.ne.b32 p, %0, 0;\n"
+        "   @p cp.async.cg.shared.global [%1], [%2], 16, %3;\n"
+        "}\n"
+        :
+        : "r"((int)nop_predicate), "r"(shr_int), "l"(gbl), "r"(bytes_to_copy));
+}
+
+__device__ __forceinline__ void memcpy_async_fence() { asm volatile("cp.async.commit_group;\n"); }
+
+template <int stages>
+__device__ __forceinline__ void memcpy_async_wait()
+{
+    static_assert(stages <= 8);
+
+    asm volatile("cp.async.wait_group %0;\n" : : "n"(stages));
+}
+
+// TODO: The tail complete should be a known compile time artifact, should try and induce this
+// without all of the branches from the call-site. This is a hacky solution.
+template <>
+__device__ __forceinline__ void tail_complete_wait<1>(int remaining_stages)
+{
+    if (remaining_stages == 0) memcpy_async_wait<0>();
+}
+
+template <>
+__device__ __forceinline__ void tail_complete_wait<2>(int remaining_stages)
+{
+    if (remaining_stages == 1)
+        memcpy_async_wait<1>();
+    else if (remaining_stages == 0)
+        memcpy_async_wait<0>();
+}
+
+template <>
+__device__ __forceinline__ void tail_complete_wait<3>(int remaining_stages)
+{
+    if (remaining_stages == 2)
+        memcpy_async_wait<2>();
+    else if (remaining_stages == 1)
+        memcpy_async_wait<1>();
+    else if (remaining_stages == 0)
+        memcpy_async_wait<0>();
+}
+
+template <>
+__device__ __forceinline__ void tail_complete_wait<4>(int remaining_stages)
+{
+    if (remaining_stages == 3)
+        memcpy_async_wait<3>();
+    else if (remaining_stages == 2)
+        memcpy_async_wait<2>();
+    else if (remaining_stages == 1)
+        memcpy_async_wait<1>();
+    else if (remaining_stages == 0)
+        memcpy_async_wait<0>();
+}
+
+template <>
+__device__ __forceinline__ void tail_complete_wait<5>(int remaining_stages)
+{
+    if (remaining_stages == 4)
+        memcpy_async_wait<4>();
+    else if (remaining_stages == 3)
+        memcpy_async_wait<3>();
+    else if (remaining_stages == 2)
+        memcpy_async_wait<2>();
+    else if (remaining_stages == 1)
+        memcpy_async_wait<1>();
+    else if (remaining_stages == 0)
+        memcpy_async_wait<0>();
+}
+
+template <>
+__device__ __forceinline__ void tail_complete_wait<6>(int remaining_stages)
+{
+    if (remaining_stages == 5)
+        memcpy_async_wait<5>();
+    else if (remaining_stages == 4)
+        memcpy_async_wait<4>();
+    else if (remaining_stages == 3)
+        memcpy_async_wait<3>();
+    else if (remaining_stages == 2)
+        memcpy_async_wait<2>();
+    else if (remaining_stages == 1)
+        memcpy_async_wait<1>();
+    else if (remaining_stages == 0)
+        memcpy_async_wait<0>();
+}
+#endif
+
+}  // namespace mem_access
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/multi_tensor_apply.dp.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/multi_tensor_apply.dp.hpp
deleted file mode 100644
index 0c22494..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/includes/multi_tensor_apply.dp.hpp
+++ /dev/null
@@ -1,180 +0,0 @@
-/* Copyright 2020 The Microsoft DeepSpeed Team
-   Copyright NVIDIA/apex
-   This file is adapted from fused adam in NVIDIA/apex, commit a109f85
-*/
-#pragma once
-
-#include <ATen/ATen.h>
-#include <ATen/AccumulateType.h>
-#if __has_include(<sycl/sycl.hpp>)
-#include <sycl/sycl.hpp>
-#elif __has_include(<CL/sycl.hpp>)
-#include <CL/sycl.hpp>
-#else
-#error "Unsupported compiler"
-#endif
-#include "compat.h"
-#include "context.hpp"
-
-#include <assert.h>
-#include <xpu/Stream.h>
-
-// #include <iostream>
-
-// This header is the one-stop shop for all your multi-tensor apply needs.
-
-constexpr int depth_to_max_tensors[5] = {110, 64, 48, 36, 30};
-constexpr int depth_to_max_blocks[5] = {320, 320, 320, 320, 320};
-
-template <int n>
-struct TensorListMetadata {
-    void* addresses[n][depth_to_max_tensors[n - 1]];
-    int sizes[depth_to_max_tensors[n - 1]];
-    unsigned char block_to_tensor[depth_to_max_blocks[n - 1]];
-    int block_to_chunk[depth_to_max_blocks[n - 1]];  // I fear this needs to be a
-                                                     // full int.
-    int start_tensor_this_launch;
-};
-
-template <typename T>
-SYCL_EXTERNAL void AdamFunctor(sycl::nd_item<1> item_ct1,
-                               int chunk_size,
-                               int* noop_gmem,
-                               const int tensor_loc,
-                               const int chunk_idx,
-                               int n,
-                               T* g,
-                               T* p,
-                               T* m,
-                               T* v,
-                               const float beta1,
-                               const float beta2,
-                               const float beta1_correction,
-                               const float beta2_correction,
-                               const float epsilon,
-                               const float lr,
-                               const int mode,
-                               const float decay);
-
-template <int depth, typename T>
-void multi_tensor_apply(int block_size,
-                        int chunk_size,
-                        const at::Tensor& noop_flag,
-                        const std::vector<std::vector<at::Tensor>>& tensor_lists,
-                        const float beta1,
-                        const float beta2,
-                        const float beta_correction1,
-                        const float beta_correction2,
-                        const float epsilon,
-                        const float lr,
-                        const int mode,
-                        const float decay)
-{
-    TORCH_CHECK(tensor_lists.size() == depth, "tensor_lists.size() != depth");
-    int len0 = tensor_lists[0].size();
-    TORCH_CHECK(len0 > 0, "tensor_lists[0].size() is not > 0");
-    auto ref_device = tensor_lists[0][0].device();
-    TORCH_CHECK(ref_device.type() == at::kXPU, "expected input to be on XPU");
-    for (int l = 0; l < tensor_lists.size(); l++)  // No range-based for because I need indices
-    {
-        TORCH_CHECK(tensor_lists[l].size() == len0, "Size mismatch among tensor lists");
-        for (int t = 0; t < tensor_lists[l].size(); t++) {
-            bool contiguous_memory = tensor_lists[l][t].is_contiguous();
-#ifdef VERSION_GE_1_5
-            contiguous_memory = (contiguous_memory ||
-                                 tensor_lists[l][t].is_contiguous(at::MemoryFormat::ChannelsLast));
-#endif
-            TORCH_CHECK(contiguous_memory, "A tensor was not contiguous.");
-            TORCH_CHECK(tensor_lists[l][t].device() == ref_device,
-                        "A tensor was not on the same device as the first tensor");
-            TORCH_CHECK(tensor_lists[l][t].numel() == tensor_lists[0][t].numel(), "Size mismatch");
-        }
-    }
-
-    int ntensors = tensor_lists[0].size();
-
-    TensorListMetadata<depth> tl;
-
-    sycl::queue* stream = SyclContext::Instance().GetCurrentStream();
-
-    tl.start_tensor_this_launch = 0;
-    int loc_block_info = 0;
-    int loc_tensor_info = 0;
-    for (int t = 0; t < ntensors; t++) {
-        tl.sizes[loc_tensor_info] = tensor_lists[0][t].numel();
-        for (int d = 0; d < depth; d++)
-            tl.addresses[d][loc_tensor_info] = tensor_lists[d][t].data_ptr();
-        loc_tensor_info++;
-
-        int chunks_this_tensor = (tensor_lists[0][t].numel() + chunk_size - 1) / chunk_size;
-
-        for (int chunk = 0; chunk < chunks_this_tensor; chunk++) {
-            // std::cout << chunks_this_tensor << std::endl;
-            tl.block_to_tensor[loc_block_info] = loc_tensor_info - 1;
-            tl.block_to_chunk[loc_block_info] = chunk;
-            loc_block_info++;
-
-            bool tensors_full = (loc_tensor_info == depth_to_max_tensors[depth - 1] &&
-                                 chunk == chunks_this_tensor - 1);
-            bool blocks_full = (loc_block_info == depth_to_max_blocks[depth - 1]);
-            bool last_chunk = (t == ntensors - 1 && chunk == chunks_this_tensor - 1);
-            if (tensors_full || blocks_full || last_chunk) {
-                int* data_ptr = noop_flag.DATA_PTR<int>();
-                sycl::buffer<unsigned char, 1> block_to_tensor_buf(&(tl.block_to_tensor[0]), {320});
-                sycl::buffer<int, 1> block_to_chunk_buf(&(tl.block_to_chunk[0]), {320});
-                sycl::buffer<void*, 2> addresses_buf(&(tl.addresses[0][0]), {4, 36});
-                sycl::buffer<int, 1> sizes_buf(&(tl.sizes[0]), {36});
-                sycl::buffer<int, 1> data_buf(data_ptr, noop_flag.numel());
-                stream->submit([&](sycl::handler& cgh) {
-                    sycl::accessor tl_block_to_tensor(block_to_tensor_buf, cgh, sycl::read_only);
-                    sycl::accessor tl_block_to_chunk(block_to_chunk_buf, cgh, sycl::read_only);
-                    sycl::accessor tl_addresses(addresses_buf, cgh, sycl::read_only);
-                    sycl::accessor tl_sizes(sizes_buf, cgh, sycl::read_only);
-                    sycl::accessor data_acc(data_buf, cgh, sycl::read_only);
-                    cgh.parallel_for(sycl::nd_range<1>(loc_block_info * block_size, block_size),
-                                     [=](sycl::nd_item<1> item_ct1) {
-                                         int tensor_loc = tl_block_to_tensor[item_ct1.get_group(0)];
-                                         int chunk_idx = tl_block_to_chunk[item_ct1.get_group(0)];
-                                         int n = tl_sizes[tensor_loc];
-                                         T* g = (T*)tl_addresses[0][tensor_loc];
-                                         T* p = (T*)tl_addresses[1][tensor_loc];
-                                         T* m = (T*)tl_addresses[2][tensor_loc];
-                                         T* v = (T*)tl_addresses[3][tensor_loc];
-
-                                         AdamFunctor<T>(item_ct1,
-                                                        chunk_size,
-                                                        data_acc.get_pointer(),
-                                                        tensor_loc,
-                                                        chunk_idx,
-                                                        n,
-                                                        g,
-                                                        p,
-                                                        m,
-                                                        v,
-                                                        beta1,
-                                                        beta2,
-                                                        beta_correction1,
-                                                        beta_correction2,
-                                                        epsilon,
-                                                        lr,
-                                                        mode,
-                                                        decay);
-                                     });
-                });
-
-                // Reset.  The control flow possibilities here make my brain hurt.
-                loc_block_info = 0;
-                if (chunk == chunks_this_tensor - 1) {
-                    loc_tensor_info = 0;
-                    tl.start_tensor_this_launch = t + 1;
-                } else {
-                    tl.sizes[0] = tl.sizes[loc_tensor_info - 1];
-                    for (int d = 0; d < depth; d++)
-                        tl.addresses[d][0] = tl.addresses[d][loc_tensor_info - 1];
-                    loc_tensor_info = 1;
-                    tl.start_tensor_this_launch = t;
-                }
-            }
-        }
-    }
-}
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/normalize_layer.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/normalize_layer.hpp
deleted file mode 100644
index 58f6284..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/includes/normalize_layer.hpp
+++ /dev/null
@@ -1,203 +0,0 @@
-#pragma once
-
-#if __has_include(<sycl/sycl.hpp>)
-#include <sycl/sycl.hpp>
-#elif __has_include(<CL/sycl.hpp>)
-#include <CL/sycl.hpp>
-#else
-#error "Unsupported compiler"
-#endif
-#include "custom_sycl_layers.hpp"
-
-template <typename T>
-class Normalize_Layer {
-public:
-    struct Config {
-        uint32_t batchSize;
-        uint32_t seqLength;
-        uint32_t hiddenDim;
-        float epsilon;
-        bool training;
-        bool useMean;
-        Config(uint32_t batch,
-               uint32_t seq,
-               uint32_t h,
-               float epsilon = 1e-12,
-               bool training = true,
-               bool useMean = true)
-            : batchSize(batch),
-              seqLength(seq),
-              hiddenDim(h),
-              epsilon(epsilon),
-              training(training),
-              useMean(useMean)
-        {
-        }
-    };
-
-    Normalize_Layer(Config config)
-        : config_(config), vars(nullptr), means(nullptr), vals_hat(nullptr)
-    {
-    }
-
-    ~Normalize_Layer() {}
-
-    void ForwardCheckpoint(int bsz,  // batch * seq
-                           T* vals,
-                           const T* residual,
-                           const T* gamma,
-                           const T* betta,
-                           sycl::queue* stream,
-                           bool preLayerNorm = false)
-    {
-        launch_bias_residual_layer_norm(vals,
-                                        residual,
-                                        gamma,
-                                        betta,
-                                        config_.epsilon,
-                                        bsz,
-                                        config_.hiddenDim,
-                                        stream,
-                                        preLayerNorm,
-                                        config_.training,
-                                        vars,
-                                        means);
-    }
-
-    void Forward(int bsz,
-                 T* vals,
-                 const T* residual,
-                 const T* gamma,
-                 const T* betta,
-                 sycl::queue* stream,
-                 bool preLayerNorm = false)
-    {
-        launch_bias_residual_layer_norm(vals,
-                                        residual,
-                                        gamma,
-                                        betta,
-                                        config_.epsilon,
-                                        bsz,
-                                        config_.hiddenDim,
-                                        stream,
-                                        preLayerNorm,
-                                        config_.training,
-                                        vars);
-    }
-
-    void Backward(int bsz,
-                  const T* out_grad,
-                  const T* gamma,
-                  T* gamma_grad,
-                  T* betta_grad,
-                  sycl::queue* stream[2],
-                  T* inp_grad_out,
-                  const T* norm_in = nullptr)
-    {
-        launch_layerNorm_backward(out_grad,
-                                  norm_in,
-                                  vars,
-                                  means,
-                                  gamma,
-                                  gamma_grad,
-                                  betta_grad,
-                                  inp_grad_out,
-                                  bsz,
-                                  config_.hiddenDim,
-                                  stream);
-    }
-
-    void Backward(int bsz,
-                  const T* out_grad,
-                  const T* gamma,
-                  const T* betta,
-                  T* gamma_grad,
-                  T* betta_grad,
-                  sycl::queue* stream[2],
-                  T* inp_grad_out,
-                  const T* norm_out)
-    {
-        launch_layerNorm_backward(out_grad,
-                                  norm_out,
-                                  vars,
-                                  gamma,
-                                  gamma_grad,
-                                  betta_grad,
-                                  inp_grad_out,
-                                  bsz,
-                                  config_.hiddenDim,
-                                  stream,
-                                  !config_.useMean,
-                                  betta);
-    }
-
-    void BackwardFusedAdd(int bsz,
-                          const T* out_grad1,
-                          const T* out_grad2,
-                          const T* gamma,
-                          T* gamma_grad,
-                          T* betta_grad,
-                          sycl::queue* stream[2],
-                          T* inp_grad_out,
-                          const T* norm_in = nullptr)
-    {
-        launch_layerNorm_backward_fused_add(out_grad1,
-                                            out_grad2,
-                                            norm_in,
-                                            vars,
-                                            means,
-                                            gamma,
-                                            gamma_grad,
-                                            betta_grad,
-                                            inp_grad_out,
-                                            bsz,
-                                            config_.hiddenDim,
-                                            stream);
-    }
-
-    void BackwardFusedAdd(int bsz,
-                          const T* out_grad1,
-                          const T* out_grad2,
-                          const T* gamma,
-                          const T* betta,
-                          T* gamma_grad,
-                          T* betta_grad,
-                          sycl::queue* stream[2],
-                          T* inp_grad_out,
-                          const T* norm_out)
-    {
-        launch_layerNorm_backward_fused_add(out_grad1,
-                                            out_grad2,
-                                            norm_out,
-                                            vars,
-                                            gamma,
-                                            gamma_grad,
-                                            betta_grad,
-                                            inp_grad_out,
-                                            bsz,
-                                            config_.hiddenDim,
-                                            stream,
-                                            !config_.useMean,
-                                            betta);
-    }
-
-    inline bool UseMean() const { return config_.useMean; }
-
-    inline void SetVar(T* variance)
-    {
-        if (!variance) { throw std::runtime_error("Normalize variance is null."); }
-        vars = variance;
-    }
-
-    inline void SetMean(T* mean)
-    {
-        if (!mean) { throw std::runtime_error("Normalize mean is null."); }
-        means = mean;
-    }
-
-private:
-    Config config_;
-    T* vars;
-    T* means;
-    T* vals_hat;
-};
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/onednn_wrappers.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/onednn_wrappers.hpp
deleted file mode 100644
index 157ec4a..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/includes/onednn_wrappers.hpp
+++ /dev/null
@@ -1,38 +0,0 @@
-#pragma once
-#if __has_include(<sycl/sycl.hpp>)
-#include <sycl/sycl.hpp>
-using namespace sycl;
-#elif __has_include(<CL/sycl.hpp>)
-#include <CL/sycl.hpp>
-using namespace cl::sycl;
-#else
-#error "Unsupported compiler"
-#endif
-#include <ext/oneapi/bfloat16.hpp>
-
-using bf16 = sycl::ext::oneapi::bfloat16;
-
-int onednn_matmul_ex(sycl::queue* handle,
-                     bool trans_src,
-                     bool trans_wgt,
-                     int m,
-                     int n,
-                     int k,
-                     const float alpha,
-                     const float beta,
-                     const bf16* src_ptr,
-                     const bf16* wgt_ptr,
-                     bf16* dst_ptr);
-
-int onednn_batchgemm(sycl::queue* handle,
-                     int m,
-                     int n,
-                     int k,
-                     const float alpha,
-                     const float beta,
-                     const bf16* src_ptr,
-                     const bf16* wgt_ptr,
-                     bf16* dst_ptr,
-                     bool trans_src,
-                     bool trans_wgt,
-                     int batch);
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/quantization.h b/intel_extension_for_deepspeed/op_builder/csrc/includes/quantization.h
new file mode 100644
index 0000000..c70ef14
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/includes/quantization.h
@@ -0,0 +1,109 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+#pragma once
+
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include "ds_kernel_utils.h"
+
+namespace quantize {
+
+enum class Type { Symmetric, Asymmetric };
+
+struct PackedInt4 {
+    int8_t high : 4;
+    int8_t low : 4;
+};
+
+DS_HD_INLINE bool requires_offset(Type qType) { return qType == Type::Asymmetric; }
+
+}  // namespace quantize
+
+void launch_quant(int8_t* output_data,
+                  float* params,
+                  const sycl::half* input_data,
+                  const int groups,
+                  const int elems_per_group,
+                  const int num_bits,
+                  const quantize::Type quant_type,
+                  dpct::queue_ptr stream);
+
+template <typename T>
+void launch_dequantize_kernel(T* dequant_data,
+                              const int8_t* q_data,
+                              const float* q_params,
+                              quantize::Type q_type,
+                              int num_bits,
+                              int elems_per_group,
+                              int total_elems,
+                              dpct::queue_ptr stream);
+
+void launch_swizzled_quant(int8_t* q_data,
+                           float* q_scales,
+                           const sycl::half* input_data,
+                           int num_bits,
+                           quantize::Type q_type,
+                           int groups,
+                           int elems_per_group,
+                           int pipelining,
+                           int nodes,
+                           int devices_per_node,
+                           dpct::queue_ptr stream);
+
+void launch_dequant_reduce(int8_t* reduced_data,
+                           float* reduced_scales,
+                           const int8_t* input_data,
+                           const float* input_scales,
+                           int num_gpus,
+                           int num_bits,
+                           quantize::Type quant_type,
+                           int out_groups,
+                           int elems_per_out_group,
+                           int elems_per_in_tensor,
+                           int groups_per_in_tensor,
+                           int elems_per_in_group,
+                           dpct::queue_ptr stream);
+
+template <typename T>
+void launch_fake_quantize_kernel(T* vals,
+                                 int total_count,
+                                 int group_num,
+                                 int num_bits,
+                                 dpct::queue_ptr stream);
+template <typename T>
+void launch_sr_fake_quantize_kernel(T* vals,
+                                    int total_count,
+                                    int group_num,
+                                    int num_bits,
+                                    dpct::queue_ptr stream);
+template <typename T>
+void launch_fake_quantize_kernel_asym(T* vals,
+                                      int total_count,
+                                      int group_num,
+                                      int num_bits,
+                                      dpct::queue_ptr stream);
+template <typename T>
+void launch_sr_fake_quantize_kernel_asym(T* vals,
+                                         int total_count,
+                                         int group_num,
+                                         int num_bits,
+                                         dpct::queue_ptr stream);
+
+void launch_dequantize_int4_to_half_experimental(uint8_t* data_in,
+                                                 sycl::half* data_out,
+                                                 sycl::half* scale_buffer,
+                                                 sycl::half* min_val_buffer,
+                                                 int num_group,
+                                                 int group_size,
+                                                 dpct::queue_ptr stream);
+
+void launch_dequantize_int8_to_half_experimental(uint8_t* data_in,
+                                                 sycl::half* data_out,
+                                                 sycl::half* scale_buffer,
+                                                 sycl::half* min_val_buffer,
+                                                 int num_group,
+                                                 int group_size,
+                                                 dpct::queue_ptr stream);
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/quantization_utils.h b/intel_extension_for_deepspeed/op_builder/csrc/includes/quantization_utils.h
new file mode 100644
index 0000000..82bceba
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/includes/quantization_utils.h
@@ -0,0 +1,477 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include <cassert>
+#include "conversion_utils.h"
+#include "ds_kernel_utils.h"
+#include "memory_access_utils.h"
+#include "quantization.h"
+#include "reduction_utils.h"
+
+#pragma once
+
+using rop = reduce::ROpType;
+
+namespace quantize {
+constexpr int granularity = 16;
+constexpr int h_per_load = granularity / sizeof(sycl::half);
+constexpr int h2_per_load = granularity / sizeof(sycl::half2);
+constexpr int max_threads = 1024;
+
+/*
+Class to hold the quantization parameters for a given tensor.
+Holds the implementation of the quantization operation.
+*/
+template <Type qType, int numBits>
+class Params {
+public:
+    /*
+    Quantization implementation, supports
+    1) 4 Bit
+    2) 8 Bit
+    3) Symmetric
+    4) Asymmetric
+    Function Arguments :
+        val : The sycl::half value to quantize.
+    */
+    DS_D_INLINE int8_t quantize(sycl::half val);
+
+    template <typename T>
+    DS_D_INLINE T dequantize(int8_t val);
+
+    DS_D_INLINE void store(float* params, int group_index);
+
+    // Initialize from memory
+    DS_D_INLINE Params(const float* params, int group_index);
+};
+
+template <int numBits>
+class Params<Type::Symmetric, numBits> {
+public:
+    float scale;
+
+    DS_D_INLINE Params(float max)
+    {
+        if (max == 0) {
+            scale = 1.0;
+        } else {
+            scale = (1 << numBits) / (2 * max);
+        }
+    }
+
+    DS_D_INLINE int8_t quantize(sycl::half val)
+    {
+        constexpr int32_t q_min = -(1 << (numBits - 1));
+        constexpr int32_t q_max = (1 << (numBits - 1)) - 1;
+
+        float val_f = conversion::to<float>(val) * scale;
+        int32_t data_i32 = conversion::to<int32_t>(val_f);
+        data_i32 = dpct::min(sycl::max(data_i32, q_min), q_max);
+        return (int8_t)data_i32;
+    }
+
+    template <typename T>
+    DS_D_INLINE T dequantize(int8_t val)
+    {
+        const float val_deq_f = conversion::to<float>(val) * scale;
+        return conversion::to<T>(val_deq_f);
+    }
+
+    DS_D_INLINE void store(float* params, int group_index)
+    {
+        const float store_scale = 1 / scale;
+        mem_access::store_global<sizeof(float)>(params + group_index, &store_scale);
+    }
+
+    DS_D_INLINE Params(const float* params, int group_index)
+    {
+        mem_access::load_global<sizeof(float)>(&scale, params + group_index);
+    }
+};
+
+template <int numBits>
+class Params<Type::Asymmetric, numBits> {
+public:
+    float scale;
+    float offset;
+
+    DS_D_INLINE Params(float max, float min)
+    {
+        if (max == min) {
+            scale = 1.0;
+        } else {
+            scale = ((1 << numBits)) / (max - min);
+        }
+        offset = (max + min) / 2;
+    }
+
+    DS_D_INLINE int8_t quantize(sycl::half val)
+    {
+        constexpr int32_t q_min = -(1 << (numBits - 1));
+        constexpr int32_t q_max = (1 << (numBits - 1)) - 1;
+
+        float val_f = (conversion::to<float>(val) - offset) * scale;
+        int32_t data_i32 = conversion::to<int32_t>(val_f);
+        data_i32 = dpct::min(sycl::max(data_i32, q_min), q_max);
+        return (int8_t)data_i32;
+    }
+
+    template <typename T>
+    DS_D_INLINE T dequantize(int8_t val)
+    {
+        const float val_deq_f = ((conversion::to<float>(val)) * scale) + offset;
+        return conversion::to<sycl::half>(val_deq_f);
+    }
+
+    DS_D_INLINE void store(float* params, int group_index)
+    {
+        // Codegen should turn this into stg.64
+        const float store_scale = 1 / scale;
+        mem_access::store_global<sizeof(float)>(params + 2 * group_index, &store_scale);
+        mem_access::store_global<sizeof(float)>(params + 2 * group_index + 1, &offset);
+    }
+
+    DS_D_INLINE Params(const float* params, int group_index)
+    {
+        // Codegen should turn this into ldg.64
+        mem_access::load_global<sizeof(float)>(&scale, params + 2 * group_index);
+        mem_access::load_global<sizeof(float)>(&offset, params + 2 * group_index + 1);
+    }
+};
+
+/*
+Group stats tracks the necessary statistics about the quantized group
+to abstract the particulars for the main loop.
+*/
+template <Type qType>
+class GroupStats {
+public:
+    DS_D_INLINE void update(sycl::half2 val);
+
+    DS_D_INLINE void reduce(sycl::group<3>& tb, sycl::sub_group& warp);
+};
+
+template <>
+class GroupStats<Type::Symmetric> {
+public:
+    // Symmetric quantization only tracks the maximum absolute value
+    sycl::half2 cur_max;
+    float max;
+
+    /*
+    Technically, this would give bad results if there
+    are 0 values to process since the reduction would
+    give -inf instead of 0. We do not consider this
+    to be a reasonable edge case.
+    */
+    DS_D_INLINE GroupStats() { cur_max = reduce::init<rop::Max, sycl::half2>(); }
+
+    /*
+    Updated the running absmax used to calculate params.
+    Function Arguments :
+        val : The sycl::half2 value to update the running min and max with.
+    */
+    DS_D_INLINE void update(sycl::half2 val)
+    {
+        cur_max = reduce::element<rop::Max>(cur_max, sycl::fabs(val));
+    }
+
+    /*
+    Function to return calculated quantization params.
+    Template Arguments :
+        numBits -   Number of bits in quantized element.    int : 8 or 4
+    Function Arguments :
+        tb      -   Threadblock object. auto
+        warp    -   Warp object.        auto
+    */
+    template <int numBits, int threads_per_group>
+    DS_D_INLINE Params<Type::Symmetric, numBits> get_params(sycl::group<3>& tb,
+                                                            sycl::sub_group& warp)
+    {
+        const sycl::float2 partial_max = conversion::to<sycl::float2>(cur_max);
+        float max = reduce::element<rop::Max>(partial_max.x(), partial_max.y());
+
+        reduce::partitioned_block<rop::Max, threads_per_group>(tb, warp, max);
+        Params<Type::Symmetric, numBits> params(max);
+
+        return params;
+    }
+};
+
+template <>
+class GroupStats<Type::Asymmetric> {
+public:
+    sycl::half2 cur_max;
+    sycl::half2 cur_min;
+
+    /*
+    Initialize cur_max to -inf, cur_min to inf since
+    we are doing a true range analysis.
+    */
+    DS_D_INLINE GroupStats()
+    {
+        cur_max = reduce::init<rop::Max, sycl::half2>();
+        cur_min = reduce::init<rop::Min, sycl::half2>();
+    }
+
+    /*
+    Updated the running min and max used to calculate params.
+    Function Arguments :
+        val : The sycl::half2 value to update the running min and max with.
+    */
+    DS_D_INLINE void update(sycl::half2 val)
+    {
+        cur_max = reduce::element<rop::Max>(cur_max, val);
+        cur_min = reduce::element<rop::Min>(cur_min, val);
+    }
+
+    /*
+    Function to return calculated quantization params.
+    Template Arguments :
+        numBits -   Number of bits in quantized element.    int : 8 or 4
+    Function Arguments :
+        tb      -   Threadblock object. auto
+        warp    -   Warp object.        auto
+    */
+    template <int numBits, int threads_per_group>
+    DS_D_INLINE Params<Type::Asymmetric, numBits> get_params(sycl::group<3>& tb,
+                                                             sycl::sub_group& warp)
+    {
+        const sycl::float2 partial_max = conversion::to<sycl::float2>(cur_max);
+        float max = reduce::element<rop::Max>(partial_max.x(), partial_max.y());
+
+        const sycl::float2 partial_min = conversion::to<sycl::float2>(cur_min);
+        float min = reduce::element<rop::Min>(partial_min.x(), partial_min.y());
+
+        reduce::partitioned_block<rop::Max, rop::Min, threads_per_group>(tb, warp, max, min);
+
+        Params<Type::Asymmetric, numBits> params(max, min);
+
+        return params;
+    }
+};
+
+/*
+Device function that quantizes 16 bytes of sycl::half type input data.
+Template Arguments :
+    numBits -   Number of bits in quantized element.    int : 8 or 4
+    qType   - Type of quantization to perform.          Type::Symmetric or Type::Asymmetric
+Function Arguments :
+    local_output -  Pointer to local memory to store quantized data.    int8_t*
+    data         -  Pointer to input data.                              sycl::half*
+    Params       -  Parameters for quantization.                        Params<qType, numBits>
+*/
+template <int numBits, Type qType>
+DS_D_INLINE void _chunk(int8_t* local_output,
+                        const sycl::half* data,
+                        Params<qType, numBits> q_params);
+
+/*
+Device function that quantizes 16 bytes of sycl::half2 type input data.
+Template Arguments :
+    numBits -   Number of bits in quantized element.    int : 8 or 4
+    qType   -   Type of quantization to perform.        Type::Symmetric or Type::Asymmetric
+Function Arguments :
+    local_output -  Pointer to local memory to store quantized data.    int8_t*
+    data         -  Pointer to input data.                              sycl::half2*
+    Params       -  Parameters for quantization.                        Params<qType, numBits>
+*/
+template <int numBits, Type qType>
+DS_D_INLINE void _chunk(int8_t* local_output,
+                        const sycl::half2* data,
+                        Params<qType, numBits> q_params);
+
+/*
+Helper function to do serial reduction on register-file arrays.
+Template Arguments :
+    qType       -   Type of quantization to perform.        Type::Symmetric or Type::Asymmetric
+    numChunks   -   Number of bits in quantized element.    int : 8 or 4
+Function Arguments :
+    local_buffer    -   Pointer memory with input half2 data to be quantized.
+*/
+template <Type qType, int numChunks>
+DS_D_INLINE GroupStats<qType> _local_serial_reduce(sycl::half2* local_buffer);
+
+/*
+The main loop of the kernel that quantizes array in local memory of sycl::half2 type input data, when
+Quantization parameters are pre-computed.
+Template Arguments :
+    qType       -   Type of quantization to perform.            Type::Symmetric or Type::Asymmetric
+    numBits     -   Number of bits in quantized element.        int : 8 or 4
+    numChunks   -   Number of chunks(16 bytes of Input data).   int : 8 or 4
+Function Arguments :
+    local_buffer    -   Pointer memory with input half2 data to be quantized.
+    scales          -   Pointer to output scales.
+    offsets         -   Pointer to output offsets.
+    output_data     -   Pointer to output data.
+    elems_per_group -   Number of elements to quantize in a group.
+    q_params        -   Quantization parameters.
+*/
+template <int numBits, Type qType, int numChunks, int threads_per_group, int max_threads>
+DS_D_INLINE void local_array(sycl::group<3>& tb,
+                             sycl::sub_group& warp,
+                             sycl::half2* local_buffer,
+                             float* __restrict__ scales,
+                             float* __restrict__ offsets,
+                             int8_t* __restrict__ output_data,
+                             const int& elems_per_group,
+                             const int& groups,
+                             Params<qType, numBits> q_params);
+
+/*
+The main loop of the kernel that quantizes array in local memory of sycl::half2 type input data.
+This function computes quantization parameters for each group.
+Template Arguments :
+    qType   -   Type of quantization to perform.                Type::Symmetric or Type::Asymmetric
+    numBits     -   Number of bits in quantized element.        int : 8 or 4
+    numChunks   -   Number of chunks(16 bytes of Input data).   int : 8 or 4
+Function Arguments :
+    local_buffer    -   Pointer memory with input half2 data to be quantized.
+    scales          -   Pointer to output scales.
+    offsets         -   Pointer to output offsets.
+    output_data     -   Pointer to output data.
+    elems_per_group -   Number of elements to quantize in a group.
+*/
+template <Type qType, int numBits, int numChunks, int threads_per_group, int max_threads>
+void local_array(sycl::half2* local_buffer,
+                 float* __restrict__ scales,
+                 float* __restrict__ offsets,
+                 int8_t* __restrict__ output_data,
+                 const int& elems_per_group,
+                 const int& groups);
+
+template <int numBits, Type qType>
+DS_D_INLINE void _chunk(int8_t* local_output,
+                        const sycl::half* data,
+                        Params<qType, numBits> q_params)
+{
+    constexpr int32_t elems = 16 / sizeof(sycl::half);
+    constexpr int32_t num_elems_packed = 8 / numBits;
+
+#pragma unroll
+    for (int i = 0, oi = 0; i < elems; i += num_elems_packed, oi++) {
+        if (num_elems_packed == 1) {
+            // TODO(cmikeh2): refactor to use conversion utils
+            local_output[i] = q_params.quantize(data[i]);
+        } else if (num_elems_packed == 2) {
+            int8_t data_i8_1 = q_params.quantize(data[i]);
+            int8_t data_i8_2 = q_params.quantize(data[i + 1]);
+            auto data_i8 = PackedInt4{data_i8_2, data_i8_1};
+            local_output[oi] = *((int8_t*)(&data_i8));
+        }
+    }
+}
+
+template <int numBits, Type qType>
+DS_D_INLINE void _chunk(int8_t* local_output,
+                        const sycl::half2* data,
+                        Params<qType, numBits> q_params)
+{
+    const sycl::half* data_cast = reinterpret_cast<const sycl::half*>(data);
+    _chunk<numBits>(local_output, data_cast, q_params);
+}
+
+template <Type qType, int numChunks>
+DS_D_INLINE GroupStats<qType> _local_serial_reduce(sycl::half2* local_buffer)
+{
+    GroupStats<qType> stats;
+#pragma unroll
+    for (int i = 0; i < numChunks * h2_per_load; i++) { stats.update(local_buffer[i]); }
+
+    return stats;
+}
+
+template <Type qType, int numBits, int numChunks, int threads_per_group, int max_threads>
+DS_D_INLINE void local_array(sycl::group<3>& tb,
+                             sycl::sub_group& warp,
+                             sycl::half2* local_buffer,
+                             float* __restrict__ global_params,
+                             int8_t* __restrict__ output_data,
+                             const int& elems_per_group,
+                             const int& groups,
+                             Params<qType, numBits> q_params)
+{
+    constexpr int num_ele_int8 = 8 / numBits;
+    constexpr int num_int8_out = quantize::h_per_load / num_ele_int8;
+
+    // Indexing offsets
+    const int block_num =
+        (tb.get_group_id()[2] * max_threads / threads_per_group) + tb.get_local_id()[1];
+    const int block_offset = block_num * elems_per_group;
+    const int elem_offset = tb.get_local_id()[2] * quantize::h_per_load;
+    const int base_offset = (block_offset + elem_offset) / num_ele_int8;
+    const int stride = sycl::ext::oneapi::experimental::this_group<3>().get_local_linear_range() *
+                       quantize::h_per_load / num_ele_int8;
+
+    int8_t local_output[num_int8_out];
+
+    if (tb.get_local_id()[2] == 0 && block_num < groups) {
+        q_params.store(
+            global_params,
+            (tb.get_group_id()[2] * max_threads / threads_per_group) + tb.get_local_id()[1]);
+    }
+#pragma unroll
+    for (int i = 0; i < numChunks; i++) {
+        if (elem_offset + i * stride * num_ele_int8 < elems_per_group && block_num < groups) {
+            quantize::_chunk<numBits, qType>(
+                local_output, local_buffer + i * quantize::h2_per_load, q_params);
+            mem_access::store_global<num_int8_out>(output_data + (base_offset + i * stride),
+                                                   local_output);
+        }
+    }
+}
+
+template <Type qType, int numBits, int numChunks, int threads_per_group, int max_threads>
+DS_D_INLINE void local_array(sycl::group<3>& tb,
+                             sycl::sub_group& warp,
+                             sycl::half* local_buffer,
+                             float* __restrict__ global_params,
+                             int8_t* __restrict__ output_data,
+                             const int& elems_per_group,
+                             const int& groups,
+                             Params<qType, numBits> q_params)
+{
+    sycl::half2* local_buffer_h2 = reinterpret_cast<sycl::half2*>(local_buffer);
+
+    quantize::local_array<qType, numBits, numChunks, threads_per_group, max_threads>(
+        tb, warp, local_buffer, global_params, output_data, elems_per_group, groups, q_params);
+}
+
+template <Type qType,
+          int numBits,
+          int numChunks,
+          int threads_per_group = max_threads,
+          int max_threads = 256>
+void local_array(sycl::half2* local_buffer,
+                 float* __restrict__ global_params,
+                 int8_t* __restrict__ output_data,
+                 const int& elems_per_group,
+                 const int& groups)
+{
+    sycl::group<3> tb = sycl::ext::oneapi::experimental::this_group<3>();
+    sycl::sub_group warp = sycl::ext::oneapi::experimental::this_sub_group();
+
+    auto group_stats = _local_serial_reduce<qType, numChunks>(local_buffer);
+    auto params = group_stats.template get_params<numBits, threads_per_group>(tb, warp);
+
+    quantize::local_array<qType, numBits, numChunks, threads_per_group, max_threads>(
+        tb, warp, local_buffer, global_params, output_data, elems_per_group, groups, params);
+}
+
+template <Type qType, int numBits, int numChunks, int threads_per_group, int max_threads>
+void local_array(sycl::half* local_buffer,
+                 float* __restrict__ global_params,
+                 int8_t* __restrict__ output_data,
+                 const int& elems_per_group,
+                 const int& groups)
+{
+    sycl::half2* local_buffer_h2 = reinterpret_cast<sycl::half2*>(local_buffer);
+    quantize::local_array<qType, numBits, numChunks, threads_per_group, max_threads>(
+        local_buffer_h2, global_params, output_data, elems_per_group, groups);
+}
+
+}  // namespace quantize
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/reduction_utils.h b/intel_extension_for_deepspeed/op_builder/csrc/includes/reduction_utils.h
new file mode 100644
index 0000000..c21d19e
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/includes/reduction_utils.h
@@ -0,0 +1,804 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+#pragma once
+
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include "conversion_utils.h"
+#include "ds_kernel_utils.h"
+#include "memory_access_utils.h"
+
+namespace reduce {
+
+enum class ROpType {
+    // Addition
+    Add,
+
+    // Maximum reduction
+    Max,
+
+    // Minimum reduction
+    Min,
+};
+
+constexpr int max_threads = 1024;
+constexpr int max_warps = max_threads / hw_warp_size;
+
+/*
+High level API. The API takes in a set of operations and variables
+and performs that reduction operation on that variable. The reductions
+of each of the arguments are completely independent of each other (
+i.e., the val1-op1 combination has no impact on val2-op2).
+
+Example usage:
+``` cpp
+float max_val;
+float min_val;
+reduce::block<rop::Max, rop::Min>(tb, warp, max_val, min_val);
+```
+
+TODO(cmikeh2): In theory, we might be able to do this sequentially with
+device functions and rely on the assembler correctly behaving. My initial
+instinct is this won't work, but if it does it would reduce implementation
+cost significantly.
+
+TODO(cmikeh2): We need to support sub-block reductions. The warp intrinsic
+currently supports this (more incidentally than anything else). It is not
+uncommon in something like softmax or a fused attention kernel to map multiple
+reductions to a thread block, but each reduction itself is only scoped
+to part of the threads (i.e block size = 512, 128 threads per reduction).
+*/
+template <ROpType Op, int warp_bound = max_warps>
+DS_D_INLINE void block(sycl::group<3>& tb, sycl::sub_group& warp, float& val);
+
+template <ROpType Op1, ROpType Op2, int warp_bound = max_warps>
+DS_D_INLINE void block(sycl::group<3>& tb, sycl::sub_group& warp, float& val1, float& val2);
+
+template <ROpType Op1, ROpType Op2, ROpType Op3, int warp_bound = max_warps>
+DS_D_INLINE void block(sycl::group<3>& tb,
+                       sycl::sub_group& warp,
+                       float& val1,
+                       float& val2,
+                       float& val3);
+
+template <ROpType Op1, ROpType Op2, ROpType Op3, ROpType Op4, int warp_bound = max_warps>
+DS_D_INLINE void block(sycl::group<3>& tb,
+                       sycl::sub_group& warp,
+                       float& val1,
+                       float& val2,
+                       float& val3,
+                       float& val4);
+
+/*
+The partitioned block is a special case of the above where in the warps of a threadblock are
+partitioned into separate independent reductions. For example, I might have an 8 warp thread block
+in which each pair of warps is processing an independent piece of data. I would then reduce that
+data with the something like the following:
+``` cpp
+float max_val;
+reduce::partitioned_block<rop::Max, 2>(tb, warp, max_val);
+```
+After which, each pair of warps would have coherent data with each other. Note, this API will not
+provide correct results if the number of warps per partition is not a power of 2.
+*/
+template <ROpType Op, int num_threads>
+DS_D_INLINE void partitioned_block(sycl::group<3>& tb, sycl::sub_group& warp, float& val);
+
+template <ROpType Op1, ROpType Op2, int num_threads>
+DS_D_INLINE void partitioned_block(sycl::group<3>& tb,
+                                   sycl::sub_group& warp,
+                                   float& val1,
+                                   float& val2);
+
+template <ROpType Op1, ROpType Op2, ROpType Op3, int num_threads>
+DS_D_INLINE void partitioned_block(sycl::group<3>& tb,
+                                   sycl::sub_group& warp,
+                                   float& val1,
+                                   float& val2,
+                                   float& val3);
+
+template <ROpType Op1, ROpType Op2, ROpType Op3, ROpType Op4, int num_threads>
+DS_D_INLINE void partitioned_block(sycl::group<3>& tb,
+                                   sycl::sub_group& warp,
+                                   float& val1,
+                                   float& val2,
+                                   float& val3,
+                                   float& val4);
+
+/*
+Single element reduction primitives. Used inside serial collection
+loops.
+
+Example usage:
+using rop = reduce::OpType;
+float min = init<rop::Min>();
+for (int i = 0; i < 4; i++) {
+    min = reduce::element<rop::Min>(min, data[i]);
+}
+*/
+
+template <ROpType Op, typename T>
+DS_D_INLINE T element(const T lhs, const T rhs);
+
+template <ROpType OType, typename T = float>
+DS_D_INLINE T init();
+
+/********************** Internal reduction APIs **********************/
+
+/*
+Single element "reductions". TODO(cmikeh2): this sort of "op" concept
+should be refactored into its own implementation at some point. This interface
+may be easily expanded for new types/operations, but the typical reductions
+we need are covered with min/max/add on float.
+
+NOTE: there is no mean reduction because that relies on knowledge of how
+many values were already reduced into each scalar. Implementing this on top
+of reduce should be straightforward (can just wrap the sum reduction) and
+would be a good extension of the header.
+*/
+
+DS_D_INLINE int _warp_rank()
+{
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    const int thread_rank =
+        item_ct1.get_local_id(2) + item_ct1.get_local_id(1) * item_ct1.get_local_range(2) +
+        item_ct1.get_local_id(0) * item_ct1.get_local_range(2) * item_ct1.get_local_range(1);
+    return thread_rank / hw_warp_size;
+}
+
+/* Float element reduce implementations */
+template <>
+DS_D_INLINE float element<ROpType::Add>(const float lhs, const float rhs)
+{
+    return lhs + rhs;
+}
+
+template <>
+DS_D_INLINE float element<ROpType::Max>(const float lhs, const float rhs)
+{
+    return sycl::fmax((float)lhs, (float)rhs);
+}
+
+template <>
+DS_D_INLINE float element<ROpType::Min>(const float lhs, const float rhs)
+{
+    return sycl::fmin((float)lhs, (float)rhs);
+}
+
+/* sycl::half element reduce implementation */
+template <>
+DS_D_INLINE sycl::half element<ROpType::Add>(const sycl::half lhs, const sycl::half rhs)
+{
+    return lhs + rhs;
+}
+
+template <>
+DS_D_INLINE sycl::half element<ROpType::Max>(const sycl::half lhs, const sycl::half rhs)
+{
+#if DPCT_COMPATIBILITY_TEMP >= 800
+    // Intrinsic limited to Ampere + newer
+    return __hmax(lhs, rhs);
+#else
+    return (lhs > rhs) ? lhs : rhs;
+#endif
+}
+
+template <>
+DS_D_INLINE sycl::half element<ROpType::Min>(const sycl::half lhs, const sycl::half rhs)
+{
+#if DPCT_COMPATIBILITY_TEMP >= 800
+    // Intrinsic limited to Ampere + newer
+    return __hmin(lhs, rhs);
+#else
+    return (lhs < rhs) ? lhs : rhs;
+#endif
+}
+
+/* sycl::half2 element reduce implementation */
+template <>
+DS_D_INLINE sycl::half2 element<ROpType::Add>(const sycl::half2 lhs, const sycl::half2 rhs)
+{
+    return lhs + rhs;
+}
+
+template <>
+DS_D_INLINE sycl::half2 element<ROpType::Max>(const sycl::half2 lhs, const sycl::half2 rhs)
+{
+#if DPCT_COMPATIBILITY_TEMP >= 800
+    return __hmax2(lhs, rhs);
+#else
+    sycl::half2 ret_val;
+    ret_val.x() = (lhs.x() > rhs.x()) ? lhs.x() : rhs.x();
+    ret_val.y() = (lhs.y() > rhs.y()) ? lhs.y() : rhs.y();
+    return ret_val;
+#endif
+}
+
+template <>
+DS_D_INLINE sycl::half2 element<ROpType::Min>(const sycl::half2 lhs, const sycl::half2 rhs)
+{
+#if DPCT_COMPATIBILITY_TEMP >= 800
+    return __hmin2(lhs, rhs);
+#else
+    sycl::half2 ret_val;
+    ret_val.x() = (lhs.x() < rhs.x()) ? lhs.x() : rhs.x();
+    ret_val.y() = (lhs.y() < rhs.y()) ? lhs.y() : rhs.y();
+    return ret_val;
+#endif
+}
+
+template <>
+DS_D_INLINE int32_t element<ROpType::Add>(const int32_t lhs, const int32_t rhs)
+{
+    return lhs + rhs;
+}
+
+template <>
+DS_D_INLINE int32_t element<ROpType::Max>(const int32_t lhs, const int32_t rhs)
+{
+    return (lhs > rhs) ? lhs : rhs;
+}
+
+template <>
+DS_D_INLINE int32_t element<ROpType::Min>(const int32_t lhs, const int32_t rhs)
+{
+    return (lhs < rhs) ? lhs : rhs;
+}
+
+template <>
+DS_D_INLINE uint32_t element<ROpType::Add>(const uint32_t lhs, const uint32_t rhs)
+{
+    return lhs + rhs;
+}
+
+template <>
+DS_D_INLINE uint32_t element<ROpType::Max>(const uint32_t lhs, const uint32_t rhs)
+{
+    return (lhs > rhs) ? lhs : rhs;
+}
+
+template <>
+DS_D_INLINE uint32_t element<ROpType::Min>(const uint32_t lhs, const uint32_t rhs)
+{
+    return (lhs < rhs) ? lhs : rhs;
+}
+
+template <>
+DS_D_INLINE int64_t element<ROpType::Add>(const int64_t lhs, const int64_t rhs)
+{
+    return lhs + rhs;
+}
+
+template <>
+DS_D_INLINE int64_t element<ROpType::Max>(const int64_t lhs, const int64_t rhs)
+{
+    return (lhs > rhs) ? lhs : rhs;
+}
+
+template <>
+DS_D_INLINE int64_t element<ROpType::Min>(const int64_t lhs, const int64_t rhs)
+{
+    return (lhs < rhs) ? lhs : rhs;
+}
+
+/*
+Reduction initialization primitives
+*/
+template <>
+DS_D_INLINE float init<ROpType::Add>()
+{
+    return 0.0f;
+}
+
+template <>
+DS_D_INLINE float init<ROpType::Min>()
+{
+    // Positive infinity
+    return INFINITY;
+}
+
+template <>
+DS_D_INLINE float init<ROpType::Max>()
+{
+    // Negative infinity
+    return -INFINITY;
+}
+
+template <>
+DS_D_INLINE sycl::half init<ROpType::Add>()
+{
+    constexpr uint16_t zero = {0x0000};
+    return sycl::half(zero);
+}
+
+template <>
+DS_D_INLINE sycl::half init<ROpType::Min>()
+{
+    constexpr uint16_t inf = {0x7C00};
+    return sycl::half(inf);
+}
+
+template <>
+DS_D_INLINE sycl::half init<ROpType::Max>()
+{
+    constexpr uint16_t neg_inf = {0xFC00};
+    return sycl::half(neg_inf);
+}
+
+template <>
+DS_D_INLINE sycl::half2 init<ROpType::Add>()
+{
+#ifdef __HIP_PLATFORM_AMD__
+    return sycl::half2{_Float16_2{0x0000, 0x0000}};
+#else
+    constexpr sycl::half2 zero = {0x0000, 0x0000};
+    return sycl::half2(zero);
+#endif
+}
+
+template <>
+DS_D_INLINE sycl::half2 init<ROpType::Min>()
+{
+#ifdef __HIP_PLATFORM_AMD__
+    return sycl::half2{_Float16_2{0x7C00, 0x7C00}};
+#else
+    constexpr sycl::half2 inf = {0x7C00, 0x7C00};
+    return sycl::half2(inf);
+#endif
+}
+
+template <>
+DS_D_INLINE sycl::half2 init<ROpType::Max>()
+{
+#ifdef __HIP_PLATFORM_AMD__
+    return sycl::half2{_Float16_2{0xFC00, 0xFC00}};
+#else
+    constexpr sycl::half2 neg_inf = {0xFC00, 0xFC00};
+    return sycl::half2(neg_inf);
+#endif
+}
+
+template <>
+DS_D_INLINE int32_t init<ROpType::Add>()
+{
+    return 0;
+}
+
+template <>
+DS_D_INLINE int32_t init<ROpType::Min>()
+{
+    return 0x7FFFFFFF;
+}
+
+template <>
+DS_D_INLINE int32_t init<ROpType::Max>()
+{
+    return 0x80000000;
+}
+
+template <>
+DS_D_INLINE uint32_t init<ROpType::Add>()
+{
+    return 0;
+}
+
+template <>
+DS_D_INLINE uint32_t init<ROpType::Min>()
+{
+    return 0xFFFFFFFF;
+}
+
+template <>
+DS_D_INLINE uint32_t init<ROpType::Max>()
+{
+    return 0;
+}
+
+template <>
+DS_D_INLINE int64_t init<ROpType::Add>()
+{
+    return 0;
+}
+
+template <>
+DS_D_INLINE int64_t init<ROpType::Min>()
+{
+    return 0x7FFFFFFFFFFFFFFF;
+}
+
+template <>
+DS_D_INLINE int64_t init<ROpType::Max>()
+{
+    return 0x8000000000000000;
+}
+
+template <>
+DS_D_INLINE uint64_t init<ROpType::Add>()
+{
+    return 0;
+}
+
+template <>
+DS_D_INLINE uint64_t init<ROpType::Min>()
+{
+    return 0xFFFFFFFFFFFFFFFF;
+}
+
+template <>
+DS_D_INLINE uint64_t init<ROpType::Max>()
+{
+    return 0;
+}
+
+template <ROpType Op, typename T>
+DS_D_INLINE void init(T* data)
+{
+    data[0] = init<Op, T>();
+}
+
+template <ROpType Op1, ROpType Op2, typename T>
+DS_D_INLINE void init(T* data)
+{
+    data[0] = init<Op1, T>();
+    data[1] = init<Op2, T>();
+}
+
+template <ROpType Op1, ROpType Op2, ROpType Op3, typename T>
+DS_D_INLINE void init(T* data)
+{
+    data[0] = init<Op1, T>();
+    data[1] = init<Op2, T>();
+    data[2] = init<Op3, T>();
+}
+
+template <ROpType Op1, ROpType Op2, ROpType Op3, ROpType Op4, typename T>
+DS_D_INLINE void init(T* data)
+{
+    data[0] = init<Op1, T>();
+    data[1] = init<Op2, T>();
+    data[2] = init<Op3, T>();
+    data[3] = init<Op4, T>();
+}
+
+/*
+Warp reduction primitives
+
+`reduction_width` is an unsafe template parameter, that is that
+when using `reduction_width` < hw_warp_size the warp is partitioned
+into `hw_warp_size` / `reduction_width` groups of partial sums.
+
+If someone can figure out how to use variadic templates in a reasonable way
+here (fold is C++17 only and I don't think helps and recursion feels like
+huge overkill that harms readability) that would be wonderful.
+*/
+
+template <typename T, ROpType Op, int reduce_width = hw_warp_size>
+DS_D_INLINE void _warp(sycl::sub_group& warp, T* data)
+{
+#pragma unroll
+    for (int i = 1; i < reduce_width; i *= 2) {
+        data[0] = element<Op>(data[0],
+                              sycl::permute_group_by_xor(
+                                  sycl::ext::oneapi::experimental::this_sub_group(), data[0], i));
+    }
+}
+
+template <typename T, ROpType Op1, ROpType Op2, int reduce_width = hw_warp_size>
+DS_D_INLINE void _warp(sycl::sub_group& warp, T* data)
+{
+#pragma unroll
+    for (int i = 1; i < reduce_width; i *= 2) {
+        data[0] = element<Op1>(data[0],
+                               sycl::permute_group_by_xor(
+                                   sycl::ext::oneapi::experimental::this_sub_group(), data[0], i));
+        data[1] = element<Op2>(data[1],
+                               sycl::permute_group_by_xor(
+                                   sycl::ext::oneapi::experimental::this_sub_group(), data[1], i));
+    }
+}
+
+template <typename T, ROpType Op1, ROpType Op2, ROpType Op3, int reduce_width = hw_warp_size>
+DS_D_INLINE void _warp(sycl::sub_group& warp, T* data)
+{
+#pragma unroll
+    for (int i = 1; i < reduce_width; i *= 2) {
+        data[0] = element<Op1>(data[0], warp.shuffle_xor(data[0], i));
+        data[1] = element<Op2>(data[1], warp.shuffle_xor(data[1], i));
+        data[2] = element<Op3>(data[2], warp.shuffle_xor(data[2], i));
+    }
+}
+
+template <typename T,
+          ROpType Op1,
+          ROpType Op2,
+          ROpType Op3,
+          ROpType Op4,
+          int reduce_width = hw_warp_size>
+DS_D_INLINE void _warp(sycl::sub_group& warp, T* data)
+{
+#pragma unroll
+    for (int i = 1; i < reduce_width; i *= 2) {
+        data[0] = element<Op1>(data[0], warp.shuffle_xor(data[0], i));
+        data[1] = element<Op2>(data[1], warp.shuffle_xor(data[1], i));
+        data[2] = element<Op3>(data[2], warp.shuffle_xor(data[2], i));
+        data[3] = element<Op4>(data[3], warp.shuffle_xor(data[3], i));
+    }
+}
+
+/*
+Implementation for primary block reduction that serves both `block` and
+`partitioned_block`.
+
+Total warps refers to the reduction width of the reduction, not
+the number of warps in the block (which may exceed that
+if the block is partitioned or if we do a conservative bound at
+compile time).
+*/
+template <typename T, int total_warps, ROpType... Ops>
+DS_D_INLINE void _block(sycl::group<3>& tb, sycl::sub_group& warp_arg, T* data)
+{
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    constexpr int elems = sizeof...(Ops);
+    constexpr int bytes = sizeof(T);
+    // Unused when `partition_size == 1` or total_warps == 1
+    /*
+    DPCT1115:0: The sycl::ext::oneapi::group_local_memory is used to allocate group-local memory at
+    the none kernel functor scope of a work-group data parallel kernel. You may need to adjust the
+    code.
+    */
+    auto& reduce_buffer = *sycl::ext::oneapi::group_local_memory_for_overwrite<T[max_warps * elems]>(
+        sycl::ext::oneapi::experimental::this_group<3>());
+
+#ifdef __HIP_PLATFORM_AMD__
+    const int total_threads = blockDim.x * blockDim.y * blockDim.z;
+    const int running_warps = total_threads / hw_warp_size;
+#else
+    /*
+    DPCT1007:7: Migration of cooperative_groups::thread_block_tile::meta_group_size is not
+    supported.
+    */
+    const int running_warps = warp_arg.get_group_range().size();
+#endif
+
+    // Always perform warp-scope reduction
+    _warp<T, Ops...>(warp_arg, data);
+
+    // If max_warps == 1 let's skip the runtime check
+    if (total_warps != 1) {
+        if (sycl::ext::oneapi::experimental::this_sub_group().get_local_linear_id() == 0) {
+#pragma unroll
+            for (int i = 0; i < elems; i++) {
+                mem_access::store_shared<bytes>(reduce_buffer + elems * _warp_rank() + i, data + i);
+            }
+        }
+
+        // Synchronization inside block-uniform conditional is safe
+        /*
+        DPCT1065:1: Consider replacing sycl::nd_item::barrier() with
+        sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if
+        there is no access to global memory.
+        */
+        item_ct1.barrier();
+
+        if (_warp_rank() == 0) {
+            if (sycl::ext::oneapi::experimental::this_sub_group().get_local_linear_id() <
+                running_warps) {
+#pragma unroll
+                for (int i = 0; i < elems; i++) {
+                    mem_access::load_shared<bytes>(
+                        data + i,
+                        reduce_buffer +
+                            elems * sycl::ext::oneapi::experimental::this_sub_group()
+                                        .get_local_linear_id() +
+                            i);
+                }
+            } else {
+                init<Ops...>(data);
+            }
+
+            _warp<T, Ops..., total_warps>(warp_arg, data);
+
+#pragma unroll
+            for (int i = 0; i < elems; i++) {
+                mem_access::store_shared<bytes>(
+                    reduce_buffer +
+                        elems * sycl::ext::oneapi::experimental::this_sub_group()
+                                    .get_local_linear_id() +
+                        i,
+                    data + i);
+            }
+        }
+
+        // Synchronization inside block-uniform conditional is safe
+        /*
+        DPCT1065:2: Consider replacing sycl::nd_item::barrier() with
+        sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if
+        there is no access to global memory.
+        */
+        item_ct1.barrier();
+
+#pragma unroll
+        for (int i = 0; i < elems; i++) {
+            mem_access::load_shared<bytes>(data + i, reduce_buffer + _warp_rank() * elems + i);
+        }
+    }
+}
+
+/*
+Main API implementations. For the most part, they just convert the individual
+variables into arrays, which makes working with them easier with a single
+implementation. In theory, we could use the `_block` implementation as another
+option, but the nature of using a pointer is a little less safe and this allows
+us to obfuscate the details of the partitioned implementation.
+*/
+template <ROpType Op, int warp_bound>
+DS_D_INLINE void block(sycl::group<3>& tb, sycl::sub_group& warp, float& val)
+{
+    _block<float, warp_bound, Op>(tb, warp, &val);
+}
+
+template <ROpType Op1, ROpType Op2, int warp_bound>
+DS_D_INLINE void block(sycl::group<3>& tb, sycl::sub_group& warp, float& val1, float& val2)
+{
+    float data[2] = {val1, val2};
+    _block<float, warp_bound, Op1, Op2>(tb, warp, data);
+    val1 = data[0];
+    val2 = data[1];
+}
+
+template <ROpType Op1, ROpType Op2, ROpType Op3, int warp_bound>
+DS_D_INLINE void block(sycl::group<3>& tb,
+                       sycl::sub_group& warp,
+                       float& val1,
+                       float& val2,
+                       float& val3)
+{
+    float data[3] = {val1, val2, val3};
+    _block<float, warp_bound, Op1, Op2, Op3>(tb, warp, data);
+    val1 = data[0];
+    val2 = data[1];
+    val3 = data[2];
+}
+
+template <ROpType Op1, ROpType Op2, ROpType Op3, ROpType Op4, int warp_bound>
+DS_D_INLINE void block(sycl::group<3>& tb,
+                       sycl::sub_group& warp,
+                       float& val1,
+                       float& val2,
+                       float& val3,
+                       float& val4)
+{
+    float data[4] = {val1, val2, val3, val4};
+    _block<float, warp_bound, Op1, Op2, Op3, Op4>(tb, warp, data);
+    val1 = data[0];
+    val2 = data[1];
+    val3 = data[2];
+    val4 = data[3];
+}
+
+/*
+Note: for the partitioned blocks, the implementation does not support non-power of 2 blocks in order
+to shorten block scale reduction length.
+*/
+template <ROpType Op, int num_threads>
+DS_D_INLINE void partitioned_block(sycl::group<3>& tb, sycl::sub_group& warp, float& val)
+{
+    if (num_threads <= hw_warp_size) {
+        _warp<float, Op, num_threads>(warp, &val);
+    } else {
+        constexpr int num_warps = num_threads / hw_warp_size;
+        _block<float, num_warps, Op>(tb, warp, &val);
+    }
+}
+
+template <ROpType Op1, ROpType Op2, int num_threads>
+DS_D_INLINE void partitioned_block(sycl::group<3>& tb,
+                                   sycl::sub_group& warp,
+                                   float& val1,
+                                   float& val2)
+{
+    float data[2] = {val1, val2};
+
+    if (num_threads <= hw_warp_size) {
+        _warp<float, Op1, Op2, num_threads>(warp, data);
+    } else {
+        constexpr int num_warps = num_threads / hw_warp_size;
+        _block<float, num_warps, Op1, Op2>(tb, warp, data);
+    }
+
+    val1 = data[0];
+    val2 = data[1];
+}
+
+template <ROpType Op1, ROpType Op2, ROpType Op3, int num_threads>
+DS_D_INLINE void partitioned_block(sycl::group<3>& tb,
+                                   sycl::sub_group& warp,
+                                   float& val1,
+                                   float& val2,
+                                   float& val3)
+{
+    float data[3] = {val1, val2, val3};
+
+    if (num_threads <= hw_warp_size) {
+        _warp<float, Op1, Op2, Op3, num_threads>(warp, data);
+    } else {
+        constexpr int num_warps = num_threads / hw_warp_size;
+        _block<float, num_warps, Op1, Op2, Op3>(tb, warp, data);
+    }
+
+    val1 = data[0];
+    val2 = data[1];
+    val3 = data[2];
+}
+
+template <ROpType Op1, ROpType Op2, ROpType Op3, ROpType Op4, int num_threads>
+DS_D_INLINE void partitioned_block(sycl::group<3>& tb,
+                                   sycl::sub_group& warp,
+                                   float& val1,
+                                   float& val2,
+                                   float& val3,
+                                   float& val4)
+{
+    float data[4] = {val1, val2, val3, val4};
+
+    if (num_threads <= hw_warp_size) {
+        _warp<float, Op1, Op2, Op3, Op4, num_threads>(warp, data);
+    } else {
+        constexpr int num_warps = num_threads / hw_warp_size;
+        _block<float, num_warps, Op1, Op2, Op3, Op4>(tb, warp, data);
+    }
+
+    val1 = data[0];
+    val2 = data[1];
+    val3 = data[2];
+    val4 = data[3];
+}
+
+/*
+Arg-reduce is a specialization of the above. We only support this with a single reduction
+parameter. This only works for max/min reductions.
+*/
+
+__dpct_align__(8) struct IdxReduceResult {
+    /*
+    NOTE: ORDERING MATTERS HERE! The idx is the least significant set of bits
+    and the val is the most significant. Changing the order of this declaration
+    will break the code.
+    */
+    int idx;
+    float val;
+};
+
+template <ROpType Op, int warpBound>
+DS_D_INLINE IdxReduceResult
+idx_reduce(sycl::group<3>& tb, sycl::sub_group& warp, float val, int idx)
+{
+    IdxReduceResult res = {idx, val};
+
+    // Clear out the nan. This shouldn't be an issue for our initial applications
+    if (sycl::isnan(val)) res.val = init<Op>();
+
+    // Can do float compares as integers. By packing the index into the lower bits
+    // we can just do a single int64 rather than a branch, compare, and select.
+    // One side benefit of this is that it is by nature a stable algorithm and
+    // will always bias ties to the higher index.
+    int64_t* res_as_int = reinterpret_cast<int64_t*>(&res);
+
+    // The way floating point compare works is normally to perform a sign comparison
+    // and if they match, then do a comparison of the rest of the bits as unsigned
+    // integers. Since we are bundling these, that means for negative values we need
+    // to reverse the sort order, which we can do with an XOR.
+    if (val < 0) { *res_as_int ^= 0x7fffffff00000000; }
+
+    _block<int64_t, warpBound, Op>(tb, warp, res_as_int);
+
+    // Sign bit is preserved, so we can check if we need to invert the mantissa back
+    if (res.val < 0) { *res_as_int ^= 0x7fffffff00000000; }
+
+    return res;
+}
+
+}  // namespace reduce
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/simd.h b/intel_extension_for_deepspeed/op_builder/csrc/includes/simd.h
new file mode 100644
index 0000000..f77568b
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/includes/simd.h
@@ -0,0 +1,198 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+#pragma once
+
+#if (__x86_64__ || __i386__)
+#include <cpuid.h>
+#include <x86intrin.h>
+#endif
+
+#define TILE (128 * 1024 * 1024)
+#if defined(__AVX512__) or defined(__AVX256__)
+
+#define ROUND_DOWN(size, step) ((size) & ~((step)-1))
+
+#if defined(__AVX512__)
+#define SIMD_STORE(a, d) _mm512_storeu_ps(a, d)
+#define SIMD_LOAD(x) _mm512_loadu_ps(x)
+#define SIMD_SET(x) _mm512_set1_ps(x)
+#define SIMD_ADD(x, y) _mm512_add_ps(x, y)
+#define SIMD_MUL(x, y) _mm512_mul_ps(x, y)
+#define SIMD_FMA(x, y, c) _mm512_fmadd_ps(x, y, c)
+#define SIMD_SQRT(x) _mm512_sqrt_ps(x)
+#define SIMD_DIV(x, y) _mm512_div_ps(x, y)
+#define SIMD_AND(x, y) _mm512_and_ps(x, y)
+#define SIMD_ANDNOT(x, y) _mm512_andnot_ps(x, y)
+#define SIMD_OR(x, y) _mm512_or_ps(x, y)
+#define SIMD_XOR(x, y) _mm512_xor_ps(x, y)
+#define SIMD_WIDTH 16
+
+#define SIMD_LOAD2(x, h) \
+    ((h) ? _mm512_cvtph_ps(_mm256_castps_si256(_mm256_loadu_ps(x))) : _mm512_loadu_ps(x))
+#define SIMD_STORE2(x, d, h)                                                                      \
+    ((h) ? _mm256_store_ps(x, _mm256_castsi256_ps(_mm512_cvtps_ph(d, _MM_FROUND_TO_NEAREST_INT))) \
+         : _mm512_storeu_ps(x, d))
+
+#define INTV __m256i
+#elif defined(__AVX256__)
+#define SIMD_STORE(a, d) _mm256_storeu_ps(a, d)
+#define SIMD_LOAD(x) _mm256_loadu_ps(x)
+#define SIMD_SET(x) _mm256_set1_ps(x)
+#define SIMD_ADD(x, y) _mm256_add_ps(x, y)
+#define SIMD_MUL(x, y) _mm256_mul_ps(x, y)
+#define SIMD_FMA(x, y, c) _mm256_fmadd_ps(x, y, c)
+#define SIMD_SQRT(x) _mm256_sqrt_ps(x)
+#define SIMD_DIV(x, y) _mm256_div_ps(x, y)
+#define SIMD_AND(x, y) _mm256_and_ps(x, y)
+#define SIMD_ANDNOT(x, y) _mm256_andnot_ps(x, y)
+#define SIMD_OR(x, y) _mm256_or_ps(x, y)
+#define SIMD_XOR(x, y) _mm256_xor_ps(x, y)
+#define SIMD_WIDTH 8
+
+#define SIMD_LOAD2(x, h) \
+    ((h) ? _mm256_cvtph_ps(_mm_loadu_si128((const __m128i*)x)) : _mm256_loadu_ps(x))
+#define SIMD_STORE2(x, d, h)                                                                \
+    ((h) ? _mm_store_ps(x, _mm_castsi128_ps(_mm256_cvtps_ph(d, _MM_FROUND_TO_NEAREST_INT))) \
+         : _mm256_storeu_ps(x, d))
+
+#define INTV __m128i
+#endif
+
+union AVX_Data {
+#if defined(__AVX512__)
+    __m512 data;
+#elif defined(__AVX256__)
+    __m256 data;
+#endif
+    // float data_f[16];
+};
+
+template <int span>
+inline void simd_store(float* dst, AVX_Data* src, bool half_precision)
+{
+    size_t width = (half_precision ? SIMD_WIDTH / 2 : SIMD_WIDTH);
+#pragma unroll
+    for (size_t i = 0; i < span; ++i) { SIMD_STORE2(dst + width * i, src[i].data, half_precision); }
+}
+template <int span>
+inline void simd_load(AVX_Data* dst, float* src, bool half_precision)
+{
+    size_t width = (half_precision ? 1 : SIMD_WIDTH);
+#pragma unroll
+    for (size_t i = 0; i < span; ++i) { dst[i].data = SIMD_LOAD2(src + width * i, half_precision); }
+}
+template <int span>
+inline void simd_fma(AVX_Data* dst, AVX_Data* src_m_l, AVX_Data src_m_r, AVX_Data* src_a)
+{
+#pragma unroll
+    for (size_t i = 0; i < span; ++i) {
+        dst[i].data = SIMD_FMA(src_m_l[i].data, src_m_r.data, src_a[i].data);
+    }
+}
+template <int span>
+inline void simd_fma(AVX_Data* dst, AVX_Data* src_m_l, AVX_Data src_m_r, AVX_Data src_a)
+{
+#pragma unroll
+    for (size_t i = 0; i < span; ++i) {
+        dst[i].data = SIMD_FMA(src_m_l[i].data, src_m_r.data, src_a.data);
+    }
+}
+template <int span>
+inline void simd_fma(AVX_Data* dst, AVX_Data* src_m_l, AVX_Data* src_m_r, AVX_Data* src_a)
+{
+#pragma unroll
+    for (size_t i = 0; i < span; ++i) {
+        dst[i].data = SIMD_FMA(src_m_l[i].data, src_m_r[i].data, src_a[i].data);
+    }
+}
+template <int span>
+inline void simd_sqrt(AVX_Data* dst, AVX_Data* src)
+{
+#pragma unroll
+    for (size_t i = 0; i < span; ++i) { dst[i].data = SIMD_SQRT(src[i].data); }
+}
+template <int span>
+inline void simd_add(AVX_Data* dst, AVX_Data* src_a_l, AVX_Data src_a_r)
+{
+#pragma unroll
+    for (size_t i = 0; i < span; ++i) { dst[i].data = SIMD_ADD(src_a_l[i].data, src_a_r.data); }
+}
+template <int span>
+inline void simd_add(AVX_Data* dst, AVX_Data* src_a_l, AVX_Data* src_a_r)
+{
+#pragma unroll
+    for (size_t i = 0; i < span; ++i) { dst[i].data = SIMD_ADD(src_a_l[i].data, src_a_r[i].data); }
+}
+template <int span>
+inline void simd_mul(AVX_Data* dst, AVX_Data* src_a_l, AVX_Data src_a_r)
+{
+#pragma unroll
+    for (size_t i = 0; i < span; ++i) { dst[i].data = SIMD_MUL(src_a_l[i].data, src_a_r.data); }
+}
+template <int span>
+inline void simd_mul(AVX_Data* dst, AVX_Data* src_a_l, AVX_Data* src_a_r)
+{
+#pragma unroll
+    for (size_t i = 0; i < span; ++i) { dst[i].data = SIMD_MUL(src_a_l[i].data, src_a_r[i].data); }
+}
+template <int span>
+inline void simd_div(AVX_Data* dst, AVX_Data* src_a_l, AVX_Data* src_a_r)
+{
+#pragma unroll
+    for (size_t i = 0; i < span; ++i) { dst[i].data = SIMD_DIV(src_a_l[i].data, src_a_r[i].data); }
+}
+template <int span>
+inline void simd_and(AVX_Data* dst, AVX_Data* src_a_l, AVX_Data src_a_r)
+{
+#pragma unroll
+    for (size_t i = 0; i < span; ++i) { dst[i].data = SIMD_AND(src_a_l[i].data, src_a_r.data); }
+}
+template <int span>
+inline void simd_and(AVX_Data* dst, AVX_Data* src_a_l, AVX_Data* src_a_r)
+{
+#pragma unroll
+    for (size_t i = 0; i < span; ++i) { dst[i].data = SIMD_AND(src_a_l[i].data, src_a_r[i].data); }
+}
+template <int span>
+inline void simd_andnot(AVX_Data* dst, AVX_Data* src_a_l, AVX_Data src_a_r)
+{
+#pragma unroll
+    for (size_t i = 0; i < span; ++i) { dst[i].data = SIMD_ANDNOT(src_a_l[i].data, src_a_r.data); }
+}
+template <int span>
+inline void simd_andnot(AVX_Data* dst, AVX_Data* src_a_l, AVX_Data* src_a_r)
+{
+#pragma unroll
+    for (size_t i = 0; i < span; ++i) {
+        dst[i].data = SIMD_ANDNOT(src_a_l[i].data, src_a_r[i].data);
+    }
+}
+template <int span>
+inline void simd_or(AVX_Data* dst, AVX_Data* src_a_l, AVX_Data src_a_r)
+{
+#pragma unroll
+    for (size_t i = 0; i < span; ++i) { dst[i].data = SIMD_OR(src_a_l[i].data, src_a_r.data); }
+}
+template <int span>
+inline void simd_or(AVX_Data* dst, AVX_Data* src_a_l, AVX_Data* src_a_r)
+{
+#pragma unroll
+    for (size_t i = 0; i < span; ++i) { dst[i].data = SIMD_OR(src_a_l[i].data, src_a_r[i].data); }
+}
+template <int span>
+inline void simd_xor(AVX_Data* dst, AVX_Data* src_a_l, AVX_Data src_a_r)
+{
+#pragma unroll
+    for (size_t i = 0; i < span; ++i) { dst[i].data = SIMD_XOR(src_a_l[i].data, src_a_r.data); }
+}
+template <int span>
+inline void simd_xor(AVX_Data* dst, AVX_Data* src_a_l, AVX_Data* src_a_r)
+{
+#pragma unroll
+    for (size_t i = 0; i < span; ++i) { dst[i].data = SIMD_XOR(src_a_l[i].data, src_a_r[i].data); }
+}
+
+#endif
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/softmax.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/softmax.hpp
deleted file mode 100644
index 9ca9810..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/includes/softmax.hpp
+++ /dev/null
@@ -1,63 +0,0 @@
-#pragma once
-
-#include <stdio.h>
-#if __has_include(<sycl/sycl.hpp>)
-#include <sycl/sycl.hpp>
-#elif __has_include(<CL/sycl.hpp>)
-#include <CL/sycl.hpp>
-#else
-#error "Unsupported compiler"
-#endif
-#include "custom_sycl_layers.hpp"
-
-#include <fstream>
-
-template <typename T>
-class Softmax {
-public:
-    struct Config {
-        size_t batchSize;
-        size_t heads;
-        size_t seq_length;
-        size_t prob_depth;
-        float temprature;
-        bool mem_alloc;
-        Config(size_t batch, size_t h, size_t seq, int prob_size = 0, bool mem_alloc = false)
-            : batchSize(batch),
-              heads(h),
-              seq_length(seq),
-              prob_depth(prob_size),
-              temprature(1.0),
-              mem_alloc(mem_alloc)
-        {
-        }
-    };
-
-    Softmax(Config config) : config_(config) {}
-
-    ~Softmax() {}
-
-    void Forward(int bsz, T* vals, const T* attn_mask, sycl::queue* stream)
-    {
-        launch_attn_softmax<T>(vals, attn_mask, bsz, config_.heads, config_.seq_length, stream);
-    }
-
-    void Backward(int bsz, T* out_grad, const T* soft_out, sycl::queue* stream)
-    {
-        launch_attn_softmax_backward_v2<T>(
-            out_grad, soft_out, bsz, config_.heads, config_.seq_length, stream);
-    }
-
-    inline size_t GetProbDepth() const { return config_.prob_depth; }
-
-    inline size_t GetBatchSize() const { return config_.batchSize; }
-
-    inline size_t GetNumHeads() const { return config_.heads; }
-
-    inline size_t GetSeqLength() const { return config_.seq_length; }
-
-    inline void SetSeqLength(size_t seq_len) { config_.seq_length = seq_len; }
-
-private:
-    Config config_;
-};
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/strided_batch_gemm.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/strided_batch_gemm.hpp
deleted file mode 100644
index 2e0ce64..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/includes/strided_batch_gemm.hpp
+++ /dev/null
@@ -1,274 +0,0 @@
-#pragma once
-
-#include <stdio.h>
-#if __has_include(<sycl/sycl.hpp>)
-#include <sycl/sycl.hpp>
-#elif __has_include(<CL/sycl.hpp>)
-#include <CL/sycl.hpp>
-#else
-#error "Unsupported compiler"
-#endif
-#include "context.hpp"
-#include "onednn_wrappers.hpp"
-#include "onemkl_wrappers.hpp"
-
-template <typename T>
-class StridedBatchGemm {
-public:
-    struct Config {
-        int batch_size;
-        int m;
-        int n;
-        int k;
-        float alpha;
-        float beta;
-        oneapi::mkl::transpose op_A;
-        oneapi::mkl::transpose op_B;
-        std::array<int, 3> gemm_algos;
-
-        Config(int batch,
-               int mm,
-               int nn,
-               int kk,
-               float param_alpha,
-               float param_beta,
-               oneapi::mkl::transpose opA,
-               oneapi::mkl::transpose opB,
-               const std::array<int, 3>& algos)
-            : batch_size(batch),
-              m(mm),
-              n(nn),
-              k(kk),
-              alpha(param_alpha),
-              beta(param_beta),
-              op_A(opA),
-              op_B(opB),
-              gemm_algos(algos)
-        {
-        }
-        void SetConfig(int mm, int nn, int kk)
-        {
-            m = mm;
-            n = nn;
-            k = kk;
-        }
-    };
-
-    StridedBatchGemm(const Config& config) : _config(config)
-    {
-        k_buf = NULL;
-        q_buf = NULL;
-    }
-
-    virtual ~StridedBatchGemm() {}
-
-    void Forward(int bsz, T* output, const T* _buffer_a, const T* _buffer_b, sycl::queue* handle)
-    {
-        int stride_a = _config.m * _config.k;
-        int stride_b = _config.n * _config.k;
-        int stride_c = _config.m * _config.n;
-
-        if constexpr (std::is_same_v<T, bf16>) {
-            onednn_batchgemm(handle,
-                             _config.n,
-                             _config.m,
-                             _config.k,
-                             _config.alpha,
-                             _config.beta,
-                             _buffer_b,
-                             _buffer_a,
-                             output,
-                             _config.op_B == oneapi::mkl::transpose::trans,
-                             _config.op_A == oneapi::mkl::transpose::trans,
-                             bsz);
-        } else {
-            onemkl_strided_batched_gemm(handle,
-                                        _config.m,
-                                        _config.n,
-                                        _config.k,
-                                        (T)_config.alpha,
-                                        (T)_config.beta,
-                                        _buffer_a,
-                                        _buffer_b,
-                                        output,
-                                        _config.op_A,
-                                        _config.op_B,
-                                        stride_a,
-                                        stride_b,
-                                        stride_c,
-                                        bsz,
-                                        int(_config.gemm_algos[0]));
-        }
-    }
-
-    void ForwardPlusSave(T* output, const T* _buffer_a, const T* _buffer_b, sycl::queue* handle)
-    {
-        int stride_a = _config.m * _config.k;
-        int stride_b = _config.n * _config.k;
-        int stride_c = _config.m * _config.n;
-
-        if constexpr (std::is_same_v<T, bf16>) {
-            throw std::runtime_error("Unsupport bf16 strided batch gemm");
-        } else {
-            onemkl_strided_batched_gemm(handle,
-                                        _config.m,
-                                        _config.n,
-                                        _config.k,
-                                        (T)_config.alpha,
-                                        (T)_config.beta,
-                                        _buffer_a,
-                                        _buffer_b,
-                                        output,
-                                        _config.op_A,
-                                        _config.op_B,
-                                        stride_a,
-                                        stride_b,
-                                        stride_c,
-                                        _config.batch_size,
-                                        int(_config.gemm_algos[0]));
-        }
-
-        k_buf = _buffer_a;
-        q_buf = _buffer_b;
-    }
-
-    void Backward(int bsz,
-                  const T* d_output,
-                  const T* _buffer_a,
-                  const T* _buffer_b,
-                  sycl::queue* handle,
-                  T* inpGradA = nullptr,
-                  T* inpGradB = nullptr)
-    {
-        if constexpr (std::is_same_v<T, bf16>) {
-            // calculate d_A
-            if (_config.op_A == oneapi::mkl::transpose::trans) {
-                onednn_batchgemm(handle,
-                                 _config.m,
-                                 _config.k,
-                                 _config.n,
-                                 _config.alpha,
-                                 _config.beta,
-                                 d_output,
-                                 _buffer_b,
-                                 inpGradA,
-                                 true,
-                                 false,
-                                 bsz);
-
-                // Calculate d_B.
-                onednn_batchgemm(handle,
-                                 _config.n,
-                                 _config.k,
-                                 _config.m,
-                                 _config.alpha,
-                                 _config.beta,
-                                 d_output,
-                                 _buffer_a,
-                                 inpGradB,
-                                 false,
-                                 false,
-                                 bsz);
-            } else {
-                onednn_batchgemm(handle,
-                                 _config.n,
-                                 _config.m,
-                                 _config.k,
-                                 _config.alpha,
-                                 _config.beta,
-                                 _buffer_b,
-                                 d_output,
-                                 inpGradA,
-                                 true,
-                                 false,
-                                 bsz);
-
-                // Calculate d_B.
-                onednn_batchgemm(handle,
-                                 _config.n,
-                                 _config.k,
-                                 _config.m,
-                                 _config.alpha,
-                                 _config.beta,
-                                 d_output,
-                                 _buffer_a,
-                                 inpGradB,
-                                 false,
-                                 true,
-                                 bsz);
-            }
-
-        } else {
-            int mb = (_config.op_A == oneapi::mkl::transpose::trans ? _config.k : _config.m);
-            int kb = (_config.op_A == oneapi::mkl::transpose::trans ? _config.m : _config.k);
-
-            int stride_a = mb * _config.n;
-            int stride_b = _config.n * kb;
-            int stride_c = _config.m * _config.k;
-
-            // B need to transpose.
-            oneapi::mkl::transpose op_b =
-                (_config.op_B == oneapi::mkl::transpose::trans ? oneapi::mkl::transpose::nontrans
-                                                               : oneapi::mkl::transpose::trans);
-
-            // calculate d_A
-            onemkl_strided_batched_gemm(
-                handle,
-                mb,
-                kb,
-                _config.n,
-                (T)_config.alpha,
-                (T)_config.beta,
-                (_config.op_A == oneapi::mkl::transpose::trans ? _buffer_b : d_output),
-                (_config.op_A == oneapi::mkl::transpose::trans ? d_output : _buffer_b),
-                inpGradA,
-                oneapi::mkl::transpose::nontrans,
-                op_b,
-                stride_a,
-                stride_b,
-                stride_c,
-                bsz,
-                int(_config.gemm_algos[1]));
-
-            // A need to transpose.
-            oneapi::mkl::transpose op_a =
-                (_config.op_A == oneapi::mkl::transpose::trans ? oneapi::mkl::transpose::nontrans
-                                                               : oneapi::mkl::transpose::trans);
-
-            stride_a = _config.m * _config.k;
-            stride_b = _config.m * _config.n;
-            stride_c = _config.n * _config.k;
-
-            // Calculate d_B.
-            onemkl_strided_batched_gemm(handle,
-                                        _config.k,
-                                        _config.n,
-                                        _config.m,
-                                        (T)_config.alpha,
-                                        (T)_config.beta,
-                                        _buffer_a,
-                                        d_output,
-                                        inpGradB,
-                                        op_a,
-                                        oneapi::mkl::transpose::nontrans,
-                                        stride_a,
-                                        stride_b,
-                                        stride_c,
-                                        bsz,
-                                        int(_config.gemm_algos[2]));
-        }
-    }
-
-    inline int GetN() const { return _config.k; }
-
-    inline const T* GetBufferA() const { return k_buf; }
-
-    inline const T* GetBufferB() const { return q_buf; }
-
-    inline void SetConfig(int m, int n, int k) { _config.SetConfig(m, n, k); }
-
-private:
-    Config _config;
-    const T* q_buf;
-    const T* k_buf;
-};
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/includes/type_shim.hpp b/intel_extension_for_deepspeed/op_builder/csrc/includes/type_shim.h
similarity index 66%
rename from intel_extension_for_deepspeed/op_builder/csrc/includes/type_shim.hpp
rename to intel_extension_for_deepspeed/op_builder/csrc/includes/type_shim.h
index 8dcf720..e8a9e01 100644
--- a/intel_extension_for_deepspeed/op_builder/csrc/includes/type_shim.hpp
+++ b/intel_extension_for_deepspeed/op_builder/csrc/includes/type_shim.h
@@ -1,14 +1,14 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
 /* Taken from NVIDIA/apex commit 855808f3fc268e9715d613f3c2e56469d8c986d8 */
-#include <ATen/ATen.h>
-#if __has_include(<sycl/sycl.hpp>)
 #include <sycl/sycl.hpp>
-#elif __has_include(<CL/sycl.hpp>)
-#include <CL/sycl.hpp>
-#else
-#error "Unsupported compiler"
-#endif
+#include <dpct/dpct.hpp>
+#include <ATen/ATen.h>
 
-// Forward/backward compatiblity hack around
+// Forward/backward compatibility hack around
 // https://github.com/pytorch/pytorch/commit/3aeb78079bcd68282fe9117088e138b77318e288
 // pending more future-proof guidance from upstream.
 // struct TypeShim
@@ -17,7 +17,7 @@
 //   TypeShim(const at::Type& type) : payload(type) {}
 //   // Enable trivial conversion to a const at::Type& for pre-3aeb78
 //   operator const at::Type&(){ return payload; };
-//   // Enable dispatch switch statements to take *this directly for post-3aeb78
+//   // Enable dispatch switch statements to take *this directly for  post-3aeb78
 //   //operator at::ScalarType(){ return payload.; };
 // };
 
@@ -33,6 +33,11 @@
             __VA_ARGS__;                                                         \
             break;                                                               \
         }                                                                        \
+        case at::ScalarType::BFloat16: {                                         \
+            using scalar_t_##LEVEL = at::BFloat16;                               \
+            __VA_ARGS__;                                                         \
+            break;                                                               \
+        }                                                                        \
         default: AT_ERROR(#NAME, " not implemented for '", toString(TYPE), "'"); \
     }
 
@@ -53,6 +58,11 @@
             __VA_ARGS__;                                                         \
             break;                                                               \
         }                                                                        \
+        case at::ScalarType::BFloat16: {                                         \
+            using scalar_t_##LEVEL = at::BFloat16;                               \
+            __VA_ARGS__;                                                         \
+            break;                                                               \
+        }                                                                        \
         default: AT_ERROR(#NAME, " not implemented for '", toString(TYPE), "'"); \
     }
 
@@ -72,25 +82,43 @@
     }
 
 template <typename T>
-T reduce_block_into_lanes(T* x,
-                          T val,
-                          sycl::nd_item<3> item_ct1,
-                          int lanes = 1,
-                          bool share_result = false)  // lanes is intended to be <= 32.
+__dpct_inline__ T
+reduce_block_into_lanes(T* x,
+                        T val,
+                        int lanes = 1,
+                        bool share_result = false)  // lanes is intended to be <= 32.
 {
-    int tid =
-        item_ct1.get_local_id(2) + item_ct1.get_local_id(1) * item_ct1.get_local_range().get(2);
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    int tid = item_ct1.get_local_id(2) + item_ct1.get_local_id(1) * item_ct1.get_local_range(2);
     int blockSize = item_ct1.get_local_range(2) *
                     item_ct1.get_local_range(1);  // blockSize is intended to be a multiple of 32.
 
     if (blockSize >= 64) {
         x[tid] = val;
+        /*
+        DPCT1118:1: SYCL group functions and algorithms must be encountered in converged control
+        flow. You may need to adjust the code.
+        */
+        /*
+        DPCT1065:6: Consider replacing sycl::nd_item::barrier() with
+        sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if
+        there is no access to global memory.
+        */
         item_ct1.barrier();
     }
 
 #pragma unroll
     for (int i = (blockSize >> 1); i >= 64; i >>= 1) {
         if (tid < i) x[tid] = x[tid] + x[tid + i];
+        /*
+        DPCT1118:2: SYCL group functions and algorithms must be encountered in converged control
+        flow. You may need to adjust the code.
+        */
+        /*
+        DPCT1065:7: Consider replacing sycl::nd_item::barrier() with
+        sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if
+        there is no access to global memory.
+        */
         item_ct1.barrier();
     }
 
@@ -111,6 +139,15 @@ T reduce_block_into_lanes(T* x,
     if (share_result) {
         if (tid < lanes) x[tid] = final;  // EpilogueOp
         // Make sure the smem result is visible to all warps.
+        /*
+        DPCT1118:3: SYCL group functions and algorithms must be encountered in converged control
+        flow. You may need to adjust the code.
+        */
+        /*
+        DPCT1065:8: Consider replacing sycl::nd_item::barrier() with
+        sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if
+        there is no access to global memory.
+        */
         item_ct1.barrier();
     }
 
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/quantization/dequantize.dp.cpp b/intel_extension_for_deepspeed/op_builder/csrc/quantization/dequantize.dp.cpp
new file mode 100644
index 0000000..1b9a116
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/quantization/dequantize.dp.cpp
@@ -0,0 +1,91 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include "dequantization_utils.h"
+#include "memory_access_utils.h"
+
+template <typename T, int numBits, dequantize::Type qType, int unroll, int threads>
+void dequantize_kernel(T* __restrict__ dequant_data,
+                                  const int8_t* __restrict__ q_data,
+                                  const float* __restrict__ q_params,
+                                  int elems_per_group,
+                                  int total_elems)
+{
+    dequantize::to_global<T, numBits, qType, unroll, threads>(
+        dequant_data, q_data, q_params, elems_per_group, total_elems);
+}
+
+/*
+DPCT1049:47: The work-group size passed to the SYCL kernel may exceed the limit. To get the device
+limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
+*/
+#define LAUNCH_DEQUANT_KERNEL(num_bits, q_type)                                                    \
+  {                                                                                                \
+    dpct::has_capability_or_fail(stream->get_device(), {sycl::aspect::fp64, sycl::aspect::fp16});  \
+    stream->submit([&](sycl::handler& cgh) {                                                       \
+      T* dequant_data_ct0 = dequant_data;                                                          \
+      const int8_t* q_data_ct1 = q_data;                                                           \
+      const float* q_params_ct2 = q_params;                                                        \
+      auto elems_per_group_ct3 = elems_per_group;                                                  \
+      auto total_elems_ct4 = total_elems;                                                          \
+                                                                                                   \
+      cgh.parallel_for(                                                                            \
+          sycl::nd_range<3>(grid * block, block),                                                  \
+          [=](sycl::nd_item<3> item_ct1) [[intel::reqd_sub_group_size(32)]] {                      \
+            dequantize_kernel<T, num_bits, q_type, unroll, threads>(                               \
+                dequant_data_ct0, q_data_ct1, q_params_ct2, elems_per_group_ct3, total_elems_ct4); \
+          });                                                                                      \
+    });                                                                                            \
+  }
+
+template <typename T>
+void launch_dequantize_kernel(T* dequant_data,
+                              const int8_t* q_data,
+                              const float* q_params,
+                              quantize::Type q_type,
+                              int num_bits,
+                              int elems_per_group,
+                              int total_elems,
+                              dpct::queue_ptr stream)
+{
+    constexpr int unroll = 8;
+    constexpr int threads = 512;
+    constexpr int elems_per_block = unroll * threads * dequantize::granularity / (sizeof(T));
+
+    const sycl::range<3> block(1, 1, threads);
+    const sycl::range<3> grid(1, 1, (total_elems + elems_per_block - 1) / elems_per_block);
+
+    // TODO(cmikeh2): It may make sense to tune unroll, there is perf benefit for large
+    // problem sizes with this large unroll value.
+    if (num_bits == 8 && q_type == quantize::Type::Symmetric) {
+        LAUNCH_DEQUANT_KERNEL(8, quantize::Type::Symmetric);
+    } else if (num_bits == 8 && q_type == quantize::Type::Asymmetric) {
+        LAUNCH_DEQUANT_KERNEL(8, quantize::Type::Asymmetric);
+    } else if (num_bits == 4 && q_type == quantize::Type::Symmetric) {
+        LAUNCH_DEQUANT_KERNEL(4, quantize::Type::Symmetric);
+    } else if (num_bits == 4 && q_type == quantize::Type::Asymmetric) {
+        LAUNCH_DEQUANT_KERNEL(4, quantize::Type::Asymmetric);
+    }
+}
+
+template void launch_dequantize_kernel(sycl::half* dequant_data,
+                                       const int8_t* q_data,
+                                       const float* q_params,
+                                       quantize::Type q_type,
+                                       int num_bits,
+                                       int elems_per_group,
+                                       int total_elems,
+                                       dpct::queue_ptr stream);
+
+template void launch_dequantize_kernel(float* dequant_data,
+                                       const int8_t* q_data,
+                                       const float* q_params,
+                                       quantize::Type q_type,
+                                       int num_bits,
+                                       int elems_per_group,
+                                       int total_elems,
+                                       dpct::queue_ptr stream);
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/quantization/fake_quantizer.dp.cpp b/intel_extension_for_deepspeed/op_builder/csrc/quantization/fake_quantizer.dp.cpp
new file mode 100644
index 0000000..ddcafd9
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/quantization/fake_quantizer.dp.cpp
@@ -0,0 +1,1121 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include <math.h>
+#include "custom_cuda_layers.h"
+#include "memory_access_utils.h"
+
+void fake_quantize_kernel(sycl::half* vals, int group_size, int num_bits)
+{
+#if DPCT_COMPATIBILITY_TEMP >= 700 || defined(__HIP_PLATFORM_AMD__)
+
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    sycl::group<3> b = sycl::ext::oneapi::experimental::this_group<3>();
+    sycl::sub_group g = sycl::ext::oneapi::experimental::this_sub_group();
+
+    int gid = threadIdx.x >> 5;
+    int lane = threadIdx.x & 0x1f;
+    int warp_num = blockDim.x >> 5;
+    int id = threadIdx.x;
+
+    constexpr int granularity = 16;
+    constexpr int vals_per_access = granularity / sizeof(sycl::half);
+
+    sycl::half data[vals_per_access];
+
+    int group_id = blockIdx.x;
+
+    int thread_index = id * vals_per_access;
+    int reg_count = 0;
+    int offset = group_id * group_size;
+    float max = -10000.0;
+    for (int thread_index = id * vals_per_access; thread_index < group_size;
+         thread_index += blockDim.x * vals_per_access) {
+        mem_access::load_global<granularity>(data, vals + offset + thread_index);
+
+#pragma unroll
+        for (int i = 0; i < vals_per_access; i++) {
+            if (abs((float)data[i]) > max) max = abs((float)data[i]);
+        }
+    }
+
+#pragma unroll
+    for (int i = 1; i < WARP_SIZE; i <<= 1) {
+        auto temp = g.shuffle_xor(max, i);
+        if (max < temp) max = temp;
+    }
+    __shared__ float partialMax[WARP_SIZE];
+
+    if (lane == 0) partialMax[gid] = max;
+
+    b.sync();
+
+    if (lane < warp_num) max = partialMax[lane];
+
+#pragma unroll
+    for (int i = 1; i < WARP_SIZE; i <<= 1) {
+        auto temp = g.shuffle_down(max, i);
+        if (max < temp) max = temp;
+    }
+
+    max = g.shuffle(max, 0);
+
+    float q_scale = (float)(1 << num_bits) / (2 * max + 1e-5);
+    float q_scale_inv = 1 / q_scale;
+    int q_range_max = (1 << (num_bits - 1)) - 1;
+    int q_range_min = -(1 << (num_bits - 1));
+
+    for (int thread_index = id * vals_per_access; thread_index < group_size;
+         thread_index += blockDim.x * vals_per_access) {
+        mem_access::load_global<granularity>(data, vals + offset + thread_index);
+#pragma unroll
+        for (int j = 0; j < vals_per_access; j++) {
+            float q_data;
+            q_data = sycl::half2float(data[j]);
+            q_data = __float2int_rn(q_data * q_scale);
+            q_data = q_data > (q_range_max) ? (q_range_max)
+                                            : (q_data < (q_range_min) ? (q_range_min) : q_data);
+            data[j] = __float2half_rn(q_data * q_scale_inv);
+        }
+        mem_access::store_global<granularity>(vals + offset + thread_index, data);
+    }
+
+#endif
+}
+
+void fake_quantize_kernel(float* vals, int group_size, int num_bits)
+{
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    sycl::group<3> b = sycl::ext::oneapi::experimental::this_group<3>();
+    auto g = sycl::ext::oneapi::experimental::this_sub_group();
+
+    int gid = item_ct1.get_local_id(2) >> 5;
+    int lane = item_ct1.get_local_id(2) & 0x1f;
+    int warp_num = item_ct1.get_local_range(2) >> 5;
+    int id = item_ct1.get_local_id(2);
+
+    constexpr int granularity = 16;
+    constexpr int vals_per_access = granularity / sizeof(float);
+
+    float data[vals_per_access];
+
+    int bid = item_ct1.get_group(2);
+
+    int thread_index = id * vals_per_access;
+
+    int reg_count = 0;
+
+    int offset = bid * group_size;
+
+    float max = -10000.0;
+
+    for (int thread_index = id * vals_per_access; thread_index < group_size;
+         thread_index += item_ct1.get_local_range(2) * vals_per_access) {
+        mem_access::load_global<granularity>(data, vals + offset + thread_index);
+
+#pragma unroll
+        for (int i = 0; i < vals_per_access; i++) {
+            if (sycl::fabs(data[i]) > max) max = sycl::fabs(data[i]);
+        }
+    }
+
+#pragma unroll
+    for (int i = 1; i < WARP_SIZE; i <<= 1) {
+        auto temp = g.shuffle_xor(max, i);
+        if (max < temp) max = temp;
+    }
+    auto& partialMax = *sycl::ext::oneapi::group_local_memory_for_overwrite<float[WARP_SIZE]>(
+        sycl::ext::oneapi::experimental::this_group<3>());
+
+    if (lane == 0) partialMax[gid] = max;
+
+    /*
+    DPCT1065:42: Consider replacing sycl::nd_item::barrier() with
+    sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there
+    is no access to global memory.
+    */
+    item_ct1.barrier();
+
+    if (lane < warp_num) max = partialMax[lane];
+
+    /*
+    DPCT1065:43: Consider replacing sycl::nd_item::barrier() with
+    sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there
+    is no access to global memory.
+    */
+    item_ct1.barrier();
+
+#pragma unroll
+    for (int i = 1; i < warp_num; i <<= 1) {
+        auto temp = g.shuffle_down(max, i);
+        if (max < temp) max = temp;
+    }
+
+    max = g.shuffle(max, 0);
+
+    float q_scale = (1 << num_bits) / (2 * max + 1e-5);
+    float q_scale_inv = 1 / q_scale;
+
+    int q_range_max = (1 << (num_bits - 1)) - 1;
+    int q_range_min = -(1 << (num_bits - 1));
+
+    for (int thread_index = id * vals_per_access; thread_index < group_size;
+         thread_index += item_ct1.get_local_range(2) * vals_per_access) {
+        mem_access::load_global<granularity>(data, vals + offset + thread_index);
+#pragma unroll
+        for (int j = 0; j < vals_per_access; j++) {
+            float q_data;
+            q_data = sycl::vec<float, 1>{(data[j] * q_scale)}
+                         .convert<int, sycl::rounding_mode::rte>()[0];
+            q_data = q_data > (q_range_max) ? (q_range_max)
+                                            : (q_data < (q_range_min) ? (q_range_min) : q_data);
+            data[j] = sycl::round(q_data * q_scale_inv);
+        }
+        mem_access::store_global<granularity>(vals + offset + thread_index, data);
+    }
+}
+
+template <typename T>
+void launch_fake_quantize_kernel(T* vals,
+                                 int total_count,
+                                 int group_num,
+                                 int num_bits,
+                                 dpct::queue_ptr stream)
+{
+    sycl::range<3> grid_dim(1, 1, group_num);
+    sycl::range<3> block_dim(1, 1, 1024);
+
+    /*
+    DPCT1049:44: The work-group size passed to the SYCL kernel may exceed the limit. To get the
+    device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
+    */
+    {
+        dpct::has_capability_or_fail(stream->get_device(), {sycl::aspect::fp16});
+        stream->parallel_for(sycl::nd_range<3>(grid_dim * block_dim, block_dim),
+                             [=](sycl::nd_item<3> item_ct1) {
+                                 fake_quantize_kernel(vals, total_count / group_num, num_bits);
+                             });
+    }
+}
+
+template void launch_fake_quantize_kernel(float* vals,
+                                          int total_count,
+                                          int group_num,
+                                          int num_bits,
+                                          dpct::queue_ptr stream);
+template void launch_fake_quantize_kernel(sycl::half* vals,
+                                          int total_count,
+                                          int group_num,
+                                          int num_bits,
+                                          dpct::queue_ptr stream);
+
+void sr_fake_quantize_kernel(sycl::half* vals,
+                             int token_size,
+                             int token_num,
+                             int num_bits,
+                             std::pair<uint64_t, uint64_t> seed)
+{
+#if DPCT_COMPATIBILITY_TEMP >= 700 || defined(__HIP_PLATFORM_AMD__)
+
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    sycl::group<3> b = sycl::ext::oneapi::experimental::this_group<3>();
+    sycl::sub_group g = sycl::ext::oneapi::experimental::this_sub_group();
+
+    int gid = threadIdx.x >> 5;
+    int lane = threadIdx.x & 0x1f;
+    int warp_num = blockDim.x >> 5;
+
+    int idx = blockIdx.x * blockDim.x + threadIdx.x;
+
+    float2* vals_cast = reinterpret_cast<float2*>(vals);
+
+    sycl::half2 data_low[128];
+    sycl::half2 data_high[128];
+
+    int bid = blockIdx.x;
+
+    /* curandStatePhilox4_32_10_t state; */
+    /* curand_init(seed.first, idx, seed.second, &state); */
+    dpct::rng::device::rng_generator<oneapi::mkl::rng::device::philox4x32x10<1>> state;
+    state = dpct::rng::device::rng_generator<oneapi::mkl::rng::device::philox4x32x10<1>>(seed.first, {seed.second, (unsigned long)idx * 4});
+    
+    unsigned int tid = threadIdx.x;
+    int reg_count = 0;
+    int offset = bid * token_size;
+    int group_index = bid * token_size + tid;
+
+    int total_count = token_size * token_num;
+    if (group_index < total_count) {
+        // float min = 10000.0;
+        float max = -10000.0;
+        while (tid < token_size) {
+            float2 data = vals_cast[offset + tid];
+            sycl::half2* data_h = reinterpret_cast<sycl::half2*>(&data);
+            data_low[reg_count] = data_h[0];
+            data_high[reg_count] = data_h[1];
+
+            float2 data_f[2];
+            data_f[0] = sycl::half22float2(data_h[0]);
+            data_f[1] = sycl::half22float2(data_h[1]);
+
+            if (abs((float)data_f[0].x) > max) max = abs((float)data_f[0].x);
+            if (abs((float)data_f[0].y) > max) max = abs((float)data_f[0].y);
+            if (abs((float)data_f[1].x) > max) max = abs((float)data_f[1].x);
+            if (abs((float)data_f[1].y) > max) max = abs((float)data_f[1].y);
+
+            tid += blockDim.x;
+            reg_count++;
+        }
+
+#pragma unroll
+        for (int i = 1; i < WARP_SIZE; i <<= 1) {
+            auto temp = g.shuffle_xor(max, i);
+            if (max < temp) max = temp;
+        }
+
+        __shared__ float partialMax[WARP_SIZE];
+
+        if (lane == 0) partialMax[gid] = max;
+
+        b.sync();
+
+        if (lane < warp_num) max = partialMax[lane];
+
+#pragma unroll
+        for (int i = 1; i < warp_num; i <<= 1) {
+            auto temp = g.shuffle_down(max, i);
+            if (max < temp) max = temp;
+        }
+
+        max = g.shuffle(max, 0);
+
+        float q_scale_val = (float)(1 << num_bits) / (max * 2 + 1e-5);
+        float high_q = (float)((1 << (num_bits - 1)) - 1);
+        float low_q = (float)(-((1 << (num_bits - 1))));
+
+        for (int i = 0; i < reg_count; i++) {
+            int token_index = i * blockDim.x + threadIdx.x;
+            if (token_index < token_size) {
+                float2 data_f[2];
+                data_f[0] = sycl::half22float2(data_low[i]);
+                data_f[1] = sycl::half22float2(data_high[i]);
+
+                float2 q_data_int[2];
+                q_data_int[0].x = (float)((int)(data_f[0].x * q_scale_val));
+                q_data_int[0].y = (float)((int)(data_f[0].y * q_scale_val));
+                q_data_int[1].x = (float)((int)(data_f[1].x * q_scale_val));
+                q_data_int[1].y = (float)((int)(data_f[1].y * q_scale_val));
+
+                // Stochastic rounding
+                sycl::float4 rand = state.generate<oneapi::mkl::rng::device::uniform<float>, 4>();
+
+                float q_error[4];
+                q_error[0] = abs(data_f[0].x - (q_data_int[0].x / q_scale_val)) * q_scale_val;
+                q_error[1] = abs(data_f[0].y - (q_data_int[0].y / q_scale_val)) * q_scale_val;
+                q_error[2] = abs(data_f[1].x - (q_data_int[1].x / q_scale_val)) * q_scale_val;
+                q_error[3] = abs(data_f[1].y - (q_data_int[1].y / q_scale_val)) * q_scale_val;
+
+                q_data_int[0].x =
+                    (rand.x < q_error[0] && q_data_int[0].x > low_q && q_data_int[0].x < high_q)
+                        ? (q_data_int[0].x + (data_f[0].x > 0 ? 1 : -1))
+                        : q_data_int[0].x;
+                q_data_int[0].y =
+                    (rand.y < q_error[1] && q_data_int[0].y > low_q && q_data_int[0].y < high_q)
+                        ? (q_data_int[0].y + (data_f[0].y > 0 ? 1 : -1))
+                        : q_data_int[0].y;
+                q_data_int[1].x =
+                    (rand.w < q_error[2] && q_data_int[1].x > low_q && q_data_int[1].x < high_q)
+                        ? (q_data_int[1].x + (data_f[1].x > 0 ? 1 : -1))
+                        : q_data_int[1].x;
+                q_data_int[1].y =
+                    (rand.z < q_error[3] && q_data_int[1].y > low_q && q_data_int[1].y < high_q)
+                        ? (q_data_int[1].y + (data_f[1].y > 0 ? 1 : -1))
+                        : q_data_int[1].y;
+
+                data_f[0].x = q_data_int[0].x / q_scale_val;
+                data_f[0].y = q_data_int[0].y / q_scale_val;
+                data_f[1].x = q_data_int[1].x / q_scale_val;
+                data_f[1].y = q_data_int[1].y / q_scale_val;
+
+                float2 result;
+                sycl::half2* result_h = reinterpret_cast<sycl::half2*>(&result);
+                result_h[0] = __float22half2_rn(data_f[0]);
+                result_h[1] = __float22half2_rn(data_f[1]);
+
+                vals_cast[offset + token_index] = result;
+            }
+        }
+    }
+#endif
+}
+
+void sr_fake_quantize_kernel(float* vals,
+                                        int token_size,
+                                        int token_num,
+                                        int num_bits,
+                                        std::pair<uint64_t, uint64_t> seed)
+{
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    sycl::group<3> b = sycl::ext::oneapi::experimental::this_group<3>();
+    sycl::sub_group g = sycl::ext::oneapi::experimental::this_sub_group();
+
+    int gid = item_ct1.get_local_id(2) >> 5;
+    int lane = item_ct1.get_local_id(2) & 0x1f;
+    int warp_num = item_ct1.get_local_range(2) >> 5;
+    int id = item_ct1.get_local_id(2);
+
+    int idx = item_ct1.get_group(2) * item_ct1.get_local_range(2) + id;
+
+    sycl::float4* vals_cast = reinterpret_cast<sycl::float4*>(vals);
+
+    sycl::float4 data[128];
+
+    int bid = item_ct1.get_group(2);
+    int tid = item_ct1.get_local_id(2);
+    dpct::rng::device::rng_generator<oneapi::mkl::rng::device::philox4x32x10<1>> state;
+    /* curand_init(seed.first, idx, seed.second, &state); */
+    state = dpct::rng::device::rng_generator<oneapi::mkl::rng::device::philox4x32x10<1>>(seed.first, {(unsigned long)idx, seed.second * 4});
+
+    int group_index = bid * token_size + item_ct1.get_local_id(2);
+    int reg_count = 0;
+    int total_count = token_size * token_num;
+    if (group_index < total_count) {
+        // float min = 10000.0;
+        float max = -10000.0;
+
+        while (tid < token_size) {
+            data[reg_count] = vals_cast[group_index];
+
+            if (sycl::fabs(data[reg_count].x()) > max) max = sycl::fabs(data[reg_count].x());
+            if (sycl::fabs(data[reg_count].y()) > max) max = sycl::fabs(data[reg_count].y());
+            if (sycl::fabs(data[reg_count].z()) > max) max = sycl::fabs(data[reg_count].z());
+            if (sycl::fabs(data[reg_count].w()) > max) max = sycl::fabs(data[reg_count].w());
+
+            group_index += item_ct1.get_local_range(2);
+            tid += item_ct1.get_local_range(2);
+            reg_count++;
+        }
+
+#pragma unroll
+        for (int i = 1; i < WARP_SIZE; i <<= 1) {
+            auto temp = g.shuffle_xor(max, i);
+            if (max < temp) max = temp;
+        }
+        auto& partialMax = *sycl::ext::oneapi::group_local_memory_for_overwrite<float[WARP_SIZE]>(
+            sycl::ext::oneapi::experimental::this_group<3>());
+
+        if (lane == 0) partialMax[gid] = max;
+
+        /*
+        DPCT1065:45: Consider replacing sycl::nd_item::barrier() with
+        sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if
+        there is no access to global memory.
+        */
+        item_ct1.barrier();
+
+        if (lane < warp_num) max = partialMax[lane];
+
+#pragma unroll
+        for (int i = 1; i < warp_num; i <<= 1) {
+            auto temp = g.shuffle_down(max, i);
+            if (max < temp) max = temp;
+        }
+
+        max = g.shuffle(max, 0);
+
+        float q_scale_val = (float)(1 << num_bits) / (max * 2 + 1e-5);
+        float high_q = (float)((1 << (num_bits - 1)) - 1);
+        float low_q = (float)(-((1 << (num_bits - 1))));
+
+        int offset = (bid)*token_size;
+        for (int i = 0; i < reg_count; i++) {
+            group_index = i * item_ct1.get_local_range(2) + item_ct1.get_local_id(2);
+            if (group_index < token_size) {
+                sycl::float4 q_data = data[i];
+
+                sycl::float4 q_data_int;
+                q_data_int.x() = (float)((int)(q_data.x() * q_scale_val));
+                q_data_int.y() = (float)((int)(q_data.y() * q_scale_val));
+                q_data_int.w() = (float)((int)(q_data.w() * q_scale_val));
+                q_data_int.z() = (float)((int)(q_data.z() * q_scale_val));
+
+                // Stochastic rounding
+                sycl::float4 rand = state.generate<oneapi::mkl::rng::device::uniform<float>, 4>();
+
+                float q_error[4];
+                q_error[0] = sycl::fabs(q_data.x() - (q_data_int.x() / q_scale_val)) * q_scale_val;
+                q_error[1] = sycl::fabs(q_data.y() - (q_data_int.y() / q_scale_val)) * q_scale_val;
+                q_error[2] = sycl::fabs(q_data.w() - (q_data_int.w() / q_scale_val)) * q_scale_val;
+                q_error[3] = sycl::fabs(q_data.z() - (q_data_int.z() / q_scale_val)) * q_scale_val;
+
+                q_data_int.x() =
+                    (rand.x() < q_error[0] && q_data_int.x() > low_q && q_data_int.x() < high_q)
+                        ? (q_data_int.x() + (q_data.x() > 0 ? 1 : -1))
+                        : q_data_int.x();
+                q_data_int.y() =
+                    (rand.y() < q_error[1] && q_data_int.y() > low_q && q_data_int.y() < high_q)
+                        ? (q_data_int.y() + (q_data.y() > 0 ? 1 : -1))
+                        : q_data_int.y();
+                q_data_int.w() =
+                    (rand.w() < q_error[2] && q_data_int.w() > low_q && q_data_int.w() < high_q)
+                        ? (q_data_int.w() + (q_data.w() > 0 ? 1 : -1))
+                        : q_data_int.w();
+                q_data_int.z() =
+                    (rand.z() < q_error[3] && q_data_int.z() > low_q && q_data_int.z() < high_q)
+                        ? (q_data_int.z() + (q_data.z() > 0 ? 1 : -1))
+                        : q_data_int.z();
+
+                q_data_int.x() /= q_scale_val;
+                q_data_int.y() /= q_scale_val;
+                q_data_int.w() /= q_scale_val;
+                q_data_int.z() /= q_scale_val;
+
+                vals_cast[group_index + offset] = q_data_int;
+            }
+        }
+    }
+}
+
+template <typename T>
+void launch_sr_fake_quantize_kernel(T* vals,
+                                    int total_count,
+                                    int group_num,
+                                    int num_bits,
+                                    dpct::queue_ptr stream)
+{
+    sycl::range<3> block_dim(1, 1, 1024);
+    sycl::range<3> grid_dim(1, 1, group_num);
+
+    uint64_t inc = total_count / grid_dim[2] / block_dim[2];
+    std::pair<uint64_t, uint64_t> seed = TrainingContext::Instance().IncrementOffset(inc);
+
+    /*
+    DPCT1049:46: The work-group size passed to the SYCL kernel may exceed the limit. To get the
+    device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
+    */
+    {
+        dpct::has_capability_or_fail(stream->get_device(), {sycl::aspect::fp16});
+        stream->parallel_for(
+            sycl::nd_range<3>(grid_dim * block_dim, block_dim), [=](sycl::nd_item<3> item_ct1) {
+                sr_fake_quantize_kernel(
+                    vals, (total_count / group_num) / 4, group_num, num_bits, seed);
+            });
+    }
+}
+template void launch_sr_fake_quantize_kernel(float* vals,
+                                             int total_count,
+                                             int group_num,
+                                             int num_bits,
+                                             dpct::queue_ptr stream);
+template void launch_sr_fake_quantize_kernel(sycl::half* vals,
+                                             int total_count,
+                                             int group_num,
+                                             int num_bits,
+                                             dpct::queue_ptr stream);
+
+void fake_quantize_kernel_asym(sycl::half* vals, int group_size, int num_bits)
+{
+#if DPCT_COMPATIBILITY_TEMP >= 700 || defined(__HIP_PLATFORM_AMD__)
+
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    sycl::group<3> b = sycl::ext::oneapi::experimental::this_group<3>();
+    sycl::sub_group g = sycl::ext::oneapi::experimental::this_sub_group();
+
+    int gid = threadIdx.x >> 5;
+    int lane = threadIdx.x & 0x1f;
+    int warp_num = blockDim.x >> 5;
+    int id = threadIdx.x;
+
+    float2* vals_cast = reinterpret_cast<float2*>(vals);
+
+    float2 data[MAX_REG];
+
+    int group_id = blockIdx.x;
+
+    {
+        int group_index = id;
+        int reg_count = 0;
+        int offset = group_id * group_size;
+        float max = -10000.0;
+        float min = 10000.0;
+
+        while (group_index < group_size && reg_count < MAX_REG) {
+            data[reg_count] = vals_cast[offset + group_index];
+            sycl::half* data_h = reinterpret_cast<sycl::half*>(&data[reg_count]);
+
+            if (((float)data_h[0]) > max) max = (float)data_h[0];
+            if (((float)data_h[1]) > max) max = (float)data_h[1];
+            if (((float)data_h[2]) > max) max = (float)data_h[2];
+            if (((float)data_h[3]) > max) max = (float)data_h[3];
+
+            if (((float)data_h[0]) < min) min = (float)data_h[0];
+            if (((float)data_h[1]) < min) min = (float)data_h[1];
+            if (((float)data_h[2]) < min) min = (float)data_h[2];
+            if (((float)data_h[3]) < min) min = (float)data_h[3];
+
+            group_index += blockDim.x;
+            reg_count++;
+        }
+
+#pragma unroll
+        for (int i = 1; i < WARP_SIZE; i <<= 1) {
+            auto temp = g.shuffle_xor(max, i);
+            if (max < temp) max = temp;
+        }
+
+#pragma unroll
+        for (int i = 1; i < WARP_SIZE; i <<= 1) {
+            auto temp = g.shuffle_xor(min, i);
+            if (min > temp) min = temp;
+        }
+
+        __shared__ float partialMax[WARP_SIZE];
+        __shared__ float partialMin[WARP_SIZE];
+
+        if (lane == 0) partialMax[gid] = max;
+        if (lane == 0) partialMin[gid] = min;
+
+        b.sync();
+
+        if (lane < warp_num) max = partialMax[lane];
+        if (lane < warp_num) min = partialMin[lane];
+
+#pragma unroll
+        for (int i = 1; i < warp_num; i <<= 1) {
+            auto temp = g.shuffle_down(max, i);
+            if (max < temp) max = temp;
+        }
+#pragma unroll
+        for (int i = 1; i < warp_num; i <<= 1) {
+            auto temp = g.shuffle_down(min, i);
+            if (min > temp) min = temp;
+        }
+
+        max = g.shuffle(max, 0);
+        min = g.shuffle(min, 0);
+
+        float q_scale = ((max - min) + 1e-5) / (float)(1 << num_bits);
+        float q_scale_inv = 1 / q_scale;
+
+        for (int i = 0; i < reg_count; i++) {
+            group_index = i * blockDim.x + id;
+            if (group_index < group_size) {
+                sycl::half2* data_h = reinterpret_cast<sycl::half2*>(&data[i]);
+                float2 q_data[2];
+                q_data[0] = sycl::half22float2(data_h[0]);
+                q_data[1] = sycl::half22float2(data_h[1]);
+
+                float2 q_data_int[2];
+
+                q_data_int[0].x = roundf((q_data[0].x - min) * q_scale_inv);
+                q_data_int[0].y = roundf((q_data[0].y - min) * q_scale_inv);
+                q_data_int[1].x = roundf((q_data[1].x - min) * q_scale_inv);
+                q_data_int[1].y = roundf((q_data[1].y - min) * q_scale_inv);
+
+                q_data_int[0].x = q_data_int[0].x * q_scale + min;
+                q_data_int[0].y = q_data_int[0].y * q_scale + min;
+                q_data_int[1].x = q_data_int[1].x * q_scale + min;
+                q_data_int[1].y = q_data_int[1].y * q_scale + min;
+
+                data_h[0] = __float22half2_rn(q_data_int[0]);
+                data_h[1] = __float22half2_rn(q_data_int[1]);
+
+                vals_cast[offset + group_index] = data[i];
+            }
+        }
+    }
+#endif
+}
+
+void fake_quantize_kernel_asym(float* vals, int group_size, int num_bits)
+{
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    sycl::group<3> b = sycl::ext::oneapi::experimental::this_group<3>();
+    sycl::sub_group g = sycl::ext::oneapi::experimental::this_sub_group();
+
+    int gid = item_ct1.get_local_id(2) >> 5;
+    int lane = item_ct1.get_local_id(2) & 0x1f;
+    int warp_num = item_ct1.get_local_range(2) >> 5;
+    int id = item_ct1.get_local_id(2);
+
+    sycl::float4* vals_cast = reinterpret_cast<sycl::float4*>(vals);
+
+    sycl::float4 data[MAX_REG];
+
+    int bid = item_ct1.get_group(2);
+
+    int group_index = bid * group_size + id;
+    int reg_count = 0;
+
+    float max = -10000.0;
+    float min = 10000.0;
+
+    while (id < group_size && reg_count < MAX_REG) {
+        sycl::float4 data_reg = vals_cast[group_index];
+        data[reg_count] = data_reg;
+
+        if (data_reg.x() > max) max = data_reg.x();
+        if (data_reg.y() > max) max = data_reg.y();
+        if (data_reg.w() > max) max = data_reg.w();
+        if (data_reg.z() > max) max = data_reg.z();
+
+        if (data_reg.x() < min) min = data_reg.x();
+        if (data_reg.y() < min) min = data_reg.y();
+        if (data_reg.w() < min) min = data_reg.w();
+        if (data_reg.z() < min) min = data_reg.z();
+
+        group_index += item_ct1.get_local_range(2);
+        id += item_ct1.get_local_range(2);
+        reg_count++;
+    }
+    id = item_ct1.get_local_id(2);
+
+#pragma unroll
+    for (int i = 1; i < WARP_SIZE; i <<= 1) {
+        auto temp = g.shuffle_xor(max, i);
+        if (max < temp) max = temp;
+    }
+
+#pragma unroll
+    for (int i = 1; i < WARP_SIZE; i <<= 1) {
+        auto temp = g.shuffle_xor(min, i);
+        if (min > temp) min = temp;
+    }
+
+    auto& partialMax = *sycl::ext::oneapi::group_local_memory_for_overwrite<float[WARP_SIZE]>(
+        sycl::ext::oneapi::experimental::this_group<3>());
+    auto& partialMin = *sycl::ext::oneapi::group_local_memory_for_overwrite<float[WARP_SIZE]>(
+        sycl::ext::oneapi::experimental::this_group<3>());
+
+    if (lane == 0) partialMax[gid] = max;
+    if (lane == 0) partialMin[gid] = min;
+
+    /*
+    DPCT1065:47: Consider replacing sycl::nd_item::barrier() with
+    sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there
+    is no access to global memory.
+    */
+    item_ct1.barrier();
+
+    if (lane < warp_num) max = partialMax[lane];
+    if (lane < warp_num) min = partialMin[lane];
+
+#pragma unroll
+    for (int i = 1; i < warp_num; i <<= 1) {
+        auto temp = g.shuffle_down(max, i);
+        if (max < temp) max = temp;
+    }
+#pragma unroll
+    for (int i = 1; i < warp_num; i <<= 1) {
+        auto temp = g.shuffle_down(min, i);
+        if (min > temp) min = temp;
+    }
+
+    max = g.shuffle(max, 0);
+    min = g.shuffle(min, 0);
+
+    float q_scale = ((max - min) + 1e-5) / (float)(1 << num_bits);
+    float q_scale_inv = 1 / q_scale;
+    for (int i = 0; i < reg_count; i++) {
+        group_index = i * item_ct1.get_local_range(2) + id;
+        if (group_index < group_size) {
+            sycl::float4 q_data;
+            q_data = data[i];
+
+            sycl::float4 q_data_int;
+            q_data_int.x() = sycl::round((q_data.x() - min) * q_scale_inv);
+            q_data_int.y() = sycl::round((q_data.y() - min) * q_scale_inv);
+            q_data_int.w() = sycl::round((q_data.w() - min) * q_scale_inv);
+            q_data_int.z() = sycl::round((q_data.z() - min) * q_scale_inv);
+
+            q_data.x() = q_data_int.x() * q_scale + min;
+            q_data.y() = q_data_int.y() * q_scale + min;
+            q_data.w() = q_data_int.w() * q_scale + min;
+            q_data.z() = q_data_int.z() * q_scale + min;
+
+            vals_cast[group_index + bid * group_size] = q_data;
+        }
+    }
+}
+
+template <typename T>
+void launch_fake_quantize_kernel_asym(T* vals,
+                                      int total_count,
+                                      int group_num,
+                                      int num_bits,
+                                      dpct::queue_ptr stream)
+{
+    sycl::range<3> grid_dim(1, 1, group_num);
+    sycl::range<3> block_dim(1, 1, 1024);
+
+    /*
+    DPCT1049:48: The work-group size passed to the SYCL kernel may exceed the limit. To get the
+    device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
+    */
+    {
+        dpct::has_capability_or_fail(stream->get_device(), {sycl::aspect::fp16});
+        stream->parallel_for(
+            sycl::nd_range<3>(grid_dim * block_dim, block_dim), [=](sycl::nd_item<3> item_ct1) {
+                fake_quantize_kernel_asym(vals, (total_count / group_num) / 4, num_bits);
+            });
+    }
+}
+
+template void launch_fake_quantize_kernel_asym(float* vals,
+                                               int total_count,
+                                               int group_num,
+                                               int num_bits,
+                                               dpct::queue_ptr stream);
+template void launch_fake_quantize_kernel_asym(sycl::half* vals,
+                                               int total_count,
+                                               int group_num,
+                                               int num_bits,
+                                               dpct::queue_ptr stream);
+
+void sr_fake_quantize_kernel_asym(sycl::half* vals,
+                                  int token_size,
+                                  int token_num,
+                                  int num_bits,
+                                  std::pair<uint64_t, uint64_t> seed)
+{
+#if DPCT_COMPATIBILITY_TEMP >= 700 || defined(__HIP_PLATFORM_AMD__)
+
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    sycl::group<3> b = sycl::ext::oneapi::experimental::this_group<3>();
+    sycl::sub_group g = sycl::ext::oneapi::experimental::this_sub_group();
+
+    int gid = threadIdx.x >> 5;
+    int lane = threadIdx.x & 0x1f;
+    int warp_num = blockDim.x >> 5;
+
+    int idx = blockIdx.x * blockDim.x + threadIdx.x;
+
+    float2* vals_cast = reinterpret_cast<float2*>(vals);
+
+    sycl::half2 data_low[128];
+    sycl::half2 data_high[128];
+
+    int bid = blockIdx.x;
+
+    /* curandStatePhilox4_32_10_t state; */
+    /* curand_init(seed.first, idx, seed.second, &state); */
+    dpct::rng::device::rng_generator<oneapi::mkl::rng::device::philox4x32x10<1>> state;
+    state = dpct::rng::device::rng_generator<oneapi::mkl::rng::device::philox4x32x10<1>>(seed.first, {seed.second, (unsigned long)idx * 4});
+    unsigned int tid = threadIdx.x;
+    int reg_count = 0;
+    int offset = bid * token_size;
+    int group_index = bid * token_size + tid;
+
+    int total_count = token_size * token_num;
+    if (group_index < total_count) {
+        float min = 10000.0;
+        float max = -10000.0;
+        while (tid < token_size) {
+            float2 data = vals_cast[offset + tid];
+            sycl::half2* data_h = reinterpret_cast<sycl::half2*>(&data);
+            data_low[reg_count] = data_h[0];
+            data_high[reg_count] = data_h[1];
+
+            float2 data_f[2];
+            data_f[0] = sycl::half22float2(data_h[0]);
+            data_f[1] = sycl::half22float2(data_h[1]);
+
+            if (((float)data_f[0].x) > max) max = (float)data_f[0].x;
+            if (((float)data_f[0].y) > max) max = (float)data_f[0].y;
+            if (((float)data_f[1].x) > max) max = (float)data_f[1].x;
+            if (((float)data_f[1].y) > max) max = (float)data_f[1].y;
+
+            if (((float)data_f[0].x) < min) min = (float)data_f[0].x;
+            if (((float)data_f[0].y) < min) min = (float)data_f[0].y;
+            if (((float)data_f[1].x) < min) min = (float)data_f[1].x;
+            if (((float)data_f[1].y) < min) min = (float)data_f[1].y;
+
+            tid += blockDim.x;
+            reg_count++;
+        }
+
+#pragma unroll
+        for (int i = 1; i < WARP_SIZE; i <<= 1) {
+            auto temp = g.shuffle_xor(max, i);
+            if (max < temp) max = temp;
+        }
+
+#pragma unroll
+        for (int i = 1; i < WARP_SIZE; i <<= 1) {
+            auto temp = g.shuffle_xor(min, i);
+            if (min > temp) min = temp;
+        }
+
+        __shared__ float partialMax[WARP_SIZE];
+        __shared__ float partialMin[WARP_SIZE];
+
+        if (lane == 0) partialMax[gid] = max;
+        if (lane == 0) partialMin[gid] = min;
+
+        b.sync();
+
+        if (lane < warp_num) max = partialMax[lane];
+        if (lane < warp_num) min = partialMin[lane];
+
+#pragma unroll
+        for (int i = 1; i < warp_num; i <<= 1) {
+            auto temp = g.shuffle_down(max, i);
+            if (max < temp) max = temp;
+        }
+#pragma unroll
+        for (int i = 1; i < warp_num; i <<= 1) {
+            auto temp = g.shuffle_down(min, i);
+            if (min > temp) min = temp;
+        }
+
+        max = g.shuffle(max, 0);
+        min = g.shuffle(min, 0);
+
+        float q_scale_val = ((max - min) + 1e-5) / (float)(1 << num_bits);
+        float q_scale_val_inv = 1 / q_scale_val;
+        float high_q = (float)((1 << num_bits) - 1);
+
+        for (int i = 0; i < reg_count; i++) {
+            int token_index = i * blockDim.x + threadIdx.x;
+            if (token_index < token_size) {
+                float2 data_f[2];
+                data_f[0] = sycl::half22float2(data_low[i]);
+                data_f[1] = sycl::half22float2(data_high[i]);
+
+                float2 q_data_int[2];
+                q_data_int[0].x = (float)((unsigned int)((data_f[0].x - min) * q_scale_val_inv));
+                q_data_int[0].y = (float)((unsigned int)((data_f[0].y - min) * q_scale_val_inv));
+                q_data_int[1].x = (float)((unsigned int)((data_f[1].x - min) * q_scale_val_inv));
+                q_data_int[1].y = (float)((unsigned int)((data_f[1].y - min) * q_scale_val_inv));
+
+                // Stochastic rounding
+                /* float4 rand = curand_uniform4(&state); */
+                sycl::float4 rand = state.generate<oneapi::mkl::rng::device::uniform<float>, 4>();
+
+                float q_error[4];
+                q_error[0] =
+                    abs(data_f[0].x - ((q_data_int[0].x * q_scale_val) + min)) * q_scale_val_inv;
+                q_error[1] =
+                    abs(data_f[0].y - ((q_data_int[0].y * q_scale_val) + min)) * q_scale_val_inv;
+                q_error[2] =
+                    abs(data_f[1].x - ((q_data_int[1].x * q_scale_val) + min)) * q_scale_val_inv;
+                q_error[3] =
+                    abs(data_f[1].y - ((q_data_int[1].y * q_scale_val) + min)) * q_scale_val_inv;
+
+                q_data_int[0].x = (rand.x < q_error[0] && q_data_int[0].x < high_q)
+                                      ? (q_data_int[0].x + 1)
+                                      : q_data_int[0].x;
+                q_data_int[0].y = (rand.y < q_error[1] && q_data_int[0].y < high_q)
+                                      ? (q_data_int[0].y + 1)
+                                      : q_data_int[0].y;
+                q_data_int[1].x = (rand.w < q_error[2] && q_data_int[1].x < high_q)
+                                      ? (q_data_int[1].x + 1)
+                                      : q_data_int[1].x;
+                q_data_int[1].y = (rand.z < q_error[3] && q_data_int[1].y < high_q)
+                                      ? (q_data_int[1].y + 1)
+                                      : q_data_int[1].y;
+
+                data_f[0].x = q_data_int[0].x * q_scale_val + min;
+                data_f[0].y = q_data_int[0].y * q_scale_val + min;
+                data_f[1].x = q_data_int[1].x * q_scale_val + min;
+                data_f[1].y = q_data_int[1].y * q_scale_val + min;
+
+                float2 result;
+                sycl::half2* result_h = reinterpret_cast<sycl::half2*>(&result);
+                result_h[0] = __float22half2_rn(data_f[0]);
+                result_h[1] = __float22half2_rn(data_f[1]);
+
+                vals_cast[offset + token_index] = result;
+            }
+        }
+    }
+#endif
+}
+
+void sr_fake_quantize_kernel_asym(float* vals,
+                                             int token_size,
+                                             int token_num,
+                                             int num_bits,
+                                             std::pair<uint64_t, uint64_t> seed)
+{
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    sycl::group<3> b = sycl::ext::oneapi::experimental::this_group<3>();
+    sycl::sub_group g = sycl::ext::oneapi::experimental::this_sub_group();
+
+    int gid = item_ct1.get_local_id(2) >> 5;
+    int lane = item_ct1.get_local_id(2) & 0x1f;
+    int warp_num = item_ct1.get_local_range(2) >> 5;
+    int id = item_ct1.get_local_id(2);
+
+    int idx = item_ct1.get_group(2) * item_ct1.get_local_range(2) + id;
+
+    sycl::float4* vals_cast = reinterpret_cast<sycl::float4*>(vals);
+
+    sycl::float4 data[128];
+
+    int bid = item_ct1.get_group(2);
+    int tid = item_ct1.get_local_id(2);
+    dpct::rng::device::rng_generator<oneapi::mkl::rng::device::philox4x32x10<1>> state;
+    /* curand_init(seed.first, idx, seed.second, &state); */
+    state = dpct::rng::device::rng_generator<oneapi::mkl::rng::device::philox4x32x10<1>>(seed.first, {seed.second, (unsigned long)idx * 4});
+
+    int group_index = bid * token_size + item_ct1.get_local_id(2);
+    int reg_count = 0;
+    int total_count = token_size * token_num;
+    if (group_index < total_count) {
+        float min = 10000.0;
+        float max = -10000.0;
+
+        while (tid < token_size) {
+            sycl::float4 data_reg = vals_cast[group_index];
+            data[reg_count] = data_reg;
+            if (data_reg.x() > max) max = data_reg.x();
+            if (data_reg.y() > max) max = data_reg.y();
+            if (data_reg.w() > max) max = data_reg.w();
+            if (data_reg.z() > max) max = data_reg.z();
+
+            if (data_reg.x() < min) min = data_reg.x();
+            if (data_reg.y() < min) min = data_reg.y();
+            if (data_reg.w() < min) min = data_reg.w();
+            if (data_reg.z() < min) min = data_reg.z();
+
+            group_index += item_ct1.get_local_range(2);
+            tid += item_ct1.get_local_range(2);
+            reg_count++;
+        }
+
+#pragma unroll
+        for (int i = 1; i < WARP_SIZE; i <<= 1) {
+            auto temp = g.shuffle_xor(max, i);
+            if (max < temp) max = temp;
+        }
+
+#pragma unroll
+        for (int i = 1; i < WARP_SIZE; i <<= 1) {
+            auto temp = g.shuffle_xor(min, i);
+            if (min > temp) min = temp;
+        }
+
+        auto& partialMax = *sycl::ext::oneapi::group_local_memory_for_overwrite<float[WARP_SIZE]>(
+            sycl::ext::oneapi::experimental::this_group<3>());
+        auto& partialMin = *sycl::ext::oneapi::group_local_memory_for_overwrite<float[WARP_SIZE]>(
+            sycl::ext::oneapi::experimental::this_group<3>());
+
+        if (lane == 0) partialMax[gid] = max;
+        if (lane == 0) partialMin[gid] = min;
+
+        /*
+        DPCT1065:49: Consider replacing sycl::nd_item::barrier() with
+        sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if
+        there is no access to global memory.
+        */
+        item_ct1.barrier();
+
+        if (lane < warp_num) max = partialMax[lane];
+        if (lane < warp_num) min = partialMin[lane];
+
+#pragma unroll
+        for (int i = 1; i < warp_num; i <<= 1) {
+            auto temp = g.shuffle_down(max, i);
+            if (max < temp) max = temp;
+        }
+#pragma unroll
+        for (int i = 1; i < warp_num; i <<= 1) {
+            auto temp = g.shuffle_down(min, i);
+            if (min > temp) min = temp;
+        }
+
+        max = g.shuffle(max, 0);
+        min = g.shuffle(min, 0);
+
+        float q_scale_val = ((max - min) + 1e-5) / (float)(1 << num_bits);
+        float high_q = (float)((1 << num_bits) - 1);
+
+        int offset = (bid)*token_size;
+        for (int i = 0; i < reg_count; i++) {
+            group_index = i * item_ct1.get_local_range(2) + item_ct1.get_local_id(2);
+            if (group_index < token_size) {
+                sycl::float4 q_data = data[i];
+
+                sycl::float4 q_data_int;
+                q_data_int.x() = (float)((int)((q_data.x() - min) / q_scale_val));
+                q_data_int.y() = (float)((int)((q_data.y() - min) / q_scale_val));
+                q_data_int.w() = (float)((int)((q_data.w() - min) / q_scale_val));
+                q_data_int.z() = (float)((int)((q_data.z() - min) / q_scale_val));
+
+                // Stochastic rounding
+                sycl::float4 rand = state.generate<oneapi::mkl::rng::device::uniform<float>, 4>();
+
+                float q_error[4];
+                q_error[0] =
+                    sycl::fabs(q_data.x() - ((q_data_int.x() * q_scale_val) + min)) / q_scale_val;
+                q_error[1] =
+                    sycl::fabs(q_data.y() - ((q_data_int.y() * q_scale_val) + min)) / q_scale_val;
+                q_error[2] =
+                    sycl::fabs(q_data.w() - ((q_data_int.w() * q_scale_val) + min)) / q_scale_val;
+                q_error[3] =
+                    sycl::fabs(q_data.z() - ((q_data_int.z() * q_scale_val) + min)) / q_scale_val;
+
+                q_data_int.x() = (rand.x() < q_error[0] && q_data_int.x() < high_q)
+                                     ? (q_data_int.x() + 1)
+                                     : q_data_int.x();
+                q_data_int.y() = (rand.y() < q_error[1] && q_data_int.y() < high_q)
+                                     ? (q_data_int.y() + 1)
+                                     : q_data_int.y();
+                q_data_int.w() = (rand.w() < q_error[2] && q_data_int.w() < high_q)
+                                     ? (q_data_int.w() + 1)
+                                     : q_data_int.w();
+                q_data_int.z() = (rand.z() < q_error[3] && q_data_int.z() < high_q)
+                                     ? (q_data_int.z() + 1)
+                                     : q_data_int.z();
+
+                q_data_int.x() = q_data_int.x() * q_scale_val + min;
+                q_data_int.y() = q_data_int.y() * q_scale_val + min;
+                q_data_int.w() = q_data_int.w() * q_scale_val + min;
+                q_data_int.z() = q_data_int.z() * q_scale_val + min;
+
+                vals_cast[group_index + offset] = q_data_int;
+            }
+        }
+    }
+}
+template <typename T>
+void launch_sr_fake_quantize_kernel_asym(T* vals,
+                                         int total_count,
+                                         int group_num,
+                                         int num_bits,
+                                         dpct::queue_ptr stream)
+{
+    sycl::range<3> block_dim(1, 1, 1024);
+    sycl::range<3> grid_dim(1, 1, group_num);
+
+    uint64_t inc = total_count / grid_dim[2] / block_dim[2];
+    std::pair<uint64_t, uint64_t> seed = TrainingContext::Instance().IncrementOffset(inc);
+
+    /*
+    DPCT1049:50: The work-group size passed to the SYCL kernel may exceed the limit. To get the
+    device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
+    */
+    {
+        dpct::has_capability_or_fail(stream->get_device(), {sycl::aspect::fp16});
+        stream->parallel_for(
+            sycl::nd_range<3>(grid_dim * block_dim, block_dim), [=](sycl::nd_item<3> item_ct1) {
+                sr_fake_quantize_kernel(
+                    vals, (total_count / group_num) / 4, group_num, num_bits, seed);
+            });
+    }
+}
+template void launch_sr_fake_quantize_kernel_asym(float* vals,
+                                                  int total_count,
+                                                  int group_num,
+                                                  int num_bits,
+                                                  dpct::queue_ptr stream);
+template void launch_sr_fake_quantize_kernel_asym(sycl::half* vals,
+                                                  int total_count,
+                                                  int group_num,
+                                                  int num_bits,
+                                                  dpct::queue_ptr stream);
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/quantization/pt_binding.cpp b/intel_extension_for_deepspeed/op_builder/csrc/quantization/pt_binding.cpp
new file mode 100644
index 0000000..28636e5
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/quantization/pt_binding.cpp
@@ -0,0 +1,326 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+/* #include <ATen/cuda/CUDAContext.h> */
+#include <torch/extension.h>
+#include <cassert>
+#include <vector>
+#include "quantization.h"
+
+#include <ipex.h>
+
+namespace at {
+  namespace cuda {
+    dpct::queue_ptr getCurrentCUDAStream() {
+      auto device_type = c10::DeviceType::XPU;
+      c10::impl::VirtualGuardImpl impl(device_type);
+      c10::Stream c10_stream = impl.getStream(c10::Device(device_type));
+      auto& queue = xpu::get_queue_from_stream(c10_stream);
+      return &queue;
+    }
+
+    dpct::queue_ptr getStreamFromPool(bool) {
+      // not implemented
+      return nullptr;
+    }
+    
+    dpct::queue_ptr getStreamFromPool() {
+      // not implemented
+      return nullptr;
+    }
+  }
+}
+
+template <typename T>
+at::Tensor ds_quantize(at::Tensor& vals, int groups, int bits)
+{
+    auto t_size = vals.sizes();
+    int size = 1;
+    for (auto dim : t_size) size *= dim;
+
+    if ((((size / groups) - 1) / 4096 + 1) <= 256) {
+        launch_fake_quantize_kernel(
+            (T*)vals.data_ptr(), size, groups, bits, at::cuda::getCurrentCUDAStream());
+    }
+    return vals;
+}
+
+template <typename T>
+at::Tensor ds_sr_quantize(at::Tensor& vals, int groups, int bits)
+{
+    auto t_size = vals.sizes();
+    int size = 1;
+    for (auto dim : t_size) size *= dim;
+
+    if (((size / groups) / 4 / 1024) <= 256) {
+        launch_sr_fake_quantize_kernel(
+            (T*)vals.data_ptr(), size, groups, bits, at::cuda::getCurrentCUDAStream());
+    }
+    return vals;
+}
+
+template <typename T>
+at::Tensor ds_quantize_asym(at::Tensor& vals, int groups, int bits)
+{
+    auto t_size = vals.sizes();
+    int size = 1;
+    for (auto dim : t_size) size *= dim;
+
+    if ((((size / groups) - 1) / 4096 + 1) <= 256) {
+        launch_fake_quantize_kernel_asym(
+            (T*)vals.data_ptr(), size, groups, bits, at::cuda::getCurrentCUDAStream());
+    }
+    return vals;
+}
+
+template <typename T>
+at::Tensor ds_sr_quantize_asym(at::Tensor& vals, int groups, int bits)
+{
+    auto t_size = vals.sizes();
+    int size = 1;
+    for (auto dim : t_size) size *= dim;
+
+    if (((size / groups) / 4 / 1024) <= 256) {
+        launch_sr_fake_quantize_kernel_asym(
+            (T*)vals.data_ptr(), size, groups, bits, at::cuda::getCurrentCUDAStream());
+    }
+    return vals;
+}
+
+std::vector<at::Tensor> quantize_kernel(at::Tensor& input_vals,
+                                        int groups,
+                                        int numBits,
+                                        quantize::Type quantType)
+{
+    auto dtype = at::kFloat;
+    auto params_options = at::TensorOptions()
+                              .dtype(dtype)
+                              .layout(at::kStrided)
+                              .device(at::kXPU)
+                              .requires_grad(false);
+    const int param_elems = (quantize::requires_offset(quantType)) ? 2 : 1;
+    auto params = torch::empty({groups, param_elems}, params_options);
+
+    auto output_options = at::TensorOptions()
+                              .dtype(at::kChar)
+                              .layout(at::kStrided)
+                              .device(at::kXPU)
+                              .requires_grad(false);
+
+    auto output_sizes = input_vals.sizes().vec();
+    output_sizes[output_sizes.size() - 1] /= numBits == 8 ? 1 : 2;
+    auto output = torch::empty(output_sizes, output_options);
+
+    const int elems_per_group = at::numel(input_vals) / groups;
+
+    launch_quant((int8_t*)output.data_ptr(),
+                 (float*)params.data_ptr(),
+                 (sycl::half*)input_vals.data_ptr(),
+                 groups,
+                 elems_per_group,
+                 numBits,
+                 quantType,
+                 at::cuda::getCurrentCUDAStream());
+
+    return {output, params};
+}
+
+template <typename T>
+at::Tensor dequantize(at::Tensor& quantized_data,
+                      at::Tensor& params,
+                      int groups,
+                      int num_bits,
+                      quantize::Type quant_type)
+{
+    auto dtype = (std::is_same<T, float>::value) ? torch::kFloat32 : torch::kFloat16;
+    auto output_options = at::TensorOptions()
+                              .dtype(dtype)
+                              .layout(at::kStrided)
+                              .device(at::kXPU)
+                              .requires_grad(false);
+
+    auto output_sizes = quantized_data.sizes().vec();
+    output_sizes[output_sizes.size() - 1] *= num_bits == 8 ? 1 : 2;
+    auto output = torch::empty(output_sizes, output_options);
+
+    const int total_elems = at::numel(output);
+    const int elems_per_group = total_elems / groups;
+
+    launch_dequantize_kernel((T*)output.data_ptr(),
+                             (const int8_t*)quantized_data.data_ptr(),
+                             (const float*)params.data_ptr(),
+                             quant_type,
+                             num_bits,
+                             elems_per_group,
+                             total_elems,
+                             at::cuda::getCurrentCUDAStream());
+
+    return output;
+}
+
+at::Tensor dequantize_int4_to_half_experimental(at::Tensor& data_in,
+                                                at::Tensor& scale_buffer,
+                                                at::Tensor& min_val_buffer,
+                                                int num_group,
+                                                int group_size)
+{
+    auto output_options = at::TensorOptions().dtype(at::kHalf).device(at::kXPU);
+    auto output = torch::empty({num_group, group_size}, output_options);
+
+    launch_dequantize_int4_to_half_experimental((uint8_t*)data_in.data_ptr(),
+                                                (sycl::half*)output.data_ptr(),
+                                                (sycl::half*)scale_buffer.data_ptr(),
+                                                (sycl::half*)min_val_buffer.data_ptr(),
+                                                num_group,
+                                                group_size,
+                                                at::cuda::getCurrentCUDAStream());
+
+    return output;
+}
+
+at::Tensor dequantize_int8_to_half_experimental(at::Tensor& data_in,
+                                                at::Tensor& scale_buffer,
+                                                at::Tensor& min_val_buffer,
+                                                int num_group,
+                                                int group_size)
+{
+    auto output_options = at::TensorOptions().dtype(at::kHalf).device(at::kXPU);
+    auto output = torch::empty({num_group, group_size}, output_options);
+
+    launch_dequantize_int8_to_half_experimental((uint8_t*)data_in.data_ptr(),
+                                                (sycl::half*)output.data_ptr(),
+                                                (sycl::half*)scale_buffer.data_ptr(),
+                                                (sycl::half*)min_val_buffer.data_ptr(),
+                                                num_group,
+                                                group_size,
+                                                at::cuda::getCurrentCUDAStream());
+
+    return output;
+}
+
+std::vector<at::Tensor> ds_swizzle_quant(at::Tensor& input_vals,
+                                         int groups,
+                                         int num_bits,
+                                         quantize::Type quant_type,
+                                         int pipeline_size,
+                                         int nodes,
+                                         int devices_per_node)
+{
+    auto scales_options = at::TensorOptions()
+                              .dtype(at::kFloat)
+                              .layout(at::kStrided)
+                              .device(at::kXPU)
+                              .requires_grad(false);
+    const int scales_elems = (quantize::requires_offset(quant_type)) ? 2 : 1;
+    auto scales = torch::empty({groups, scales_elems}, scales_options);
+
+    auto output_options = at::TensorOptions()
+                              .dtype(at::kChar)
+                              .layout(at::kStrided)
+                              .device(at::kXPU)
+                              .requires_grad(false);
+
+    const int quantization_scalar = 8 / num_bits;
+    const int compressed_vals = at::numel(input_vals) / quantization_scalar;
+
+    auto output = torch::empty({compressed_vals}, output_options);
+    const int elems_per_group = at::numel(input_vals) / groups;
+
+    launch_swizzled_quant((int8_t*)output.data_ptr(),
+                          (float*)scales.data_ptr(),
+                          (sycl::half*)input_vals.data_ptr(),
+                          num_bits,
+                          quant_type,
+                          groups,
+                          elems_per_group,
+                          pipeline_size,
+                          nodes,
+                          devices_per_node,
+                          at::cuda::getCurrentCUDAStream());
+
+    return {output, scales};
+}
+
+std::vector<at::Tensor> quantized_reduction(at::Tensor& input_vals,
+                                            at::Tensor& input_scales,
+                                            int in_groups,
+                                            int out_groups,
+                                            int num_bits,
+                                            quantize::Type quant_type,
+                                            int devices_per_node)
+{
+    auto scales_options = at::TensorOptions()
+                              .dtype(at::kFloat)
+                              .layout(at::kStrided)
+                              .device(at::kXPU)
+                              .requires_grad(false);
+    const int scales_elems = (quantize::requires_offset(quant_type)) ? 2 : 1;
+    auto scales = torch::empty({out_groups, scales_elems}, scales_options);
+
+    auto output_options = at::TensorOptions()
+                              .dtype(at::kChar)
+                              .layout(at::kStrided)
+                              .device(at::kXPU)
+                              .requires_grad(false);
+
+    std::vector<long int> sz(input_vals.sizes().begin(), input_vals.sizes().end());
+    sz[sz.size() - 1] = sz.back() / devices_per_node;  // num of GPU per nodes
+    const int elems_per_in_tensor = at::numel(input_vals) / devices_per_node;
+    auto output = torch::empty(sz, output_options);
+
+    const int elems_per_in_group = elems_per_in_tensor / (in_groups / devices_per_node);
+    const int elems_per_out_group = elems_per_in_tensor / out_groups;
+
+    launch_dequant_reduce((int8_t*)output.data_ptr(),
+                          (float*)scales.data_ptr(),
+                          (const int8_t*)input_vals.data_ptr(),
+                          (const float*)input_scales.data_ptr(),
+                          devices_per_node,
+                          num_bits,
+                          quant_type,
+                          out_groups,
+                          elems_per_out_group,
+                          elems_per_in_tensor,
+                          in_groups / devices_per_node,
+                          elems_per_in_group,
+                          at::cuda::getCurrentCUDAStream());
+    return {output, scales};
+}
+
+PYBIND11_MODULE(TORCH_EXTENSION_NAME, m)
+{
+    m.def("ds_quantize_fp32", &ds_quantize<float>, "DeepSpeed Quantize with fp32 (CUDA)");
+    m.def("ds_quantize_fp16", &ds_quantize<sycl::half>, "DeepSpeed Quantize with fp16 (CUDA)");
+    m.def("ds_sr_quantize_fp32", &ds_sr_quantize<float>, "DeepSpeed Quantize with fp32 (CUDA)");
+    m.def(
+        "ds_sr_quantize_fp16", &ds_sr_quantize<sycl::half>, "DeepSpeed Quantize with fp16 (CUDA)");
+    m.def("ds_quantize_asym_fp32", &ds_quantize_asym<float>, "DeepSpeed Quantize with fp32 (CUDA)");
+    m.def("ds_quantize_asym_fp16",
+          &ds_quantize_asym<sycl::half>,
+          "DeepSpeed Quantize with fp16 (CUDA)");
+    m.def("ds_sr_quantize_asym_fp32",
+          &ds_sr_quantize_asym<float>,
+          "DeepSpeed Quantize with fp32 (CUDA)");
+    m.def("ds_sr_quantize_asym_fp16",
+          &ds_sr_quantize_asym<sycl::half>,
+          "DeepSpeed Quantize with fp16 (CUDA)");
+    pybind11::enum_<quantize::Type>(m, "QuantizationType")
+        .value("Symmetric", quantize::Type::Symmetric)
+        .value("Asymmetric", quantize::Type::Asymmetric)
+        .export_values();
+    m.def("quantize", &quantize_kernel);
+    m.def("dequantize", &dequantize<sycl::half>);
+    m.def("dequantize_fp32", &dequantize<float>);
+    m.def("dequantize_int4_to_half_experimental",
+          &dequantize_int4_to_half_experimental,
+          "Dequantize int4 to half (experimental)");
+    m.def("dequantize_int8_to_half_experimental",
+          &dequantize_int8_to_half_experimental,
+          "Dequantize int8 to half (experimental)");
+    m.def("swizzle_quant", &ds_swizzle_quant);
+    m.def("quantized_reduction", &quantized_reduction);
+}
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/quantization/quant_reduce.dp.cpp b/intel_extension_for_deepspeed/op_builder/csrc/quantization/quant_reduce.dp.cpp
new file mode 100644
index 0000000..ae3142f
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/quantization/quant_reduce.dp.cpp
@@ -0,0 +1,296 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include <cstdio>
+#include "dequantization_utils.h"
+#include "ds_kernel_utils.h"
+#include "memory_access_utils.h"
+#include "quantization_utils.h"
+#include "reduction_utils.h"
+
+using rop = reduce::ROpType;
+
+/*
+TODO(cmikeh2): Add implementation that better handles larger nodes. It would like make sense
+to leverage some parallel reductions here to improve performance.
+*/
+
+template <int numBits, int numTensors, int totalChunks, quantize::Type quantType>
+/*
+DPCT1110:46: The total declared local variable size in device function dequant_reduce exceeds 128
+bytes and may cause high register pressure. Consult with your hardware vendor to find the total
+register size available and adjust the code, or use smaller sub-group size to avoid high register
+pressure.
+*/
+void dequant_reduce(int8_t* reduced_data,
+                    float* reduced_scales,
+                    const int8_t* input_data,
+                    const float* input_scales,
+                    int elems_per_out_group,
+                    int elems_per_in_tensor,
+                    int groups_per_in_tensor,
+                    int elems_per_in_group,
+                    int num_tensors)
+{
+    sycl::group<3> tb = sycl::ext::oneapi::experimental::this_group<3>();
+    sycl::sub_group warp = sycl::ext::oneapi::experimental::this_sub_group();
+
+    // NOTE(cmikeh2): This probably could be hardcoded to a larger number,
+    // but that means even stronger restrictions on the number of elements per group
+    // A performance analysis here might be beneficial
+    constexpr int mem_granularity = (numBits == 8) ? 8 : 4;
+    constexpr int elems_per_load = mem_granularity / sizeof(int8_t);  // div by 1
+    constexpr int storage_values = 16 / sizeof(sycl::half2);
+
+    const int block_offset = tb.get_group_id()[2] * elems_per_out_group;
+    const int elem_offset = tb.get_local_id()[2] * elems_per_load;
+    const int base_offset = block_offset + elem_offset;
+    /*
+    DPCT1007:49: Migration of cooperative_groups::thread_block::group_dim is not supported.
+    */
+    const int stride = tb.get_group_range()[2] * elems_per_load;
+
+    sycl::half2 local_buffer[totalChunks * storage_values];
+
+    quantize::GroupStats<quantType> stats;
+
+#pragma unroll
+    for (int i = 0; i < totalChunks; i++) {
+        sycl::half2* iteration_buffer = local_buffer + i * storage_values;
+
+#pragma unroll
+        for (int j = 0; j < storage_values; j++) {
+            iteration_buffer[j] = reduce::init<rop::Add, sycl::half2>();
+        }
+
+        const int iter_offset = i * stride + base_offset;
+        const int iter_scale_idx = iter_offset / elems_per_in_group;
+        bool do_loads = i * stride + elem_offset < elems_per_out_group;
+
+        if (numTensors > 0) {
+#pragma unroll
+            for (int j = 0; j < numTensors; j++) {
+                if (do_loads) {
+                    int8_t load_buffer[elems_per_load];
+
+                    mem_access::load_global<mem_granularity>(
+                        load_buffer, input_data + j * elems_per_in_tensor + iter_offset);
+
+                    quantize::Params<quantType, numBits> params(
+                        input_scales + j * groups_per_in_tensor, iter_scale_idx);
+
+                    sycl::half2 dequant_buffer[storage_values];
+                    dequantize::chunk<numBits, quantType>(dequant_buffer, load_buffer, params);
+
+#pragma unroll
+                    for (int k = 0; k < storage_values; k++) {
+                        iteration_buffer[k] =
+                            reduce::element<rop::Add>(iteration_buffer[k], dequant_buffer[k]);
+                    }
+                }
+            }
+        } else {
+#pragma unroll 4
+            for (int j = 0; j < num_tensors; j++) {
+                if (do_loads) {
+                    int8_t load_buffer[elems_per_load];
+
+                    mem_access::load_global<mem_granularity>(
+                        load_buffer, input_data + j * elems_per_in_tensor + iter_offset);
+
+                    quantize::Params<quantType, numBits> params(
+                        input_scales + j * groups_per_in_tensor, iter_scale_idx);
+
+                    sycl::half2 dequant_buffer[storage_values];
+                    dequantize::chunk<numBits, quantType>(dequant_buffer, load_buffer, params);
+
+#pragma unroll
+                    for (int k = 0; k < storage_values; k++) {
+                        iteration_buffer[k] =
+                            reduce::element<rop::Add>(iteration_buffer[k], dequant_buffer[k]);
+                    }
+                }
+            }
+        }
+
+#pragma unroll
+        for (int j = 0; j < storage_values; j++) { stats.update(iteration_buffer[j]); }
+    }
+
+    auto params = stats.template get_params<numBits, 1024>(tb, warp);
+
+    if (tb.get_local_id()[2] == 0) { params.store(reduced_scales, tb.get_group_id()[2]); }
+
+#pragma unroll
+    for (int i = 0; i < totalChunks; i++) {
+        const int iter_offset = i * stride + base_offset;
+        if (i * stride + elem_offset < elems_per_out_group) {
+            int8_t local_output[elems_per_load];
+            quantize::_chunk<numBits, quantType>(
+                local_output, local_buffer + i * storage_values, params);
+            mem_access::store_global<mem_granularity>(reduced_data + iter_offset, local_output);
+        }
+    }
+}
+
+template <int Power>
+int32_t pow2_round(int32_t raw_value)
+{
+    return (((raw_value - 1) >> Power) + 1) << Power;
+}
+
+/*
+DPCT1049:47: The work-group size passed to the SYCL kernel may exceed the limit. To get the device
+limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
+*/
+#define LAUNCH_DEQUANT_REDUCE(num_chunks)                                                       \
+ {                                                                                              \
+  dpct::has_capability_or_fail(stream->get_device(), {sycl::aspect::fp64, sycl::aspect::fp16}); \
+  stream->submit([&](sycl::handler& cgh) {                                                      \
+   int8_t* reduced_data_ct0 = reduced_data;                                                     \
+   float* reduced_scales_ct1 = reduced_scales;                                                  \
+   const int8_t* input_data_ct2 = input_data;                                                   \
+   const float* input_scales_ct3 = input_scales;                                                \
+   auto elems_per_out_group_ct4 = elems_per_out_group;                                          \
+   auto elems_per_in_tensor_ct5 = elems_per_in_tensor;                                          \
+   auto groups_per_in_tensor_ct6 = groups_per_in_tensor;                                        \
+   auto elems_per_in_group_ct7 = elems_per_in_group;                                            \
+   auto num_tensors_ct8 = num_tensors;                                                          \
+                                                                                                \
+   cgh.parallel_for(sycl::nd_range<3>(grid * block, block),                                     \
+                    [=](sycl::nd_item<3> item_ct1) [[intel::reqd_sub_group_size(32)]] {         \
+                     dequant_reduce<numBits, numTensors, num_chunks, quantType>(                \
+                         reduced_data_ct0,                                                      \
+                         reduced_scales_ct1,                                                    \
+                         input_data_ct2,                                                        \
+                         input_scales_ct3,                                                      \
+                         elems_per_out_group_ct4,                                               \
+                         elems_per_in_tensor_ct5,                                               \
+                         groups_per_in_tensor_ct6,                                              \
+                         elems_per_in_group_ct7,                                                \
+                         num_tensors_ct8);                                                      \
+                    });                                                                         \
+  });                                                                                           \
+ }
+
+template <int numBits, int numTensors, quantize::Type quantType>
+void launch_dequant_reduce_impl(int8_t* reduced_data,
+                                float* reduced_scales,
+                                const int8_t* input_data,
+                                const float* input_scales,
+                                int out_groups,
+                                int elems_per_out_group,
+                                int elems_per_in_tensor,
+                                int groups_per_in_tensor,
+                                int elems_per_in_group,
+                                int num_tensors,
+                                dpct::queue_ptr stream)
+{
+    // This is a coincidence. This is derived by 8 halves per 16 bytes with 2-way packing for int4
+    constexpr int elems_per_thread = numBits;
+    const int one_step_threads =
+        next_pow2((elems_per_out_group + elems_per_thread - 1) / (elems_per_thread));
+    // TODO(cmikeh2): Tune this
+    const int threads = (one_step_threads < 1024) ? one_step_threads : 1024;
+
+    sycl::range<3> block(1, 1, threads);
+    sycl::range<3> grid(1, 1, out_groups);
+
+    const int elems_per_step = threads * elems_per_thread;
+    const int unroll_raw = (elems_per_out_group + elems_per_step - 1) / elems_per_step;
+
+    const int unroll = (unroll_raw >= 4) ? pow2_round<1>(unroll_raw) : unroll_raw;
+
+    if (unroll == 1) {
+        // 0-4096 elems
+        LAUNCH_DEQUANT_REDUCE(1);
+    } else if (unroll == 2) {
+        // 4097-8192 etc...
+        LAUNCH_DEQUANT_REDUCE(2);
+    } else if (unroll == 3) {
+        LAUNCH_DEQUANT_REDUCE(3);
+    } else if (unroll == 4) {
+        LAUNCH_DEQUANT_REDUCE(4);
+    } else if (unroll == 6) {
+        LAUNCH_DEQUANT_REDUCE(6);
+    } else if (unroll == 8) {
+        LAUNCH_DEQUANT_REDUCE(8);
+    } else if (unroll == 10) {
+        LAUNCH_DEQUANT_REDUCE(10);
+    } else if (unroll == 12) {
+        // 48k limit
+        LAUNCH_DEQUANT_REDUCE(12);
+    } else {
+        assert(false);
+    }
+}
+
+#define LAUNCH_DEQUANT_REDUCE_IMPL(NUM_BITS, NUM_GPUS, QUANT_TYPE)                   \
+    launch_dequant_reduce_impl<NUM_BITS, NUM_GPUS, QUANT_TYPE>(reduced_data,         \
+                                                               reduced_scales,       \
+                                                               input_data,           \
+                                                               input_scales,         \
+                                                               out_groups,           \
+                                                               elems_per_out_group,  \
+                                                               elems_per_in_tensor,  \
+                                                               groups_per_in_tensor, \
+                                                               elems_per_in_group,   \
+                                                               num_gpus,             \
+                                                               stream);
+
+void launch_dequant_reduce(int8_t* reduced_data,
+                           float* reduced_scales,
+                           const int8_t* input_data,
+                           const float* input_scales,
+                           int num_gpus,
+                           int num_bits,
+                           quantize::Type quant_type,
+                           int out_groups,
+                           int elems_per_out_group,
+                           int elems_per_in_tensor,
+                           int groups_per_in_tensor,
+                           int elems_per_in_group,
+                           dpct::queue_ptr stream)
+{
+    if (quant_type == quantize::Type::Symmetric) {
+        if (num_bits == 4) {
+            if (num_gpus == 8) {
+                LAUNCH_DEQUANT_REDUCE_IMPL(4, 8, quantize::Type::Symmetric);
+            } else if (num_gpus == 16) {
+                LAUNCH_DEQUANT_REDUCE_IMPL(4, 16, quantize::Type::Symmetric);
+            } else {
+                LAUNCH_DEQUANT_REDUCE_IMPL(4, -1, quantize::Type::Symmetric);
+            }
+        } else if (num_bits == 8) {
+            if (num_gpus == 8) {
+                LAUNCH_DEQUANT_REDUCE_IMPL(8, 8, quantize::Type::Symmetric);
+            } else if (num_gpus == 16) {
+                LAUNCH_DEQUANT_REDUCE_IMPL(8, 16, quantize::Type::Symmetric);
+            } else {
+                LAUNCH_DEQUANT_REDUCE_IMPL(8, -1, quantize::Type::Symmetric);
+            }
+        }
+    } else if (quant_type == quantize::Type::Asymmetric) {
+        if (num_bits == 4) {
+            if (num_gpus == 8) {
+                LAUNCH_DEQUANT_REDUCE_IMPL(4, 8, quantize::Type::Asymmetric);
+            } else if (num_gpus == 16) {
+                LAUNCH_DEQUANT_REDUCE_IMPL(4, 16, quantize::Type::Asymmetric);
+            } else {
+                LAUNCH_DEQUANT_REDUCE_IMPL(4, -1, quantize::Type::Asymmetric);
+            }
+        } else if (num_bits == 8) {
+            if (num_gpus == 8) {
+                LAUNCH_DEQUANT_REDUCE_IMPL(8, 8, quantize::Type::Asymmetric);
+            } else if (num_gpus == 16) {
+                LAUNCH_DEQUANT_REDUCE_IMPL(8, 16, quantize::Type::Asymmetric);
+            } else {
+                LAUNCH_DEQUANT_REDUCE_IMPL(8, -1, quantize::Type::Asymmetric);
+            }
+        }
+    }
+}
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/quantization/quantize.dp.cpp b/intel_extension_for_deepspeed/op_builder/csrc/quantization/quantize.dp.cpp
new file mode 100644
index 0000000..286258b
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/quantization/quantize.dp.cpp
@@ -0,0 +1,175 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include "ds_kernel_utils.h"
+#include "memory_access_utils.h"
+#include "quantization.h"
+#include "quantization_utils.h"
+#include "reduction_utils.h"
+
+/*
+Pure quantization kernel with no fusion.
+*/
+template <int q_bits,
+          quantize::Type quant_type,
+          int UNROLL,
+          int internal_unroll,
+          int threads_per_group,
+          int max_threads>
+/*
+DPCT1110:46: The total declared local variable size in device function cached_quantization exceeds
+128 bytes and may cause high register pressure. Consult with your hardware vendor to find the total
+register size available and adjust the code, or use smaller sub-group size to avoid high register
+pressure.
+*/
+void cached_quantization(int8_t* __restrict__ output_data,
+                         float* __restrict__ params,
+                         const sycl::half* __restrict__ input_data,
+                         int groups,
+                         int elems_per_group)
+{
+    sycl::group<3> tb = sycl::ext::oneapi::experimental::this_group<3>();
+    sycl::sub_group warp = sycl::ext::oneapi::experimental::this_sub_group();
+
+    // Indexing offsets
+    const int block_offset =
+        (tb.get_group_id()[2] * (max_threads / threads_per_group) * elems_per_group) +
+        (tb.get_local_id()[1] * elems_per_group);
+    const int elem_offset = tb.get_local_id()[2] * quantize::h_per_load;
+    const int base_offset = block_offset + elem_offset;
+    const int stride = sycl::ext::oneapi::experimental::this_group<3>().get_local_linear_range() *
+                       quantize::h_per_load;
+
+    const sycl::half* input_base = input_data + base_offset;  //..
+
+    sycl::half2 local_buffer[UNROLL * internal_unroll * quantize::h2_per_load];
+
+#pragma unroll
+    for (int i = 0; i < UNROLL; i++) {
+        // Convenience helper, should resolve to register indices and not realize.
+        sycl::half2* iteration_buffer = local_buffer + i * internal_unroll * quantize::h2_per_load;
+#pragma unroll
+        for (int j = 0; j < internal_unroll; j++) {
+            const int iteration = i * internal_unroll + j;
+            mem_access::load_global<quantize::granularity>(
+                iteration_buffer + j * quantize::h2_per_load,
+                input_base + iteration * stride,
+                elem_offset + iteration * stride < elems_per_group);
+        }
+    }
+
+    quantize::
+        local_array<quant_type, q_bits, UNROLL * internal_unroll, threads_per_group, max_threads>(
+            local_buffer, params, output_data, elems_per_group, groups);
+}
+
+/********* Launcher methods ***********/
+/*
+DPCT1049:47: The work-group size passed to the SYCL kernel may exceed the limit. To get the device
+limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
+*/
+#define LAUNCH_CACHED_QUANT_CALL(q_bits, quant_type)                                           \
+ dpct::has_capability_or_fail(stream->get_device(), {sycl::aspect::fp64, sycl::aspect::fp16}); \
+ stream->submit([&](sycl::handler& cgh) {                                                      \
+  int8_t* output_data_ct0 = output_data;                                                       \
+  float* params_ct1 = params;                                                                  \
+  const sycl::half* input_data_ct2 = input_data;                                               \
+  int groups_ct3 = groups;                                                                     \
+  int elems_per_group_ct4 = elems_per_group;                                                   \
+                                                                                               \
+  cgh.parallel_for(                                                                            \
+      sycl::nd_range<3>(grid * block, block),                                                  \
+      [=](sycl::nd_item<3> item_ct1) [[intel::reqd_sub_group_size(32)]] {                      \
+       cached_quantization<q_bits,                                                             \
+                           quant_type,                                                         \
+                           unroll_factor,                                                      \
+                           internal_unroll_l,                                                  \
+                           threads_per_group,                                                  \
+                           max_threads>(                                                       \
+           output_data_ct0, params_ct1, input_data_ct2, groups_ct3, elems_per_group_ct4);      \
+      });                                                                                      \
+ });
+
+#define LAUNCH_CACHED_QUANT(                                                        \
+    q_bits, quant_type, unroll_factor_in, internal_unroll_in, threads_per_group_in) \
+    const int unroll_factor = unroll_factor_in;                                     \
+    const int internal_unroll_l = internal_unroll_in;                               \
+    const int threads_per_group = threads_per_group_in;                             \
+    if (q_bits == 4) {                                                              \
+        if (quant_type == quantize::Type::Asymmetric) {                             \
+            LAUNCH_CACHED_QUANT_CALL(4, quantize::Type::Asymmetric)                 \
+        } else {                                                                    \
+            LAUNCH_CACHED_QUANT_CALL(4, quantize::Type::Symmetric)                  \
+        }                                                                           \
+    } else {                                                                        \
+        if (quant_type == quantize::Type::Asymmetric) {                             \
+            LAUNCH_CACHED_QUANT_CALL(8, quantize::Type::Asymmetric)                 \
+        } else {                                                                    \
+            LAUNCH_CACHED_QUANT_CALL(8, quantize::Type::Symmetric)                  \
+        }                                                                           \
+    }
+
+void launch_quant(int8_t* output_data,
+                  float* params,
+                  const sycl::half* input_data,
+                  const int groups,
+                  const int elems_per_group,
+                  const int num_bits,
+                  const quantize::Type quant_type,
+                  dpct::queue_ptr stream)
+{
+    constexpr int max_threads = 256;
+
+    constexpr int internal_unroll = 2;
+
+    const bool is_subblock_schedule = (elems_per_group <= 128) ? true : false;
+    const int h_per_step = is_subblock_schedule ? quantize::h_per_load
+                                                : quantize::h_per_load * internal_unroll;
+
+    // Scheduling concern: may be slightly faster for some inputs to assign multiple stages of
+    // warp-sized blocks rather than stepping up to 64/96 threads
+    const int one_step_threads = next_pow2((elems_per_group + h_per_step - 1) / h_per_step);
+    const int threads_per_group = (one_step_threads < max_threads) ? one_step_threads : max_threads;
+
+    const int groups_per_block =
+        is_subblock_schedule ? (max_threads + threads_per_group - 1) / threads_per_group : 1;
+    const int groups_launch = (groups_per_block + groups - 1) / groups_per_block;
+
+    sycl::range<3> block(1, groups_per_block, threads_per_group);
+    sycl::range<3> grid(1, 1, groups_launch);
+
+    const int elems_per_step = threads_per_group * h_per_step;
+    const int external_unroll = (elems_per_group + elems_per_step - 1) / elems_per_step;
+
+    if (is_subblock_schedule) {
+        // <=128
+        if (threads_per_group == 1) {
+            LAUNCH_CACHED_QUANT(num_bits, quant_type, 1, 1, 1);
+        } else if (threads_per_group == 2) {
+            LAUNCH_CACHED_QUANT(num_bits, quant_type, 1, 1, 2);
+        } else if (threads_per_group == 4) {
+            LAUNCH_CACHED_QUANT(num_bits, quant_type, 1, 1, 4);
+        } else if (threads_per_group == 8) {
+            LAUNCH_CACHED_QUANT(num_bits, quant_type, 1, 1, 8);
+        } else if (threads_per_group == 16) {
+            LAUNCH_CACHED_QUANT(num_bits, quant_type, 1, 1, 16);
+        }
+    } else if (external_unroll == 1) {
+        // 129 - 4096 elems
+        // (this can launch with 1-7 warps as well)
+        LAUNCH_CACHED_QUANT(num_bits, quant_type, 1, internal_unroll, max_threads);
+    } else if (external_unroll == 2) {
+        // 4097 - 8192 elems
+        LAUNCH_CACHED_QUANT(num_bits, quant_type, 2, internal_unroll, max_threads);
+    } else if (external_unroll == 3) {
+        // 8193 - 12288 elems
+        LAUNCH_CACHED_QUANT(num_bits, quant_type, 3, internal_unroll, max_threads);
+    } else if (external_unroll == 4) {
+        // 12289 - 16384 elems
+        LAUNCH_CACHED_QUANT(num_bits, quant_type, 4, internal_unroll, max_threads);
+    }
+}
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/quantization/quantize_intX.dp.cpp b/intel_extension_for_deepspeed/op_builder/csrc/quantization/quantize_intX.dp.cpp
new file mode 100644
index 0000000..20ade04
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/quantization/quantize_intX.dp.cpp
@@ -0,0 +1,297 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include <assert.h>
+#include "memory_access_utils.h"
+#include <cmath>
+
+template <typename T, int N>
+struct alignas(sizeof(T) * N) AlignedArray {
+    using Element = T;
+    static const int kElements = N;
+
+    AlignedArray() {}
+
+    AlignedArray(const T& rhs)
+    {
+#pragma unroll
+        for (int idx = 0; idx < kElements; ++idx) { this->at(idx) = rhs; }
+    }
+
+    T& operator[](int offset)
+    {
+        return reinterpret_cast<T&>(this->buffer[offset]);
+    }
+
+    const T& operator[](int offset) const
+    {
+        return reinterpret_cast<const T&>(this->buffer[offset]);
+    }
+
+    T& at(int offset) { return reinterpret_cast<T&>(this->buffer[offset]); }
+
+    const T& at(int offset) const
+    {
+        return reinterpret_cast<const T&>(this->buffer[offset]);
+    }
+
+    AlignedArray<T, N> operator+(const AlignedArray<T, N>& rhs) const
+    {
+        AlignedArray<T, N> ret;
+
+#pragma unroll
+        for (int idx = 0; idx < kElements; ++idx) { ret[idx] = this->at(idx) + rhs.at(idx); }
+
+        return ret;
+    }
+
+    __dpct_inline__ void clear()
+    {
+#pragma unroll
+        for (int idx = 0; idx < kElements; ++idx) { this->at(idx) = Element(0); }
+    }
+
+    Element buffer[N];
+};
+
+template <typename T>
+struct reduce_max {
+    __dpct_inline__ T operator()(const T& lhs, const T& rhs)
+    {
+        return lhs > rhs ? lhs : rhs;
+    }
+};
+
+template <typename T>
+struct reduce_min {
+    __dpct_inline__ T operator()(const T& lhs, const T& rhs)
+    {
+        return lhs < rhs ? lhs : rhs;
+    }
+};
+
+template <typename T, int N>
+struct subtract {
+    __dpct_inline__ AlignedArray<T, N> operator()(const AlignedArray<T, N>& lhs, const T& rhs)
+    {
+        AlignedArray<T, N> ret;
+
+#pragma unroll
+        for (int idx = 0; idx < N; ++idx) { ret[idx] = lhs[idx] - rhs; }
+
+        return ret;
+    }
+};
+
+template <typename T, int N>
+struct plus {
+    __dpct_inline__ AlignedArray<T, N> operator()(const AlignedArray<T, N>& lhs, const T& rhs)
+    {
+        AlignedArray<T, N> ret;
+
+#pragma unroll
+        for (int idx = 0; idx < N; ++idx) { ret[idx] = lhs[idx] + rhs; }
+
+        return ret;
+    }
+};
+
+template <typename T, int N>
+struct multiply {
+    __dpct_inline__ AlignedArray<T, N> operator()(const AlignedArray<T, N>& lhs, const T& rhs)
+    {
+        AlignedArray<T, N> ret;
+
+#pragma unroll
+        for (int idx = 0; idx < N; ++idx) { ret[idx] = lhs[idx] * rhs; }
+
+        return ret;
+    }
+};
+
+template <typename T, int N>
+struct clamp {
+    __dpct_inline__ AlignedArray<T, N> operator()(const AlignedArray<T, N>& lhs,
+                                                  const T& min_val,
+                                                  const T& max_val)
+    {
+        AlignedArray<T, N> ret;
+
+#pragma unroll
+        for (int idx = 0; idx < N; ++idx) {
+            ret[idx] = reduce_max<T>()(reduce_min<T>()(lhs[idx], max_val), min_val);
+        }
+
+        return ret;
+    }
+};
+
+template <typename T, int N>
+struct round_int;
+
+template <int N>
+struct round_int<sycl::half, N> {
+    __dpct_inline__ AlignedArray<sycl::half, N> operator()(const AlignedArray<sycl::half, N>& lhs)
+    {
+        AlignedArray<sycl::half, N> ret;
+
+#pragma unroll
+        for (int idx = 0; idx < N; ++idx) { ret[idx] = hrint(lhs[idx]); }
+
+        return ret;
+    }
+};
+
+template <typename T, int N>
+struct divide {
+    __dpct_inline__ AlignedArray<T, N> operator()(const AlignedArray<T, N>& lhs, const T& rhs)
+    {
+        AlignedArray<T, N> ret;
+
+#pragma unroll
+        for (int idx = 0; idx < N; ++idx) { ret[idx] = lhs[idx] / rhs; }
+
+        return ret;
+    }
+};
+
+template <typename T, int N, typename Reducer>
+__dpct_inline__ T to_scalar(const AlignedArray<T, N>& data)
+{
+    Reducer re;
+    T res = data[0];
+
+#pragma unroll
+    for (int idx = 1; idx < N; ++idx) { res = re(res, data[idx]); }
+
+    return res;
+}
+
+template <int N>
+__dpct_inline__ AlignedArray<sycl::half, N * 2> int4_to_half(const AlignedArray<uint8_t, N>& data)
+{
+    AlignedArray<sycl::half, N * 2> ret;
+
+#pragma unroll
+    for (int idx = 0; idx < N * 2; idx += 2) {
+        ret[idx] = sycl::half(int(data[idx / 2] >> 4));
+        ret[idx + 1] = sycl::half(int(data[idx / 2] & 0xf));
+    }
+
+    return ret;
+}
+
+void dequantize_int4_to_half(uint8_t* data_in,
+                             sycl::half* data_out,
+                             sycl::half* scale_buffer,
+                             sycl::half* min_val_buffer,
+                             int num_group,
+                             int group_size)
+{
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    using AccessType = AlignedArray<uint8_t, 4>;
+    using AccessTypeOut = AlignedArray<sycl::half, 8>;
+
+    for (int idx = item_ct1.get_local_id(2) + item_ct1.get_group(2) * item_ct1.get_local_range(2);
+         idx < num_group * group_size / 8;
+         idx += item_ct1.get_local_range(2) * item_ct1.get_group_range(2)) {
+        int id_group = idx / (group_size / 8);
+        AccessType value = reinterpret_cast<AccessType*>(data_in)[idx];
+        sycl::half scale = scale_buffer[id_group];
+        sycl::half min_value = min_val_buffer[id_group];
+
+        AccessTypeOut output = int4_to_half(value);
+        output = divide<sycl::half, 8>()(output, scale);
+        output = plus<sycl::half, 8>()(output, min_value);
+
+        reinterpret_cast<AccessTypeOut*>(data_out)[idx] = output;
+    }
+}
+
+void launch_dequantize_int4_to_half_experimental(uint8_t* data_in,
+                                                 sycl::half* data_out,
+                                                 sycl::half* scale_buffer,
+                                                 sycl::half* min_val_buffer,
+                                                 int num_group,
+                                                 int group_size,
+                                                 dpct::queue_ptr stream)
+{
+    int num_warp = num_group / 4;
+    int num_block = num_warp / 8;  // 256 trd / block
+
+    {
+        dpct::has_capability_or_fail(stream->get_device(), {sycl::aspect::fp16});
+        stream->parallel_for(
+            sycl::nd_range<3>(sycl::range<3>(1, 1, num_block) * sycl::range<3>(1, 1, 256),
+                              sycl::range<3>(1, 1, 256)),
+            [=](sycl::nd_item<3> item_ct1) {
+                dequantize_int4_to_half(
+                    data_in, data_out, scale_buffer, min_val_buffer, num_group, group_size);
+            });
+    }
+}
+
+template <int N>
+__dpct_inline__ AlignedArray<sycl::half, N> int8_to_half(const AlignedArray<uint8_t, N>& data)
+{
+    AlignedArray<sycl::half, N> ret;
+
+#pragma unroll
+    for (int idx = 0; idx < N; idx += 1) { ret[idx] = sycl::half(int(data[idx])); }
+
+    return ret;
+}
+
+void dequantize_int8_to_half(uint8_t* data_in,
+                             sycl::half* data_out,
+                             sycl::half* scale_buffer,
+                             sycl::half* min_val_buffer,
+                             int num_group,
+                             int group_size)
+{
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    using AccessType = AlignedArray<uint8_t, 8>;
+    using AccessTypeOut = AlignedArray<sycl::half, 8>;
+
+    for (int idx = item_ct1.get_local_id(2) + item_ct1.get_group(2) * item_ct1.get_local_range(2);
+         idx < num_group * group_size / 8;
+         idx += item_ct1.get_local_range(2) * item_ct1.get_group_range(2)) {
+        int id_group = idx / (group_size / 8);
+        AccessType value = reinterpret_cast<AccessType*>(data_in)[idx];
+        sycl::half scale = scale_buffer[id_group];
+        sycl::half min_value = min_val_buffer[id_group];
+
+        AccessTypeOut output = int8_to_half(value);
+        output = divide<sycl::half, 8>()(output, scale);
+        output = plus<sycl::half, 8>()(output, min_value);
+
+        reinterpret_cast<AccessTypeOut*>(data_out)[idx] = output;
+    }
+}
+
+void launch_dequantize_int8_to_half_experimental(uint8_t* data_in,
+                                                 sycl::half* data_out,
+                                                 sycl::half* scale_buffer,
+                                                 sycl::half* min_val_buffer,
+                                                 int num_group,
+                                                 int group_size,
+                                                 dpct::queue_ptr stream)
+{
+    int num_warp = num_group / 4;
+    int num_block = num_warp / 8;  // 256 trd / block
+
+    {
+        dpct::has_capability_or_fail(stream->get_device(), {sycl::aspect::fp16});
+        stream->parallel_for(
+            sycl::nd_range<3>(sycl::range<3>(1, 1, num_block) * sycl::range<3>(1, 1, 256),
+                              sycl::range<3>(1, 1, 256)),
+            [=](sycl::nd_item<3> item_ct1) {
+                dequantize_int8_to_half(
+                    data_in, data_out, scale_buffer, min_val_buffer, num_group, group_size);
+            });
+    }
+}
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/quantization/swizzled_quantize.dp.cpp b/intel_extension_for_deepspeed/op_builder/csrc/quantization/swizzled_quantize.dp.cpp
new file mode 100644
index 0000000..2863276
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/quantization/swizzled_quantize.dp.cpp
@@ -0,0 +1,233 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include "memory_access_utils.h"
+#include "quantization_utils.h"
+#include "reduction_utils.h"
+
+using rop = reduce::ROpType;
+
+namespace swiz_quant {
+constexpr int max_threads = 512;
+constexpr int min_threads = 32;
+
+constexpr int step_granularity = 2;
+constexpr int h_per_step = step_granularity * quantize::h_per_load;
+}  // namespace swiz_quant
+
+template <int numBits, int totalChunks, int threads, quantize::Type quantType>
+/*
+DPCT1110:46: The total declared local variable size in device function swizzled_quant_kernel exceeds
+128 bytes and may cause high register pressure. Consult with your hardware vendor to find the total
+register size available and adjust the code, or use smaller sub-group size to avoid high register
+pressure.
+*/
+void swizzled_quant_kernel(int8_t* quantized_data,
+                           float* quantized_scales,
+                           const sycl::half* uncompressed_data,
+                           int elems_per_group,
+                           int nodes,
+                           int devices_per_node)
+{
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    sycl::group<3> tb = sycl::ext::oneapi::experimental::this_group<3>();
+    sycl::sub_group warp = sycl::ext::oneapi::experimental::this_sub_group();
+
+    // Indexing offsets, same as normal quantization for in-case
+    const int block_rank =
+        item_ct1.get_group(2) + item_ct1.get_group(1) * item_ct1.get_group_range(2) +
+        item_ct1.get_group(0) * item_ct1.get_group_range(2) * item_ct1.get_group_range(1);
+    const int block_offset = block_rank * elems_per_group;
+    const int elem_offset = tb.get_local_id()[2] * quantize::h_per_load;
+    const int base_offset = block_offset + elem_offset;
+    const int stride = sycl::ext::oneapi::experimental::this_group<3>().get_local_linear_range() *
+                       quantize::h_per_load;
+    const sycl::half* input_base = uncompressed_data + base_offset;
+
+    // Local buffer
+    sycl::half2 local_buffer[totalChunks * quantize::h2_per_load];
+
+    quantize::GroupStats<quantType> stats;
+#pragma unroll
+    for (int i = 0; i < totalChunks; i++) {
+        sycl::half2* iteration_buffer = local_buffer + i * quantize::h2_per_load;
+
+        mem_access::load_global<quantize::granularity>(
+            iteration_buffer, input_base + i * stride, elem_offset + i * stride < elems_per_group);
+
+#pragma unroll
+        for (int j = 0; j < quantize::h2_per_load; j++) { stats.update(iteration_buffer[j]); }
+    }
+
+    auto params = stats.template get_params<numBits, threads>(tb, warp);
+
+    const int partition_id = item_ct1.get_group(0);
+    const int partition_offset = partition_id / devices_per_node;
+    const int partition_base = (partition_id % devices_per_node) * nodes;
+    const int pipelining_offset = item_ct1.get_group(1) * (devices_per_node * nodes);
+    const int output_partition = (pipelining_offset + partition_base + partition_offset);
+
+    constexpr int out_scalar_effect = 8 / numBits;
+    const int out_block_rank =
+        output_partition * item_ct1.get_group_range(2) + item_ct1.get_group(2);
+    const int out_block_offset = out_block_rank * elems_per_group / out_scalar_effect;
+    const int out_base_offset = out_block_offset + elem_offset / out_scalar_effect;
+    int8_t* out_base = quantized_data + out_base_offset;
+
+    const int out_stride = stride / out_scalar_effect;
+    constexpr int num_int8_out = quantize::h_per_load / out_scalar_effect;
+
+    if (tb.get_local_id()[2] == 0) { params.store(quantized_scales, out_block_rank); }
+
+#pragma unroll
+    for (int i = 0; i < totalChunks; i++) {
+        if (i * stride + elem_offset < elems_per_group) {
+            int8_t local_output[quantize::h_per_load / out_scalar_effect];
+            quantize::_chunk<numBits, quantType>(
+                local_output, local_buffer + i * quantize::h2_per_load, params);
+            mem_access::store_global<num_int8_out>(out_base + i * out_stride, local_output);
+        }
+    }
+}
+
+/*
+DPCT1049:47: The work-group size passed to the SYCL kernel may exceed the limit. To get the device
+limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
+*/
+#define LAUNCH_SWIZZLE_QUANT(total_chunks, threads)                                               \
+  {                                                                                               \
+    dpct::has_capability_or_fail(stream->get_device(), {sycl::aspect::fp64, sycl::aspect::fp16}); \
+    stream->submit([&](sycl::handler& cgh) {                                                      \
+      int8_t* q_data_ct0 = q_data;                                                                \
+      float* q_scales_ct1 = q_scales;                                                             \
+      const sycl::half* input_data_ct2 = input_data;                                              \
+      auto elems_per_group_ct3 = elems_per_group;                                                 \
+      auto nodes_ct4 = nodes;                                                                     \
+      auto devices_per_node_ct5 = devices_per_node;                                               \
+                                                                                                  \
+      cgh.parallel_for(sycl::nd_range<3>(grid * block, block),                                    \
+                       [=](sycl::nd_item<3> item_ct1) [[intel::reqd_sub_group_size(32)]] {        \
+                         swizzled_quant_kernel<numBits, total_chunks, threads, qType>(            \
+                             q_data_ct0,                                                          \
+                             q_scales_ct1,                                                        \
+                             input_data_ct2,                                                      \
+                             elems_per_group_ct3,                                                 \
+                             nodes_ct4,                                                           \
+                             devices_per_node_ct5);                                               \
+                       });                                                                        \
+    });                                                                                           \
+  }
+
+/*
+Swizzled quantization reorganizes the quantized groups in order to better facilitate
+communication. As an example of the partitioning scheme we have the following example
+of 2 node, 4 device swizzling:
+
+ --- --- --- --- --- --- --- ---
+| 0 | 1 | 2 | 3 | 4 | 5 | 6 | 7 |
+ --- --- --- --- --- --- --- ---
+becomes
+ --- --- --- --- --- --- --- ---
+| 0 | 4 | 1 | 5 | 2 | 6 | 3 | 7 |
+ --- --- --- --- --- --- --- ---
+
+Multiple quantization groups may be mapped into a single partition. In order to better support
+later pipelining, we may also perform an additional slicing. In two-way slicing, for instance,
+the first halves of each partition are concatenated.
+*/
+
+template <int numBits, quantize::Type qType>
+void launch_swizzled_quant_impl(int8_t* q_data,
+                                float* q_scales,
+                                const sycl::half* input_data,
+                                int groups,
+                                int elems_per_group,
+                                int pipelining,
+                                int nodes,
+                                int devices_per_node,
+                                dpct::queue_ptr stream)
+{
+    const int one_step_threads =
+        next_pow2((elems_per_group + swiz_quant::h_per_step - 1) / swiz_quant::h_per_step);
+    const int max_threads = (one_step_threads < swiz_quant::max_threads) ? one_step_threads
+                                                                         : swiz_quant::max_threads;
+    const int threads = (max_threads < swiz_quant::min_threads) ? swiz_quant::min_threads
+                                                                : max_threads;
+
+    sycl::range<3> block(1, 1, threads);
+    const int groups_per_partition = groups / (nodes * devices_per_node);
+    assert(groups_per_partition % pipelining == 0);
+    const int contiguous_groups = groups_per_partition / pipelining;
+    const int partitions = nodes * devices_per_node;
+    sycl::range<3> grid(partitions, pipelining, contiguous_groups);
+
+    const int elems_per_step = threads * swiz_quant::h_per_step;
+    const int external_unroll = ((elems_per_group + elems_per_step - 1) / elems_per_step);
+    const int total_unroll = external_unroll * swiz_quant::step_granularity;
+
+    assert(total_unroll % 2 == 0);
+
+    if (threads == 32) {
+        LAUNCH_SWIZZLE_QUANT(2, 32);
+    } else if (threads == 64) {
+        LAUNCH_SWIZZLE_QUANT(2, 64);
+    } else if (threads == 128) {
+        LAUNCH_SWIZZLE_QUANT(2, 128);
+    } else if (threads == 256) {
+        LAUNCH_SWIZZLE_QUANT(2, 256);
+    } else if (threads == 512) {
+        if (total_unroll == 2) {
+            LAUNCH_SWIZZLE_QUANT(2, 512);
+        } else if (total_unroll == 4) {
+            LAUNCH_SWIZZLE_QUANT(4, 512);
+        } else if (total_unroll == 6) {
+            LAUNCH_SWIZZLE_QUANT(6, 512);
+        } else if (total_unroll == 8) {
+            LAUNCH_SWIZZLE_QUANT(8, 512);
+        } else if (total_unroll == 10) {
+            LAUNCH_SWIZZLE_QUANT(10, 512);
+        }
+    }
+}
+
+#define DISPATCH_SWIZZLE_QUANT(num_bits, qtype)                   \
+    launch_swizzled_quant_impl<num_bits, qtype>(q_data,           \
+                                                q_scales,         \
+                                                input_data,       \
+                                                groups,           \
+                                                elems_per_group,  \
+                                                pipelining,       \
+                                                nodes,            \
+                                                devices_per_node, \
+                                                stream);
+
+void launch_swizzled_quant(int8_t* q_data,
+                           float* q_scales,
+                           const sycl::half* input_data,
+                           int num_bits,
+                           quantize::Type q_type,
+                           int groups,
+                           int elems_per_group,
+                           int pipelining,
+                           int nodes,
+                           int devices_per_node,
+                           dpct::queue_ptr stream)
+{
+    if (num_bits == 4) {
+        if (q_type == quantize::Type::Asymmetric) {
+            DISPATCH_SWIZZLE_QUANT(4, quantize::Type::Asymmetric);
+        } else if (q_type == quantize::Type::Symmetric) {
+            DISPATCH_SWIZZLE_QUANT(4, quantize::Type::Symmetric);
+        }
+    } else if (num_bits == 8) {
+        if (q_type == quantize::Type::Asymmetric) {
+            DISPATCH_SWIZZLE_QUANT(8, quantize::Type::Asymmetric);
+        } else if (q_type == quantize::Type::Symmetric) {
+            DISPATCH_SWIZZLE_QUANT(8, quantize::Type::Symmetric);
+        }
+    }
+}
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/dropout_kernels.dp.cpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/dropout_kernels.dp.cpp
deleted file mode 100644
index 25e5e0f..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/transformer/dropout_kernels.dp.cpp
+++ /dev/null
@@ -1,1201 +0,0 @@
-#if __has_include(<sycl/sycl.hpp>)
-#include <sycl/sycl.hpp>
-using namespace sycl;
-#elif __has_include(<CL/sycl.hpp>)
-#include <CL/sycl.hpp>
-using namespace cl::sycl;
-#else
-#error "Unsupported compiler"
-#endif
-#include "custom_sycl_layers.hpp"
-
-const int unroll_factor = 4;
-
-void dropout_kernel(const int N,
-                    const float ratio,
-                    float* out,
-                    const float* Xdata,
-                    uint8_t* mask,
-                    const std::pair<uint64_t, uint64_t>& seed,
-                    nd_item<3> item_ct1)
-{
-    const float scale = 1. / (1. - ratio);
-    size_t idx = item_ct1.get_group(2) * item_ct1.get_local_range(2) + item_ct1.get_local_id(2);
-
-    oneapi::mkl::rng::device::philox4x32x10<4> engine(seed.first, {idx * 4, seed.second});
-    oneapi::mkl::rng::device::uniform<> distr;
-
-    DPCPP_1D_KERNEL_LOOP(j, N / unroll_factor)
-    {
-        float4 rand = oneapi::mkl::rng::device::generate(distr, engine);
-        uint8_t m[unroll_factor];
-
-        m[0] = (uint8_t)(rand.x() > ratio);
-        m[1] = (uint8_t)(rand.y() > ratio);
-        m[2] = (uint8_t)(rand.z() > ratio);
-        m[3] = (uint8_t)(rand.w() > ratio);
-
-        int i = j * unroll_factor;
-
-        mask[i] = (uint8_t)m[0];
-        mask[i + 1] = (uint8_t)m[1];
-        mask[i + 2] = (uint8_t)m[2];
-        mask[i + 3] = (uint8_t)m[3];
-
-        out[i] = Xdata[i] * scale * m[0];
-        out[i + 1] = Xdata[i + 1] * scale * m[1];
-        out[i + 2] = Xdata[i + 2] * scale * m[2];
-        out[i + 3] = Xdata[i + 3] * scale * m[3];
-    }
-    int high_index = ((((N / unroll_factor) - 1) / item_ct1.get_local_range().get(2) + 1) *
-                      (unroll_factor * item_ct1.get_local_range().get(2))) +
-                     item_ct1.get_local_id(2);
-    if (N > high_index) {
-        float4 rand = oneapi::mkl::rng::device::generate(distr, engine);
-        float* rand_data = &(rand.x());
-        int k = 0;
-        for (int i = high_index; i < N; i++) {
-            uint8_t m = (uint8_t)(rand_data[k++] > ratio);
-            out[i] = Xdata[i] * scale * m;
-            mask[i] = m;
-        }
-    }
-}
-
-void dropout_kernel(const int N,
-                    const float ratio,
-                    bf16* out,
-                    const bf16* Xdata,
-                    uint8_t* mask,
-                    const std::pair<uint64_t, uint64_t>& seed,
-                    nd_item<3> item_ct1)
-{
-    const float scale = 1. / (1. - ratio);
-    size_t idx = item_ct1.get_group(2) * item_ct1.get_local_range(2) + item_ct1.get_local_id(2);
-
-    oneapi::mkl::rng::device::philox4x32x10<4> engine(seed.first, {idx * 4, seed.second});
-    oneapi::mkl::rng::device::uniform<> distr;
-
-    ushort* out_cast = reinterpret_cast<ushort*>(out);
-    const ushort* Xdata_cast = reinterpret_cast<const ushort*>(Xdata);
-
-    DPCPP_1D_KERNEL_LOOP(j, N / unroll_factor)
-    {
-        float4 rand = oneapi::mkl::rng::device::generate(distr, engine);
-        uint8_t m[unroll_factor];
-
-        m[0] = (uint8_t)(rand.x() > ratio);
-        m[1] = (uint8_t)(rand.y() > ratio);
-        m[2] = (uint8_t)(rand.z() > ratio);
-        m[3] = (uint8_t)(rand.w() > ratio);
-
-        int i = j * unroll_factor;
-
-        mask[i] = (uint8_t)m[0];
-        mask[i + 1] = (uint8_t)m[1];
-        mask[i + 2] = (uint8_t)m[2];
-        mask[i + 3] = (uint8_t)m[3];
-
-        out_cast[i] = bf16(float(Xdata_cast[i]) * scale * m[0]);
-        out_cast[i + 1] = bf16(float(Xdata_cast[i + 1]) * scale * m[1]);
-        out_cast[i + 2] = bf16(float(Xdata_cast[i + 2]) * scale * m[2]);
-        out_cast[i + 3] = bf16(float(Xdata_cast[i + 3]) * scale * m[3]);
-    }
-    int high_index = ((((N / unroll_factor) - 1) / item_ct1.get_local_range().get(2) + 1) *
-                      (unroll_factor * item_ct1.get_local_range().get(2))) +
-                     item_ct1.get_local_id(2);
-    if (N > high_index) {
-        float4 rand = oneapi::mkl::rng::device::generate(distr, engine);
-        float* rand_data = &(rand.x());
-        int k = 0;
-        for (int i = high_index; i < N; i++) {
-            uint8_t m = (uint8_t)(rand_data[k++] > ratio);
-            out_cast[i] = bf16(float(Xdata_cast[i]) * scale * m);
-            mask[i] = m;
-        }
-    }
-}
-
-void dropout_kernel(const int N,
-                    const float ratio,
-                    half* out,
-                    const half* Xdata,
-                    uint8_t* mask,
-                    const std::pair<uint64_t, uint64_t>& seed,
-                    nd_item<3> item_ct1)
-{
-    const float scale = 1. / (1. - ratio);
-
-    size_t idx = item_ct1.get_group(2) * item_ct1.get_local_range(2) + item_ct1.get_local_id(2);
-
-    oneapi::mkl::rng::device::philox4x32x10<4> engine(seed.first, {idx * 4, seed.second});
-    oneapi::mkl::rng::device::uniform<> distr;
-
-#ifdef __STOCHASTIC_MODE__
-
-    const half2 h_scale = vec<float, 2>{scale}.convert<float>();
-    const float2* x_cast = reinterpret_cast<const float2*>(Xdata);
-    float2* out_cast = reinterpret_cast<float2*>(out);
-    uint32_t* mask_cast = reinterpret_cast<uint32_t*>(mask);
-
-    uint32_t m_32;
-    uint8_t* m = reinterpret_cast<uint8_t*>(&m_32);
-
-    float2 result_f;
-    half2* result_h = reinterpret_cast<half2*>(&result_f);
-    half2 mask_h[2];
-    float2 mask_f[2];
-
-    DPCPP_1D_KERNEL_LOOP(j, N / unroll_factor)
-    {
-        float2 x_f = x_cast[j];
-        half2* x_h = reinterpret_cast<half2*>(&x_f);
-
-        float4 rand = oneapi::mkl::rng::device::generate(distr, engine);
-
-        m[0] = (uint8_t)(rand.x > ratio);
-        m[1] = (uint8_t)(rand.y > ratio);
-        m[2] = (uint8_t)(rand.z > ratio);
-        m[3] = (uint8_t)(rand.w > ratio);
-
-        float* mask_f_data = &mask_f[0].x;
-#pragma unroll
-        for (int i = 0; i < unroll_factor; i++) mask_f_data[i] = (float)(m[i]);
-
-        mask_h[0] = mask_f[0].convert<half>();
-        mask_h[1] = mask_f[1].convert<half> 9;
-
-        result_h[0] = x_h[0] * h_scale * mask_h[0];
-        result_h[1] = x_h[1] * h_scale * mask_h[1];
-
-        out_cast[j] = result_f;
-
-        mask_cast[j] = m_32;
-    }
-
-#else
-
-    DPCPP_1D_KERNEL_LOOP(j, N / unroll_factor)
-    {
-        int i = j * unroll_factor;
-
-        const half2* vals_half = reinterpret_cast<const half2*>(Xdata + i);
-        float2 vals_half_f[2];
-        vals_half_f[0] = vals_half[0].convert<float, rounding_mode::automatic>();
-        vals_half_f[1] = vals_half[1].convert<float, rounding_mode::automatic>();
-
-        uint8_t m[unroll_factor];
-        float4 rand = oneapi::mkl::rng::device::generate(distr, engine);
-        m[0] = (uint8_t)(rand.x() > ratio);
-        m[1] = (uint8_t)(rand.y() > ratio);
-        m[2] = (uint8_t)(rand.z() > ratio);
-        m[3] = (uint8_t)(rand.w() > ratio);
-
-        out[i] = vec<float, 1>{vals_half_f[0].x() * scale * m[0]}
-                     .convert<half, rounding_mode::automatic>()[0];
-        out[i + 1] = vec<float, 1>{vals_half_f[0].y() * scale * m[1]}
-                         .convert<half, rounding_mode::automatic>()[0];
-        out[i + 2] = vec<float, 1>{vals_half_f[1].x() * scale * m[2]}
-                         .convert<half, rounding_mode::automatic>()[0];
-        out[i + 3] = vec<float, 1>{vals_half_f[1].y() * scale * m[3]}
-                         .convert<half, rounding_mode::automatic>()[0];
-
-        mask[i] = m[0];
-        mask[i + 1] = m[1];
-        mask[i + 2] = m[2];
-        mask[i + 3] = m[3];
-    }
-
-#endif
-    int high_index = ((((N / unroll_factor) - 1) / item_ct1.get_local_range().get(2) + 1) *
-                      (unroll_factor * item_ct1.get_local_range().get(2))) +
-                     item_ct1.get_local_id(2);
-    if (N > high_index) {
-        float4 rand = oneapi::mkl::rng::device::generate(distr, engine);
-        float* rand_data = &(rand.x());
-        int k = 0;
-        for (int i = high_index; i < N; i++) {
-            uint8_t m = (uint8_t)(rand_data[k++] > ratio);
-            out[i] = vec<float, 1>{(float)Xdata[i] * scale * m}
-                         .convert<half, rounding_mode::automatic>()[0];
-            mask[i] = m;
-        }
-    }
-}
-
-void dropout_kernel_bwd(const int N,
-                        const float ratio,
-                        const float* Xdata,
-                        float* out,
-                        uint8_t* mask,
-                        const std::pair<uint64_t, uint64_t>& seed,
-                        nd_item<3> item_ct1)
-{
-    const float scale = 1. / (1. - ratio);
-    DPCPP_1D_KERNEL_LOOP(j, N / unroll_factor)
-    {
-        int i = j * unroll_factor;
-
-        out[i] = mask[i] * Xdata[i] * scale;
-        out[i + 1] = mask[i + 1] * Xdata[i + 1] * scale;
-        out[i + 2] = mask[i + 2] * Xdata[i + 2] * scale;
-        out[i + 3] = mask[i + 3] * Xdata[i + 3] * scale;
-    }
-    int high_index = ((((N / unroll_factor) - 1) / item_ct1.get_local_range().get(2) + 1) *
-                      (unroll_factor * item_ct1.get_local_range().get(2))) +
-                     item_ct1.get_local_id(2);
-    if (N > high_index) {
-        for (int i = high_index; i < N; i++) { out[i] = mask[i] * Xdata[i] * scale; }
-    }
-}
-
-void dropout_kernel_bwd(const int N,
-                        const float ratio,
-                        const bf16* Xdata,
-                        bf16* out,
-                        uint8_t* mask,
-                        const std::pair<uint64_t, uint64_t>& seed,
-                        nd_item<3> item_ct1)
-{
-    const float scale = 1. / (1. - ratio);
-
-    const ushort* Xdata_cast = reinterpret_cast<const ushort*>(Xdata);
-    ushort* out_cast = reinterpret_cast<ushort*>(out);
-
-    DPCPP_1D_KERNEL_LOOP(j, N / unroll_factor)
-    {
-        int i = j * unroll_factor;
-
-        out_cast[i] = bf16(mask[i] * float(Xdata_cast[i]) * scale);
-        out_cast[i + 1] = bf16(mask[i + 1] * float(Xdata_cast[i + 1]) * scale);
-        out_cast[i + 2] = bf16(mask[i + 2] * float(Xdata_cast[i + 2]) * scale);
-        out_cast[i + 3] = bf16(mask[i + 3] * float(Xdata_cast[i + 3]) * scale);
-    }
-    int high_index = ((((N / unroll_factor) - 1) / item_ct1.get_local_range().get(2) + 1) *
-                      (unroll_factor * item_ct1.get_local_range().get(2))) +
-                     item_ct1.get_local_id(2);
-    if (N > high_index) {
-        for (int i = high_index; i < N; i++) {
-            out_cast[i] = bf16(mask[i] * float(Xdata_cast[i]) * scale);
-        }
-    }
-}
-
-void dropout_kernel_bwd(const int N,
-                        const float ratio,
-                        const half* Xdata,
-                        half* out,
-                        uint8_t* mask,
-                        const std::pair<uint64_t, uint64_t>& seed,
-                        nd_item<3> item_ct1)
-{
-    const float scale = 1. / (1. - ratio);
-
-#ifdef __STOCHASTIC_MODE__
-
-    const half2 h_scale = vec<float, 2>{scale}.convert<half>();
-
-    const float2* x_cast = reinterpret_cast<const float2*>(Xdata);
-    float2* out_cast = reinterpret_cast<float2*>(out);
-    uint32_t* mask_cast = reinterpret_cast<uint32_t*>(mask);
-
-    DPCPP_1D_KERNEL_LOOP(j, N / unroll_factor)
-    {
-        float2 x_f = x_cast[j];
-        half2* x_h = reinterpret_cast<half2*>(&x_f);
-
-        uint32_t m_32 = mask_cast[j];
-        uint8_t* m = (uint8_t*)&m_32;
-
-        half2 mask_h[2];
-        float2 mask_f[2];
-
-        float* mask_f_data = &mask_f[0].x;
-#pragma unroll
-        for (int i = 0; i < unroll_factor; i++) mask_f_data[i] = (float)(m[i]);
-
-#pragma unroll
-        for (int i = 0; i < 2; i++) mask_h[i] = __float22half2_rn(mask_f[i]);
-
-        float2 result_f;
-        half2* result_h = reinterpret_cast<half2*>(&result_f);
-
-        result_h[0] = x_h[0] * h_scale * mask_h[0];
-        result_h[1] = x_h[1] * h_scale * mask_h[1];
-
-        out_cast[j] = result_f;
-    }
-
-#else
-
-    const half h_scale = vec<float, 1>{scale}.convert<half, rounding_mode::automatic>()[0];
-    const half h_zero = vec<float, 1>{0.0}.convert<half, rounding_mode::automatic>()[0];
-
-    DPCPP_1D_KERNEL_LOOP(j, N / unroll_factor)
-    {
-        int i = j * unroll_factor;
-
-        const half2* vals_half = reinterpret_cast<const half2*>(Xdata + i);
-
-        uint8_t* m = mask + i;
-
-        float2 vals_half_f[2];
-
-        vals_half_f[0] = vals_half[0].convert<float, rounding_mode::automatic>();
-        vals_half_f[1] = vals_half[1].convert<float, rounding_mode::automatic>();
-
-        out[i] = vec<float, 1>{vals_half_f[0].x() * scale * m[0]}
-                     .convert<half, rounding_mode::automatic>()[0];
-        out[i + 1] = vec<float, 1>{vals_half_f[0].y() * scale * m[1]}
-                         .convert<half, rounding_mode::automatic>()[0];
-        out[i + 2] = vec<float, 1>{vals_half_f[1].x() * scale * m[2]}
-                         .convert<half, rounding_mode::automatic>()[0];
-        out[i + 3] = vec<float, 1>{vals_half_f[1].y() * scale * m[3]}
-                         .convert<half, rounding_mode::automatic>()[0];
-    }
-
-#endif
-    int high_index = ((((N / unroll_factor) - 1) / item_ct1.get_local_range().get(2) + 1) *
-                      (unroll_factor * item_ct1.get_local_range().get(2))) +
-                     item_ct1.get_local_id(2);
-    if (N > high_index) {
-        for (int i = high_index; i < N; i++) {
-            out[i] = vec<float, 1>{(float)Xdata[i] * scale * mask[i]}
-                         .convert<half, rounding_mode::automatic>()[0];
-        }
-    }
-}
-
-template <typename T>
-void launch_dropout(T* out,
-                    const T* vals,
-                    uint8_t* mask,
-                    int total_count,
-                    int dim,
-                    float ratio,
-                    queue* stream,
-                    bool bwd)
-{
-    /*
-     * dropout.Forward
-     */
-    assert(unroll_factor == 4);
-
-    range<3> grid_dim = range<3>(1, 1, DS_GET_BLOCKS(total_count / unroll_factor));
-    range<3> block_dim = range<3>(1, 1, DS_CUDA_NUM_THREADS);
-
-    if (dim > 512) {
-        block_dim[2] >>= 1;
-        grid_dim[2] <<= 1;
-    }
-    uint64_t inc = total_count / grid_dim[2] / block_dim[2];
-    std::pair<uint64_t, uint64_t> seed = SyclContext::Instance().IncrementOffset(inc);
-    if (bwd)
-        stream->submit([&](handler& cgh) {
-            cgh.parallel_for(
-                nd_range<3>(grid_dim * block_dim, block_dim), [=](nd_item<3> item_ct1) {
-                    dropout_kernel_bwd(total_count, ratio, vals, out, mask, seed, item_ct1);
-                });
-        });
-    else
-        stream->submit([&](handler& cgh) {
-            cgh.parallel_for(
-                nd_range<3>(grid_dim * block_dim, block_dim), [=](nd_item<3> item_ct1) {
-                    dropout_kernel(total_count, ratio, out, vals, mask, seed, item_ct1);
-                });
-        });
-}
-
-template void launch_dropout(float* out,
-                             const float* vals,
-                             uint8_t* mask,
-                             int total_count,
-                             int dim,
-                             float ratio,
-                             queue* stream,
-                             bool);
-template void launch_dropout(bf16* out,
-                             const bf16* vals,
-                             uint8_t* mask,
-                             int total_count,
-                             int dim,
-                             float ratio,
-                             queue* stream,
-                             bool);
-template void launch_dropout(half* out,
-                             const half* vals,
-                             uint8_t* mask,
-                             int total_count,
-                             int dim,
-                             float ratio,
-                             queue* stream,
-                             bool);
-
-void dropout_grad_kernel(const int N,
-                         const float scale,
-                         float* Xdata,
-                         uint8_t* mask,
-                         nd_item<3> item_ct1)
-{
-    DPCPP_1D_KERNEL_LOOP(i, N) { Xdata[i] *= scale * mask[i]; }
-}
-
-void dropout_grad_kernel(const int N,
-                         const float scale,
-                         bf16* Xdata,
-                         uint8_t* mask,
-                         nd_item<3> item_ct1)
-{
-    ushort* Xdata_cast = reinterpret_cast<ushort*>(Xdata);
-    DPCPP_1D_KERNEL_LOOP(i, N)
-    {
-        Xdata_cast[i] = bf16(float(Xdata_cast[i]) * scale * mask[i]);
-    }
-}
-
-void dropout_grad_kernel(const int N,
-                         const float scale,
-                         half* Xdata,
-                         uint8_t* mask,
-                         nd_item<3> item_ct1)
-{
-    const half2 h_scale = float2{scale, scale}.convert<half, rounding_mode::rte>();
-    float2* x_cast = reinterpret_cast<float2*>(Xdata);
-    uint32_t* mask_cast = reinterpret_cast<uint32_t*>(mask);
-
-    DPCPP_1D_KERNEL_LOOP(j, N / unroll_factor)
-    {
-        float2 x_data = x_cast[j];
-        uint32_t m_32 = mask_cast[j];
-        uint8_t* m = (uint8_t*)&m_32;
-
-        float2 result_f;
-        half2* result_h = reinterpret_cast<half2*>(&result_f);
-
-#ifdef __STOCHASTIC_MODE__
-
-        half2* x_data_h = reinterpret_cast<half2*>(&x_data);
-        half2 mask_h[2];
-        float2 mask_f[2];
-
-        float* mask_f_data = &mask_f[0].x;
-#pragma unroll
-        for (int i = 0; i < unroll_factor; i++) *(mask_f_data++) = (float)(m[i]);
-
-        mask_h[0] = __float22half2_rn(mask_f[0]);
-        mask_h[1] = __float22half2_rn(mask_f[1]);
-
-        result_h[0] = x_data_h[0] * h_scale * mask_h[0];
-        result_h[1] = x_data_h[1] * h_scale * mask_h[1];
-
-#else
-
-        half* x_data_h = reinterpret_cast<half*>(&x_data);
-        float2 result[2];
-
-        result[0].x() = (float)x_data_h[0] * scale * m[0];
-        result[0].y() = (float)x_data_h[1] * scale * m[1];
-        result[1].x() = (float)x_data_h[2] * scale * m[2];
-        result[1].y() = (float)x_data_h[3] * scale * m[3];
-
-        result_h[0] = result[0].convert<half, rounding_mode::rte>();
-        result_h[1] = result[1].convert<half, rounding_mode::rte>();
-
-#endif
-        x_cast[j] = result_f;
-    }
-    int high_index = ((((N / unroll_factor) - 1) / item_ct1.get_local_range().get(2) + 1) *
-                      (unroll_factor * item_ct1.get_local_range().get(2))) +
-                     item_ct1.get_local_id(2);
-    if (N > high_index) {
-        for (int i = high_index; i < N; i++) {
-            Xdata[i] = vec<float, 1>{(float)Xdata[i] * scale * mask[i]}
-                           .convert<half, rounding_mode::automatic>()[0];
-        }
-    }
-}
-
-template <typename T>
-void launch_dropout_grad(T* vals, uint8_t* mask, int total_count, float ratio, queue* stream)
-{
-    /*
-     * Dropout.Backward0
-     */
-    assert(unroll_factor == 4);
-
-    const float scale = 1. / (1. - ratio);
-    range<3> grid_dim = range<3>(1, 1, DS_GET_BLOCKS(total_count / unroll_factor));
-    range<3> block_dim = range<3>(1, 1, DS_CUDA_NUM_THREADS);
-    stream->submit([&](handler& cgh) {
-        cgh.parallel_for(nd_range<3>(grid_dim * block_dim, block_dim), [=](nd_item<3> item_ct1) {
-            dropout_grad_kernel(total_count, scale, vals, mask, item_ct1);
-        });
-    });
-}
-
-template void launch_dropout_grad(float* vals,
-                                  uint8_t* mask,
-                                  int total_count,
-                                  float ratio,
-                                  queue* stream);
-template void launch_dropout_grad(bf16* vals,
-                                  uint8_t* mask,
-                                  int total_count,
-                                  float ratio,
-                                  queue* stream);
-template void launch_dropout_grad(half* vals,
-                                  uint8_t* mask,
-                                  int total_count,
-                                  float ratio,
-                                  queue* stream);
-
-void dropout_grad_kernel(const int N,
-                         const float scale,
-                         const float* Xdata,
-                         float* out,
-                         uint8_t* mask,
-                         nd_item<3> item_ct1)
-{
-    DPCPP_1D_KERNEL_LOOP(i, N) { out[i] = Xdata[i] * scale * mask[i]; }
-}
-
-void dropout_grad_kernel(const int N,
-                         const float scale,
-                         const bf16* Xdata,
-                         bf16* out,
-                         uint8_t* mask,
-                         nd_item<3> item_ct1)
-{
-    const ushort* Xdata_cast = reinterpret_cast<const ushort*>(Xdata);
-    ushort* out_cast = reinterpret_cast<ushort*>(out);
-    DPCPP_1D_KERNEL_LOOP(i, N)
-    {
-        out_cast[i] = bf16(float(Xdata_cast[i]) * scale * mask[i]);
-    }
-}
-
-void dropout_grad_kernel(const int N,
-                         const float scale,
-                         const half* Xdata,
-                         half* out,
-                         uint8_t* mask,
-                         nd_item<3> item_ct1)
-{
-    const float2* x_cast = reinterpret_cast<const float2*>(Xdata);
-    float2* out_cast = reinterpret_cast<float2*>(out);
-    const uint32_t* mask_cast = reinterpret_cast<const uint32_t*>(mask);
-
-    float2 result_f;
-    half2* result_h = reinterpret_cast<half2*>(&result_f);
-
-    DPCPP_1D_KERNEL_LOOP(j, N / unroll_factor)
-    {
-        float2 x_data = x_cast[j];
-        uint32_t m_32 = mask_cast[j];
-        uint8_t* m = (uint8_t*)&m_32;
-
-        half* x_data_h = reinterpret_cast<half*>(&x_data);
-        float2 result[2];
-
-        result[0].x() = (float)x_data_h[0] * scale * m[0];
-        result[0].y() = (float)x_data_h[1] * scale * m[1];
-        result[1].x() = (float)x_data_h[2] * scale * m[2];
-        result[1].y() = (float)x_data_h[3] * scale * m[3];
-
-        result_h[0] = result[0].convert<half, rounding_mode::rte>();
-        result_h[1] = result[1].convert<half, rounding_mode::rte>();
-
-        out_cast[j] = result_f;
-    }
-    int high_index = ((((N / unroll_factor) - 1) / item_ct1.get_local_range().get(2) + 1) *
-                      (unroll_factor * item_ct1.get_local_range().get(2))) +
-                     item_ct1.get_local_id(2);
-    if (N > high_index) {
-        for (int i = high_index; i < N; i++) {
-            out[i] = vec<float, 1>{(float)Xdata[i] * scale * mask[i]}
-                         .convert<half, rounding_mode::automatic>()[0];
-        }
-    }
-}
-
-template <typename T>
-void launch_dropout_grad(T* vals_out,
-                         const T* vals,
-                         uint8_t* mask,
-                         int total_count,
-                         float ratio,
-                         queue* stream)
-{
-    /*
-     * Dropout.Backward1
-     */
-    assert(unroll_factor == 4);
-
-    const float scale = 1. / (1. - ratio);
-    range<3> grid_dim = range<3>(1, 1, DS_GET_BLOCKS(total_count / unroll_factor));
-    range<3> block_dim = range<3>(1, 1, DS_CUDA_NUM_THREADS);
-    stream->submit([&](handler& cgh) {
-        cgh.parallel_for(nd_range<3>(grid_dim * block_dim, block_dim), [=](nd_item<3> item_ct1) {
-            dropout_grad_kernel(total_count, scale, vals, vals_out, mask, item_ct1);
-        });
-    });
-}
-template void launch_dropout_grad(float* vals_out,
-                                  const float* vals,
-                                  uint8_t* mask,
-                                  int total_count,
-                                  float ratio,
-                                  queue* stream);
-template void launch_dropout_grad(bf16* vals_out,
-                                  const bf16* vals,
-                                  uint8_t* mask,
-                                  int total_count,
-                                  float ratio,
-                                  queue* stream);
-template void launch_dropout_grad(half* vals_out,
-                                  const half* vals,
-                                  uint8_t* mask,
-                                  int total_count,
-                                  float ratio,
-                                  queue* stream);
-
-/*
- * not called in transformer kernel Shi Yuankun 2021/10/21
- */
-void dropout_kernel(const int N,
-                    const int dim,
-                    const float ratio,
-                    const float* bias,
-                    float* Xdata,
-                    uint8_t* mask,
-                    const std::pair<uint64_t, uint64_t>& seed,
-                    nd_item<3> item_ct1)
-{
-    const float scale = 1. / (1. - ratio);
-    size_t idx =
-        item_ct1.get_group(2) * item_ct1.get_local_range().get(2) + item_ct1.get_local_id(2);
-    int tid = item_ct1.get_local_id(2) % (dim / unroll_factor);
-
-    oneapi::mkl::rng::device::philox4x32x10<4> engine(seed.first, {idx * 4, seed.second});
-    oneapi::mkl::rng::device::uniform<> distr;
-
-    float4* Xdata_cast = reinterpret_cast<float4*>(Xdata);
-    uint32_t* mask_32 = reinterpret_cast<uint32_t*>(mask);
-    const float4* bias_cast = reinterpret_cast<const float4*>(bias);
-
-    DPCPP_1D_KERNEL_LOOP(j, N)
-    {
-        float4 rand = oneapi::mkl::rng::device::generate(distr, engine);
-        uint32_t m_32;
-        uint8_t* m = (uint8_t*)&m_32;
-
-        m[0] = (uint8_t)(rand.x() > ratio);
-        m[1] = (uint8_t)(rand.y() > ratio);
-        m[2] = (uint8_t)(rand.z() > ratio);
-        m[3] = (uint8_t)(rand.w() > ratio);
-
-        float4 x_data = Xdata_cast[j];
-        float4 b_data = bias_cast[j % (dim / unroll_factor)];
-
-        x_data.x() += b_data.x();
-        x_data.y() += b_data.y();
-        x_data.z() += b_data.z();
-        x_data.w() += b_data.w();
-
-        x_data.x() = x_data.x() * scale * m[0];
-        x_data.y() = x_data.y() * scale * m[1];
-        x_data.z() = x_data.z() * scale * m[2];
-        x_data.w() = x_data.w() * scale * m[3];
-
-        mask_32[j] = m_32;
-        Xdata_cast[j] = x_data;
-    }
-    int high_index = ((((N / unroll_factor) - 1) / item_ct1.get_local_range().get(2) + 1) *
-                      (unroll_factor * item_ct1.get_local_range().get(2))) +
-                     item_ct1.get_local_id(2);
-    if (N > high_index) {
-        float4 rand = oneapi::mkl::rng::device::generate(distr, engine);
-        float* rand_data = &(rand.x());
-        int k = 0;
-        for (int i = high_index; i < N; i++) {
-            float x_data = Xdata[i] + bias[i % dim];
-            uint8_t m = (uint8_t)(rand_data[k++] > ratio);
-            Xdata[i] = x_data * scale * m;
-            mask[i] = m;
-        }
-    }
-}
-
-void dropout_kernel(const int N,
-                    const int dim,
-                    const float ratio,
-                    const half* bias,
-                    half* Xdata,
-                    uint8_t* mask,
-                    const std::pair<uint64_t, uint64_t>& seed,
-                    nd_item<3> item_ct1)
-{
-    const float scale = 1. / (1. - ratio);
-    size_t idx =
-        item_ct1.get_group(2) * item_ct1.get_local_range().get(2) + item_ct1.get_local_id(2);
-    int tid = item_ct1.get_local_id(2) % (dim / unroll_factor);
-
-    oneapi::mkl::rng::device::philox4x32x10<4> engine(seed.first, {idx * 4, seed.second});
-    oneapi::mkl::rng::device::uniform<> distr;
-
-    float2* Xdata_cast = reinterpret_cast<float2*>(Xdata);
-    uint32_t* mask_32 = reinterpret_cast<uint32_t*>(mask);
-    const float2* bias_cast = reinterpret_cast<const float2*>(bias);
-
-    DPCPP_1D_KERNEL_LOOP(j, N)
-    {
-        float4 rand = oneapi::mkl::rng::device::generate(distr, engine);
-
-        float2 data_f;
-        half2* data_h = reinterpret_cast<half2*>(&data_f);
-
-        float2 bias_f;
-        half2* bias_h = reinterpret_cast<half2*>(&bias_f);
-
-        data_f = Xdata_cast[j];
-        bias_f = bias_cast[j % (dim / unroll_factor)];
-
-        float2 data_h_0 = data_h[0].convert<float, rounding_mode::automatic>();
-        float2 data_h_1 = data_h[1].convert<float, rounding_mode::automatic>();
-
-        float2 bias_h_0 = bias_h[0].convert<float, rounding_mode::automatic>();
-        float2 bias_h_1 = bias_h[1].convert<float, rounding_mode::automatic>();
-
-        data_h_0.x() += bias_h_0.x();
-        data_h_0.y() += bias_h_0.y();
-        data_h_1.x() += bias_h_1.x();
-        data_h_1.y() += bias_h_1.y();
-
-        uint32_t m_32;
-        uint8_t* m = (uint8_t*)&m_32;
-
-        m[0] = (uint8_t)(rand.x() > ratio);
-        m[1] = (uint8_t)(rand.y() > ratio);
-        m[2] = (uint8_t)(rand.z() > ratio);
-        m[3] = (uint8_t)(rand.w() > ratio);
-
-        data_h_0.x() =
-            vec<float, 1>{data_h_0.x() * scale * m[0]}.convert<half, rounding_mode::automatic>()[0];
-        data_h_0.y() =
-            vec<float, 1>{data_h_0.y() * scale * m[1]}.convert<half, rounding_mode::automatic>()[0];
-        data_h_1.x() =
-            vec<float, 1>{data_h_1.x() * scale * m[2]}.convert<half, rounding_mode::automatic>()[0];
-        data_h_1.y() =
-            vec<float, 1>{data_h_1.y() * scale * m[3]}.convert<half, rounding_mode::automatic>()[0];
-
-        float2 result_f;
-        half2* result_h = reinterpret_cast<half2*>(&result_f);
-
-        result_h[0] = data_h_0.convert<half, rounding_mode::rte>();
-        result_h[1] = data_h_1.convert<half, rounding_mode::rte>();
-
-        Xdata_cast[j] = result_f;
-        mask_32[j] = m_32;
-    }
-    int high_index = ((((N / unroll_factor) - 1) / item_ct1.get_local_range().get(2) + 1) *
-                      (unroll_factor * item_ct1.get_local_range().get(2))) +
-                     item_ct1.get_local_id(2);
-    if (N > high_index) {
-        float4 rand = oneapi::mkl::rng::device::generate(distr, engine);
-        float* rand_data = &(rand.x());
-        int k = 0;
-        for (int i = high_index; i < N; i++) {
-            float x_data = (float)Xdata[i] + (float)bias[i % dim];
-            uint8_t m = (uint8_t)(rand_data[k++] > ratio);
-            Xdata[i] =
-                vec<float, 1>{x_data * scale * m}.convert<half, rounding_mode::automatic>()[0];
-            mask[i] = m;
-        }
-    }
-}
-
-template <typename T>
-void launch_dropout(T* out,
-                    const T* bias,
-                    uint8_t* mask,
-                    int batch,
-                    int dim,
-                    float ratio,
-                    queue* stream)
-{
-    assert(unroll_factor == 4);
-
-    int total_count = batch * dim / unroll_factor;
-
-    range<3> grid_dim = range<3>(1, 1, DS_GET_BLOCKS(total_count));
-    range<3> block_dim = range<3>(1, 1, DS_CUDA_NUM_THREADS);
-
-    uint64_t inc = (batch * dim) / grid_dim[2] / block_dim[2];
-    std::pair<uint64_t, uint64_t> seed = SyclContext::Instance().IncrementOffset(inc);
-    stream->submit([&](handler& cgh) {
-        cgh.parallel_for(nd_range<3>(grid_dim * block_dim, block_dim), [=](nd_item<3> item_ct1) {
-            dropout_kernel(total_count, dim, ratio, bias, out, mask, seed, item_ct1);
-        });
-    });
-}
-
-template void launch_dropout(float*,
-                             const float* bias,
-                             uint8_t* mask,
-                             int batch,
-                             int dim,
-                             float ratio,
-                             queue* stream);
-template void launch_dropout(half*,
-                             const half* bias,
-                             uint8_t* mask,
-                             int batch,
-                             int dim,
-                             float ratio,
-                             queue* stream);
-
-void dropout_kernel(const int N,
-                    const int dim,
-                    const float ratio,
-                    const float* input,
-                    const float* residual,
-                    const float* bias,
-                    float* out,
-                    uint8_t* mask,
-                    const std::pair<uint64_t, uint64_t>& seed,
-                    nd_item<3> item_ct1)
-{
-    const float scale = 1. / (1. - ratio);
-    size_t idx =
-        item_ct1.get_group(2) * item_ct1.get_local_range().get(2) + item_ct1.get_local_id(2);
-    int tid = item_ct1.get_local_id(2) % (dim / unroll_factor);
-
-    oneapi::mkl::rng::device::philox4x32x10<4> engine(seed.first, {idx * 4, seed.second});
-    oneapi::mkl::rng::device::uniform<> distr;
-
-    float4* out_cast = reinterpret_cast<float4*>(out);
-    uint32_t* mask_32 = reinterpret_cast<uint32_t*>(mask);
-
-    const float4* bias_cast = reinterpret_cast<const float4*>(bias);
-    const float4* residual_cast = reinterpret_cast<const float4*>(residual);
-    const float4* input_cast = reinterpret_cast<const float4*>(input);
-
-    DPCPP_1D_KERNEL_LOOP(j, N)
-    {
-        float4 rand = oneapi::mkl::rng::device::generate(distr, engine);
-
-        uint32_t m_32;
-        uint8_t* m = (uint8_t*)&m_32;
-
-        m[0] = (uint8_t)(rand.x() > ratio);
-        m[1] = (uint8_t)(rand.y() > ratio);
-        m[2] = (uint8_t)(rand.z() > ratio);
-        m[3] = (uint8_t)(rand.w() > ratio);
-
-        float4 out_data;
-        float4 b_data = bias_cast[j % (dim / unroll_factor)];
-        float4 res_data = residual_cast[j];
-        float4 inp_data = input_cast[j];
-
-        out_data.x() = (b_data.x() + inp_data.x());
-        out_data.y() = (b_data.y() + inp_data.y());
-        out_data.z() = (b_data.z() + inp_data.z());
-        out_data.w() = (b_data.w() + inp_data.w());
-
-        out_data.x() = out_data.x() * scale * m[0];
-        out_data.y() = out_data.y() * scale * m[1];
-        out_data.z() = out_data.z() * scale * m[2];
-        out_data.w() = out_data.w() * scale * m[3];
-
-        out_data.x() += res_data.x();
-        out_data.y() += res_data.y();
-        out_data.z() += res_data.z();
-        out_data.w() += res_data.w();
-
-        mask_32[j] = m_32;
-        out_cast[j] = out_data;
-    }
-    int high_index = ((((N / unroll_factor) - 1) / item_ct1.get_local_range().get(2) + 1) *
-                      (unroll_factor * item_ct1.get_local_range().get(2))) +
-                     item_ct1.get_local_id(2);
-    if (N > high_index) {
-        float4 rand = oneapi::mkl::rng::device::generate(distr, engine);
-        float* rand_data = &(rand.x());
-        int k = 0;
-        for (int i = high_index; i < N; i++) {
-            float x_data = input[i] + bias[i % dim];
-            uint8_t m = (uint8_t)(rand_data[k++] > ratio);
-            x_data = x_data * scale * m;
-            x_data += residual[i];
-
-            out[i] = x_data;
-            mask[i] = m;
-        }
-    }
-}
-
-void dropout_kernel(const int N,
-                    const int dim,
-                    const float ratio,
-                    const bf16* input,
-                    const bf16* residual,
-                    const bf16* bias,
-                    bf16* out,
-                    uint8_t* mask,
-                    const std::pair<uint64_t, uint64_t>& seed,
-                    nd_item<3> item_ct1)
-{
-    const float scale = 1. / (1. - ratio);
-    size_t idx =
-        item_ct1.get_group(2) * item_ct1.get_local_range().get(2) + item_ct1.get_local_id(2);
-    int tid = item_ct1.get_local_id(2) % (dim / unroll_factor);
-
-    oneapi::mkl::rng::device::philox4x32x10<4> engine(seed.first, {idx * 4, seed.second});
-    oneapi::mkl::rng::device::uniform<> distr;
-
-    ushort4* out_cast = reinterpret_cast<ushort4*>(out);
-    uint32_t* mask_32 = reinterpret_cast<uint32_t*>(mask);
-
-    const ushort4* bias_cast = reinterpret_cast<const ushort4*>(bias);
-    const ushort4* residual_cast = reinterpret_cast<const ushort4*>(residual);
-    const ushort4* input_cast = reinterpret_cast<const ushort4*>(input);
-
-    DPCPP_1D_KERNEL_LOOP(j, N)
-    {
-        float4 rand = oneapi::mkl::rng::device::generate(distr, engine);
-
-        uint32_t m_32;
-        uint8_t* m = (uint8_t*)&m_32;
-
-        m[0] = (uint8_t)(rand.x() > ratio);
-        m[1] = (uint8_t)(rand.y() > ratio);
-        m[2] = (uint8_t)(rand.z() > ratio);
-        m[3] = (uint8_t)(rand.w() > ratio);
-
-        float4 out_data;
-        float4 b_data = {
-            float(bias_cast[j % (dim / unroll_factor)].x()),
-            float(bias_cast[j % (dim / unroll_factor)].y()),
-            float(bias_cast[j % (dim / unroll_factor)].z()),
-            float(bias_cast[j % (dim / unroll_factor)].w()),
-        };
-        float4 res_data = {float(residual_cast[j].x()),
-                           float(residual_cast[j].y()),
-                           float(residual_cast[j].z()),
-                           float(residual_cast[j].w())};
-        float4 inp_data = {float(input_cast[j].x()),
-                           float(input_cast[j].y()),
-                           float(input_cast[j].z()),
-                           float(input_cast[j].w())};
-
-        out_data.x() = (b_data.x() + inp_data.x());
-        out_data.y() = (b_data.y() + inp_data.y());
-        out_data.z() = (b_data.z() + inp_data.z());
-        out_data.w() = (b_data.w() + inp_data.w());
-
-        out_data.x() = out_data.x() * scale * m[0];
-        out_data.y() = out_data.y() * scale * m[1];
-        out_data.z() = out_data.z() * scale * m[2];
-        out_data.w() = out_data.w() * scale * m[3];
-
-        out_data.x() += res_data.x();
-        out_data.y() += res_data.y();
-        out_data.z() += res_data.z();
-        out_data.w() += res_data.w();
-
-        mask_32[j] = m_32;
-        out_cast[j] = {bf16(out_data.x()),
-                       bf16(out_data.y()),
-                       bf16(out_data.z()),
-                       bf16(out_data.w())};
-    }
-    int high_index = ((((N / unroll_factor) - 1) / item_ct1.get_local_range().get(2) + 1) *
-                      (unroll_factor * item_ct1.get_local_range().get(2))) +
-                     item_ct1.get_local_id(2);
-    if (N > high_index) {
-        ushort* out_cast = reinterpret_cast<ushort*>(out);
-        const ushort* bias_cast = reinterpret_cast<const ushort*>(bias);
-        const ushort* residual_cast = reinterpret_cast<const ushort*>(residual);
-        const ushort* input_cast = reinterpret_cast<const ushort*>(input);
-        float4 rand = oneapi::mkl::rng::device::generate(distr, engine);
-        float* rand_data = &(rand.x());
-        int k = 0;
-        for (int i = high_index; i < N; i++) {
-            float x_data = float(input_cast[i]) + float(bias_cast[i % dim]);
-            uint8_t m = (uint8_t)(rand_data[k++] > ratio);
-            x_data = x_data * scale * m;
-            x_data += float(residual_cast[i]);
-
-            out_cast[i] = bf16(x_data);
-            mask[i] = m;
-        }
-    }
-}
-
-void dropout_kernel(const int N,
-                    const int dim,
-                    const float ratio,
-                    const half* input,
-                    const half* residual,
-                    const half* bias,
-                    half* out,
-                    uint8_t* mask,
-                    const std::pair<uint64_t, uint64_t>& seed,
-                    nd_item<3> item_ct1)
-{
-    const float scale = 1. / (1. - ratio);
-    size_t idx = item_ct1.get_group(2) * item_ct1.get_local_range(2) + item_ct1.get_local_id(2);
-    int tid = item_ct1.get_local_id(2) % (dim / unroll_factor);
-
-    oneapi::mkl::rng::device::philox4x32x10<4> engine(seed.first, {idx * 4, seed.second});
-    oneapi::mkl::rng::device::uniform<> distr;
-
-    float2* out_cast = reinterpret_cast<float2*>(out);
-    uint32_t* mask_32 = reinterpret_cast<uint32_t*>(mask);
-
-    const float2* bias_cast = reinterpret_cast<const float2*>(bias);
-    const float2* residual_cast = reinterpret_cast<const float2*>(residual);
-    const float2* input_cast = reinterpret_cast<const float2*>(input);
-
-    DPCPP_1D_KERNEL_LOOP(j, N)
-    {
-        float4 rand = oneapi::mkl::rng::device::generate(distr, engine);
-
-        float2 data_f;
-        half2* data_h = reinterpret_cast<half2*>(&data_f);
-
-        float2 bias_f;
-        half2* bias_h = reinterpret_cast<half2*>(&bias_f);
-
-        float2 residual_f;
-        half2* residual_h = reinterpret_cast<half2*>(&residual_f);
-
-        float2 input_f;
-        half2* input_h = reinterpret_cast<half2*>(&input_f);
-
-        bias_f = bias_cast[j % (dim / unroll_factor)];
-        residual_f = residual_cast[j];
-        input_f = input_cast[j];
-
-        float2 data_h_0 = data_h[0].convert<float, rounding_mode::automatic>();
-        float2 data_h_1 = data_h[1].convert<float, rounding_mode::automatic>();
-
-        float2 bias_h_0 = bias_h[0].convert<float, rounding_mode::automatic>();
-        float2 bias_h_1 = bias_h[1].convert<float, rounding_mode::automatic>();
-
-        float2 residual_h_0 = residual_h[0].convert<float, rounding_mode::automatic>();
-        float2 residual_h_1 = residual_h[1].convert<float, rounding_mode::automatic>();
-
-        float2 input_h_0 = input_h[0].convert<float, rounding_mode::automatic>();
-        float2 input_h_1 = input_h[1].convert<float, rounding_mode::automatic>();
-
-        data_h_0.x() = (bias_h_0.x() + input_h_0.x());
-        data_h_0.y() = (bias_h_0.y() + input_h_0.y());
-        data_h_1.x() = (bias_h_1.x() + input_h_1.x());
-        data_h_1.y() = (bias_h_1.y() + input_h_1.y());
-
-        uint32_t m_32;
-        uint8_t* m = (uint8_t*)&m_32;
-
-        m[0] = (uint8_t)(rand.x() > ratio);
-        m[1] = (uint8_t)(rand.y() > ratio);
-        m[2] = (uint8_t)(rand.z() > ratio);
-        m[3] = (uint8_t)(rand.w() > ratio);
-
-        data_h_0.x() =
-            vec<float, 1>{data_h_0.x() * scale * m[0]}.convert<half, rounding_mode::automatic>()[0];
-        data_h_0.y() =
-            vec<float, 1>{data_h_0.y() * scale * m[1]}.convert<half, rounding_mode::automatic>()[0];
-        data_h_1.x() =
-            vec<float, 1>{data_h_1.x() * scale * m[2]}.convert<half, rounding_mode::automatic>()[0];
-        data_h_1.y() =
-            vec<float, 1>{data_h_1.y() * scale * m[3]}.convert<half, rounding_mode::automatic>()[0];
-
-        data_h_0.x() += residual_h_0.x();
-        data_h_0.y() += residual_h_0.y();
-        data_h_1.x() += residual_h_1.x();
-        data_h_1.y() += residual_h_1.y();
-
-        float2 result_f;
-        half2* result_h = reinterpret_cast<half2*>(&result_f);
-
-        result_h[0] = data_h_0.convert<half, rounding_mode::rte>();
-        result_h[1] = data_h_1.convert<half, rounding_mode::rte>();
-
-        out_cast[j] = result_f;
-        mask_32[j] = m_32;
-    }
-    int high_index = ((((N / unroll_factor) - 1) / item_ct1.get_local_range().get(2) + 1) *
-                      (unroll_factor * item_ct1.get_local_range().get(2))) +
-                     item_ct1.get_local_id(2);
-    if (N > high_index) {
-        float4 rand = oneapi::mkl::rng::device::generate(distr, engine);
-        float* rand_data = &(rand.x());
-        int k = 0;
-        for (int i = high_index; i < N; i++) {
-            float x_data = (float)input[i] + (float)bias[i % dim];
-            uint8_t m = (uint8_t)(rand_data[k++] > ratio);
-            x_data = x_data * scale * m;
-            x_data += (float)residual[i];
-
-            out[i] = vec<float, 1>{x_data}.convert<half, rounding_mode::automatic>()[0];
-            mask[i] = m;
-        }
-    }
-}
-
-template <typename T>
-void launch_dropout(T* out,
-                    const T* input,
-                    const T* residual,
-                    const T* bias,
-                    uint8_t* mask,
-                    int batch,
-                    int dim,
-                    float ratio,
-                    queue* stream)
-{
-    assert(unroll_factor == 4);
-
-    int total_count = batch * dim / unroll_factor;
-    range<3> grid_dim = range<3>(1, 1, DS_GET_BLOCKS(total_count));
-    range<3> block_dim = range<3>(1, 1, DS_CUDA_NUM_THREADS);
-
-    uint64_t inc = (batch * dim) / grid_dim[2] / block_dim[2];
-    std::pair<uint64_t, uint64_t> seed = SyclContext::Instance().IncrementOffset(inc);
-
-    stream->submit([&](handler& cgh) {
-        cgh.parallel_for(nd_range<3>(grid_dim * block_dim, block_dim), [=](nd_item<3> item_ct1) {
-            dropout_kernel(
-                total_count, dim, ratio, input, residual, bias, out, mask, seed, item_ct1);
-        });
-    });
-}
-
-template void launch_dropout(float*,
-                             const float*,
-                             const float* residual,
-                             const float* bias,
-                             uint8_t* mask,
-                             int batch,
-                             int dim,
-                             float ratio,
-                             queue* stream);
-template void launch_dropout(bf16*,
-                             const bf16*,
-                             const bf16* residual,
-                             const bf16* bias,
-                             uint8_t* mask,
-                             int batch,
-                             int dim,
-                             float ratio,
-                             queue* stream);
-template void launch_dropout(half*,
-                             const half*,
-                             const half* residual,
-                             const half* bias,
-                             uint8_t* mask,
-                             int batch,
-                             int dim,
-                             float ratio,
-                             queue* stream);
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/ds_dropout_sycl.dp.cpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/ds_dropout_sycl.dp.cpp
deleted file mode 100644
index 9a91502..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/transformer/ds_dropout_sycl.dp.cpp
+++ /dev/null
@@ -1,92 +0,0 @@
-#include "common.hpp"
-#include "context.hpp"
-#include "dropout.hpp"
-
-template <typename T>
-std::vector<torch::Tensor> dropout_forward(float ratio,
-                                           uint32_t dim,
-                                           int bsz,
-                                           const torch::Tensor& vals)
-{
-    CHECK_INPUT(vals);
-    auto output = torch::empty_like(vals);
-
-    auto uint8_options = torch::TensorOptions()
-                             .dtype(torch::kInt8)
-                             .layout(torch::kStrided)
-                             .device(torch::kXPU)
-                             .requires_grad(false);
-
-    auto mask = torch::empty({bsz, dim}, uint8_options);
-
-    const T* input_ptr = (const T*)vals.data_ptr();
-    T* output_ptr = (T*)output.data_ptr();
-    uint8_t* mask_ptr = (uint8_t*)mask.data_ptr();
-
-    sycl::queue* q = ::SyclContext::Instance().GetCurrentStream();
-    Dropout<T> _dropout = Dropout<T>(typename Dropout<T>::Config(ratio, dim));
-    _dropout.SetMask(mask_ptr);
-    _dropout.Forward(bsz, output_ptr, input_ptr, q);
-    return {output, mask};
-}
-
-template <typename T>
-std::vector<torch::Tensor> dropout_forward_with_bias(float ratio,
-                                                     uint32_t dim,
-                                                     int bsz,
-                                                     const torch::Tensor& vals,
-                                                     const torch::Tensor& bias,
-                                                     const torch::Tensor& residual)
-{
-    CHECK_INPUT(vals);
-    CHECK_INPUT(bias);
-    CHECK_INPUT(residual);
-    auto output = torch::empty_like(vals);
-
-    auto uint8_options = torch::TensorOptions()
-                             .dtype(torch::kInt8)
-                             .layout(torch::kStrided)
-                             .device(torch::kXPU)
-                             .requires_grad(false);
-
-    auto mask = torch::empty({bsz, dim}, uint8_options);
-
-    const T* input_ptr = (const T*)vals.data_ptr();
-    const T* bias_ptr = (const T*)bias.data_ptr();
-    const T* residual_ptr = (const T*)residual.data_ptr();
-    T* output_ptr = (T*)output.data_ptr();
-    uint8_t* mask_ptr = (uint8_t*)mask.data_ptr();
-
-    sycl::queue* q = ::SyclContext::Instance().GetCurrentStream();
-    Dropout<T> _dropout = Dropout<T>(typename Dropout<T>::Config(ratio, dim));
-    _dropout.SetMask(mask_ptr);
-    _dropout.ForwardWithBias(bsz, output_ptr, input_ptr, residual_ptr, bias_ptr, q);
-    return {output, mask};
-}
-
-template <typename T>
-std::vector<torch::Tensor> dropout_backward(float ratio,
-                                            uint32_t dim,
-                                            int bsz,
-                                            torch::Tensor& vals,
-                                            torch::Tensor& mask,
-                                            bool in_place)
-{
-    CHECK_INPUT(vals);
-    CHECK_INPUT(mask);
-    sycl::queue* q = ::SyclContext::Instance().GetCurrentStream();
-    Dropout<T> _dropout = Dropout<T>(typename Dropout<T>::Config(ratio, dim));
-    uint8_t* mask_ptr = (uint8_t*)mask.data_ptr();
-    _dropout.SetMask(mask_ptr);
-    if (in_place) {
-        T* d_input_ptr = (T*)vals.data_ptr();
-        _dropout.Backward(bsz, d_input_ptr, q);
-        return {vals};
-    } else {
-        auto output = torch::empty_like(vals);
-        const T* d_input_ptr = (const T*)vals.data_ptr();
-        T* d_output_ptr = (T*)output.data_ptr();
-        _dropout.Backward(bsz, d_output_ptr, d_input_ptr, q);
-        return {output};
-    }
-}
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/ds_feedforward_sycl.dp.cpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/ds_feedforward_sycl.dp.cpp
deleted file mode 100644
index 5232bb5..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/transformer/ds_feedforward_sycl.dp.cpp
+++ /dev/null
@@ -1,81 +0,0 @@
-#include "common.hpp"
-#include "context.hpp"
-#include "feed_forward.hpp"
-
-template <typename T>
-std::vector<torch::Tensor> feedforward_forward(int bsz,
-                                               int seq_len,
-                                               int hidden_size,
-                                               const torch::Tensor& input,
-                                               const torch::Tensor& weights)
-{
-    CHECK_INPUT(input);
-    CHECK_INPUT(weights);
-
-    int batchSize = bsz * seq_len;
-    int inputSize = hidden_size;
-    int outputSize = 3 * hidden_size;
-    auto options = torch::TensorOptions()
-                       .dtype(input.options().dtype())
-                       .layout(torch::kStrided)
-                       .device(torch::kXPU)
-                       .requires_grad(true);
-
-    const T* input_ptr = (const T*)input.data_ptr();
-    const T* weights_ptr = (const T*)weights.data_ptr();
-
-    auto output = torch::empty({bsz, seq_len, outputSize}, options);
-
-    T* output_ptr = (T*)output.data_ptr();
-
-    sycl::queue* q = ::SyclContext::Instance().GetCurrentStream();
-
-    FeedForward<T> _ff =
-        FeedForward<T>(typename FeedForward<T>::Config(batchSize, outputSize, inputSize));
-
-    _ff.Forward(batchSize, input_ptr, weights_ptr, output_ptr, q);
-    return {output};
-}
-
-template <typename T>
-std::vector<torch::Tensor> feedforward_backward(int bsz,
-                                                int seq_len,
-                                                int hidden_size,
-                                                const torch::Tensor& grad_out,
-                                                const torch::Tensor& input,
-                                                const torch::Tensor& weights)
-{
-    CHECK_INPUT(grad_out);
-    CHECK_INPUT(input);
-    CHECK_INPUT(weights);
-
-    int batchSize = bsz * seq_len;
-    int inputSize = hidden_size;
-    int outputSize = 3 * hidden_size;
-
-    auto options = torch::TensorOptions()
-                       .dtype(input.options().dtype())
-                       .layout(torch::kStrided)
-                       .device(torch::kXPU)
-                       .requires_grad(true);
-
-    const T* grad_out_ptr = (const T*)grad_out.data_ptr();
-    const T* input_ptr = (const T*)input.data_ptr();
-    const T* weights_ptr = (const T*)weights.data_ptr();
-
-    auto grad_weights = torch::empty(weights.sizes(), options);
-    auto grad_bias = torch::empty({outputSize}, options);
-    auto grad_input = torch::empty(input.sizes(), options);
-
-    T* grad_w_ptr = (T*)grad_weights.data_ptr();
-    T* grad_b_ptr = (T*)grad_bias.data_ptr();
-    T* grad_i_ptr = (T*)grad_input.data_ptr();
-    sycl::queue* q = ::SyclContext::Instance().GetCurrentStream();
-
-    FeedForward<T> _ff =
-        FeedForward<T>(typename FeedForward<T>::Config(batchSize, outputSize, inputSize));
-
-    _ff.Backward(
-        batchSize, grad_out_ptr, input_ptr, weights_ptr, grad_w_ptr, grad_b_ptr, q, q, grad_i_ptr);
-    return {grad_input, grad_weights, grad_bias};
-}
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/ds_gelu_sycl.dp.cpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/ds_gelu_sycl.dp.cpp
deleted file mode 100644
index 0a05c41..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/transformer/ds_gelu_sycl.dp.cpp
+++ /dev/null
@@ -1,39 +0,0 @@
-#include "common.hpp"
-#include "context.hpp"
-#include "gelu.hpp"
-
-template <typename T>
-std::vector<torch::Tensor> gelu_forward(int intermediate_size,
-                                        int bsz_seq,
-                                        const torch::Tensor& input,
-                                        const torch::Tensor& bias)
-{
-    CHECK_INPUT(input);
-    CHECK_INPUT(bias);
-    const T* input_ptr = (const T*)input.data_ptr();
-    const T* bias_ptr = (const T*)bias.data_ptr();
-    auto output = torch::empty_like(input);
-    T* output_ptr = (T*)output.data_ptr();
-    sycl::queue* q = ::SyclContext::Instance().GetCurrentStream();
-    Gelu<T> _gelu = Gelu<T>(typename Gelu<T>::Config(intermediate_size));
-    _gelu.ForwardWithBiasAdd(bsz_seq, input_ptr, bias_ptr, output_ptr, q);
-    return {output};
-}
-
-template <typename T>
-std::vector<torch::Tensor> gelu_backward(torch::Tensor& d_output,
-                                         int intermediate_size,
-                                         int bsz_seq,
-                                         const torch::Tensor& input,
-                                         const torch::Tensor& bias)
-{
-    CHECK_INPUT(input);
-    CHECK_INPUT(bias);
-    const T* input_ptr = (const T*)input.data_ptr();
-    const T* bias_ptr = (const T*)bias.data_ptr();
-    T* d_output_ptr = (T*)d_output.data_ptr();
-    sycl::queue* q = ::SyclContext::Instance().GetCurrentStream();
-    Gelu<T> _gelu = Gelu<T>(typename Gelu<T>::Config(intermediate_size));
-    _gelu.Backward(bsz_seq, d_output_ptr, input_ptr, bias_ptr, q);
-    return {d_output};
-}
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/ds_layer_reorder_sycl.dp.cpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/ds_layer_reorder_sycl.dp.cpp
deleted file mode 100644
index c9fd531..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/transformer/ds_layer_reorder_sycl.dp.cpp
+++ /dev/null
@@ -1,122 +0,0 @@
-#include "common.hpp"
-#include "context.hpp"
-#include "custom_sycl_layers.hpp"
-#include "general_kernels.hpp"
-
-template <typename T>
-std::vector<torch::Tensor> transform4d_0213(const torch::Tensor& input,
-                                            int batch,
-                                            int seq_len,
-                                            int hidden_size,
-                                            int num_heads,
-                                            int trans_count)
-{
-    CHECK_INPUT(input);
-    auto options = torch::TensorOptions()
-                       .dtype(input.options().dtype())
-                       .layout(torch::kStrided)
-                       .device(torch::kXPU)
-                       .requires_grad(true);
-
-    torch::Tensor output;
-    if (trans_count == 3)
-        // trans_count=3
-        output = torch::empty({batch, seq_len, 3, num_heads, hidden_size / num_heads}, options);
-    else
-        // for 1 attn_o_inp, trans_count=1
-        output = torch::empty({batch, seq_len, num_heads, hidden_size / num_heads}, options);
-
-    sycl::queue* q = ::SyclContext::Instance().GetCurrentStream();
-
-    const T* input_ptr = (const T*)input.data_ptr();
-    T* output_ptr = (T*)output.data_ptr();
-    // trans_count=1
-    // launch_transform4d_0213(output_ptr, input_ptr, batch, num_heads, seq_len,
-    // hidden_size, q, 1);
-    // trans_count=3
-    launch_transform4d_0213(
-        output_ptr, input_ptr, batch, num_heads, seq_len, hidden_size, q, trans_count);
-    return {output};
-}
-
-template <typename T>
-std::vector<torch::Tensor> bias_add_transform_0213(const torch::Tensor& input,
-                                                   const torch::Tensor& bias,
-                                                   int batch,
-                                                   int seq_len,
-                                                   int hidden_size,
-                                                   int num_heads)
-{
-    CHECK_INPUT(input);
-    CHECK_INPUT(bias);
-    auto options = torch::TensorOptions()
-                       .dtype(input.options().dtype())
-                       .layout(torch::kStrided)
-                       .device(torch::kXPU)
-                       .requires_grad(true);
-
-    auto output = torch::empty({3, batch, num_heads, seq_len, hidden_size / num_heads}, options);
-
-    sycl::queue* q = ::SyclContext::Instance().GetCurrentStream();
-
-    const T* input_ptr = (const T*)input.data_ptr();
-    const T* bias_ptr = (const T*)bias.data_ptr();
-    T* output_ptr = (T*)output.data_ptr();
-    launch_bias_add_transform_0213(
-        output_ptr, input_ptr, bias_ptr, batch, seq_len, hidden_size, num_heads, q, 3);
-    return {output};
-}
-
-template <typename T>
-std::vector<torch::Tensor> transform_0213(const torch::Tensor& input,
-                                          int batch,
-                                          int seq_len,
-                                          int hidden_size,
-                                          int num_heads)
-{
-    CHECK_INPUT(input);
-
-    auto options = torch::TensorOptions()
-                       .dtype(input.options().dtype())
-                       .layout(torch::kStrided)
-                       .device(torch::kXPU)
-                       .requires_grad(true);
-
-    auto output = torch::empty({batch, num_heads, seq_len, hidden_size / num_heads}, options);
-
-    sycl::queue* q = ::SyclContext::Instance().GetCurrentStream();
-
-    const T* input_ptr = (const T*)input.data_ptr();
-    T* output_ptr = (T*)output.data_ptr();
-
-    launch_transform_0213(output_ptr, input_ptr, batch, seq_len, hidden_size, num_heads, q);
-    return {output};
-}
-
-template <typename T>
-std::vector<torch::Tensor> fused_add2(const torch::Tensor& input1,
-                                      const torch::Tensor& input2,
-                                      int batch,
-                                      int seq_len,
-                                      int hidden_size)
-{
-    CHECK_INPUT(input1);
-    CHECK_INPUT(input2);
-
-    auto options = torch::TensorOptions()
-                       .dtype(input1.options().dtype())
-                       .layout(torch::kStrided)
-                       .device(torch::kXPU)
-                       .requires_grad(true);
-
-    auto output = torch::empty({batch, seq_len, hidden_size}, options);
-
-    sycl::queue* q = ::SyclContext::Instance().GetCurrentStream();
-
-    const T* input_ptr1 = (const T*)input1.data_ptr();
-    const T* input_ptr2 = (const T*)input2.data_ptr();
-    T* output_ptr = (T*)output.data_ptr();
-
-    launch_fused_add2(output_ptr, input_ptr1, input_ptr2, batch, seq_len, hidden_size, q);
-    return {output};
-}
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/ds_normalize_sycl.dp.cpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/ds_normalize_sycl.dp.cpp
deleted file mode 100644
index d779a12..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/transformer/ds_normalize_sycl.dp.cpp
+++ /dev/null
@@ -1,151 +0,0 @@
-#include "common.hpp"
-#include "context.hpp"
-#include "normalize_layer.hpp"
-
-template <typename T>
-std::vector<torch::Tensor> normalize_forward(const int batch,
-                                             const int seq_len,
-                                             const int hidden_size,
-                                             const torch::Tensor& residual,
-                                             const torch::Tensor& gamma,
-                                             const torch::Tensor& betta,
-                                             torch::Tensor& mean,
-                                             torch::Tensor& var,
-                                             const bool preln,
-                                             const bool wmean,
-                                             const float epsilon)
-{
-    CHECK_INPUT(residual);
-    CHECK_INPUT(gamma);
-    CHECK_INPUT(betta);
-
-    int bsz_seq = batch * seq_len;
-
-    auto options = torch::TensorOptions()
-                       .dtype(residual.options().dtype())
-                       .layout(torch::kStrided)
-                       .device(torch::kXPU)
-                       .requires_grad(true);
-
-    auto output = torch::empty({batch, seq_len, hidden_size}, options);
-
-    T* output_ptr = (T*)output.data_ptr();
-    T* mean_ptr = (T*)mean.data_ptr();
-    T* var_ptr = (T*)var.data_ptr();
-    const T* residual_ptr = (const T*)residual.data_ptr();
-    const T* gamma_ptr = (const T*)gamma.data_ptr();
-    const T* betta_ptr = (const T*)betta.data_ptr();
-
-    sycl::queue* q = ::SyclContext::Instance().GetCurrentStream();
-    Normalize_Layer<T> _norm(
-        typename Normalize_Layer<T>::Config(batch, seq_len, hidden_size, epsilon, true, wmean));
-    _norm.SetMean(mean_ptr);
-    _norm.SetVar(var_ptr);
-
-    if (wmean)
-        _norm.ForwardCheckpoint(bsz_seq, output_ptr, residual_ptr, gamma_ptr, betta_ptr, q);
-    else
-        _norm.Forward(bsz_seq, output_ptr, residual_ptr, gamma_ptr, betta_ptr, q);
-    return {output};
-}
-
-template <typename T>
-std::vector<torch::Tensor> normalize_backward(const int batch,
-                                              const int seq_len,
-                                              const int hidden_size,
-                                              const torch::Tensor& input,
-                                              const torch::Tensor& gamma,
-                                              const torch::Tensor& betta,
-                                              const torch::Tensor& output,
-                                              const torch::Tensor& out1_grad,
-                                              const torch::Tensor& out2_grad,
-                                              torch::Tensor& mean,
-                                              torch::Tensor& var,
-                                              const bool preln,
-                                              const bool wmean,
-                                              const float epsilon)
-{
-    CHECK_INPUT(input);
-    CHECK_INPUT(output);
-    CHECK_INPUT(out1_grad);
-    CHECK_INPUT(out2_grad);
-    CHECK_INPUT(gamma);
-    CHECK_INPUT(betta);
-    int bsz_seq = batch * seq_len;
-
-    auto options = torch::TensorOptions()
-                       .dtype(input.options().dtype())
-                       .layout(torch::kStrided)
-                       .device(torch::kXPU)
-                       .requires_grad(true);
-
-    auto gamma_grad = torch::empty({hidden_size}, options);
-    auto betta_grad = torch::empty({hidden_size}, options);
-    auto input_grad = torch::empty({batch, seq_len, hidden_size}, options);
-
-    const T* input_ptr = (const T*)input.data_ptr();
-    const T* out1_grad_ptr = (const T*)out1_grad.data_ptr();
-    const T* out2_grad_ptr = (const T*)out2_grad.data_ptr();
-    const T* gamma_ptr = (const T*)gamma.data_ptr();
-    const T* betta_ptr = (const T*)betta.data_ptr();
-    const T* output_ptr = (const T*)output.data_ptr();
-    T* gamma_grad_ptr = (T*)gamma_grad.data_ptr();
-    T* betta_grad_ptr = (T*)betta_grad.data_ptr();
-    T* inp_grad_ptr = (T*)input_grad.data_ptr();
-    T* mean_ptr = (T*)mean.data_ptr();
-    T* var_ptr = (T*)var.data_ptr();
-    sycl::queue* q = ::SyclContext::Instance().GetCurrentStream();
-
-    Normalize_Layer<T> _norm(
-        typename Normalize_Layer<T>::Config(batch, seq_len, hidden_size, epsilon, true, wmean));
-    sycl::queue* qs[2] = {q, q};
-
-    _norm.SetMean(mean_ptr);
-    _norm.SetVar(var_ptr);
-
-    if (preln) {
-        if (wmean)
-            _norm.BackwardFusedAdd(bsz_seq,
-                                   out1_grad_ptr,
-                                   out2_grad_ptr,
-                                   gamma_ptr,
-                                   gamma_grad_ptr,
-                                   betta_grad_ptr,
-                                   qs,
-                                   inp_grad_ptr,
-                                   input_ptr);
-        else
-            _norm.BackwardFusedAdd(bsz_seq,
-                                   out1_grad_ptr,
-                                   out2_grad_ptr,
-                                   gamma_ptr,
-                                   betta_ptr,
-                                   gamma_grad_ptr,
-                                   betta_grad_ptr,
-                                   qs,
-                                   inp_grad_ptr,
-                                   output_ptr);
-    } else {
-        if (wmean)
-            _norm.Backward(bsz_seq,
-                           out1_grad_ptr,
-                           gamma_ptr,
-                           gamma_grad_ptr,
-                           betta_grad_ptr,
-                           qs,
-                           inp_grad_ptr,
-                           input_ptr);
-        else {
-            _norm.Backward(bsz_seq,
-                           out1_grad_ptr,
-                           gamma_ptr,
-                           betta_ptr,
-                           gamma_grad_ptr,
-                           betta_grad_ptr,
-                           qs,
-                           inp_grad_ptr,
-                           output_ptr);
-        }
-    }
-    return {input_grad, gamma_grad, betta_grad};
-}
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/ds_softmax_sycl.dp.cpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/ds_softmax_sycl.dp.cpp
deleted file mode 100644
index 557fcb4..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/transformer/ds_softmax_sycl.dp.cpp
+++ /dev/null
@@ -1,44 +0,0 @@
-#include "common.hpp"
-#include "context.hpp"
-#include "softmax.hpp"
-
-template <typename T>
-std::vector<torch::Tensor> softmax_forward(int bsz,
-                                           int seq_len,
-                                           int num_heads,
-                                           torch::Tensor& inout,
-                                           const torch::Tensor& mask)
-{
-    CHECK_INPUT(inout);
-    CHECK_INPUT(mask);
-
-    T* inout_ptr = (T*)inout.data_ptr();
-    const T* mask_ptr = (const T*)mask.data_ptr();
-
-    sycl::queue* q = ::SyclContext::Instance().GetCurrentStream();
-    Softmax<T> _softmax = Softmax<T>(typename Softmax<T>::Config(bsz, num_heads, seq_len));
-    _softmax.SetSeqLength(seq_len);
-    _softmax.Forward(bsz, inout_ptr, mask_ptr, q);
-    return {inout};
-}
-
-template <typename T>
-std::vector<torch::Tensor> softmax_backward(int bsz,
-                                            int seq_len,
-                                            int num_heads,
-                                            torch::Tensor& out_grad,
-                                            const torch::Tensor& input)
-{
-    CHECK_INPUT(out_grad);
-    CHECK_INPUT(input);
-
-    T* out_grad_ptr = (T*)out_grad.data_ptr();
-    const T* input_ptr = (const T*)input.data_ptr();
-
-    sycl::queue* q = ::SyclContext::Instance().GetCurrentStream();
-    Softmax<T> _softmax = Softmax<T>(typename Softmax<T>::Config(bsz, num_heads, seq_len));
-    _softmax.SetSeqLength(seq_len);
-
-    _softmax.Backward(bsz, out_grad_ptr, input_ptr, q);
-    return {out_grad};
-}
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/ds_stridedbatchgemm_sycl.dp.cpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/ds_stridedbatchgemm_sycl.dp.cpp
deleted file mode 100644
index 3f67b33..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/transformer/ds_stridedbatchgemm_sycl.dp.cpp
+++ /dev/null
@@ -1,95 +0,0 @@
-#include "common.hpp"
-#include "context.hpp"
-#include "strided_batch_gemm.hpp"
-
-template <typename T>
-std::vector<torch::Tensor> stridedbatchgemm_forward(const int batchSize,
-                                                    const int m,
-                                                    const int n,
-                                                    const int k,
-                                                    const float alpha,
-                                                    const float beta,
-                                                    const torch::Tensor& matA,
-                                                    const torch::Tensor& matB)
-{
-    CHECK_INPUT(matA);
-    CHECK_INPUT(matB);
-
-    auto options = torch::TensorOptions()
-                       .dtype(matA.options().dtype())
-                       .layout(torch::kStrided)
-                       .device(torch::kXPU)
-                       .requires_grad(true);
-
-    StridedBatchGemm<T> _sbgemm =
-        StridedBatchGemm<T>(typename StridedBatchGemm<T>::Config(batchSize,
-                                                                 m,
-                                                                 n,
-                                                                 k,
-                                                                 alpha,
-                                                                 beta,
-                                                                 oneapi::mkl::transpose::trans,
-                                                                 oneapi::mkl::transpose::nontrans,
-                                                                 {0, 0, 0}));
-
-    const T* matA_ptr = (const T*)matA.data_ptr();
-    const T* matB_ptr = (const T*)matB.data_ptr();
-
-    auto matC = torch::empty({batchSize, n, m}, options);
-
-    T* matC_ptr = (T*)matC.data_ptr();
-
-    sycl::queue* q = ::SyclContext::Instance().GetCurrentStream();
-
-    _sbgemm.Forward(batchSize, matC_ptr, matA_ptr, matB_ptr, q);
-    return {matC};
-}
-
-template <typename T>
-std::vector<torch::Tensor> stridedbatchgemm_backward(const int batchSize,
-                                                     const int m,
-                                                     const int n,
-                                                     const int k,
-                                                     const float alpha,
-                                                     const float beta,
-                                                     const torch::Tensor& grad_matC,
-                                                     const torch::Tensor& matA,
-                                                     const torch::Tensor& matB)
-{
-    CHECK_INPUT(grad_matC);
-    CHECK_INPUT(matA);
-    CHECK_INPUT(matB);
-
-    auto options = torch::TensorOptions()
-                       .dtype(matA.options().dtype())
-                       .layout(torch::kStrided)
-                       .device(torch::kXPU)
-                       .requires_grad(true);
-
-    StridedBatchGemm<T> _sbgemm =
-        StridedBatchGemm<T>(typename StridedBatchGemm<T>::Config(batchSize,
-                                                                 m,
-                                                                 n,
-                                                                 k,
-                                                                 alpha,
-                                                                 beta,
-                                                                 oneapi::mkl::transpose::trans,
-                                                                 oneapi::mkl::transpose::nontrans,
-                                                                 {0, 0, 0}));
-
-    const T* grad_c_ptr = (const T*)grad_matC.data_ptr();
-    const T* matA_ptr = (const T*)matA.data_ptr();
-    const T* matB_ptr = (const T*)matB.data_ptr();
-
-    auto grad_matA = torch::empty(matA.sizes(), options);
-    auto grad_matB = torch::empty(matB.sizes(), options);
-    CHECK_INPUT(grad_matA);
-    CHECK_INPUT(grad_matB);
-
-    T* grad_a_ptr = (T*)grad_matA.data_ptr();
-    T* grad_b_ptr = (T*)grad_matB.data_ptr();
-    sycl::queue* q = ::SyclContext::Instance().GetCurrentStream();
-
-    _sbgemm.Backward(batchSize, grad_c_ptr, matA_ptr, matB_ptr, q, grad_a_ptr, grad_b_ptr);
-    return {grad_matA, grad_matB};
-}
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/ds_transformer_sycl.dp.cpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/ds_transformer_sycl.dp.cpp
deleted file mode 100644
index 8668270..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/transformer/ds_transformer_sycl.dp.cpp
+++ /dev/null
@@ -1,1087 +0,0 @@
-#include "ds_transformer_sycl.hpp"
-#include <torch/extension.h>
-#if __has_include(<sycl/sycl.hpp>)
-#include <sycl/sycl.hpp>
-#elif __has_include(<CL/sycl.hpp>)
-#include <CL/sycl.hpp>
-#else
-#error "Unsupported compiler"
-#endif
-#include <cmath>
-#include <oneapi/mkl.hpp>
-#include <type_traits>
-#include <unordered_map>
-#include <vector>
-#include "Timer.hpp"
-#include "common.hpp"
-#include "context.hpp"
-#include "custom_sycl_layers.hpp"
-#include "onednn_wrappers.hpp"
-#include "onemkl_wrappers.hpp"
-
-static std::unordered_map<int, std::shared_ptr<void>> s_transformer_layers;
-
-const int init_seq_length = 128;
-
-// C++ interface
-
-template <typename T>
-size_t get_workspace_size(int maxBatchSize,
-                          int seq_len,
-                          int hidden_size,
-                          int intermediate_size,
-                          int heads,
-                          bool training,
-                          bool gelu_checkpoint)
-{
-    size_t workSpacesize = 4 * (size_t(maxBatchSize) * seq_len * hidden_size);
-    if (training) {
-        workSpacesize += 2 * (size_t(maxBatchSize) * seq_len * hidden_size);
-        workSpacesize += ((std::max)((size_t(maxBatchSize) * seq_len * intermediate_size),
-                                     2 * (size_t(maxBatchSize) * heads * seq_len * seq_len)));
-        if (gelu_checkpoint)
-            workSpacesize += 2 * (size_t(maxBatchSize) * seq_len * intermediate_size);
-    }
-    return workSpacesize;  // * sizeof(T);
-}
-
-// NOTE: AT_ASSERT has become AT_CHECK on master after 0.4.
-#define CHECK_XPU(x) AT_ASSERTM(x.is_xpu(), #x " must be a XPU tensor")
-#define CHECK_CONTIGUOUS(x) AT_ASSERTM(x.is_contiguous(), #x " must be contiguous")
-#define CHECK_INPUT(x) \
-    CHECK_XPU(x);      \
-    CHECK_CONTIGUOUS(x)
-
-template <typename T>
-BertTransformerLayer<T>::BertTransformerLayer(int layer_id,
-                                              int batch_size,
-                                              int hidden_size,
-                                              int num_heads,
-                                              int intermediate_size,
-                                              int seq_length,
-                                              float attn_prob_dropout_ratio,
-                                              float hidden_output_dropout_ratio,
-                                              float layer_norm_eps,
-                                              bool pre_or_postLayerNorm,
-                                              const std::vector<std::array<int, 3>>& gemm_algos,
-                                              bool attn_dropout_checkpoint,
-                                              bool normalize_invertible,
-                                              bool gelu_checkpoint,
-                                              bool stochastic_mode)
-    : _layer_id(layer_id),
-      _batch_size(batch_size),
-      _hidden_size(hidden_size),
-      _heads(num_heads),
-      _intermediate_size(intermediate_size),
-      _seq_length(seq_length),
-      _training(true),
-      _pre_or_postLayerNorm(pre_or_postLayerNorm),
-      _attn_dropout_checkpoint(attn_dropout_checkpoint),
-      _normalize_invertible(normalize_invertible),
-      _gelu_checkpoint(gelu_checkpoint),
-      _stochastic_mode(stochastic_mode),
-      _stream(::SyclContext::Instance().GetCurrentStream()),
-      _onemklQ(::SyclContext::Instance().GetCurrentStream()),
-      _qkv_linear(
-          typename FeedForward<T>::Config(batch_size * seq_length, 3 * hidden_size, hidden_size)),
-      _attn_out_linear(
-          typename FeedForward<T>::Config(batch_size * seq_length, hidden_size, hidden_size)),
-      _attn_layer_norm(typename Normalize_Layer<T>::Config(batch_size,
-                                                           seq_length,
-                                                           hidden_size,
-                                                           layer_norm_eps,
-                                                           true,
-                                                           !normalize_invertible)),
-      _layer_norm(typename Normalize_Layer<T>::Config(batch_size,
-                                                      seq_length,
-                                                      hidden_size,
-                                                      layer_norm_eps,
-                                                      true,
-                                                      !normalize_invertible)),
-      _ff1(typename FeedForward<T>::Config(batch_size * seq_length,
-                                           _intermediate_size,
-                                           hidden_size)),
-      _ff2(typename FeedForward<T>::Config(batch_size * seq_length,
-                                           hidden_size,
-                                           _intermediate_size)),
-      _softmax(typename Softmax<T>::Config(batch_size, num_heads, seq_length)),
-      _gelu(typename Gelu<T>::Config(_intermediate_size)),
-      _attn_prob_dropout(typename Dropout<T>::Config(attn_prob_dropout_ratio, _seq_length)),
-      _attn_output_dropout(typename Dropout<T>::Config(hidden_output_dropout_ratio, _hidden_size)),
-      _layer_output_dropout(typename Dropout<T>::Config(hidden_output_dropout_ratio, _hidden_size)),
-      _attn_scores(typename StridedBatchGemm<T>::Config(_batch_size * _heads,
-                                                        _seq_length,
-                                                        _seq_length,
-                                                        _hidden_size / _heads,
-                                                        //(T(1.0) / T(sqrt(_hidden_size / _heads))),
-                                                        float(1.0 / sqrt(_hidden_size / _heads)),
-                                                        float(0.0),
-                                                        oneapi::mkl::transpose::trans,
-                                                        oneapi::mkl::transpose::nontrans,
-                                                        gemm_algos[3])),
-      _attn_context(typename StridedBatchGemm<T>::Config(_batch_size * _heads,
-                                                         _hidden_size / _heads,
-                                                         _seq_length,
-                                                         _seq_length,
-                                                         float(1.0),
-                                                         float(0.0),
-                                                         oneapi::mkl::transpose::nontrans,
-                                                         oneapi::mkl::transpose::nontrans,
-                                                         gemm_algos[4]))
-{
-    assert(_hidden_size % _heads == 0);
-    Initialize();
-}
-
-template <typename T>
-BertTransformerLayer<T>::~BertTransformerLayer()
-{
-}
-
-template <typename T>
-void BertTransformerLayer<T>::Initialize()
-{
-}
-
-template <typename T>
-void BertTransformerLayer<T>::Forward(int bsz,
-                                      const T* input_ptr,
-                                      const T* input_mask_ptr,
-                                      const T* attn_qkvw_ptr,
-                                      const T* attn_qkvb_ptr,
-                                      const T* attn_ow_ptr,
-                                      const T* attn_ob_ptr,
-                                      const T* attn_nw_ptr,
-                                      const T* attn_nb_ptr,
-                                      const T* inter_w_ptr,
-                                      const T* inter_b_ptr,
-                                      const T* output_w_ptr,
-                                      const T* output_b_ptr,
-                                      const T* norm_w_ptr,
-                                      const T* norm_b_ptr,
-                                      T* out_ptr,
-                                      T* inp_norm_ptr,
-                                      T* q_tf_ptr,
-                                      T* k_tf_ptr,
-                                      T* v_tf_ptr,
-                                      T* soft_out_ptr,
-                                      T* ctx_bufB_ptr,
-                                      T* attn_o_inp_ptr,
-                                      T* add_res_ptr,
-                                      T* ff1_inp_ptr,
-                                      T* gelu_inp_ptr,
-                                      T* ff2_inp_ptr)
-{
-    if (!_stochastic_mode) _stream->wait();
-
-    T* workspace = static_cast<T*>(::SyclContext::Instance().GetWorkSpace());
-    size_t small_buf_size = bsz * _seq_length * _hidden_size;
-    T* buf_0 = workspace;
-    T* buf_1 = buf_0 + small_buf_size;
-    T* buf_2 = buf_1;
-
-    if (_normalize_invertible) {
-        add_res_ptr = buf_1 + 3 * small_buf_size;
-        buf_2 = add_res_ptr;
-    }
-    if (_gelu_checkpoint) buf_2 += small_buf_size;
-    if (_attn_dropout_checkpoint)
-        ctx_bufB_ptr =
-            (_gelu_checkpoint ? (buf_2 + (_intermediate_size / _hidden_size) * small_buf_size)
-                              : (buf_1 + 4 * small_buf_size));
-
-    int bsz_seq = bsz * _seq_length;
-
-    if (_pre_or_postLayerNorm) {
-        if (_layer_norm.UseMean()) {
-            _layer_norm.ForwardCheckpoint(
-                bsz_seq, inp_norm_ptr, input_ptr, norm_w_ptr, norm_b_ptr, _stream, true);
-
-        } else {
-            // input_ptr[bsz_seq], norm_w_ptr[_seq_length], norm_b_ptr[_seq_length]
-            // --> inp_norm_ptr[bsz_seq]
-            _layer_norm.Forward(
-                bsz_seq, inp_norm_ptr, input_ptr, norm_w_ptr, norm_b_ptr, _stream, true);
-        }
-    }
-
-    if (_pre_or_postLayerNorm) {
-        _qkv_linear.Forward(bsz_seq, inp_norm_ptr, attn_qkvw_ptr, buf_0, _onemklQ);
-    } else {
-        _qkv_linear.Forward(bsz_seq, input_ptr, attn_qkvw_ptr, buf_0, _onemklQ);
-    }
-
-    launch_bias_add_transform_0213<T>(
-        q_tf_ptr, buf_0, attn_qkvb_ptr, bsz, _seq_length, _hidden_size, _heads, _stream, 3);
-
-    int bsz_heads = bsz * _heads;
-
-    // attention scores
-    _attn_scores.Forward(bsz_heads, soft_out_ptr, k_tf_ptr, q_tf_ptr, _onemklQ);
-
-    // Softmax + Mask
-    _softmax.Forward(bsz, soft_out_ptr, input_mask_ptr, _stream);
-
-    // attn prob dropout.
-    _attn_prob_dropout.Forward(bsz_heads * _seq_length, ctx_bufB_ptr, soft_out_ptr, _stream);
-
-    // attention context
-    _attn_context.Forward(bsz_heads, buf_1, v_tf_ptr, ctx_bufB_ptr, _onemklQ);
-
-    launch_transform4d_0213<T>(
-        attn_o_inp_ptr, buf_1, bsz, _heads, _seq_length, _hidden_size, _stream, 1);
-
-    if (_pre_or_postLayerNorm) {
-        _attn_out_linear.Forward(bsz_seq, attn_o_inp_ptr, attn_ow_ptr, buf_1, _onemklQ);
-    } else {
-        _attn_out_linear.Forward(bsz_seq, attn_o_inp_ptr, attn_ow_ptr, ff1_inp_ptr, _onemklQ);
-    }
-
-    // attn output dropout.
-    if (_pre_or_postLayerNorm) {
-        _attn_output_dropout.ForwardWithBias(
-            bsz_seq, add_res_ptr, buf_1, input_ptr, attn_ob_ptr, _stream);
-    } else {
-        _attn_output_dropout.ForwardWithBias(
-            bsz_seq, add_res_ptr, ff1_inp_ptr, input_ptr, attn_ob_ptr, _stream);
-    }
-
-    if (_pre_or_postLayerNorm) {
-        if (_attn_layer_norm.UseMean()) {
-            _attn_layer_norm.ForwardCheckpoint(
-                bsz_seq, ff1_inp_ptr, add_res_ptr, attn_nw_ptr, attn_nb_ptr, _stream, true);
-        } else {
-            _attn_layer_norm.Forward(
-                bsz_seq, ff1_inp_ptr, add_res_ptr, attn_nw_ptr, attn_nb_ptr, _stream, true);
-        }
-    } else {
-        if (_attn_layer_norm.UseMean()) {
-            _attn_layer_norm.ForwardCheckpoint(
-                bsz_seq, ff1_inp_ptr, add_res_ptr, attn_nw_ptr, attn_nb_ptr, _stream, true);
-        } else {
-            _attn_layer_norm.Forward(
-                bsz_seq, ff1_inp_ptr, add_res_ptr, attn_nw_ptr, attn_nb_ptr, _stream, true);
-        }
-    }
-
-    _ff1.Forward(bsz_seq,
-                 ff1_inp_ptr,
-                 inter_w_ptr,
-                 (_gelu_checkpoint ? ff2_inp_ptr : gelu_inp_ptr),
-                 _onemklQ);
-
-    _gelu.ForwardWithBiasAdd(bsz_seq,
-                             (_gelu_checkpoint ? ff2_inp_ptr : gelu_inp_ptr),
-                             inter_b_ptr,
-                             (_gelu_checkpoint ? buf_2 : ff2_inp_ptr),
-                             _stream);
-
-    _ff2.Forward(
-        bsz_seq, (_gelu_checkpoint ? buf_2 : ff2_inp_ptr), output_w_ptr, out_ptr, _onemklQ);
-
-    // layer output dropout.
-    if (_pre_or_postLayerNorm) {
-        _layer_output_dropout.ForwardWithBias(
-            bsz_seq, out_ptr, out_ptr, add_res_ptr, output_b_ptr, _stream);
-    } else {
-        _layer_output_dropout.ForwardWithBias(
-            bsz_seq, inp_norm_ptr, out_ptr, ff1_inp_ptr, output_b_ptr, _stream);
-    }
-
-    if (!_pre_or_postLayerNorm) {
-        if (_layer_norm.UseMean()) {
-            _layer_norm.ForwardCheckpoint(
-                bsz_seq, out_ptr, inp_norm_ptr, norm_w_ptr, norm_b_ptr, _stream, true);
-        } else {
-            _layer_norm.Forward(
-                bsz_seq, out_ptr, inp_norm_ptr, norm_w_ptr, norm_b_ptr, _stream, true);
-        }
-    }
-}
-
-template <typename T>
-void BertTransformerLayer<T>::Backward(int bsz,
-                                       const T* grad_output_ptr,
-                                       const T* input_ptr,
-                                       const T* output_ptr,
-                                       const T* inp_norm_ptr,
-                                       const T* q_tf_ptr,
-                                       const T* k_tf_ptr,
-                                       const T* v_tf_ptr,
-                                       const T* soft_out_ptr,
-                                       const T* ctx_bufB_ptr,
-                                       const T* attn_o_inp_ptr,
-                                       const T* add_res_ptr,
-                                       const T* ff1_inp_ptr,
-                                       const T* gelu_inp_ptr,
-                                       const T* ff2_inp_ptr,
-                                       const T* input_mask_ptr,
-                                       const T* attn_qkvw_ptr,
-                                       const T* attn_ow_ptr,
-                                       const T* attn_nw_ptr,
-                                       const T* attn_nb_ptr,
-                                       const T* inter_w_ptr,
-                                       const T* inter_b_ptr,
-                                       const T* output_w_ptr,
-                                       const T* norm_w_ptr,
-                                       const T* norm_b_ptr,
-
-                                       T* grad_input_ptr,
-                                       T* grad_attn_qkvw_ptr,
-                                       T* grad_attn_qkvb_ptr,
-                                       T* grad_attn_ow_ptr,
-                                       T* grad_attn_ob_ptr,
-                                       T* grad_attn_nw_ptr,
-                                       T* grad_attn_nb_ptr,
-                                       T* grad_inter_w_ptr,
-                                       T* grad_inter_b_ptr,
-                                       T* grad_output_w_ptr,
-                                       T* grad_output_b_ptr,
-                                       T* grad_norm_w_ptr,
-                                       T* grad_norm_b_ptr)
-{
-    if (!_stochastic_mode) _stream->wait();
-
-    T* workspace = static_cast<T*>(::SyclContext::Instance().GetWorkSpace());
-    size_t small_buf_size = bsz * _seq_length * _hidden_size;
-    T* buf_0 = workspace;
-    T* buf_1 = buf_0 + small_buf_size;
-    T* buf_2 = buf_1 + small_buf_size;
-    T* buf_3 = buf_2 + small_buf_size;
-
-    T* ff2_buf = (_gelu_checkpoint ? buf_3 + (bsz * _seq_length * _intermediate_size)
-                                   : buf_3 + small_buf_size);
-    T* ctx_bufB_ptr_recomp = ff2_buf + (_seq_length * _seq_length * bsz * _heads);
-
-    sycl::queue* streams[2] = {_stream, _stream};
-
-    int bsz_seq = bsz * _seq_length;
-    int bsz_heads = bsz * _heads;
-
-    if (!_pre_or_postLayerNorm) {
-        if (_layer_norm.UseMean())
-            _layer_norm.Backward(bsz_seq,
-                                 grad_output_ptr,
-                                 norm_w_ptr,
-                                 grad_norm_w_ptr,
-                                 grad_norm_b_ptr,
-                                 streams,
-                                 buf_1,
-                                 inp_norm_ptr);
-
-        else
-            _layer_norm.Backward(bsz_seq,
-                                 grad_output_ptr,
-                                 norm_w_ptr,
-                                 norm_b_ptr,
-                                 grad_norm_w_ptr,
-                                 grad_norm_b_ptr,
-                                 streams,
-                                 buf_1,
-                                 output_ptr);
-    }
-
-    if (_pre_or_postLayerNorm)
-        _layer_output_dropout.Backward(bsz_seq, buf_0, grad_output_ptr, _stream);
-    else
-        _layer_output_dropout.Backward(bsz_seq, buf_0, buf_1, _stream);
-
-    const T* layer_dropout_buf = _layer_output_dropout.HasDropout()
-                                     ? buf_0
-                                     : (_pre_or_postLayerNorm ? grad_output_ptr : buf_1);
-
-    if (_gelu_checkpoint)
-        _gelu.ForwardWithBiasAdd(bsz_seq, ff2_inp_ptr, inter_b_ptr, buf_2, _stream);
-    _ff2.Backward(bsz_seq,
-                  layer_dropout_buf,
-                  (_gelu_checkpoint ? buf_2 : ff2_inp_ptr),
-                  output_w_ptr,
-                  grad_output_w_ptr,
-                  grad_output_b_ptr,
-                  _onemklQ,
-                  _stream,
-                  ff2_buf);
-
-    _gelu.Backward(
-        bsz_seq, ff2_buf, (_gelu_checkpoint ? ff2_inp_ptr : gelu_inp_ptr), inter_b_ptr, _stream);
-
-    _ff1.Backward(bsz_seq,
-                  ff2_buf,
-                  ff1_inp_ptr,
-                  inter_w_ptr,
-                  grad_inter_w_ptr,
-                  grad_inter_b_ptr,
-                  _onemklQ,
-                  _stream,
-                  buf_3);
-
-    if (!_pre_or_postLayerNorm)
-        launch_fused_add2<T>(buf_2, buf_3, buf_1, bsz, _seq_length, _hidden_size, _stream);
-
-    if (_pre_or_postLayerNorm) {
-        if (_attn_layer_norm.UseMean())
-            _attn_layer_norm.BackwardFusedAdd(bsz_seq,
-                                              buf_3,
-                                              grad_output_ptr,
-                                              attn_nw_ptr,
-                                              grad_attn_nw_ptr,
-                                              grad_attn_nb_ptr,
-                                              streams,
-                                              buf_0,
-                                              add_res_ptr);
-
-        else
-            _attn_layer_norm.BackwardFusedAdd(bsz_seq,
-                                              buf_3,
-                                              grad_output_ptr,
-                                              attn_nw_ptr,
-                                              attn_nb_ptr,
-                                              grad_attn_nw_ptr,
-                                              grad_attn_nb_ptr,
-                                              streams,
-                                              buf_0,
-                                              ff1_inp_ptr);
-    } else {
-        if (_attn_layer_norm.UseMean())
-            _attn_layer_norm.Backward(bsz_seq,
-                                      buf_2,
-                                      attn_nw_ptr,
-                                      grad_attn_nw_ptr,
-                                      grad_attn_nb_ptr,
-                                      streams,
-                                      buf_0,
-                                      add_res_ptr);
-
-        else
-            _attn_layer_norm.Backward(bsz_seq,
-                                      buf_2,
-                                      attn_nw_ptr,
-                                      attn_nb_ptr,
-                                      grad_attn_nw_ptr,
-                                      grad_attn_nb_ptr,
-                                      streams,
-                                      buf_0,
-                                      ff1_inp_ptr);
-    }
-
-    _attn_output_dropout.Backward(bsz_seq, buf_2, buf_0, _stream);
-
-    T* attn_output_dropout_buf = _attn_output_dropout.HasDropout() ? buf_2 : buf_0;
-
-    _attn_out_linear.Backward(bsz_seq,
-                              attn_output_dropout_buf,
-                              attn_o_inp_ptr,
-                              attn_ow_ptr,
-                              grad_attn_ow_ptr,
-                              grad_attn_ob_ptr,
-                              _onemklQ,
-                              _stream,
-                              buf_1);
-
-    launch_transform_0213<T>(buf_2, buf_1, bsz, _seq_length, _hidden_size, _heads, _stream);
-
-    if (_attn_prob_dropout.HasDropout()) {
-        if (_attn_dropout_checkpoint) {
-            _attn_prob_dropout.Forward(
-                bsz_heads * _seq_length, ctx_bufB_ptr_recomp, soft_out_ptr, _stream, true);
-        }
-
-        _attn_context.Backward(bsz_heads,
-                               buf_2,
-                               v_tf_ptr,
-                               (_attn_dropout_checkpoint ? ctx_bufB_ptr_recomp : ctx_bufB_ptr),
-                               _onemklQ,
-                               buf_3,
-                               ff2_buf);
-    } else {
-        _attn_context.Backward(bsz_heads, buf_2, v_tf_ptr, soft_out_ptr, _onemklQ, buf_3, ff2_buf);
-    }
-
-    _attn_prob_dropout.Backward(bsz_heads * _seq_length, ff2_buf, _stream);
-
-    _softmax.Backward(bsz, ff2_buf, soft_out_ptr, _stream);
-
-    _attn_scores.Backward(bsz_heads, ff2_buf, k_tf_ptr, q_tf_ptr, _onemklQ, buf_2, buf_1);
-
-    // the size of input (buf_1) relates to the last argument (trans_count), in
-    // this case, buf_1 connected with buf_2 and buf_3 are all inputs
-    launch_transform4d_0213(ff2_buf, buf_1, bsz, _heads, _seq_length, _hidden_size, _stream, 3);
-
-    T* grad_out_buffer = (T*)malloc_shared(10 * sizeof(T), *_stream);
-    T* input_buffer = (T*)malloc_shared(10 * sizeof(T), *_stream);
-    T* weight_buffer = (T*)malloc_shared(10 * sizeof(T), *_stream);
-    T* grad_weight_buffer = (T*)malloc_shared(10 * sizeof(T), *_stream);
-    T* grad_bias_buffer = (T*)malloc_shared(10 * sizeof(T), *_stream);
-    if (_pre_or_postLayerNorm) {
-        _qkv_linear.Backward(bsz_seq,
-                             ff2_buf,
-                             inp_norm_ptr,
-                             attn_qkvw_ptr,
-                             grad_attn_qkvw_ptr,
-                             grad_attn_qkvb_ptr,
-                             _onemklQ,
-                             _stream,
-                             buf_2);
-    } else {
-        _qkv_linear.Backward(bsz_seq,
-                             ff2_buf,
-                             input_ptr,
-                             attn_qkvw_ptr,
-                             grad_attn_qkvw_ptr,
-                             grad_attn_qkvb_ptr,
-                             _onemklQ,
-                             _stream,
-                             buf_2);
-    }
-
-    if (_pre_or_postLayerNorm) {
-        if (_layer_norm.UseMean()) {
-            _layer_norm.BackwardFusedAdd(bsz_seq,
-                                         buf_2,
-                                         buf_0,
-                                         norm_w_ptr,
-                                         grad_norm_w_ptr,
-                                         grad_norm_b_ptr,
-                                         streams,
-                                         grad_input_ptr,
-                                         input_ptr);
-        }
-
-        else {
-            _layer_norm.BackwardFusedAdd(bsz_seq,
-                                         buf_2,
-                                         buf_0,
-                                         norm_w_ptr,
-                                         norm_b_ptr,
-                                         grad_norm_w_ptr,
-                                         grad_norm_b_ptr,
-                                         streams,
-                                         grad_input_ptr,
-                                         inp_norm_ptr);
-        }
-    } else {
-        launch_fused_add2<T>(grad_input_ptr, buf_2, buf_0, bsz, _seq_length, _hidden_size, _stream);
-        _stream->submit([&](sycl::handler& cgh) {
-            cgh.single_task([=]() {
-                for (int i = 0; i < 10; ++i) {
-                    grad_out_buffer[i] = ff2_buf[i];
-                    input_buffer[i] = inp_norm_ptr[i];
-                    weight_buffer[i] = attn_qkvw_ptr[i];
-                    grad_weight_buffer[i] = grad_attn_qkvw_ptr[i];
-                    grad_bias_buffer[i] = grad_attn_qkvb_ptr[i];
-                }
-            });
-        });
-    }
-
-    _stream->wait();
-}
-
-template <typename T>
-void BertTransformerLayer<T>::SetTrainingMode(bool training)
-{
-    // Dropout will be skipped when not in training model.
-    _attn_prob_dropout.SetTrainingMode(training);
-    _attn_output_dropout.SetTrainingMode(training);
-    _layer_output_dropout.SetTrainingMode(training);
-}
-
-template <typename T>
-void BertTransformerLayer<T>::SetIntermediateBuffers(uint8_t* attn_prob_dropout_mask_ptr,
-                                                     uint8_t* attn_output_dropout_mask_ptr,
-                                                     uint8_t* layer_output_dropout_mask_ptr,
-                                                     T* attn_layer_norm_var,
-                                                     T* attn_layer_norm_mean,
-                                                     T* layer_norm_var,
-                                                     T* layer_norm_mean)
-{
-    _attn_prob_dropout.SetMask(attn_prob_dropout_mask_ptr);
-    _attn_output_dropout.SetMask(attn_output_dropout_mask_ptr);
-    _layer_output_dropout.SetMask(layer_output_dropout_mask_ptr);
-
-    _attn_layer_norm.SetVar(attn_layer_norm_var);
-    _attn_layer_norm.SetMean(attn_layer_norm_mean);
-    _layer_norm.SetVar(layer_norm_var);
-    _layer_norm.SetMean(layer_norm_mean);
-}
-
-template <typename T>
-void BertTransformerLayer<T>::SetSeqLength(int seq_len)
-{
-    _seq_length = seq_len;
-
-    _softmax.SetSeqLength(_seq_length);
-    _attn_prob_dropout.SetDimension(_seq_length);
-    _attn_scores.SetConfig(_seq_length, _seq_length, _hidden_size / _heads);
-    _attn_context.SetConfig(_hidden_size / _heads, _seq_length, _seq_length);
-}
-
-template <typename T>
-int create_transformer_layer(int layer_id,
-                             int batch_size,
-                             int hidden_dim,
-                             int num_heads,
-                             int intermediate_size,
-                             float attn_dropout_ratio,
-                             float hidden_dropout_ratio,
-                             float layer_norm_eps,
-                             int seed,
-                             bool pre_or_postLayerNorm,
-                             bool test_gemm,
-                             bool attn_dropout_checkpoint,
-                             bool normalize_invertible,
-                             bool gelu_checkpoint,
-                             bool stochastic_mode)
-{
-    ::SyclContext::Instance().SetSeed(seed);
-    ::SyclContext::Instance().TestGemmFP16(
-        test_gemm, batch_size, init_seq_length, num_heads, hidden_dim / num_heads);
-    auto layer = std::make_shared<BertTransformerLayer<T>>(layer_id,
-                                                           batch_size,
-                                                           hidden_dim,
-                                                           num_heads,
-                                                           intermediate_size,
-                                                           init_seq_length,
-                                                           attn_dropout_ratio,
-                                                           hidden_dropout_ratio,
-                                                           layer_norm_eps,
-                                                           pre_or_postLayerNorm,
-                                                           ::SyclContext::Instance().GetGemmAlgos(),
-                                                           attn_dropout_checkpoint,
-                                                           normalize_invertible,
-                                                           gelu_checkpoint,
-                                                           stochastic_mode);
-    s_transformer_layers[layer_id] = layer;
-
-    std::string dtype = (std::is_same<T, sycl::half>::value)
-                            ? "half"
-                            : ((std::is_same<T, bf16>::value) ? "bf16" : "float");
-
-    std::cout << "layer #" << layer_id << " is created with date type [" << dtype << "]."
-              << std::endl;
-
-    return 0;
-}
-
-template <typename T>
-std::vector<torch::Tensor> ds_transformer_forward(int layer_id,
-                                                  const torch::Tensor& input,
-                                                  const torch::Tensor& input_mask,
-                                                  const torch::Tensor& attn_qkvw,
-                                                  const torch::Tensor& attn_qkvb,
-                                                  const torch::Tensor& attn_ow,
-                                                  const torch::Tensor& attn_ob,
-                                                  const torch::Tensor& attn_nw,
-                                                  const torch::Tensor& attn_nb,
-                                                  const torch::Tensor& inter_w,
-                                                  const torch::Tensor& inter_b,
-                                                  const torch::Tensor& output_w,
-                                                  const torch::Tensor& output_b,
-                                                  const torch::Tensor& norm_w,
-                                                  const torch::Tensor& norm_b,
-                                                  bool training_mode,
-                                                  bool prelayernorm,
-                                                  bool attn_dropout_checkpoint,
-                                                  bool normalize_invertible,
-                                                  bool gelu_checkpoint)
-{
-    CHECK_INPUT(input);
-    CHECK_INPUT(input_mask);
-    CHECK_INPUT(attn_qkvw);
-    CHECK_INPUT(attn_qkvb);
-    CHECK_INPUT(attn_ow);
-    CHECK_INPUT(attn_ob);
-    CHECK_INPUT(attn_nw);
-    CHECK_INPUT(attn_nb);
-    CHECK_INPUT(inter_w);
-    CHECK_INPUT(inter_b);
-    CHECK_INPUT(output_w);
-    CHECK_INPUT(output_b);
-    CHECK_INPUT(norm_w);
-    CHECK_INPUT(norm_b);
-
-    int bsz = input.size(0);
-
-    const T* input_ptr = (const T*)input.data_ptr();
-    const T* input_mask_ptr = (const T*)input_mask.data_ptr();
-    const T* attn_qkvw_ptr = (const T*)attn_qkvw.data_ptr();
-    const T* attn_qkvb_ptr = (const T*)attn_qkvb.data_ptr();
-    const T* attn_ow_ptr = (const T*)attn_ow.data_ptr();
-    const T* attn_ob_ptr = (const T*)attn_ob.data_ptr();
-    const T* attn_nw_ptr = (const T*)attn_nw.data_ptr();
-    const T* attn_nb_ptr = (const T*)attn_nb.data_ptr();
-    const T* inter_w_ptr = (const T*)inter_w.data_ptr();
-    const T* inter_b_ptr = (const T*)inter_b.data_ptr();
-    const T* output_w_ptr = (const T*)output_w.data_ptr();
-    const T* output_b_ptr = (const T*)output_b.data_ptr();
-    const T* norm_w_ptr = (const T*)norm_w.data_ptr();
-    const T* norm_b_ptr = (const T*)norm_b.data_ptr();
-
-    auto output = torch::empty_like(input);
-    T* out_ptr = (T*)output.data_ptr();
-
-    auto options = torch::TensorOptions()
-                       .dtype(input.options().dtype())
-                       .layout(torch::kStrided)
-                       .device(torch::kXPU)
-                       .requires_grad(true);
-
-    auto uint8_options = torch::TensorOptions()
-                             .dtype(torch::kInt8)
-                             .layout(torch::kStrided)
-                             .device(torch::kXPU)
-                             .requires_grad(false);
-
-    std::shared_ptr<BertTransformerLayer<T>> layer =
-        std::static_pointer_cast<BertTransformerLayer<T>>(s_transformer_layers[layer_id]);
-
-    int seq_len = layer->GetSeqLength();
-    if (input.size(1) != seq_len) {
-        seq_len = input.size(1);
-        layer->SetSeqLength(seq_len);
-    }
-
-    auto workspace = torch::empty({(long)get_workspace_size<T>(bsz,
-                                                               seq_len,
-                                                               layer->GetHiddenSize(),
-                                                               layer->GetIntermediateSize(),
-                                                               layer->GetNumHeads(),
-                                                               layer->IsTrainingMode(),
-                                                               layer->GeluCheckpoint())},
-                                  options);
-    ::SyclContext::Instance().SetWorkSpace((T*)workspace.data_ptr());
-
-    auto inp_norm = ((prelayernorm || !normalize_invertible) ? torch::empty_like(input) : output);
-    auto add_res = (normalize_invertible ? inp_norm : torch::empty_like(input));
-    auto attn_o_inp = torch::empty_like(input);
-    auto qkv_tf = torch::empty({(bsz * seq_len), output_w.size(0) * 3}, options);
-
-    auto attn_prob_dropout_mask =
-        torch::empty({(bsz * layer->GetNumHeads() * seq_len), seq_len}, uint8_options);
-    auto attn_output_dropout_mask =
-        torch::empty({(bsz * seq_len), layer->GetHiddenSize()}, uint8_options);
-    auto layer_output_dropout_mask =
-        torch::empty({(bsz * seq_len), layer->GetHiddenSize()}, uint8_options);
-
-    auto attn_layer_norm_var = torch::empty({(bsz * seq_len)}, options);
-    auto attn_layer_norm_mean = torch::empty({(bsz * seq_len)}, options);
-    auto layer_norm_var = torch::empty({(bsz * seq_len)}, options);
-    auto layer_norm_mean = torch::empty({(bsz * seq_len)}, options);
-
-    T* inp_norm_ptr = (T*)inp_norm.data_ptr();
-    T* add_res_ptr = (T*)add_res.data_ptr();
-    T* q_tf_ptr = (T*)qkv_tf.data_ptr();
-    T* k_tf_ptr = q_tf_ptr + (bsz * seq_len * output_w.size(0));  //(T*)k_tf.data_ptr();
-    T* v_tf_ptr = k_tf_ptr + (bsz * seq_len * output_w.size(0));  //(T*)v_tf.data_ptr();
-    T* attn_o_inp_ptr = (T*)attn_o_inp.data_ptr();
-
-    torch::Tensor ff2_inp = torch::empty({(bsz * seq_len), output_w.size(1)}, options);
-    torch::Tensor gelu_inp =
-        (gelu_checkpoint ? ff2_inp : torch::empty({(bsz * seq_len), output_w.size(1)}, options));
-    auto ff1_inp = torch::empty_like(input);
-    T* ff2_inp_ptr = (T*)ff2_inp.data_ptr();
-    T* gelu_inp_ptr = (T*)gelu_inp.data_ptr();
-    T* ff1_inp_ptr = (T*)ff1_inp.data_ptr();
-
-    torch::Tensor soft_out =
-        torch::empty({(bsz * layer->GetNumHeads() * seq_len), seq_len}, options);
-    torch::Tensor ctx_bufB =
-        (attn_dropout_checkpoint
-             ? soft_out
-             : torch::empty({(bsz * layer->GetNumHeads() * seq_len), seq_len}, options));
-    T* soft_out_ptr = (T*)soft_out.data_ptr();
-    T* ctx_bufB_ptr = (T*)ctx_bufB.data_ptr();
-
-    layer->SetTrainingMode(training_mode);
-    layer->SetIntermediateBuffers((uint8_t*)attn_prob_dropout_mask.data_ptr(),
-                                  (uint8_t*)attn_output_dropout_mask.data_ptr(),
-                                  (uint8_t*)layer_output_dropout_mask.data_ptr(),
-                                  (T*)attn_layer_norm_var.data_ptr(),
-                                  (T*)attn_layer_norm_mean.data_ptr(),
-                                  (T*)layer_norm_var.data_ptr(),
-                                  (T*)layer_norm_mean.data_ptr());
-
-    layer->Forward(bsz,
-                   input_ptr,
-                   input_mask_ptr,
-                   attn_qkvw_ptr,
-                   attn_qkvb_ptr,
-                   attn_ow_ptr,
-                   attn_ob_ptr,
-                   attn_nw_ptr,
-                   attn_nb_ptr,
-                   inter_w_ptr,
-                   inter_b_ptr,
-                   output_w_ptr,
-                   output_b_ptr,
-                   norm_w_ptr,
-                   norm_b_ptr,
-                   out_ptr,
-                   inp_norm_ptr,
-                   q_tf_ptr,
-                   k_tf_ptr,
-                   v_tf_ptr,
-                   soft_out_ptr,
-                   ctx_bufB_ptr,
-                   attn_o_inp_ptr,
-                   add_res_ptr,
-                   ff1_inp_ptr,
-                   gelu_inp_ptr,
-                   ff2_inp_ptr);
-
-    return {output,
-            inp_norm,
-            qkv_tf,
-            soft_out,
-            ctx_bufB,
-            attn_o_inp,
-            add_res,
-            ff1_inp,
-            gelu_inp,
-            ff2_inp,
-            attn_prob_dropout_mask,
-            attn_output_dropout_mask,
-            layer_output_dropout_mask,
-            attn_layer_norm_var,
-            attn_layer_norm_mean,
-            layer_norm_var,
-            layer_norm_mean};
-}
-
-template <typename T>
-std::vector<torch::Tensor> ds_transformer_backward(int layer_id,
-                                                   const torch::Tensor& grad_output,
-                                                   const torch::Tensor& output,
-                                                   const torch::Tensor& inp_norm,
-                                                   const torch::Tensor& qkv_tf,
-                                                   const torch::Tensor& soft_out,
-                                                   const torch::Tensor& ctx_bufB,
-                                                   const torch::Tensor& attn_o_inp,
-                                                   const torch::Tensor& add_res,
-                                                   const torch::Tensor& ff1_inp,
-                                                   const torch::Tensor& gelu_inp,
-                                                   const torch::Tensor& ff2_inp,
-                                                   const torch::Tensor& attn_prob_dropout_mask,
-                                                   const torch::Tensor& attn_output_dropout_mask,
-                                                   const torch::Tensor& layer_output_dropout_mask,
-                                                   const torch::Tensor& attn_layer_norm_var,
-                                                   const torch::Tensor& attn_layer_norm_mean,
-                                                   const torch::Tensor& layer_norm_var,
-                                                   const torch::Tensor& layer_norm_mean,
-                                                   const torch::Tensor& input,
-                                                   const torch::Tensor& input_mask,
-                                                   const torch::Tensor& attn_qkvw,
-                                                   const torch::Tensor& attn_qkvb,
-                                                   const torch::Tensor& attn_ow,
-                                                   const torch::Tensor& attn_ob,
-                                                   const torch::Tensor& attn_nw,
-                                                   const torch::Tensor& attn_nb,
-                                                   const torch::Tensor& inter_w,
-                                                   const torch::Tensor& inter_b,
-                                                   const torch::Tensor& output_w,
-                                                   const torch::Tensor& output_b,
-                                                   const torch::Tensor& norm_w,
-                                                   const torch::Tensor& norm_b)
-{
-    auto g_output = grad_output.contiguous();
-    CHECK_INPUT(g_output);
-    CHECK_INPUT(output);
-    CHECK_INPUT(inp_norm);
-    CHECK_INPUT(qkv_tf);
-    CHECK_INPUT(add_res);
-    CHECK_INPUT(soft_out);
-    CHECK_INPUT(ctx_bufB);
-    CHECK_INPUT(attn_o_inp);
-    CHECK_INPUT(ff1_inp);
-    CHECK_INPUT(gelu_inp);
-    CHECK_INPUT(ff2_inp);
-    CHECK_INPUT(input);
-    CHECK_INPUT(input_mask);
-    CHECK_INPUT(attn_qkvw);
-    CHECK_INPUT(attn_qkvb);
-    CHECK_INPUT(attn_ow);
-    CHECK_INPUT(attn_ob);
-    CHECK_INPUT(attn_nw);
-    CHECK_INPUT(attn_nb);
-    CHECK_INPUT(inter_w);
-    CHECK_INPUT(inter_b);
-    CHECK_INPUT(output_w);
-    CHECK_INPUT(output_b);
-    CHECK_INPUT(norm_w);
-    CHECK_INPUT(norm_b);
-
-    int bsz = g_output.size(0);
-
-    std::shared_ptr<BertTransformerLayer<T>> layer =
-        std::static_pointer_cast<BertTransformerLayer<T>>(s_transformer_layers[layer_id]);
-
-    int seq_len = layer->GetSeqLength();
-    if (g_output.size(1) != seq_len) {
-        seq_len = g_output.size(1);
-        layer->SetSeqLength(seq_len);
-    }
-    auto options = torch::TensorOptions()
-                       .dtype(g_output.options().dtype())
-                       .layout(torch::kStrided)
-                       .device(torch::kXPU)
-                       .requires_grad(true);
-    auto workspace = torch::empty({(long)get_workspace_size<T>(bsz,
-                                                               seq_len,
-                                                               layer->GetHiddenSize(),
-                                                               layer->GetIntermediateSize(),
-                                                               layer->GetNumHeads(),
-                                                               layer->IsTrainingMode(),
-                                                               layer->GeluCheckpoint())},
-                                  options);
-    ::SyclContext::Instance().SetWorkSpace((T*)workspace.data_ptr());
-
-    auto grad_input = torch::empty_like(input);
-    auto grad_attn_qkvw = torch::empty_like(attn_qkvw);
-    auto grad_attn_qkvb = torch::empty_like(attn_qkvb);
-    auto grad_attn_ow = torch::empty_like(attn_ow);
-    auto grad_attn_ob = torch::empty_like(attn_ob);
-    auto grad_attn_nw = torch::empty_like(attn_nw);
-    auto grad_attn_nb = torch::empty_like(attn_nb);
-    auto grad_inter_w = torch::empty_like(inter_w);
-    auto grad_inter_b = torch::empty_like(inter_b);
-    auto grad_output_w = torch::empty_like(output_w);
-    auto grad_output_b = torch::empty_like(output_b);
-    auto grad_norm_w = torch::empty_like(norm_w);
-    auto grad_norm_b = torch::empty_like(norm_b);
-
-    // inputs.
-    const T* grad_output_ptr = (const T*)g_output.data_ptr();
-    const T* input_ptr = (const T*)input.data_ptr();
-    const T* output_ptr = (const T*)output.data_ptr();
-    const T* inp_norm_ptr = (const T*)inp_norm.data_ptr();
-    const T* q_tf_ptr = (const T*)qkv_tf.data_ptr();
-    const T* add_res_ptr = (const T*)add_res.data_ptr();
-    const T* k_tf_ptr =
-        q_tf_ptr + (bsz * layer->GetSeqLength() * output_w.size(0));  //(const T*)k_tf.data_ptr();
-    const T* v_tf_ptr =
-        k_tf_ptr + (bsz * layer->GetSeqLength() * output_w.size(0));  //(const T*)v_tf.data_ptr();
-    const T* ff1_inp_ptr = (const T*)ff1_inp.data_ptr();
-    const T* gelu_inp_ptr = (const T*)gelu_inp.data_ptr();
-    const T* ff2_inp_ptr = (const T*)ff2_inp.data_ptr();
-    const T* ctx_bufB_ptr = (const T*)ctx_bufB.data_ptr();
-    const T* soft_out_ptr = (const T*)soft_out.data_ptr();
-    const T* attn_o_inp_ptr = (const T*)attn_o_inp.data_ptr();
-    const T* input_mask_ptr = (const T*)input_mask.data_ptr();
-    const T* attn_qkvw_ptr = (const T*)attn_qkvw.data_ptr();
-    const T* attn_ow_ptr = (const T*)attn_ow.data_ptr();
-    const T* attn_nw_ptr = (const T*)attn_nw.data_ptr();
-    const T* attn_nb_ptr = (const T*)attn_nb.data_ptr();
-    const T* inter_w_ptr = (const T*)inter_w.data_ptr();
-    const T* inter_b_ptr = (const T*)inter_b.data_ptr();
-    const T* output_w_ptr = (const T*)output_w.data_ptr();
-    const T* norm_w_ptr = (const T*)norm_w.data_ptr();
-    const T* norm_b_ptr = (const T*)norm_b.data_ptr();
-
-    // outputs.
-    T* grad_input_ptr = (T*)grad_input.data_ptr();
-    T* grad_attn_qkvw_ptr = (T*)grad_attn_qkvw.data_ptr();
-    T* grad_attn_qkvb_ptr = (T*)grad_attn_qkvb.data_ptr();
-    T* grad_attn_ow_ptr = (T*)grad_attn_ow.data_ptr();
-    T* grad_attn_ob_ptr = (T*)grad_attn_ob.data_ptr();
-    T* grad_attn_nw_ptr = (T*)grad_attn_nw.data_ptr();
-    T* grad_attn_nb_ptr = (T*)grad_attn_nb.data_ptr();
-    T* grad_inter_w_ptr = (T*)grad_inter_w.data_ptr();
-    T* grad_inter_b_ptr = (T*)grad_inter_b.data_ptr();
-    T* grad_output_w_ptr = (T*)grad_output_w.data_ptr();
-    T* grad_output_b_ptr = (T*)grad_output_b.data_ptr();
-    T* grad_norm_w_ptr = (T*)grad_norm_w.data_ptr();
-    T* grad_norm_b_ptr = (T*)grad_norm_b.data_ptr();
-
-    layer->SetIntermediateBuffers((uint8_t*)attn_prob_dropout_mask.data_ptr(),
-                                  (uint8_t*)attn_output_dropout_mask.data_ptr(),
-                                  (uint8_t*)layer_output_dropout_mask.data_ptr(),
-                                  (T*)attn_layer_norm_var.data_ptr(),
-                                  (T*)attn_layer_norm_mean.data_ptr(),
-                                  (T*)layer_norm_var.data_ptr(),
-                                  (T*)layer_norm_mean.data_ptr());
-
-    layer->Backward(bsz,
-                    grad_output_ptr,
-                    input_ptr,
-                    output_ptr,
-                    inp_norm_ptr,
-                    q_tf_ptr,
-                    k_tf_ptr,
-                    v_tf_ptr,
-                    soft_out_ptr,
-                    ctx_bufB_ptr,
-                    attn_o_inp_ptr,
-                    add_res_ptr,
-                    ff1_inp_ptr,
-                    gelu_inp_ptr,
-                    ff2_inp_ptr,
-                    input_mask_ptr,
-                    attn_qkvw_ptr,
-                    attn_ow_ptr,
-                    attn_nw_ptr,
-                    attn_nb_ptr,
-                    inter_w_ptr,
-                    inter_b_ptr,
-                    output_w_ptr,
-                    norm_w_ptr,
-                    norm_b_ptr,
-
-                    grad_input_ptr,
-                    grad_attn_qkvw_ptr,
-                    grad_attn_qkvb_ptr,
-                    grad_attn_ow_ptr,
-                    grad_attn_ob_ptr,
-                    grad_attn_nw_ptr,
-                    grad_attn_nb_ptr,
-                    grad_inter_w_ptr,
-                    grad_inter_b_ptr,
-                    grad_output_w_ptr,
-                    grad_output_b_ptr,
-                    grad_norm_w_ptr,
-                    grad_norm_b_ptr);
-
-    return {grad_input,
-            grad_attn_qkvw,
-            grad_attn_qkvb,
-            grad_attn_ow,
-            grad_attn_ob,
-            grad_attn_nw,
-            grad_attn_nb,
-            grad_inter_w,
-            grad_inter_b,
-            grad_output_w,
-            grad_output_b,
-            grad_norm_w,
-            grad_norm_b};
-}
-
-PYBIND11_MODULE(TORCH_EXTENSION_NAME, m)
-{
-    m.def("forward_fp32",
-          &ds_transformer_forward<float>,
-          "DeepSpeed Transformer forward with fp32 (DPCPP)");
-    m.def("forward_fp16",
-          &ds_transformer_forward<sycl::half>,
-          "DeepSpeed Transformer forward with fp16 (DPCPP)");
-    m.def("forward_bf16",
-          &ds_transformer_forward<bf16>,
-          "DeepSpeed Transformer forward with bf16 (DPCPP)");
-    m.def("backward_fp32",
-          &ds_transformer_backward<float>,
-          "DeepSpeed Transformer backward with fp32 (DPCPP)");
-    m.def("backward_fp16",
-          &ds_transformer_backward<sycl::half>,
-          "DeepSpeed Transformer backward with fp16 (DPCPP)");
-    m.def("backward_bf16",
-          &ds_transformer_backward<bf16>,
-          "DeepSpeed Transformer backward with bf16 (DPCPP)");
-    m.def("create_transformer_layer_fp32",
-          &create_transformer_layer<float>,
-          "Create DeepSpeed Transformer Transformer Layer with fp32 (DPCPP)");
-    m.def("create_transformer_layer_fp16",
-          &create_transformer_layer<sycl::half>,
-          "Create DeepSpeed Transformer Transformer Layer with fp16 (DPCPP)");
-    m.def("create_transformer_layer_bf16",
-          &create_transformer_layer<bf16>,
-          "Create DeepSpeed Transformer Transformer Layer with bf16 (DPCPP)");
-}
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/gelu_kernels.dp.cpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/gelu_kernels.dp.cpp
deleted file mode 100644
index 6991dcb..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/transformer/gelu_kernels.dp.cpp
+++ /dev/null
@@ -1,454 +0,0 @@
-#if __has_include(<sycl/sycl.hpp>)
-#include <sycl/sycl.hpp>
-using namespace sycl;
-#elif __has_include(<CL/sycl.hpp>)
-#include <CL/sycl.hpp>
-using namespace cl::sycl;
-#else
-#error "Unsupported compiler"
-#endif
-#include <ext/oneapi/bfloat16.hpp>
-
-using bf16 = sycl::ext::oneapi::bfloat16;
-
-inline float gelu(const float x)
-{
-    const float sqrt_param = 0.79788456080286535587989211986876f;
-    const float mul_param = 0.044715;
-    return x * 0.5f * (1.0f + tanh(sqrt_param * (x + mul_param * x * x * x)));
-}
-
-inline float d_gelu(const float x)
-{
-    const float sqrt_param = 0.79788456080286535587989211986876f;
-    const float mul_param = 0.044715;
-
-    float x2mul = x * x * mul_param;
-    float tan_h = tanh(sqrt_param * (x + x * x2mul));
-    float dg1 = 0.5f * (1.0f + tan_h);
-    float dg2 = x * 0.5f * sqrt_param * (1 - tan_h * tan_h);
-    float dg3 = dg2 * 3 * x2mul;
-    return (dg1 + dg2 + dg3);
-}
-
-/*
-  Fused bias add with GELU
-
-  Loads a vector of 4 elements each iteration, for stride
-  iterations. It was written with the intention to launch 256 thread
-  threadblocks, so to launch for bert-large, we would set ITERATIONS
-  to 4. This is currently done automatically as a heuristic, setting
-  the number of iterations as blocks of 1024.
-
-  For FP16, the values are loaded from memory as half, but converted
-  to FP32 for the arithmetic itself, to prevent numerous overflow on
-  the intermediate hyperbolic tangent, since there's no intrinsic
-  that computes it directly.
-*/
-
-void gelu_kernel(const float* input,
-                 float* vals,
-                 int row_stride,
-                 int iterations,
-                 nd_item<3> item_ct1)
-{
-    int row = item_ct1.get_group(2);
-    int id = item_ct1.get_local_id(2);
-    int loop_stride = item_ct1.get_local_range(2);
-
-    const float4* input_cast = reinterpret_cast<const float4*>(input);
-    float4* vals_cast = reinterpret_cast<float4*>(vals);
-
-    for (int i = 0; i < iterations; i++) {
-        if (i * loop_stride + id < row_stride) {
-            float4 data = input_cast[row * row_stride + i * loop_stride + id];
-
-            data.x() = gelu(data.x());
-            data.y() = gelu(data.y());
-            data.z() = gelu(data.z());
-            data.w() = gelu(data.w());
-
-            vals_cast[row * row_stride + i * loop_stride + id] = data;
-        }
-    }
-}
-
-void gelu_kernel(const half* input, half* vals, int row_stride, int iterations, nd_item<3> item_ct1)
-{
-    int row = item_ct1.get_group(2);
-    int id = item_ct1.get_local_id(2);
-    int loop_stride = item_ct1.get_local_range(2);
-
-    const float2* input_cast = reinterpret_cast<const float2*>(input);
-    float2* vals_cast = reinterpret_cast<float2*>(vals);
-
-    for (int i = 0; i < iterations; i++) {
-        if (i * loop_stride + id < row_stride) {
-            float2 vals_vec = input_cast[row * row_stride + i * loop_stride + id];
-
-            half2* vals_half = reinterpret_cast<half2*>(&vals_vec);
-
-            float2 low_data = vals_half[0].convert<float>();   // __half22float2(vals_half[0]);
-            float2 high_data = vals_half[1].convert<float>();  // __half22float2(vals_half[1]);
-
-            low_data.x() = gelu(low_data.x());
-            low_data.y() = gelu(low_data.y());
-            high_data.x() = gelu(high_data.x());
-            high_data.y() = gelu(high_data.y());
-
-            vals_half[0] = low_data.convert<half>();   // __float22half2_rn(low_data);
-            vals_half[1] = high_data.convert<half>();  // __float22half2_rn(high_data);
-
-            vals_cast[row * row_stride + i * loop_stride + id] = vals_vec;
-        }
-    }
-}
-
-void fused_bias_gelu(const float* input,
-                     const float* bias,
-                     float* vals,
-                     int row_stride,
-                     int iterations,
-                     nd_item<3> item_ct1)
-{
-    int row = item_ct1.get_group(2);
-    int id = item_ct1.get_local_id(2);
-    int loop_stride = item_ct1.get_local_range(2);
-
-    const float4* input_cast = reinterpret_cast<const float4*>(input);
-    float4* vals_cast = reinterpret_cast<float4*>(vals);
-    const float4* bias_cast = reinterpret_cast<const float4*>(bias);
-
-    for (int i = 0; i < iterations; i++) {
-        if (i * loop_stride + id < row_stride) {
-            float4 data = input_cast[row * row_stride + i * loop_stride + id];
-            float4 bias_data = bias_cast[i * loop_stride + id];
-
-            data.x() += bias_data.x();
-            data.y() += bias_data.y();
-            data.z() += bias_data.z();
-            data.w() += bias_data.w();
-
-            data.x() = gelu(data.x());
-            data.y() = gelu(data.y());
-            data.z() = gelu(data.z());
-            data.w() = gelu(data.w());
-
-            vals_cast[row * row_stride + i * loop_stride + id] = data;
-        }
-    }
-}
-
-void fused_bias_gelu(const bf16* input,
-                     const bf16* bias,
-                     bf16* vals,
-                     int row_stride,
-                     int iterations,
-                     nd_item<3> item_ct1)
-{
-    int row = item_ct1.get_group(2);
-    int id = item_ct1.get_local_id(2);
-    int loop_stride = item_ct1.get_local_range(2);
-
-    const ushort4* input_cast = reinterpret_cast<const ushort4*>(input);
-    ushort4* vals_cast = reinterpret_cast<ushort4*>(vals);
-    const ushort4* bias_cast = reinterpret_cast<const ushort4*>(bias);
-
-    for (int i = 0; i < iterations; i++) {
-        if (i * loop_stride + id < row_stride) {
-            ushort4 vals_vec = input_cast[row * row_stride + i * loop_stride + id];
-            ushort4 bias_vec = bias_cast[i * loop_stride + id];
-
-            float4 data = {float(vals_vec.x()),
-                           float(vals_vec.y()),
-                           float(vals_vec.z()),
-                           float(vals_vec.w())};
-            float4 bias = {float(bias_vec.x()),
-                           float(bias_vec.y()),
-                           float(bias_vec.z()),
-                           float(bias_vec.w())};
-
-            data += bias;
-
-            data.x() = gelu(data.x());
-            data.y() = gelu(data.y());
-            data.z() = gelu(data.z());
-            data.w() = gelu(data.w());
-
-            vals_vec.x() = bf16(data.x());
-            vals_vec.y() = bf16(data.y());
-            vals_vec.z() = bf16(data.z());
-            vals_vec.w() = bf16(data.w());
-
-            vals_cast[row * row_stride + i * loop_stride + id] = vals_vec;
-        }
-    }
-}
-
-void fused_bias_gelu(const half* input,
-                     const half* bias,
-                     half* vals,
-                     int row_stride,
-                     int iterations,
-                     nd_item<3> item_ct1)
-{
-    int row = item_ct1.get_group(2);
-    int id = item_ct1.get_local_id(2);
-    int loop_stride = item_ct1.get_local_range(2);
-
-    const float2* input_cast = reinterpret_cast<const float2*>(input);
-    float2* vals_cast = reinterpret_cast<float2*>(vals);
-    const float2* bias_cast = reinterpret_cast<const float2*>(bias);
-
-    for (int i = 0; i < iterations; i++) {
-        if (i * loop_stride + id < row_stride) {
-            float2 vals_vec = input_cast[row * row_stride + i * loop_stride + id];
-            float2 bias_vec = bias_cast[i * loop_stride + id];
-
-            half2* vals_half = reinterpret_cast<half2*>(&vals_vec);
-            half2* bias_half = reinterpret_cast<half2*>(&bias_vec);
-
-            float2 low_data = vals_half[0].convert<float>();   // __half22float2(vals_half[0]);
-            float2 high_data = vals_half[1].convert<float>();  // __half22float2(vals_half[1]);
-
-            float2 low_bias = bias_half[0].convert<float>();   // __half22float2(bias_half[0]);
-            float2 high_bias = bias_half[1].convert<float>();  // __half22float2(bias_half[1]);
-
-            low_data.x() += low_bias.x();
-            low_data.y() += low_bias.y();
-            high_data.x() += high_bias.x();
-            high_data.y() += high_bias.y();
-
-            low_data.x() = gelu(low_data.x());
-            low_data.y() = gelu(low_data.y());
-            high_data.x() = gelu(high_data.x());
-            high_data.y() = gelu(high_data.y());
-
-            vals_half[0] = low_data.convert<half>();   // __float22half2_rn(low_data);
-            vals_half[1] = high_data.convert<half>();  // __float22half2_rn(high_data);
-
-            vals_cast[row * row_stride + i * loop_stride + id] = vals_vec;
-        }
-    }
-}
-
-void d_gelu_func(float* d_output,
-                 const float* gelu_input,
-                 const float* bias,
-                 int row_stride,
-                 int iterations,
-                 nd_item<3> item_ct1)
-{
-    int row = item_ct1.get_group(2);
-    int id = item_ct1.get_local_id(2);
-    int loop_stride = item_ct1.get_local_range(2);
-
-    float4* d_output_cast = reinterpret_cast<float4*>(d_output);
-    const float4* gelu_input_cast = reinterpret_cast<const float4*>(gelu_input);
-    const float4* bias_cast = reinterpret_cast<const float4*>(bias);
-
-    for (int i = 0; i < iterations; i++) {
-        if (i * loop_stride + id < row_stride) {
-            float4 output_data = d_output_cast[row * row_stride + i * loop_stride + id];
-            float4 gelu_input_data = gelu_input_cast[row * row_stride + i * loop_stride + id];
-            float4 bias_data = bias_cast[i * loop_stride + id];
-
-            gelu_input_data.x() += bias_data.x();
-            gelu_input_data.y() += bias_data.y();
-            gelu_input_data.z() += bias_data.z();
-            gelu_input_data.w() += bias_data.w();
-
-            output_data.x() *= d_gelu(gelu_input_data.x());
-            output_data.y() *= d_gelu(gelu_input_data.y());
-            output_data.z() *= d_gelu(gelu_input_data.z());
-            output_data.w() *= d_gelu(gelu_input_data.w());
-
-            d_output_cast[row * row_stride + i * loop_stride + id] = output_data;
-        }
-    }
-}
-
-void d_gelu_func(bf16* d_output,
-                 const bf16* gelu_input,
-                 const bf16* bias,
-                 int row_stride,
-                 int iterations,
-                 nd_item<3> item_ct1)
-{
-    int row = item_ct1.get_group(2);
-    int id = item_ct1.get_local_id(2);
-    int loop_stride = item_ct1.get_local_range(2);
-
-    ushort4* d_output_cast = reinterpret_cast<ushort4*>(d_output);
-    const ushort4* gelu_input_cast = reinterpret_cast<const ushort4*>(gelu_input);
-    const ushort4* bias_cast = reinterpret_cast<const ushort4*>(bias);
-
-    for (int i = 0; i < iterations; i++) {
-        if (i * loop_stride + id < row_stride) {
-            ushort4 output_vec = d_output_cast[row * row_stride + i * loop_stride + id];
-            ushort4 gelu_input_vec = gelu_input_cast[row * row_stride + i * loop_stride + id];
-            ushort4 bias_vec = bias_cast[i * loop_stride + id];
-
-            float4 gelu_input_data = {float(gelu_input_vec.x()),
-                                      float(gelu_input_vec.y()),
-                                      float(gelu_input_vec.z()),
-                                      float(gelu_input_vec.w())};
-            float4 bias_data = {
-                float(bias_vec.x()),
-                float(bias_vec.y()),
-                float(bias_vec.z()),
-                float(bias_vec.w()),
-            };
-            float4 output_data = {
-                float(output_vec.x()),
-                float(output_vec.y()),
-                float(output_vec.z()),
-                float(output_vec.w()),
-            };
-
-            gelu_input_data.x() += bias_data.x();
-            gelu_input_data.y() += bias_data.y();
-            gelu_input_data.z() += bias_data.z();
-            gelu_input_data.w() += bias_data.w();
-
-            output_data.x() *= d_gelu(gelu_input_data.x());
-            output_data.y() *= d_gelu(gelu_input_data.y());
-            output_data.z() *= d_gelu(gelu_input_data.z());
-            output_data.w() *= d_gelu(gelu_input_data.w());
-
-            output_vec.x() = bf16(output_data.x());
-            output_vec.y() = bf16(output_data.y());
-            output_vec.z() = bf16(output_data.z());
-            output_vec.w() = bf16(output_data.w());
-            d_output_cast[row * row_stride + i * loop_stride + id] = output_vec;
-        }
-    }
-}
-
-void d_gelu_func(half* d_output,
-                 const half* gelu_input,
-                 const half* bias,
-                 int row_stride,
-                 int iterations,
-                 nd_item<3> item_ct1)
-{
-    int row = item_ct1.get_group(2);
-    int id = item_ct1.get_local_id(2);
-    int loop_stride = item_ct1.get_local_range(2);
-
-    float2* d_output_cast = reinterpret_cast<float2*>(d_output);
-    const float2* gelu_input_cast = reinterpret_cast<const float2*>(gelu_input);
-    const float2* bias_cast = reinterpret_cast<const float2*>(bias);
-
-#pragma unroll
-    for (int i = 0; i < iterations; i++) {
-        if (i * loop_stride + id < row_stride) {
-            float2 output_data = d_output_cast[row * row_stride + i * loop_stride + id];
-            float2 gelu_input_data = gelu_input_cast[row * row_stride + i * loop_stride + id];
-            float2 bias_vec = bias_cast[i * loop_stride + id];
-
-            half2* output_data_half = reinterpret_cast<half2*>(&output_data);
-            half2* gelu_input_data_half = reinterpret_cast<half2*>(&gelu_input_data);
-            half2* bias_half = reinterpret_cast<half2*>(&bias_vec);
-
-            float2 output_half_0 =
-                output_data_half[0].convert<float>();  // __half22float2(output_data_half[0]);
-            float2 output_half_1 =
-                output_data_half[1].convert<float>();  // __half22float2(output_data_half[1]);
-
-            float2 gelu_input_half_0 =
-                gelu_input_data_half[0]
-                    .convert<float>();  // __half22float2(gelu_input_data_half[0]);
-            float2 gelu_input_half_1 =
-                gelu_input_data_half[1]
-                    .convert<float>();  // __half22float2(gelu_input_data_half[1]);
-
-            float2 bias_half_0 = bias_half[0].convert<float>();  // __half22float2(bias_half[0]);
-            float2 bias_half_1 = bias_half[1].convert<float>();  // __half22float2(bias_half[1]);
-
-            gelu_input_half_0.x() += bias_half_0.x();
-            gelu_input_half_0.y() += bias_half_0.y();
-            gelu_input_half_1.x() += bias_half_1.x();
-            gelu_input_half_1.y() += bias_half_1.y();
-
-            output_half_0.x() *= d_gelu(gelu_input_half_0.x());
-            output_half_0.y() *= d_gelu(gelu_input_half_0.y());
-            output_half_1.x() *= d_gelu(gelu_input_half_1.x());
-            output_half_1.y() *= d_gelu(gelu_input_half_1.y());
-
-            float2 result;
-            half2* result_half2 = reinterpret_cast<half2*>(&result);
-
-            result_half2[0] = output_half_0.convert<half>();  // __float22half2_rn(output_half_0);
-            result_half2[1] = output_half_1.convert<half>();  // __float22half2_rn(output_half_1);
-
-            d_output_cast[row * row_stride + i * loop_stride + id] = result;
-        }
-    }
-}
-
-template <typename T>
-void launch_bias_gelu(const T* input,
-                      const T* bias,
-                      T* output,
-                      int intermediate_size,
-                      int batch_size,
-                      queue* stream)
-{
-    int iterations = (intermediate_size + 1023) / 1024;
-    int threads = (intermediate_size - 1) / (iterations * 4) + 1;
-    range<3> block_dims(1, 1, threads);
-    range<3> grid_dims(1, 1, batch_size);
-
-    stream->submit([&](handler& cgh) {
-        cgh.parallel_for(nd_range<3>(grid_dims * block_dims, block_dims), [=](nd_item<3> item_ct1) {
-            fused_bias_gelu(input, bias, output, intermediate_size / 4, iterations, item_ct1);
-        });
-    });
-}
-
-template <typename T>
-void launch_gelu(const T* input, T* output, int intermediate_size, int batch_size, queue* stream)
-{
-    int iterations = (intermediate_size + 1023) / 1024;
-    int threads = (intermediate_size - 1) / (iterations * 4) + 1;
-    range<3> block_dims(1, 1, threads);
-    range<3> grid_dims(1, 1, batch_size);
-
-    stream->submit([&](handler& cgh) {
-        cgh.parallel_for(nd_range<3>(grid_dims * block_dims, block_dims), [=](nd_item<3> item_ct1) {
-            gelu_kernel(input, output, intermediate_size / 4, iterations, item_ct1);
-        });
-    });
-}
-
-template void launch_bias_gelu<float>(const float*, const float*, float*, int, int, queue*);
-template void launch_bias_gelu<half>(const half*, const half*, half*, int, int, queue*);
-template void launch_bias_gelu<bf16>(const bf16*, const bf16*, bf16*, int, int, queue*);
-
-template void launch_gelu<float>(const float*, float*, int, int, queue*);
-template void launch_gelu<half>(const half*, half*, int, int, queue*);
-
-template <typename T>
-void launch_d_gelu(T* d_output,
-                   const T* input,
-                   const T* bias,
-                   int intermediate_size,
-                   int batch_size,
-                   queue* stream)
-{
-    int iterations = (intermediate_size + 1023) / 1024;
-    int threads = (intermediate_size - 1) / (iterations * 4) + 1;
-    range<3> block_dims(1, 1, threads);
-    range<3> grid_dims(1, 1, batch_size);
-
-    stream->submit([&](handler& cgh) {
-        cgh.parallel_for(nd_range<3>(grid_dims * block_dims, block_dims), [=](nd_item<3> item_ct1) {
-            d_gelu_func(d_output, input, bias, intermediate_size / 4, iterations, item_ct1);
-        });
-    });
-}
-
-template void launch_d_gelu<float>(float*, const float*, const float*, int, int, queue*);
-template void launch_d_gelu<half>(half*, const half*, const half*, int, int, queue*);
-template void launch_d_gelu<bf16>(bf16*, const bf16*, const bf16*, int, int, queue*);
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/general_kernels.dp.cpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/general_kernels.dp.cpp
deleted file mode 100644
index ce64746..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/transformer/general_kernels.dp.cpp
+++ /dev/null
@@ -1,546 +0,0 @@
-#include "general_kernels.hpp"
-#if __has_include(<sycl/sycl.hpp>)
-#include <sycl/sycl.hpp>
-using namespace sycl;
-#elif __has_include(<CL/sycl.hpp>)
-#include <CL/sycl.hpp>
-using namespace cl::sycl;
-#else
-#error "Unsupported compiler"
-#endif
-
-constexpr int MAX_SG_NUM = 32;
-constexpr int MAX_SG_NUM1 = MAX_SG_NUM + 1;
-template <typename T>
-void column_sum_reduce(const T* inp, T* out, int rows, int width, nd_item<3> item_ct1, float* tile)
-{
-    group<3> b = item_ct1.get_group();
-    sub_group sg = item_ct1.get_sub_group();
-
-    int idx = item_ct1.get_local_range(2) * item_ct1.get_group(2) + item_ct1.get_local_id(2);
-
-    int y_stride = width * MAX_SG_NUM;
-
-    float localSum = 0;
-
-    // Loop across matrix height
-    if (idx < width) {
-        int offset = item_ct1.get_local_id(1) * width + idx;
-        for (int r = item_ct1.get_local_id(1); r < rows; r += MAX_SG_NUM) {
-            localSum += (float)inp[offset];
-            offset += y_stride;
-        }
-    }
-
-    tile[item_ct1.get_local_id(2) * MAX_SG_NUM1 + item_ct1.get_local_id(1)] = localSum;
-
-    item_ct1.barrier();
-
-    // Sum the shared buffer.
-    float sum = tile[item_ct1.get_local_id(1) * MAX_SG_NUM1 + item_ct1.get_local_id(2)];
-
-#ifndef __STOCHASTIC_MODE__
-    item_ct1.barrier();
-#endif
-
-    for (int i = 1; i < MAX_SG_NUM; i <<= 1) { sum += sg.shuffle_down(sum, i); }
-
-    if (item_ct1.get_local_id(2) == 0) {
-        int pos = item_ct1.get_group(2) * MAX_SG_NUM + item_ct1.get_local_id(1);
-        if (pos < width) out[pos] = sum;
-    }
-}
-
-template <>
-void column_sum_reduce(const bf16* inp,
-                       bf16* out,
-                       int rows,
-                       int width,
-                       nd_item<3> item_ct1,
-                       float* tile)
-{
-    group<3> b = item_ct1.get_group();
-    sub_group sg = item_ct1.get_sub_group();
-
-    int idx = item_ct1.get_local_range(2) * item_ct1.get_group(2) + item_ct1.get_local_id(2);
-
-    int y_stride = width * MAX_SG_NUM;
-
-    float localSum = 0;
-    ushort* inp_cast = (ushort*)inp;
-    ushort* out_cast = (ushort*)out;
-    // Loop across matrix height
-    if (idx < width) {
-        int offset = item_ct1.get_local_id(1) * width + idx;
-        for (int r = item_ct1.get_local_id(1); r < rows; r += MAX_SG_NUM) {
-            localSum += float(inp_cast[offset]);
-            offset += y_stride;
-        }
-    }
-
-    tile[item_ct1.get_local_id(2) * MAX_SG_NUM1 + item_ct1.get_local_id(1)] = localSum;
-
-    item_ct1.barrier();
-
-    // Sum the shared buffer.
-    float sum = tile[item_ct1.get_local_id(1) * MAX_SG_NUM1 + item_ct1.get_local_id(2)];
-
-#ifndef __STOCHASTIC_MODE__
-    item_ct1.barrier();
-#endif
-
-    for (int i = 1; i < MAX_SG_NUM; i <<= 1) { sum += sg.shuffle_down(sum, i); }
-
-    if (item_ct1.get_local_id(2) == 0) {
-        int pos = item_ct1.get_group(2) * MAX_SG_NUM + item_ct1.get_local_id(1);
-        if (pos < width) out_cast[pos] = bf16(sum);
-    }
-}
-
-template <typename T>
-void launch_fuse_transpose_bias_kernel(const T* inp, T* out, int rows, int cols, queue* stream)
-{
-    range<3> grid_dim(1, 1, (cols - 1) / MAX_SG_NUM + 1);
-    range<3> block_dim(1, MAX_SG_NUM, MAX_SG_NUM);
-
-    stream->submit([&](handler& cgh) {
-        accessor<float, 2, access_mode::read_write, access::target::local> tile(
-            range<2>(MAX_SG_NUM, MAX_SG_NUM1), cgh);
-        cgh.parallel_for(nd_range<3>(grid_dim * block_dim, block_dim),
-                         [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                             column_sum_reduce<T>(
-                                 inp, out, rows, cols, item_ct1, tile.get_pointer());
-                         });
-    });
-}
-
-template void launch_fuse_transpose_bias_kernel<float>(const float* inp,
-                                                       float* out,
-                                                       int rows,
-                                                       int cols,
-                                                       queue* stream);
-template void launch_fuse_transpose_bias_kernel<bf16>(const bf16* inp,
-                                                      bf16* out,
-                                                      int rows,
-                                                      int cols,
-                                                      queue* stream);
-template void launch_fuse_transpose_bias_kernel<half>(const half* inp,
-                                                      half* out,
-                                                      int rows,
-                                                      int cols,
-                                                      queue* stream);
-
-void fused_add2_kernel(const int N,
-                       float* out,
-                       const float* inp1,
-                       const float* inp2,
-                       nd_item<3> item_ct1)
-{
-    const float4* inp1_4 = reinterpret_cast<const float4*>(inp1);
-    const float4* inp2_4 = reinterpret_cast<const float4*>(inp2);
-    float4* out_4 = reinterpret_cast<float4*>(out);
-
-    DPCPP_1D_KERNEL_LOOP(j, N)
-    {
-        float4 val;
-        float4 inp1_reg = inp1_4[j];
-        float4 inp2_reg = inp2_4[j];
-
-        val.x() = inp1_reg.x() + inp2_reg.x();
-        val.y() = inp1_reg.y() + inp2_reg.y();
-        val.z() = inp1_reg.z() + inp2_reg.z();
-        val.w() = inp1_reg.w() + inp2_reg.w();
-
-        out_4[j] = val;
-    }
-}
-
-void fused_add2_kernel(const int N,
-                       bf16* out,
-                       const bf16* inp1,
-                       const bf16* inp2,
-                       nd_item<3> item_ct1)
-{
-    const ushort4* inp1_cast = reinterpret_cast<const ushort4*>(inp1);
-    const ushort4* inp2_cast = reinterpret_cast<const ushort4*>(inp2);
-    ushort4* out_cast = reinterpret_cast<ushort4*>(out);
-
-    DPCPP_1D_KERNEL_LOOP(j, N)
-    {
-        float4 val;
-        float4 inp1_reg = {float(inp1_cast[j].x()),
-                           float(inp1_cast[j].y()),
-                           float(inp1_cast[j].z()),
-                           float(inp1_cast[j].w())};
-        float4 inp2_reg = {float(inp2_cast[j].x()),
-                           float(inp2_cast[j].y()),
-                           float(inp2_cast[j].z()),
-                           float(inp2_cast[j].w())};
-
-        val.x() = inp1_reg.x() + inp2_reg.x();
-        val.y() = inp1_reg.y() + inp2_reg.y();
-        val.z() = inp1_reg.z() + inp2_reg.z();
-        val.w() = inp1_reg.w() + inp2_reg.w();
-
-        out_cast[j] = {bf16(val.x()),
-                       bf16(val.y()),
-                       bf16(val.z()),
-                       bf16(val.w())};
-    }
-}
-
-void fused_add2_kernel(const int N,
-                       half* out,
-                       const half* inp1,
-                       const half* inp2,
-                       nd_item<3> item_ct1)
-{
-    float2 inp1_4;
-    float2 inp2_4;
-
-    half2* inp1_h = reinterpret_cast<half2*>(&inp1_4);
-    half2* inp2_h = reinterpret_cast<half2*>(&inp2_4);
-
-    const float2* inp1_arr = reinterpret_cast<const float2*>(inp1);
-    const float2* inp2_arr = reinterpret_cast<const float2*>(inp2);
-
-    DPCPP_1D_KERNEL_LOOP(j, N)
-    {
-        inp1_4 = inp1_arr[j];
-        inp2_4 = inp2_arr[j];
-
-        float2 inp1_h_f_0 = inp1_h[0].convert<float, rounding_mode::automatic>();
-        float2 inp1_h_f_1 = inp1_h[1].convert<float, rounding_mode::automatic>();
-
-        float2 inp2_h_f_0 = inp2_h[0].convert<float, rounding_mode::automatic>();
-        float2 inp2_h_f_1 = inp2_h[1].convert<float, rounding_mode::automatic>();
-
-        inp1_h_f_0.x() += inp2_h_f_0.x();
-        inp1_h_f_0.y() += inp2_h_f_0.y();
-        inp1_h_f_1.x() += inp2_h_f_1.x();
-        inp1_h_f_1.y() += inp2_h_f_1.y();
-
-        float2 val_f;
-        half2* val_h = reinterpret_cast<half2*>(&val_f);
-
-        val_h[0] = inp1_h_f_0.convert<half, rounding_mode::rte>();
-        val_h[1] = inp1_h_f_1.convert<half, rounding_mode::rte>();
-
-        float2* out_4 = reinterpret_cast<float2*>(out);
-        out_4[j] = val_f;
-    }
-}
-
-template <typename T>
-void launch_fused_add2(T* out,
-                       const T* inp1,
-                       const T* inp2,
-                       int batch_size,
-                       int seq_length,
-                       int hidden_dim,
-                       queue* stream)
-{
-    int total_count = batch_size * seq_length * hidden_dim / 4;
-    range<3> grid_dim = range<3>(1, 1, DS_GET_BLOCKS(total_count));  //(batch_size * seq_length);
-
-    range<3> block_dim = range<3>(1, 1, DS_CUDA_NUM_THREADS);  //(hidden_dim / 4);
-    stream->submit([&](handler& cgh) {
-        cgh.parallel_for(nd_range<3>(grid_dim * block_dim, block_dim), [=](nd_item<3> item_ct1) {
-            fused_add2_kernel(total_count, out, inp1, inp2, item_ct1);
-        });
-    });
-}
-
-template void launch_fused_add2<float>(float* out,
-                                       const float* inp1,
-                                       const float* inp2,
-                                       int batch_size,
-                                       int seq_length,
-                                       int hidden_dim,
-                                       queue* stream);
-template void launch_fused_add2<bf16>(bf16* out,
-                                      const bf16* inp1,
-                                      const bf16* inp2,
-                                      int batch_size,
-                                      int seq_length,
-                                      int hidden_dim,
-                                      queue* stream);
-template void launch_fused_add2<half>(half* out,
-                                      const half* inp1,
-                                      const half* inp2,
-                                      int batch_size,
-                                      int seq_length,
-                                      int hidden_dim,
-                                      queue* stream);
-
-void fused_add3_kernel(float* out,
-                       const float* inp1,
-                       const float* inp2,
-                       const float* inp3,
-                       int size,
-                       int row_stride,
-                       nd_item<3> item_ct1)
-{
-    int row = item_ct1.get_group(2);
-    int id = item_ct1.get_local_id(2);
-
-    const float4* inp1_4 = reinterpret_cast<const float4*>(inp1);
-    const float4* inp2_4 = reinterpret_cast<const float4*>(inp2);
-    const float4* inp3_4 = reinterpret_cast<const float4*>(inp3);
-
-    float4* out_4 = reinterpret_cast<float4*>(out);
-
-    float4 val;
-    float4 inp1_reg = inp1_4[row * row_stride + id];
-    float4 inp2_reg = inp2_4[row * row_stride + id];
-    float4 inp3_reg = inp3_4[row * row_stride + id];
-
-    val.x() = inp1_reg.x() + inp2_reg.x() + inp3_reg.x();
-    val.y() = inp1_reg.y() + inp2_reg.y() + inp3_reg.y();
-    val.z() = inp1_reg.z() + inp2_reg.z() + inp3_reg.z();
-    val.w() = inp1_reg.w() + inp2_reg.w() + inp3_reg.w();
-
-    out_4[row * row_stride + id] = val;
-}
-
-void fused_add3_kernel(half* out,
-                       const half* inp1,
-                       const half* inp2,
-                       const half* inp3,
-                       int size,
-                       int row_stride,
-                       nd_item<3> item_ct1)
-{
-    int row = item_ct1.get_group(2);
-    int id = item_ct1.get_local_id(2);
-    const float2* inp1_arr = reinterpret_cast<const float2*>(inp1);
-    const float2* inp2_arr = reinterpret_cast<const float2*>(inp2);
-    const float2* inp3_arr = reinterpret_cast<const float2*>(inp3);
-
-    float2 inp1_4 = inp1_arr[row * row_stride + id];
-    float2 inp2_4 = inp2_arr[row * row_stride + id];
-    float2 inp3_4 = inp3_arr[row * row_stride + id];
-
-    half2* inp1_h = reinterpret_cast<half2*>(&inp1_4);
-    half2* inp2_h = reinterpret_cast<half2*>(&inp2_4);
-    half2* inp3_h = reinterpret_cast<half2*>(&inp3_4);
-
-    float2 inp1_h_f_0 = inp1_h[0].convert<float, rounding_mode::automatic>();
-    float2 inp1_h_f_1 = inp1_h[1].convert<float, rounding_mode::automatic>();
-
-    float2 inp2_h_f_0 = inp2_h[0].convert<float, rounding_mode::automatic>();
-    float2 inp2_h_f_1 = inp2_h[1].convert<float, rounding_mode::automatic>();
-
-    float2 inp3_h_f_0 = inp3_h[0].convert<float, rounding_mode::automatic>();
-    float2 inp3_h_f_1 = inp3_h[1].convert<float, rounding_mode::automatic>();
-
-    inp1_h_f_0.x() += (inp2_h_f_0.x() + inp3_h_f_0.x());
-    inp1_h_f_0.y() += (inp2_h_f_0.y() + inp3_h_f_0.y());
-    inp1_h_f_1.x() += (inp2_h_f_1.x() + inp3_h_f_1.x());
-    inp1_h_f_1.y() += (inp2_h_f_1.y() + inp3_h_f_1.y());
-
-    float2 val_f;
-    half2* val_h = reinterpret_cast<half2*>(&val_f);
-
-    val_h[0] = inp1_h_f_0.convert<half, rounding_mode::rte>();
-    val_h[1] = inp1_h_f_1.convert<half, rounding_mode::rte>();
-
-    float2* out_4 = reinterpret_cast<float2*>(out);
-    out_4[row * row_stride + id] = val_f;
-}
-
-template <>
-void launch_fused_add3<float>(float* out,
-                              const float* inp1,
-                              const float* inp2,
-                              const float* inp3,
-                              int batch_size,
-                              int seq_length,
-                              int hidden_size,
-                              queue* stream)
-{
-    range<3> grid_dim(1, 1, batch_size * seq_length);
-    range<3> block_dim(1, 1, hidden_size / 4);
-
-    stream->submit([&](handler& cgh) {
-        cgh.parallel_for(nd_range<3>(grid_dim * block_dim, block_dim), [=](nd_item<3> item_ct1) {
-            fused_add3_kernel(out,
-                              inp1,
-                              inp2,
-                              inp3,
-                              (batch_size * seq_length * hidden_size),
-                              hidden_size / 4,
-                              item_ct1);
-        });
-    });
-}
-
-template <>
-void launch_fused_add3<half>(half* out,
-                             const half* inp1,
-                             const half* inp2,
-                             const half* inp3,
-                             int batch_size,
-                             int seq_length,
-                             int hidden_size,
-                             queue* stream)
-{
-    range<3> grid_dim(1, 1, batch_size * seq_length);
-
-    range<3> block_dim(1, 1, hidden_size / 4);
-
-    stream->submit([&](handler& cgh) {
-        cgh.parallel_for(nd_range<3>(grid_dim * block_dim, block_dim), [=](nd_item<3> item_ct1) {
-            fused_add3_kernel(out,
-                              inp1,
-                              inp2,
-                              inp3,
-                              (batch_size * seq_length * hidden_size),
-                              hidden_size / 4,
-                              item_ct1);
-        });
-    });
-}
-
-void fused_add4_kernel(float* out,
-                       const float* inp1,
-                       const float* inp2,
-                       const float* inp3,
-                       const float* inp4,
-                       int size,
-                       int row_stride,
-                       nd_item<3> item_ct1)
-{
-    int row = item_ct1.get_group(2);
-    int id = item_ct1.get_local_id(2);
-
-    const float4* inp1_4 = reinterpret_cast<const float4*>(inp1);
-    const float4* inp2_4 = reinterpret_cast<const float4*>(inp2);
-    const float4* inp3_4 = reinterpret_cast<const float4*>(inp3);
-    const float4* inp4_4 = reinterpret_cast<const float4*>(inp4);
-    float4* out_4 = reinterpret_cast<float4*>(out);
-
-    float4 val;
-    float4 inp1_reg = inp1_4[row * row_stride + id];
-    float4 inp2_reg = inp2_4[row * row_stride + id];
-    float4 inp3_reg = inp3_4[row * row_stride + id];
-    float4 inp4_reg = inp4_4[row * row_stride + id];
-
-    val.x() = inp1_reg.x() + inp2_reg.x() + inp3_reg.x() + inp4_reg.x();
-    val.y() = inp1_reg.y() + inp2_reg.y() + inp3_reg.y() + inp4_reg.y();
-    val.z() = inp1_reg.z() + inp2_reg.z() + inp3_reg.z() + inp4_reg.z();
-    val.w() = inp1_reg.w() + inp2_reg.w() + inp3_reg.w() + inp4_reg.w();
-
-    out_4[row * row_stride + id] = val;
-}
-
-void fused_add4_kernel(half* out,
-                       const half* inp1,
-                       const half* inp2,
-                       const half* inp3,
-                       const half* inp4,
-                       int size,
-                       int row_stride,
-                       nd_item<3> item_ct1)
-{
-    int row = item_ct1.get_group(2);
-    int id = item_ct1.get_local_id(2);
-    const float2* inp1_arr = reinterpret_cast<const float2*>(inp1);
-    const float2* inp2_arr = reinterpret_cast<const float2*>(inp2);
-    const float2* inp3_arr = reinterpret_cast<const float2*>(inp3);
-    const float2* inp4_arr = reinterpret_cast<const float2*>(inp4);
-
-    float2 inp1_4 = inp1_arr[row * row_stride + id];
-    float2 inp2_4 = inp2_arr[row * row_stride + id];
-    float2 inp3_4 = inp3_arr[row * row_stride + id];
-    float2 inp4_4 = inp4_arr[row * row_stride + id];
-
-    half2* inp1_h = reinterpret_cast<half2*>(&inp1_4);
-    half2* inp2_h = reinterpret_cast<half2*>(&inp2_4);
-    half2* inp3_h = reinterpret_cast<half2*>(&inp3_4);
-    half2* inp4_h = reinterpret_cast<half2*>(&inp4_4);
-
-    float2 inp1_h_f_0 = inp1_h[0].convert<float, rounding_mode::automatic>();
-    float2 inp1_h_f_1 = inp1_h[1].convert<float, rounding_mode::automatic>();
-
-    float2 inp2_h_f_0 = inp2_h[0].convert<float, rounding_mode::automatic>();
-    float2 inp2_h_f_1 = inp2_h[1].convert<float, rounding_mode::automatic>();
-
-    float2 inp3_h_f_0 = inp3_h[0].convert<float, rounding_mode::automatic>();
-    float2 inp3_h_f_1 = inp3_h[1].convert<float, rounding_mode::automatic>();
-
-    float2 inp4_h_f_0 = inp4_h[0].convert<float, rounding_mode::automatic>();
-    float2 inp4_h_f_1 = inp4_h[1].convert<float, rounding_mode::automatic>();
-
-    inp1_h_f_0.x() += (inp2_h_f_0.x() + inp3_h_f_0.x() + inp4_h_f_0.x());
-    inp1_h_f_0.y() += (inp2_h_f_0.y() + inp3_h_f_0.y() + inp4_h_f_0.y());
-    inp1_h_f_1.x() += (inp2_h_f_1.x() + inp3_h_f_1.x() + inp4_h_f_1.x());
-    inp1_h_f_1.y() += (inp2_h_f_1.y() + inp3_h_f_1.y() + inp4_h_f_1.y());
-
-    float2 val_f;
-    half2* val_h = reinterpret_cast<half2*>(&val_f);
-
-    val_h[0] = inp1_h_f_0.convert<half, rounding_mode::rte>();
-    val_h[1] = inp1_h_f_1.convert<half, rounding_mode::rte>();
-
-    float2* out_4 = reinterpret_cast<float2*>(out);
-    out_4[row * row_stride + id] = val_f;
-}
-
-template <>
-void launch_fused_add4<float>(float* out,
-                              const float* inp1,
-                              const float* inp2,
-                              const float* inp3,
-                              const float* inp4,
-                              int batch_size,
-                              int seq_length,
-                              int hidden_size,
-                              queue* stream)
-{
-    range<3> grid_dim(1, 1, batch_size * seq_length);
-
-    range<3> block_dim(1, 1, hidden_size / 4);
-
-    stream->submit([&](handler& cgh) {
-        cgh.parallel_for(nd_range<3>(grid_dim * block_dim, block_dim), [=](nd_item<3> item_ct1) {
-            fused_add4_kernel(out,
-                              inp1,
-                              inp2,
-                              inp3,
-                              inp4,
-                              (batch_size * seq_length * hidden_size),
-                              hidden_size / 4,
-                              item_ct1);
-        });
-    });
-}
-
-template <>
-void launch_fused_add4<half>(half* out,
-                             const half* inp1,
-                             const half* inp2,
-                             const half* inp3,
-                             const half* inp4,
-                             int batch_size,
-                             int seq_length,
-                             int hidden_size,
-                             queue* stream)
-{
-    range<3> grid_dim(1, 1, batch_size * seq_length);
-
-    range<3> block_dim(1, 1, hidden_size / 4);
-
-    stream->submit([&](handler& cgh) {
-        cgh.parallel_for(nd_range<3>(grid_dim * block_dim, block_dim), [=](nd_item<3> item_ct1) {
-            fused_add4_kernel(out,
-                              inp1,
-                              inp2,
-                              inp3,
-                              inp4,
-                              (batch_size * seq_length * hidden_size),
-                              hidden_size / 4,
-                              item_ct1);
-        });
-    });
-}
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/apply_rotary_pos_emb.dp.cpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/apply_rotary_pos_emb.dp.cpp
new file mode 100644
index 0000000..80a6b5e
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/apply_rotary_pos_emb.dp.cpp
@@ -0,0 +1,246 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include "conversion_utils.h"
+#ifdef __HIP_PLATFORM_AMD__
+#include "hip/hip_cooperative_groups.h"
+#else
+#endif
+#include "ds_kernel_utils.h"
+#include "inference_cuda_layers.h"
+#include "memory_access_utils.h"
+
+#ifndef __HIP_PLATFORM_AMD__
+#endif
+
+namespace rot_half {
+constexpr int threads = 256;
+}  // namespace rot_half
+
+template <typename T, int threadsPerHead, int granularity>
+/*
+DPCT1110:3: The total declared local variable size in device function apply_rotary_pos_half exceeds
+128 bytes and may cause high register pressure. Consult with your hardware vendor to find the total
+register size available and adjust the code, or use smaller sub-group size to avoid high register
+pressure.
+*/
+void apply_rotary_pos_half(T* mixed_query,
+                           T* key_layer,
+                           unsigned rotary_dim,
+                           unsigned seq_len,
+                           unsigned seq_offset,
+                           unsigned num_heads,
+                           unsigned head_size,
+                           unsigned total_count,
+                           float rope_theta,
+                           int max_out_tokens)
+{
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    constexpr int T_per_thread = granularity / sizeof(T);
+    constexpr int heads_per_block = rot_half::threads / threadsPerHead;
+
+    sycl::group<3> tb = sycl::ext::oneapi::experimental::this_group<3>();
+    auto head_group =
+        sycl::ext::oneapi::experimental::this_sub_group();
+
+    const int head_idx =
+        item_ct1.get_group(2) * heads_per_block + item_ct1.get_local_id(2) / threadsPerHead;
+    const int cur_seq_idx = head_idx % seq_len;
+    const int offset = head_idx * head_size;
+    const int k_offset = (cur_seq_idx + (head_idx / seq_len) * max_out_tokens) * head_size;
+
+    const int seq_idx = cur_seq_idx + seq_offset;
+    const int half_dim = rotary_dim >> 1;
+    const int half_dim_threads = half_dim / T_per_thread;
+
+    if (head_idx < total_count) {
+        /*
+        DPCT1007:0: Migration of thread_rank is not supported.
+        */
+        const int base_neuron_idx = head_group.get_local_linear_id() * T_per_thread;
+
+        T q[T_per_thread], k[T_per_thread];
+        mem_access::load_global<granularity>(q, mixed_query + offset + base_neuron_idx);
+        mem_access::load_global<granularity>(k, key_layer + k_offset + base_neuron_idx);
+
+#pragma unroll
+        for (int i = 0; i < T_per_thread; i++) {
+            const int neuron_idx = base_neuron_idx + i;
+            if (neuron_idx < rotary_dim) {
+                float inv_freq = (float)((neuron_idx % half_dim) * 2) / (float)rotary_dim;
+                inv_freq = 1.0 / dpct::pow(rope_theta, inv_freq) * (float)seq_idx;
+
+                float rotary_sign = (neuron_idx > (half_dim - 1) ? -1.0 : 1.0);
+                float q_rot = conversion::to<float>(q[i]) * rotary_sign;
+                float k_rot = conversion::to<float>(k[i]) * rotary_sign;
+
+                const int target_lane = (neuron_idx < half_dim)
+                                            /*
+                                            DPCT1007:1: Migration of thread_rank is not supported.
+                                            */
+                                            ? head_group.get_local_linear_id() + half_dim_threads
+                                            /*
+                                            DPCT1007:2: Migration of thread_rank is not supported.
+                                            */
+                                            : head_group.get_local_linear_id() - half_dim_threads;
+
+                /*
+                DPCT1007:5: Migration of cooperative_groups::thread_block_tile::shfl is not
+                supported.
+                */
+                const float q_rot_temp = head_group.shuffle(q_rot, target_lane);
+                /*
+                DPCT1007:6: Migration of cooperative_groups::thread_block_tile::shfl is not
+                supported.
+                */
+                const float k_rot_temp = head_group.shuffle(k_rot, target_lane);
+
+                q[i] = conversion::to<T>(conversion::to<float>(q[i]) * sycl::cos(inv_freq) +
+                                         q_rot_temp * sycl::sin(inv_freq));
+                k[i] = conversion::to<T>(conversion::to<float>(k[i]) * sycl::cos(inv_freq) +
+                                         k_rot_temp * sycl::sin(inv_freq));
+            }
+        }
+
+        mem_access::store_global<granularity>(mixed_query + offset + base_neuron_idx, q);
+        mem_access::store_global<granularity>(key_layer + k_offset + base_neuron_idx, k);
+    }
+}
+
+/*
+DPCT1049:4: The work-group size passed to the SYCL kernel may exceed the limit. To get the device
+limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
+*/
+#define LAUNCH_ROT_POS_EMB_HALF(HEAD_THREADS, ALIGNMENT)                                          \
+  {                                                                                               \
+    dpct::has_capability_or_fail(stream->get_device(), {sycl::aspect::fp64, sycl::aspect::fp16}); \
+    stream->submit([&](sycl::handler& cgh) {                                                      \
+      T* mixed_query_ct0 = mixed_query;                                                           \
+      T* key_layer_ct1 = key_layer;                                                               \
+      auto rotary_dim_ct2 = rotary_dim;                                                           \
+      auto seq_len_ct3 = seq_len;                                                                 \
+      auto offset_ct4 = offset;                                                                   \
+      auto num_heads_ct5 = num_heads;                                                             \
+      auto head_size_ct6 = head_size;                                                             \
+      auto total_count_ct7 = total_count;                                                         \
+      auto rope_theta_ct8 = rope_theta;                                                           \
+      auto max_out_tokens_ct9 = max_out_tokens;                                                   \
+                                                                                                  \
+      cgh.parallel_for(sycl::nd_range<3>(grid * block, block),                                    \
+                       [=](sycl::nd_item<3> item_ct1) [[intel::reqd_sub_group_size(32)]] {        \
+                         apply_rotary_pos_half<T, HEAD_THREADS, ALIGNMENT>(mixed_query_ct0,       \
+                                                                           key_layer_ct1,         \
+                                                                           rotary_dim_ct2,        \
+                                                                           seq_len_ct3,           \
+                                                                           offset_ct4,            \
+                                                                           num_heads_ct5,         \
+                                                                           head_size_ct6,         \
+                                                                           total_count_ct7,       \
+                                                                           rope_theta_ct8,        \
+                                                                           max_out_tokens_ct9);   \
+                       });                                                                        \
+    });                                                                                           \
+  }
+
+#ifdef __HIP_PLATFORM_AMD__
+#define LAUNCH_FOR_ALIGNMENT(ALIGNMENT)         \
+    if (threads_per_head == 4) {                \
+        LAUNCH_ROT_POS_EMB_HALF(4, ALIGNMENT);  \
+    } else if (threads_per_head == 8) {         \
+        LAUNCH_ROT_POS_EMB_HALF(8, ALIGNMENT);  \
+    } else if (threads_per_head == 16) {        \
+        LAUNCH_ROT_POS_EMB_HALF(16, ALIGNMENT); \
+    } else if (threads_per_head == 32) {        \
+        LAUNCH_ROT_POS_EMB_HALF(32, ALIGNMENT); \
+    } else if (threads_per_head == 64) {        \
+        LAUNCH_ROT_POS_EMB_HALF(64, ALIGNMENT); \
+    } else {                                    \
+        assert(false);                          \
+    }
+#else
+#define LAUNCH_FOR_ALIGNMENT(ALIGNMENT)         \
+    if (threads_per_head == 4) {                \
+        LAUNCH_ROT_POS_EMB_HALF(4, ALIGNMENT);  \
+    } else if (threads_per_head == 8) {         \
+        LAUNCH_ROT_POS_EMB_HALF(8, ALIGNMENT);  \
+    } else if (threads_per_head == 16) {        \
+        LAUNCH_ROT_POS_EMB_HALF(16, ALIGNMENT); \
+    } else if (threads_per_head == 32) {        \
+        LAUNCH_ROT_POS_EMB_HALF(32, ALIGNMENT); \
+    } else {                                    \
+        assert(false);                          \
+    }
+#endif
+
+template <typename T>
+void launch_apply_rotary_pos_emb(T* mixed_query,
+                                 T* key_layer,
+                                 unsigned head_size,
+                                 unsigned seq_len,
+                                 unsigned rotary_dim,
+                                 unsigned offset,
+                                 unsigned num_heads,
+                                 unsigned batch,
+                                 float rope_theta,
+                                 dpct::queue_ptr stream,
+                                 int max_out_tokens)
+{
+    const int half_dim = rotary_dim >> 1;
+
+    int alignment = sizeof(T);
+    if (half_dim % (16 / sizeof(T)) == 0) {
+        alignment = 16;
+    } else if (half_dim % (8 / sizeof(T)) == 0) {
+        alignment = 8;
+    } else if (half_dim % (4 / sizeof(T)) == 0) {
+        alignment = 4;
+    } else {
+        assert(false);
+    }
+    const int T_per_elem = alignment / sizeof(T);
+
+    int total_count = batch * num_heads * seq_len;
+
+    const int padded_head_size = next_pow2(head_size);
+
+    assert(padded_head_size <= hw_warp_size * T_per_elem);
+
+    const int threads_per_head = padded_head_size / T_per_elem;
+    const int heads_per_block = rot_half::threads / threads_per_head;
+
+    sycl::range<3> block(1, 1, rot_half::threads);
+    sycl::range<3> grid(1, 1, (total_count + heads_per_block - 1) / heads_per_block);
+
+    if (alignment == 4) {
+        LAUNCH_FOR_ALIGNMENT(4);
+    } else if (alignment == 8) {
+        LAUNCH_FOR_ALIGNMENT(8);
+    } else if (alignment == 16) {
+        LAUNCH_FOR_ALIGNMENT(16);
+    } else {
+        assert(false);
+    }
+}
+
+#define INSTANTIATE_LAUNCH_ROTARY_POS_EMB(T)                      \
+    template void launch_apply_rotary_pos_emb<T>(T*,              \
+                                                 T*,              \
+                                                 unsigned,        \
+                                                 unsigned,        \
+                                                 unsigned,        \
+                                                 unsigned,        \
+                                                 unsigned,        \
+                                                 unsigned,        \
+                                                 float,           \
+                                                 dpct::queue_ptr, \
+                                                 int);
+
+INSTANTIATE_LAUNCH_ROTARY_POS_EMB(float);
+#ifdef BF16_AVAILABLE
+INSTANTIATE_LAUNCH_ROTARY_POS_EMB(sycl::ext::oneapi::bfloat16);
+#endif
+INSTANTIATE_LAUNCH_ROTARY_POS_EMB(sycl::half);
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/dequantize.dp.cpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/dequantize.dp.cpp
new file mode 100644
index 0000000..26f8ff4
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/dequantize.dp.cpp
@@ -0,0 +1,178 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include "conversion_utils.h"
+#include "inference_cuda_layers.h"
+
+#define MAX_QUANTIZE_GROUPING 1024
+
+#define loop_unroll 1
+#define loop_unroll_bits 1
+
+template <typename T>
+void dequantize_kernel(T* output,
+                                  const int8_t* input,
+                                  const float* qscale,
+                                  int output_size,
+                                  int hidden_dim,
+                                  int groups,
+                                  int merge_count)
+{
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    unsigned merge_hidden = hidden_dim >> merge_count;
+    unsigned quantization_stride = (merge_hidden * output_size) / groups;
+
+    unsigned bid = item_ct1.get_group(2);
+    unsigned tid = item_ct1.get_local_id(2);
+
+    while (tid < output_size) {
+        unsigned w_index = bid / merge_hidden;
+        unsigned q_index = tid + bid * output_size;
+
+        auto q = input[q_index];
+
+        unsigned merge_hidden_total = w_index * merge_hidden;
+        unsigned scale_index =
+            ((((bid - merge_hidden_total) + tid * merge_hidden) / quantization_stride)
+             << merge_count) +
+            w_index;
+
+        float scale_data = qscale[scale_index];
+
+        output[q_index] = conversion::to<T>(scale_data * (float)q);
+        tid += item_ct1.get_local_range(2);
+    }
+}
+
+template <typename T>
+void launch_dequantize(T* output,
+                       const int8_t* input,
+                       const float* qscale,
+                       unsigned output_size,
+                       unsigned hidden_dim,
+                       unsigned groups,
+                       unsigned merge_count,
+                       dpct::queue_ptr stream)
+{
+    unsigned threads = 1024;
+    sycl::range<3> block_dims(1, 1, threads);
+    sycl::range<3> grid_dims(1, 1, hidden_dim);
+
+    /*
+    DPCT1049:0: The work-group size passed to the SYCL kernel may exceed the limit. To get the
+    device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
+    */
+    {
+        dpct::has_capability_or_fail(stream->get_device(),
+                                     {sycl::aspect::fp64, sycl::aspect::fp16});
+        stream->parallel_for(
+            sycl::nd_range<3>(grid_dims * block_dims, block_dims), [=](sycl::nd_item<3> item_ct1) {
+                dequantize_kernel(
+                    output, input, qscale, output_size, hidden_dim, groups, merge_count);
+            });
+    }
+}
+
+#define INSTANTIATE_DEQUANTIZE_MERGE(T) \
+    template void launch_dequantize<T>( \
+        T*, const int8_t*, const float*, unsigned, unsigned, unsigned, unsigned, dpct::queue_ptr);
+
+INSTANTIATE_DEQUANTIZE_MERGE(float);
+#ifdef BF16_AVAILABLE
+INSTANTIATE_DEQUANTIZE_MERGE(sycl::ext::oneapi::bfloat16);
+#endif
+INSTANTIATE_DEQUANTIZE_MERGE(sycl::half);
+
+void dequantize_kernel(float* output,
+                                  const int8_t* input,
+                                  const float* qscale,
+                                  int hidden_dim,
+                                  unsigned merge_hidden,
+                                  int cnt)
+{
+}
+
+template <typename T>
+void dequantize_kernel(T* output,
+                                  const int8_t* input,
+                                  const float* qscale,
+                                  unsigned hidden_dim,
+                                  unsigned merge_hidden,
+                                  int cnt)
+{
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    unsigned bid = item_ct1.get_group(2) * item_ct1.get_group_range(1) + item_ct1.get_group(1);
+    unsigned tid = item_ct1.get_local_id(2);
+
+    float local_scale = qscale[item_ct1.get_group(2)];
+
+    const float* input_cast = reinterpret_cast<const float*>(input);
+    sycl::float2* output_cast = reinterpret_cast<sycl::float2*>(output);
+
+    input_cast += bid * merge_hidden;
+    output_cast += bid * merge_hidden;
+
+    for (int c = 0; c < cnt; c++) {
+        if (tid < merge_hidden) {
+            float q = input_cast[tid];
+            int8_t* q_int8 = (int8_t*)&q;
+
+            sycl::float2 q_f;
+            T* q_h = (T*)&q_f;
+
+            q_h[0] = conversion::to<T>(local_scale * (float)q_int8[0]);
+            q_h[1] = conversion::to<T>(local_scale * (float)q_int8[1]);
+            q_h[2] = conversion::to<T>(local_scale * (float)q_int8[2]);
+            q_h[3] = conversion::to<T>(local_scale * (float)q_int8[3]);
+            output_cast[tid] = q_f;
+            tid += item_ct1.get_local_range(2);
+        }
+    }
+}
+
+template <typename T>
+void launch_dequantize(T* output,
+                       const int8_t* input,
+                       const float* qscale,
+                       unsigned output_size,
+                       unsigned hidden_dim,
+                       unsigned groups,
+                       dpct::queue_ptr stream)
+{
+    unsigned threads = 1024;
+    hidden_dim /= 4;
+    unsigned thd_cnt = (hidden_dim - 1) / threads + 1;
+
+    assert(output_size % groups == 0);
+    unsigned blocks = output_size / groups;
+
+    sycl::range<3> block_dims(1, 1, threads);
+    sycl::range<3> grid_dims(1, blocks, groups);
+
+    /*
+    DPCT1049:1: The work-group size passed to the SYCL kernel may exceed the limit. To get the
+    device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
+    */
+    {
+        dpct::has_capability_or_fail(stream->get_device(),
+                                     {sycl::aspect::fp64, sycl::aspect::fp16});
+        stream->parallel_for(
+            sycl::nd_range<3>(grid_dims * block_dims, block_dims), [=](sycl::nd_item<3> item_ct1) {
+                dequantize_kernel(output, input, qscale, hidden_dim, hidden_dim, thd_cnt);
+            });
+    }
+}
+
+#define INSTANTIATE_DEQUANTIZE_NO_MERGE(T) \
+    template void launch_dequantize<T>(    \
+        T*, const int8_t*, const float*, unsigned, unsigned, unsigned, dpct::queue_ptr);
+
+INSTANTIATE_DEQUANTIZE_NO_MERGE(float);
+#ifdef BF16_AVAILABLE
+INSTANTIATE_DEQUANTIZE_NO_MERGE(sycl::ext::oneapi::bfloat16);
+#endif
+INSTANTIATE_DEQUANTIZE_NO_MERGE(sycl::half);
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/gelu.cpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/gelu.cpp
deleted file mode 100644
index 14352d2..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/gelu.cpp
+++ /dev/null
@@ -1,354 +0,0 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
-
-#include "compatible.hpp"
-#include "conversion_utils.hpp"
-#include "memory_access_utils.hpp"
-#include "inference_sycl_layers.hpp"
-
-#define MAX_CAP 4
-#define MAX_SEQ 2048
-
-inline float gelu(const float x) {
-  const float sqrt_param = 0.79788456080286535587989211986876f;
-  const float mul_param = 0.044715;
-  return x * 0.5f * (1.0f + tanhf(sqrt_param * (x + mul_param * x * x * x)));
-}
-
-/*
-In-place gelu(biasAdd(x)) for channels last
-*/
-template <typename T>
-class  fused_bias_gelu {
-private:
-  T *input; 
-  const T *bias; 
-  int total_count;
-  int intermediate_size;
-
-public:
-  fused_bias_gelu(T *input, const T *bias, int total_count, int intermediate_size): input(input), bias(bias), total_count(total_count), intermediate_size(intermediate_size) {};
-
-  void operator()(sycl::nd_item<1> pos) const {
-
-  // Input restriction: intermediate_size % vals_per_access == 0
-  constexpr int granularity = 16;
-  constexpr int values_per_access = granularity / sizeof(T);
-
-  const int offset =
-      (pos.get_group(0) * pos.get_local_range(0) + pos.get_local_id(0)) *
-      values_per_access;
-
-  if (offset < total_count) {
-    T data[values_per_access];
-    T data_bias[values_per_access];
-    mem_access::load_global<granularity>(data, input + offset);
-    mem_access::load_global<granularity>(data_bias,
-                                         bias + (offset % intermediate_size));
-
-#pragma unroll
-    for (int i = 0; i < values_per_access; i++) {
-      float data_f = conversion::to<float>(data[i]);
-      float bias_f = conversion::to<float>(data_bias[i]);
-      data[i] = conversion::to<T>(gelu(data_f + bias_f));
-    }
-
-    mem_access::store_global<granularity>(input + offset, data);
-  }
-
-  };
-
-};
-
-
-template <typename T>
-void launch_bias_gelu(T *input, const T *bias, int intermediate_size,
-                      int batch_size, sycl::queue stream) {
-  constexpr int threads = 1024;
-  constexpr int granularity = 16;
-
-  const int total_count = batch_size * intermediate_size;
-  const int elems_per_block = threads * (granularity / sizeof(T));
-
-  sycl::range<1> block_dims(threads);
-  sycl::range<1> grid_dims(((total_count + elems_per_block - 1) / elems_per_block) * threads);
-
-  fused_bias_gelu<T> fn(input, bias, total_count, intermediate_size);
-  stream.submit([&](sycl::handler &cmd_list) {
-      cmd_list.parallel_for(sycl::nd_range<1>{grid_dims, block_dims}, fn);
-      });
-
-}
-
-template class fused_bias_gelu<half>;
-template class fused_bias_gelu<bf16>;
-template class fused_bias_gelu<float>;
-
-template void launch_bias_gelu<float>(float *, const float *, int, int, sycl::queue);
-template void launch_bias_gelu<bf16>(bf16 *, const bf16 *, int, int, sycl::queue);
-template void launch_bias_gelu<half>(half *, const half *, int, int, sycl::queue);
-
-/*
-In-place channels-last bias add
-*/
-template <typename T>
-class fused_bias_add {
-
-private:
-  T *input; 
-  const T *bias;
-  int total_count;
-  int intermediate_size;
-
-public:
-  fused_bias_add(T *input, const T *bias, int total_count,
-                 int intermediate_size): input(input), bias(bias), total_count(total_count), intermediate_size(intermediate_size) {};
-
-  void operator()(sycl::nd_item<1> pos) const {
-
-  // Input restriction: intermediate_size % vals_per_access == 0
-  constexpr int granularity = 16;
-  constexpr int values_per_access = granularity / sizeof(T);
-  const int offset =
-      (pos.get_group(0) * pos.get_local_range(0) + pos.get_local_id(0)) *
-      values_per_access;
-
-  if (offset < total_count) {
-    T data[values_per_access];
-    T data_bias[values_per_access];
-    mem_access::load_global<granularity>(data, input + offset);
-    mem_access::load_global<granularity>(data_bias,
-                                         bias + (offset % intermediate_size));
-
-#pragma unroll
-    for (int i = 0; i < values_per_access; i++) {
-      float data_f = conversion::to<float>(data[i]);
-      float bias_f = conversion::to<float>(data_bias[i]);
-      data[i] = conversion::to<T>(data_f + bias_f);
-    }
-
-    mem_access::store_global<granularity>(input + offset, data);
-  }
-
-  };
-};
-
-
-template <typename T>
-void launch_bias_add(T *input, const T *bias, int intermediate_size,
-                     int batch_size, sycl::queue stream) {
-  constexpr int threads = 1024;
-  constexpr int granularity = 16;
-
-  const int total_count = batch_size * intermediate_size;
-  const int elems_per_block = threads * (granularity / sizeof(T));
-
-  sycl::range<1> block_dims(threads);
-  sycl::range<1> grid_dims(((total_count + elems_per_block - 1) / elems_per_block) * threads);
-
-  fused_bias_add<T> fn(input, bias, total_count, intermediate_size);
-  stream.submit([&](sycl::handler &cmd_list) {
-      cmd_list.parallel_for(sycl::nd_range<1>{grid_dims, block_dims}, fn);
-      });
-}
-
-template void launch_bias_add<float>(float *, const float *, int, int, sycl::queue);
-template void launch_bias_add<bf16>(bf16 *, const bf16 *, int, int, sycl::queue);
-template void launch_bias_add<half>(half *, const half *, int, int, sycl::queue);
-
-template <typename T> class fused_bias_residual {
-
-private:
-  T *residual;
-  const T *hidden_state;
-  const T *attn;
-  const T *bias;
-  const T *attn_bias;
-  const int total_count;
-  const int intermediate_size;
-  const float mp_scale;
-  const bool preln;
-
-public:
-  fused_bias_residual(T *residual, const T *hidden_state, const T *attn,
-                      const T *bias, const T *attn_bias, const int total_count,
-                      const int intermediate_size, const float mp_scale,
-                      const bool preln)
-      : residual(residual), hidden_state(hidden_state), attn(attn), bias(bias),
-        attn_bias(attn_bias), total_count(total_count),
-        intermediate_size(intermediate_size), mp_scale(mp_scale),
-        preln(preln){};
-
-  void operator()(sycl::nd_item<1> pos) const {
-    using T2 = typename std::conditional<std::is_same<T, half>::value, half2,
-                                         float2>::type;
-
-    float2 *res_fl2_ptr = reinterpret_cast<float2 *>(residual);
-    const float2 *hs_fl2_ptr = reinterpret_cast<const float2 *>(hidden_state);
-    const float2 *attn_fl2_ptr = reinterpret_cast<const float2 *>(attn);
-    const float2 *bias_fl2_ptr = reinterpret_cast<const float2 *>(bias);
-    const float2 *attn_bias_fl2_ptr =
-        reinterpret_cast<const float2 *>(attn_bias);
-    const int offset =
-        pos.get_group(0) * pos.get_local_range(0) + pos.get_local_id(0);
-
-    if (offset < total_count) {
-      float2 res_fl2 = res_fl2_ptr[offset];
-      const float2 hs_fl2 = hs_fl2_ptr[offset];
-      const float2 attn_fl2 = attn_fl2_ptr[offset];
-      const float2 bias_fl2 = bias_fl2_ptr[offset % intermediate_size];
-      const float2 attn_bias_fl2 =
-          attn_bias_fl2_ptr[offset % intermediate_size];
-
-      T2 *res_half2 = reinterpret_cast<T2 *>(&res_fl2);
-      const T2 *hs_half2 = reinterpret_cast<const T2 *>(&hs_fl2);
-      const T2 *attn_half2 = reinterpret_cast<const T2 *>(&attn_fl2);
-      const T2 *bias_half2 = reinterpret_cast<const T2 *>(&bias_fl2);
-      const T2 *attn_bias_half2 = reinterpret_cast<const T2 *>(&attn_bias_fl2);
-
-      float2 res_low = conversion::to<float2>(res_half2[0]);
-      float2 res_high = conversion::to<float2>(res_half2[1]);
-
-      const float2 hs_low = conversion::to<float2>(hs_half2[0]);
-      const float2 hs_high = conversion::to<float2>(hs_half2[1]);
-
-      const float2 attn_low = conversion::to<float2>(attn_half2[0]);
-      const float2 attn_high = conversion::to<float2>(attn_half2[1]);
-
-      const float2 bias_low = conversion::to<float2>(bias_half2[0]);
-      const float2 bias_high = conversion::to<float2>(bias_half2[1]);
-
-      const float2 attn_bias_low = conversion::to<float2>(attn_bias_half2[0]);
-      const float2 attn_bias_high = conversion::to<float2>(attn_bias_half2[1]);
-
-      if (preln) {
-        // residual = (residual + attention + bias + attention_bias) *
-        // mp_scale + hidden_state
-        res_low.x() =
-            (res_low.x() + attn_low.x() + bias_low.x() + attn_bias_low.x()) *
-                mp_scale +
-            hs_low.x();
-        res_low.y() =
-            (res_low.y() + attn_low.y() + bias_low.y() + attn_bias_low.y()) *
-                mp_scale +
-            hs_low.y();
-        res_high.x() = (res_high.x() + attn_high.x() + bias_high.x() +
-                        attn_bias_high.x()) *
-                           mp_scale +
-                       hs_high.x();
-        res_high.y() = (res_high.y() + attn_high.y() + bias_high.y() +
-                        attn_bias_high.y()) *
-                           mp_scale +
-                       hs_high.y();
-      } else {
-        // residual += hidden_state + bias
-        res_low.x() = (res_low.x() + hs_low.x() + bias_low.x());
-        res_low.y() = (res_low.y() + hs_low.y() + bias_low.y());
-        res_high.x() = (res_high.x() + hs_high.x() + bias_high.x());
-        res_high.y() = (res_high.y() + hs_high.y() + bias_high.y());
-      }
-      res_half2[0] = conversion::to<T2>(res_low);
-      res_half2[1] = conversion::to<T2>(res_high);
-
-      res_fl2_ptr[offset] = res_fl2;
-    }
-  };
-};
-
-template <> class fused_bias_residual<float> {
-
-private:
-  float *residual;
-  const float *hidden_state;
-  const float *attn;
-  const float *bias;
-  const float *attn_bias;
-  const int total_count;
-  const int intermediate_size;
-  const float mp_scale;
-  const bool preln;
-
-public:
-  fused_bias_residual(float *residual, const float *hidden_state,
-                      const float *attn, const float *bias,
-                      const float *attn_bias, const int total_count,
-                      const int intermediate_size, const float mp_scale,
-                      const bool preln)
-      : residual(residual), hidden_state(hidden_state), attn(attn), bias(bias),
-        attn_bias(attn_bias), total_count(total_count),
-        intermediate_size(intermediate_size), mp_scale(mp_scale),
-        preln(preln){};
-
-  void operator()(sycl::nd_item<1> pos) const {
-
-    float4 *res_fl4_ptr = reinterpret_cast<float4 *>(residual);
-    const float4 *hs_fl4_ptr = reinterpret_cast<const float4 *>(hidden_state);
-    const float4 *attn_fl4_ptr = reinterpret_cast<const float4 *>(attn);
-    const float4 *bias_fl4_ptr = reinterpret_cast<const float4 *>(bias);
-    const float4 *attn_bias_fl4_ptr =
-        reinterpret_cast<const float4 *>(attn_bias);
-    const int offset =
-        pos.get_group(0) * pos.get_local_range(0) + pos.get_local_id(0);
-
-    if (offset < total_count) {
-      float4 res_fl4 = res_fl4_ptr[offset];
-      const float4 hs_fl4 = hs_fl4_ptr[offset];
-      const float4 attn_fl4 = attn_fl4_ptr[offset];
-      const float4 bias_fl4 = bias_fl4_ptr[offset % intermediate_size];
-      const float4 attn_bias_fl4 =
-          attn_bias_fl4_ptr[offset % intermediate_size];
-      if (preln) {
-        // residual = (residual + attention + bias + attention_bias) *
-        // mp_scale + hidden_state
-        res_fl4.x() =
-            (res_fl4.x() + attn_fl4.x() + bias_fl4.x() + attn_bias_fl4.x()) *
-                mp_scale +
-            (hs_fl4.x());
-        res_fl4.y() =
-            (res_fl4.y() + attn_fl4.y() + bias_fl4.y() + attn_bias_fl4.y()) *
-                mp_scale +
-            (hs_fl4.y());
-        res_fl4.z() =
-            (res_fl4.z() + attn_fl4.z() + bias_fl4.z() + attn_bias_fl4.z()) *
-                mp_scale +
-            (hs_fl4.z());
-        res_fl4.w() =
-            (res_fl4.w() + attn_fl4.w() + bias_fl4.w() + attn_bias_fl4.w()) *
-                mp_scale +
-            (hs_fl4.w());
-      } else {
-        // residual += hidden_state + bias
-        res_fl4.x() = res_fl4.x() + hs_fl4.x() + bias_fl4.x();
-        res_fl4.y() = res_fl4.y() + hs_fl4.y() + bias_fl4.y();
-        res_fl4.z() = res_fl4.z() + hs_fl4.z() + bias_fl4.z();
-        res_fl4.w() = res_fl4.w() + hs_fl4.w() + bias_fl4.w();
-      }
-      res_fl4_ptr[offset] = res_fl4;
-    }
-  };
-};
-
-
-template <typename T>
-void launch_bias_residual(T *residual, T *hidden_state, T *attn, T *bias,
-                          T *attn_bias, int batch, int hidden_dim, int mp_size,
-                          bool preln, sycl::queue stream) {
-  int total_count = batch * hidden_dim / 4;
-
-  sycl::range<1> block_dims(1024);
-  sycl::range<1> grid_dims(((total_count - 1) / 1024 + 1) * 1024);
-
-  fused_bias_residual<T> fn(residual, hidden_state, attn, bias, attn_bias,
-                            total_count, hidden_dim / 4, 1.0 / mp_size, preln);
-  stream.submit([&](sycl::handler &cmd_list) {
-    cmd_list.parallel_for(sycl::nd_range<1>{grid_dims, block_dims}, fn);
-  });
-}
-
-template void launch_bias_residual<float>(float *, float *, float *, float *,
-                                          float *, int, int, int, bool, sycl::queue);
-template void launch_bias_residual<bf16>(bf16 *, bf16 *, bf16 *, bf16 *, bf16 *,
-                                         int, int, int, bool, sycl::queue);
-template void launch_bias_residual<half>(half *, half *, half *, half *, half *,
-                                         int, int, int, bool, sycl::queue);
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/gelu.dp.cpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/gelu.dp.cpp
new file mode 100644
index 0000000..d02eaff
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/gelu.dp.cpp
@@ -0,0 +1,854 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include "conversion_utils.h"
+#include "inference_cuda_layers.h"
+#include "memory_access_utils.h"
+
+#define MAX_CAP 4
+#define MAX_SEQ 2048
+
+// only used to avoid compilation error due to lack of definition.
+#ifndef BF16_AVAILABLE
+using __nv_bfloat162 = sycl::half2;
+#endif
+
+inline float gelu(const float x)
+{
+    constexpr float sqrt_param = 0.79788456080286535587989211986876f;
+    constexpr float mul_param = 0.044715;
+    return x * 0.5f * (1.0f + sycl::tanh(sqrt_param * (x + mul_param * x * x * x)));
+}
+
+/*
+In-place gelu(biasAdd(x)) for channels last
+*/
+template <typename T>
+void fused_bias_gelu(T* input, const T* bias, int total_count, int intermediate_size)
+{
+    // Input restriction: intermediate_size % vals_per_access == 0
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    constexpr int granularity = 16;
+    constexpr int values_per_access = granularity / sizeof(T);
+    const int offset =
+        (item_ct1.get_group(2) * item_ct1.get_local_range(2) + item_ct1.get_local_id(2)) *
+        values_per_access;
+
+    if (offset < total_count) {
+        T data[values_per_access];
+        T data_bias[values_per_access];
+        mem_access::load_global<granularity>(data, input + offset);
+        mem_access::load_global<granularity>(
+            data_bias, bias + (offset % intermediate_size), bias != nullptr);
+
+#pragma unroll
+        for (int i = 0; i < values_per_access; i++) {
+            float data_f = conversion::to<float>(data[i]);
+            float bias_f = conversion::to<float>(data_bias[i]);
+            data[i] = conversion::to<T>(gelu(data_f + bias_f));
+        }
+
+        mem_access::store_global<granularity>(input + offset, data);
+    }
+}
+
+template <typename T>
+void launch_bias_gelu(T* input,
+                      const T* bias,
+                      int intermediate_size,
+                      int batch_size,
+                      dpct::queue_ptr stream)
+{
+    constexpr int threads = 1024;
+    constexpr int granularity = 16;
+
+    const int total_count = batch_size * intermediate_size;
+    const int elems_per_block = threads * (granularity / sizeof(T));
+    sycl::range<3> block_dims(1, 1, threads);
+    sycl::range<3> grid_dims(1, 1, (total_count + elems_per_block - 1) / elems_per_block);
+
+    /*
+    DPCT1049:0: The work-group size passed to the SYCL kernel may exceed the limit. To get the
+    device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
+    */
+    {
+        dpct::has_capability_or_fail(stream->get_device(),
+                                     {sycl::aspect::fp64, sycl::aspect::fp16});
+        stream->parallel_for(sycl::nd_range<3>(grid_dims * block_dims, block_dims),
+                             [=](sycl::nd_item<3> item_ct1) {
+                                 fused_bias_gelu(input, bias, total_count, intermediate_size);
+                             });
+    }
+}
+
+#define INSTANTIATE_LAUNCH_BIAS_GELU(T) \
+    template void launch_bias_gelu<T>(T*, const T*, int, int, dpct::queue_ptr);
+
+INSTANTIATE_LAUNCH_BIAS_GELU(float)
+#ifdef BF16_AVAILABLE
+INSTANTIATE_LAUNCH_BIAS_GELU(sycl::ext::oneapi::bfloat16)
+#endif
+INSTANTIATE_LAUNCH_BIAS_GELU(sycl::half)
+
+/*
+In-place channels-last bias add
+*/
+template <typename T>
+void fused_bias_add(T* input, const T* bias, int total_count, int intermediate_size)
+{
+    // Input restriction: intermediate_size % vals_per_access == 0
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    constexpr int granularity = 16;
+    constexpr int values_per_access = granularity / sizeof(T);
+    const int offset =
+        (item_ct1.get_group(2) * item_ct1.get_local_range(2) + item_ct1.get_local_id(2)) *
+        values_per_access;
+
+    if (offset < total_count) {
+        T data[values_per_access];
+        T data_bias[values_per_access];
+        mem_access::load_global<granularity>(data, input + offset);
+        mem_access::load_global<granularity>(
+            data_bias, bias + (offset % intermediate_size), bias != nullptr);
+
+#pragma unroll
+        for (int i = 0; i < values_per_access; i++) {
+            float data_f = conversion::to<float>(data[i]);
+            float bias_f = conversion::to<float>(data_bias[i]);
+            data[i] = conversion::to<T>(data_f + bias_f);
+        }
+
+        mem_access::store_global<granularity>(input + offset, data);
+    }
+}
+
+template <typename T>
+void launch_bias_add(T* input,
+                     const T* bias,
+                     int intermediate_size,
+                     int batch_size,
+                     dpct::queue_ptr stream)
+{
+    constexpr int threads = 1024;
+    constexpr int granularity = 16;
+
+    const int total_count = batch_size * intermediate_size;
+    const int elems_per_block = threads * (granularity / sizeof(T));
+    sycl::range<3> block_dims(1, 1, threads);
+    sycl::range<3> grid_dims(1, 1, (total_count + elems_per_block - 1) / elems_per_block);
+
+    /*
+    DPCT1049:1: The work-group size passed to the SYCL kernel may exceed the limit. To get the
+    device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
+    */
+    {
+        dpct::has_capability_or_fail(stream->get_device(),
+                                     {sycl::aspect::fp64, sycl::aspect::fp16});
+        stream->parallel_for(sycl::nd_range<3>(grid_dims * block_dims, block_dims),
+                             [=](sycl::nd_item<3> item_ct1) {
+                                 fused_bias_add(input, bias, total_count, intermediate_size);
+                             });
+    }
+}
+
+#define INSTANTIATE_LAUNCH_BIAS_ADD(T) \
+    template void launch_bias_add<T>(T*, const T*, int, int, dpct::queue_ptr);
+
+INSTANTIATE_LAUNCH_BIAS_ADD(float)
+#ifdef BF16_AVAILABLE
+INSTANTIATE_LAUNCH_BIAS_ADD(sycl::ext::oneapi::bfloat16)
+#endif
+INSTANTIATE_LAUNCH_BIAS_ADD(sycl::half)
+
+void fused_bias_residual(float* residual,
+                                    const float* hidden_state,
+                                    const float* attn,
+                                    const float* bias,
+                                    const float* attn_bias,
+                                    const int total_count,
+                                    const int intermediate_size,
+                                    const float mp_scale,
+                                    const bool preln)
+{
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    sycl::float4* res_fl4_ptr = reinterpret_cast<sycl::float4*>(residual);
+    const sycl::float4* hs_fl4_ptr = reinterpret_cast<const sycl::float4*>(hidden_state);
+    const sycl::float4* attn_fl4_ptr = reinterpret_cast<const sycl::float4*>(attn);
+    const sycl::float4* bias_fl4_ptr = reinterpret_cast<const sycl::float4*>(bias);
+    const sycl::float4* attn_bias_fl4_ptr = reinterpret_cast<const sycl::float4*>(attn_bias);
+    const int offset =
+        item_ct1.get_group(2) * item_ct1.get_local_range(2) + item_ct1.get_local_id(2);
+
+    if (offset < total_count) {
+        sycl::float4 res_fl4 = res_fl4_ptr[offset];
+        const sycl::float4 hs_fl4 = hs_fl4_ptr[offset];
+        const sycl::float4 attn_fl4 = attn_fl4_ptr[offset];
+        const sycl::float4 bias_fl4 = bias_fl4_ptr[offset % intermediate_size];
+        const sycl::float4 attn_bias_fl4 = attn_bias_fl4_ptr[offset % intermediate_size];
+        if (preln) {
+            // residual = (residual + attention + bias + attention_bias) *
+            // mp_scale + hidden_state
+            res_fl4.x() =
+                (res_fl4.x() + attn_fl4.x() + bias_fl4.x() + attn_bias_fl4.x()) * mp_scale +
+                (hs_fl4.x());
+            res_fl4.y() =
+                (res_fl4.y() + attn_fl4.y() + bias_fl4.y() + attn_bias_fl4.y()) * mp_scale +
+                (hs_fl4.y());
+            res_fl4.z() =
+                (res_fl4.z() + attn_fl4.z() + bias_fl4.z() + attn_bias_fl4.z()) * mp_scale +
+                (hs_fl4.z());
+            res_fl4.w() =
+                (res_fl4.w() + attn_fl4.w() + bias_fl4.w() + attn_bias_fl4.w()) * mp_scale +
+                (hs_fl4.w());
+        } else {
+            // residual += hidden_state + bias
+            res_fl4.x() = res_fl4.x() + hs_fl4.x() + bias_fl4.x();
+            res_fl4.y() = res_fl4.y() + hs_fl4.y() + bias_fl4.y();
+            res_fl4.z() = res_fl4.z() + hs_fl4.z() + bias_fl4.z();
+            res_fl4.w() = res_fl4.w() + hs_fl4.w() + bias_fl4.w();
+        }
+        res_fl4_ptr[offset] = res_fl4;
+    }
+}
+
+template <typename T>
+/*
+DPCT1110:2: The total declared local variable size in device function fused_bias_residual exceeds
+128 bytes and may cause high register pressure. Consult with your hardware vendor to find the total
+register size available and adjust the code, or use smaller sub-group size to avoid high register
+pressure.
+*/
+void fused_bias_residual(T* residual,
+                         const T* hidden_state,
+                         const T* attn,
+                         const T* bias,
+                         const T* attn_bias,
+                         const int total_count,
+                         const int intermediate_size,
+                         const float mp_scale,
+                         const bool preln)
+{
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    using T2 = typename std::conditional<std::is_same<T, sycl::half>::value,
+                                         sycl::half2,
+                                         sycl::marray<sycl::ext::oneapi::bfloat16, 2>>::type;
+    sycl::float2* res_fl2_ptr = reinterpret_cast<sycl::float2*>(residual);
+    const sycl::float2* hs_fl2_ptr = reinterpret_cast<const sycl::float2*>(hidden_state);
+    const sycl::float2* attn_fl2_ptr = reinterpret_cast<const sycl::float2*>(attn);
+    const sycl::float2* bias_fl2_ptr = reinterpret_cast<const sycl::float2*>(bias);
+    const sycl::float2* attn_bias_fl2_ptr = reinterpret_cast<const sycl::float2*>(attn_bias);
+    const int offset =
+        item_ct1.get_group(2) * item_ct1.get_local_range(2) + item_ct1.get_local_id(2);
+
+    if (offset < total_count) {
+        sycl::float2 res_fl2 = res_fl2_ptr[offset];
+        const sycl::float2 hs_fl2 = hs_fl2_ptr[offset];
+        const sycl::float2 attn_fl2 = attn_fl2_ptr[offset];
+        const sycl::float2 bias_fl2 = bias_fl2_ptr[offset % intermediate_size];
+        const sycl::float2 attn_bias_fl2 = attn_bias_fl2_ptr[offset % intermediate_size];
+
+        T2* res_half2 = reinterpret_cast<T2*>(&res_fl2);
+        const T2* hs_half2 = reinterpret_cast<const T2*>(&hs_fl2);
+        const T2* attn_half2 = reinterpret_cast<const T2*>(&attn_fl2);
+        const T2* bias_half2 = reinterpret_cast<const T2*>(&bias_fl2);
+        const T2* attn_bias_half2 = reinterpret_cast<const T2*>(&attn_bias_fl2);
+
+        sycl::float2 res_low = conversion::to<sycl::float2>(res_half2[0]);
+        sycl::float2 res_high = conversion::to<sycl::float2>(res_half2[1]);
+
+        const sycl::float2 hs_low = conversion::to<sycl::float2>(hs_half2[0]);
+        const sycl::float2 hs_high = conversion::to<sycl::float2>(hs_half2[1]);
+
+        const sycl::float2 attn_low = conversion::to<sycl::float2>(attn_half2[0]);
+        const sycl::float2 attn_high = conversion::to<sycl::float2>(attn_half2[1]);
+
+        const sycl::float2 bias_low = conversion::to<sycl::float2>(bias_half2[0]);
+        const sycl::float2 bias_high = conversion::to<sycl::float2>(bias_half2[1]);
+
+        const sycl::float2 attn_bias_low = conversion::to<sycl::float2>(attn_bias_half2[0]);
+        const sycl::float2 attn_bias_high = conversion::to<sycl::float2>(attn_bias_half2[1]);
+
+        if (preln) {
+            // residual = (residual + attention + bias + attention_bias) *
+            // mp_scale + hidden_state
+            res_low.x() =
+                (res_low.x() + attn_low.x() + bias_low.x() + attn_bias_low.x()) * mp_scale +
+                hs_low.x();
+            res_low.y() =
+                (res_low.y() + attn_low.y() + bias_low.y() + attn_bias_low.y()) * mp_scale +
+                hs_low.y();
+            res_high.x() =
+                (res_high.x() + attn_high.x() + bias_high.x() + attn_bias_high.x()) * mp_scale +
+                hs_high.x();
+            res_high.y() =
+                (res_high.y() + attn_high.y() + bias_high.y() + attn_bias_high.y()) * mp_scale +
+                hs_high.y();
+        } else {
+            // residual += hidden_state + bias
+            res_low.x() = (res_low.x() + hs_low.x() + bias_low.x());
+            res_low.y() = (res_low.y() + hs_low.y() + bias_low.y());
+            res_high.x() = (res_high.x() + hs_high.x() + bias_high.x());
+            res_high.y() = (res_high.y() + hs_high.y() + bias_high.y());
+        }
+        res_half2[0] = conversion::to<T2>(res_low);
+        res_half2[1] = conversion::to<T2>(res_high);
+
+        res_fl2_ptr[offset] = res_fl2;
+    }
+}
+
+template <typename T>
+void launch_bias_residual(T* residual,
+                          T* hidden_state,
+                          T* attn,
+                          T* bias,
+                          T* attn_bias,
+                          int batch,
+                          int hidden_dim,
+                          int mp_size,
+                          bool preln,
+                          dpct::queue_ptr stream)
+{
+    int total_count = batch * hidden_dim / 4;
+    sycl::range<3> block_dims(1, 1, 1024);
+    sycl::range<3> grid_dims(1, 1, (total_count - 1) / 1024 + 1);  // (batch_size);
+
+    /*
+    DPCT1049:3: The work-group size passed to the SYCL kernel may exceed the limit. To get the
+    device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
+    */
+    {
+        dpct::has_capability_or_fail(stream->get_device(),
+                                     {sycl::aspect::fp64, sycl::aspect::fp16});
+        stream->parallel_for(sycl::nd_range<3>(grid_dims * block_dims, block_dims),
+                             [=](sycl::nd_item<3> item_ct1) {
+                                 fused_bias_residual(residual,
+                                                     hidden_state,
+                                                     attn,
+                                                     bias,
+                                                     attn_bias,
+                                                     total_count,
+                                                     hidden_dim / 4,
+                                                     1.0 / mp_size,
+                                                     preln);
+                             });
+    }
+}
+
+#define INSTANTIATE_LAUNCH_BIAS_RESIDUAL(T) \
+    template void launch_bias_residual<T>(T*, T*, T*, T*, T*, int, int, int, bool, dpct::queue_ptr);
+
+INSTANTIATE_LAUNCH_BIAS_RESIDUAL(float);
+#ifdef BF16_AVAILABLE
+INSTANTIATE_LAUNCH_BIAS_RESIDUAL(sycl::ext::oneapi::bfloat16);
+#endif
+INSTANTIATE_LAUNCH_BIAS_RESIDUAL(sycl::half);
+
+void gptj_residual_add(float* residual,
+                                  const float* hidden_state,
+                                  const float* attn,
+                                  const float* bias,
+                                  const float* attn_bias,
+                                  const int total_count,
+                                  const int intermediate_size,
+                                  const float mp_scale)
+{
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    sycl::float4* res_fl4_ptr = reinterpret_cast<sycl::float4*>(residual);
+    const sycl::float4* hs_fl4_ptr = reinterpret_cast<const sycl::float4*>(hidden_state);
+    const sycl::float4* attn_fl4_ptr = reinterpret_cast<const sycl::float4*>(attn);
+    const sycl::float4* bias_fl4_ptr = reinterpret_cast<const sycl::float4*>(bias);
+    const sycl::float4* attn_bias_fl4_ptr = reinterpret_cast<const sycl::float4*>(attn_bias);
+    const int offset =
+        item_ct1.get_group(2) * item_ct1.get_local_range(2) + item_ct1.get_local_id(2);
+
+    if (offset < total_count) {
+        sycl::float4 res_fl4 = res_fl4_ptr[offset];
+        const sycl::float4 hs_fl4 = hs_fl4_ptr[offset];
+        const sycl::float4 attn_fl4 = attn_fl4_ptr[offset];
+        const sycl::float4 bias_fl4 = bias_fl4_ptr[offset % intermediate_size];
+
+        if (attn_bias) {
+            sycl::float4 attn_bias_fl4 = attn_bias_fl4_ptr[offset % intermediate_size];
+            // residual += attention_bias
+            res_fl4.x() += attn_bias_fl4.x();
+            res_fl4.y() += attn_bias_fl4.y();
+            res_fl4.z() += attn_bias_fl4.z();
+            res_fl4.w() += attn_bias_fl4.w();
+        }
+        // residual = hidden_state + attention + (residual + bias) * mp_scale
+        res_fl4.x() = hs_fl4.x() + attn_fl4.x() + (res_fl4.x() + bias_fl4.x()) * mp_scale;
+        res_fl4.y() = hs_fl4.y() + attn_fl4.y() + (res_fl4.y() + bias_fl4.y()) * mp_scale;
+        res_fl4.z() = hs_fl4.z() + attn_fl4.z() + (res_fl4.z() + bias_fl4.z()) * mp_scale;
+        res_fl4.w() = hs_fl4.w() + attn_fl4.w() + (res_fl4.w() + bias_fl4.w()) * mp_scale;
+
+        res_fl4_ptr[offset] = res_fl4;
+    }
+}
+
+template <typename T>
+/*
+DPCT1110:4: The total declared local variable size in device function gptj_residual_add exceeds 128
+bytes and may cause high register pressure. Consult with your hardware vendor to find the total
+register size available and adjust the code, or use smaller sub-group size to avoid high register
+pressure.
+*/
+void gptj_residual_add(T* residual,
+                       const T* hidden_state,
+                       const T* attn,
+                       const T* bias,
+                       const T* attn_bias,
+                       const int total_count,
+                       const int intermediate_size,
+                       const float mp_scale)
+{
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    using T2 = typename std::conditional<std::is_same<T, sycl::half>::value,
+                                         sycl::half2,
+                                         sycl::marray<sycl::ext::oneapi::bfloat16, 2>>::type;
+    sycl::float2* res_fl2_ptr = reinterpret_cast<sycl::float2*>(residual);
+    const sycl::float2* hs_fl2_ptr = reinterpret_cast<const sycl::float2*>(hidden_state);
+    const sycl::float2* attn_fl2_ptr = reinterpret_cast<const sycl::float2*>(attn);
+    const sycl::float2* bias_fl2_ptr = reinterpret_cast<const sycl::float2*>(bias);
+    const sycl::float2* attn_bias_fl2_ptr = reinterpret_cast<const sycl::float2*>(attn_bias);
+    const int offset =
+        item_ct1.get_group(2) * item_ct1.get_local_range(2) + item_ct1.get_local_id(2);
+
+    if (offset < total_count) {
+        sycl::float2 res_fl2 = res_fl2_ptr[offset];
+        const sycl::float2 hs_fl2 = hs_fl2_ptr[offset];
+        const sycl::float2 attn_fl2 = attn_fl2_ptr[offset];
+        const sycl::float2 bias_fl2 = bias_fl2_ptr[offset % intermediate_size];
+
+        T2* res_half2 = reinterpret_cast<T2*>(&res_fl2);
+        const T2* hs_half2 = reinterpret_cast<const T2*>(&hs_fl2);
+        const T2* attn_half2 = reinterpret_cast<const T2*>(&attn_fl2);
+        const T2* bias_half2 = reinterpret_cast<const T2*>(&bias_fl2);
+
+        sycl::float2 res_low = conversion::to<sycl::float2>(res_half2[0]);
+        sycl::float2 res_high = conversion::to<sycl::float2>(res_half2[1]);
+
+        const sycl::float2 hs_low = conversion::to<sycl::float2>(hs_half2[0]);
+        const sycl::float2 hs_high = conversion::to<sycl::float2>(hs_half2[1]);
+
+        const sycl::float2 attn_low = conversion::to<sycl::float2>(attn_half2[0]);
+        const sycl::float2 attn_high = conversion::to<sycl::float2>(attn_half2[1]);
+
+        const sycl::float2 bias_low = conversion::to<sycl::float2>(bias_half2[0]);
+        const sycl::float2 bias_high = conversion::to<sycl::float2>(bias_half2[1]);
+
+        if (attn_bias) {
+            const sycl::float2 attn_bias_fl2 = attn_bias_fl2_ptr[offset % intermediate_size];
+            const T2* attn_bias_half2 = reinterpret_cast<const T2*>(&attn_bias_fl2);
+            const sycl::float2 attn_bias_low = conversion::to<sycl::float2>(attn_bias_half2[0]);
+            const sycl::float2 attn_bias_high = conversion::to<sycl::float2>(attn_bias_half2[1]);
+            // residual += attention_bias
+            res_low.x() += attn_bias_low.x();
+            res_low.y() += attn_bias_low.y();
+            res_high.x() += attn_bias_high.x();
+            res_high.y() += attn_bias_high.y();
+        }
+        // residual = hidden_state + attention + (residual + bias) * mp_scale
+        res_low.x() = attn_low.x() + hs_low.x() + (res_low.x() + bias_low.x()) * mp_scale;
+        res_low.y() = attn_low.y() + hs_low.y() + (res_low.y() + bias_low.y()) * mp_scale;
+        res_high.x() = attn_high.x() + hs_high.x() + (res_high.x() + bias_high.x()) * mp_scale;
+        res_high.y() = attn_high.y() + hs_high.y() + (res_high.y() + bias_high.y()) * mp_scale;
+
+        res_half2[0] = conversion::to<T2>(res_low);
+        res_half2[1] = conversion::to<T2>(res_high);
+
+        res_fl2_ptr[offset] = res_fl2;
+    }
+}
+
+template <typename T>
+void launch_gptj_residual_add(T* residual,
+                              T* hidden_state,
+                              T* attn,
+                              T* bias,
+                              T* attn_bias,
+                              int hidden_dim,
+                              int batch,
+                              int mp_size,
+                              dpct::queue_ptr stream)
+{
+    int total_count = batch * hidden_dim / 4;
+    sycl::range<3> block_dims(1, 1, 1024);
+    sycl::range<3> grid_dims(1, 1, (total_count - 1) / 1024 + 1);  // (batch_size);
+
+    /*
+    DPCT1049:5: The work-group size passed to the SYCL kernel may exceed the limit. To get the
+    device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
+    */
+    {
+        dpct::has_capability_or_fail(stream->get_device(),
+                                     {sycl::aspect::fp64, sycl::aspect::fp16});
+        stream->parallel_for(sycl::nd_range<3>(grid_dims * block_dims, block_dims),
+                             [=](sycl::nd_item<3> item_ct1) {
+                                 gptj_residual_add(residual,
+                                                   hidden_state,
+                                                   attn,
+                                                   bias,
+                                                   attn_bias,
+                                                   total_count,
+                                                   hidden_dim / 4,
+                                                   1.0 / mp_size);
+                             });
+    }
+}
+
+#define INSTANTIATE_GPT_RES_ADD(T) \
+    template void launch_gptj_residual_add<T>(T*, T*, T*, T*, T*, int, int, int, dpct::queue_ptr);
+
+INSTANTIATE_GPT_RES_ADD(float);
+INSTANTIATE_GPT_RES_ADD(sycl::half);
+#ifdef BF16_AVAILABLE
+INSTANTIATE_GPT_RES_ADD(sycl::ext::oneapi::bfloat16);
+#endif
+
+template <typename T>
+void moe_res_matmul(T* residual, T* coef, T* mlp_out, int seq_len, int hidden_dim)
+{
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    constexpr int granularity = 16;
+    constexpr int vals_per_access = granularity / sizeof(T);
+
+    T* residual_seq = residual + item_ct1.get_group(2) * hidden_dim;
+    T* mlp_out_seq = mlp_out + item_ct1.get_group(2) * hidden_dim;
+
+    for (unsigned tid = item_ct1.get_local_id(2) * vals_per_access; tid < hidden_dim;
+         tid += item_ct1.get_local_range(2) * vals_per_access) {
+        T mlp[vals_per_access];
+        T res[vals_per_access];
+        T coef1[vals_per_access];
+        T coef2[vals_per_access];
+
+        mem_access::load_global<granularity>(mlp, mlp_out_seq + tid);
+        mem_access::load_global<granularity>(res, residual_seq + tid);
+        mem_access::load_global<granularity>(coef1, coef + tid);
+        mem_access::load_global<granularity>(coef2, coef + tid + hidden_dim);
+
+#pragma unroll
+        for (int idx = 0; idx < vals_per_access; idx++) {
+            mlp[idx] = mlp[idx] * coef2[idx] + res[idx] * coef1[idx];
+        }
+
+        mem_access::store_global<granularity>(mlp_out_seq + tid, mlp);
+    }
+}
+
+template <typename T>
+void launch_moe_res_matmul(T* residual,
+                           T* coef,
+                           T* mlp_out,
+                           int seq_len,
+                           int hidden_dim,
+                           dpct::queue_ptr stream)
+{
+    sycl::range<3> grid_dim(1, 1, seq_len);
+    sycl::range<3> block_dim(1, 1, 1024);
+    /*
+    DPCT1049:6: The work-group size passed to the SYCL kernel may exceed the limit. To get the
+    device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
+    */
+    {
+        dpct::has_capability_or_fail(stream->get_device(), {sycl::aspect::fp16});
+        stream->parallel_for(sycl::nd_range<3>(grid_dim * block_dim, block_dim),
+                             [=](sycl::nd_item<3> item_ct1) {
+                                 moe_res_matmul(residual, coef, mlp_out, seq_len, hidden_dim);
+                             });
+    }
+}
+
+#define INSTANTIATE_LAUNCH_MOE_RES_MATMUL(T) \
+    template void launch_moe_res_matmul<T>(T*, T*, T*, int, int, dpct::queue_ptr);
+
+INSTANTIATE_LAUNCH_MOE_RES_MATMUL(float);
+#ifdef BF16_AVAILABLE
+INSTANTIATE_LAUNCH_MOE_RES_MATMUL(sycl::ext::oneapi::bfloat16);
+#endif
+INSTANTIATE_LAUNCH_MOE_RES_MATMUL(sycl::half);
+
+template <typename T>
+void pad_data_kernel(T* padded_output, T* output, int head_size, int padded_head_size)
+{
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    using T2 = typename std::conditional<std::is_same<T, sycl::half>::value,
+                                         sycl::half2,
+                                         sycl::marray<sycl::ext::oneapi::bfloat16, 2>>::type;
+    sycl::float4* padded_output_cast = reinterpret_cast<sycl::float4*>(padded_output);
+    sycl::float4* output_cast = reinterpret_cast<sycl::float4*>(output);
+    int bid = item_ct1.get_group(2) * (item_ct1.get_local_range(1)) + item_ct1.get_local_id(1);
+    int idx = item_ct1.get_local_id(2);
+    padded_output_cast += (bid * padded_head_size);
+    output_cast += (bid * head_size);
+    sycl::float4 ZERO;
+    const T2 zero_h = conversion::to<T2>(0.f);
+    T2* ZERO_h = reinterpret_cast<T2*>(&ZERO);
+#pragma unroll
+    for (int i = 0; i < 4; i++) ZERO_h[i] = zero_h;
+    if (idx < head_size)
+        padded_output_cast[idx] = output_cast[idx];
+    else
+        padded_output_cast[idx] = ZERO;
+}
+
+void pad_data_kernel(float* padded_output,
+                                float* output,
+                                int head_size,
+                                int padded_head_size)
+{
+}
+
+template <typename T>
+void pad_data(T* padded_output,
+              T* output,
+              int bsz,
+              int head_size,
+              int padded_head_size,
+              dpct::queue_ptr stream)
+{
+    sycl::range<3> grid_dim(1, 1, (bsz - 1) / 16 + 1);
+    sycl::range<3> block_dim(1, 16, padded_head_size / 8);
+    /*
+    DPCT1049:7: The work-group size passed to the SYCL kernel may exceed the limit. To get the
+    device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
+    */
+    {
+        dpct::has_capability_or_fail(stream->get_device(),
+                                     {sycl::aspect::fp64, sycl::aspect::fp16});
+        stream->parallel_for(
+            sycl::nd_range<3>(grid_dim * block_dim, block_dim), [=](sycl::nd_item<3> item_ct1) {
+                pad_data_kernel(padded_output, output, head_size / 8, padded_head_size / 8);
+            });
+    }
+}
+
+#define INSTANTIATE_PAD_DATA(T) \
+    template void pad_data(T*, T*, int, int, int, dpct::queue_ptr stream);
+
+INSTANTIATE_PAD_DATA(float);
+INSTANTIATE_PAD_DATA(sycl::half);
+#ifdef BF16_AVAILABLE
+INSTANTIATE_PAD_DATA(sycl::ext::oneapi::bfloat16);
+#endif
+
+template <typename T>
+void pad_head_seq_kernel(T* padded_output,
+                                    T* output,
+                                    int seq_len,
+                                    int padded_seq_len,
+                                    int head_size,
+                                    int padded_head_size)
+{
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    using T2 = typename std::conditional<std::is_same<T, sycl::half>::value,
+                                         sycl::half2,
+                                         sycl::marray<sycl::ext::oneapi::bfloat16, 2>>::type;
+    sycl::float4* padded_output_cast = reinterpret_cast<sycl::float4*>(padded_output);
+    sycl::float4* output_cast = reinterpret_cast<sycl::float4*>(output);
+    int bsz = item_ct1.get_group(2);
+    int bid = item_ct1.get_group(1) * (item_ct1.get_local_range(1)) + item_ct1.get_local_id(1);
+    int idx = item_ct1.get_local_id(2);
+    padded_output_cast += (bsz * padded_seq_len + bid) * padded_head_size;
+    output_cast += (bsz * seq_len + bid) * head_size;
+    sycl::float4 ZERO;
+    const T2 zero_h = conversion::to<T2>(0.f);
+    T2* ZERO_h = reinterpret_cast<T2*>(&ZERO);
+#pragma unroll
+    for (int i = 0; i < 4; i++) ZERO_h[i] = zero_h;
+
+    if (idx < head_size && bid < seq_len)
+        padded_output_cast[idx] = output_cast[idx];
+    else
+        padded_output_cast[idx] = ZERO;
+}
+
+void pad_head_seq_kernel(float* padded_output,
+                                    float* output,
+                                    int seq_len,
+                                    int padded_seq_len,
+                                    int head_size,
+                                    int padded_head_size)
+{
+}
+
+template <typename T>
+void pad_head_seq(T* padded_output,
+                  T* output,
+                  int bsz,
+                  int seq_len,
+                  int padded_seq_len,
+                  int head_size,
+                  int padded_head_size,
+                  dpct::queue_ptr stream)
+{
+    sycl::range<3> grid_dim(1, padded_seq_len / 16, bsz);
+    sycl::range<3> block_dim(1, 16, padded_head_size / 8);
+    /*
+    DPCT1049:8: The work-group size passed to the SYCL kernel may exceed the limit. To get the
+    device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
+    */
+    {
+        dpct::has_capability_or_fail(stream->get_device(),
+                                     {sycl::aspect::fp64, sycl::aspect::fp16});
+        stream->parallel_for(sycl::nd_range<3>(grid_dim * block_dim, block_dim),
+                             [=](sycl::nd_item<3> item_ct1) {
+                                 pad_head_seq_kernel(padded_output,
+                                                     output,
+                                                     seq_len,
+                                                     padded_seq_len,
+                                                     head_size / 8,
+                                                     padded_head_size / 8);
+                             });
+    }
+}
+
+#define INSTANTIATE_PAD_HEAD_SEQ(T) \
+    template void pad_head_seq<T>(T*, T*, int, int, int, int, int, dpct::queue_ptr);
+
+INSTANTIATE_PAD_HEAD_SEQ(sycl::half);
+#ifdef BF16_AVAILABLE
+INSTANTIATE_PAD_HEAD_SEQ(sycl::ext::oneapi::bfloat16);
+#endif
+INSTANTIATE_PAD_HEAD_SEQ(float);
+
+// TODO(cmikeh2): evaluate different GeLU performance
+__dpct_inline__ float old_gelu(float val)
+{
+    // 1 / sqrt(2)
+    constexpr float rsqrt_2 = 0.707106769084930419922;
+    return val * 0.5f * (1.0f + sycl::erf(val * rsqrt_2));
+}
+
+namespace fused_geglu {
+constexpr int threads = 256;
+constexpr int steps = 2;
+constexpr int granularity = 16;
+}  // namespace fused_geglu
+
+__dpct_inline__ float silu(float val) { return val / (1.0f + sycl::native::exp(-val)); }
+
+template <typename T, bool useGelu>
+void fused_gate_activation(T* output,
+                                      const T* activation,
+                                      const T* bias,
+                                      int base_channels,
+                                      int output_stride,
+                                      int total_elems)
+{
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    constexpr int T_per_access = fused_geglu::granularity / sizeof(T);
+    constexpr int T_per_step = T_per_access * fused_geglu::threads;
+    constexpr int T_per_block = T_per_step * fused_geglu::steps;
+
+    const int id = item_ct1.get_group(2) * T_per_block + item_ct1.get_local_id(2) * T_per_access;
+
+#pragma unroll
+    for (int i = 0; i < fused_geglu::steps; i++) {
+        T activation_buffer_1[T_per_access];
+        T activation_buffer_2[T_per_access];
+        T bias_buffer_1[T_per_access];
+        T bias_buffer_2[T_per_access];
+
+        const int iter_id = id + T_per_step * i;
+        if (iter_id < total_elems) {
+            const int channel_id = iter_id % base_channels;
+            const int seq_id = iter_id / base_channels;
+            const int seq_offset = seq_id * base_channels * 2;
+
+            mem_access::load_global<fused_geglu::granularity>(activation_buffer_1,
+                                                              activation + seq_offset + channel_id);
+            mem_access::load_global<fused_geglu::granularity>(
+                activation_buffer_2, activation + seq_offset + channel_id + base_channels);
+            mem_access::load_global<fused_geglu::granularity>(
+                bias_buffer_1, bias + channel_id, bias != nullptr);
+            mem_access::load_global<fused_geglu::granularity>(
+                bias_buffer_2, bias + channel_id + base_channels, bias != nullptr);
+
+            // Since the GeLU is going to happen at float, might as well
+            // convert
+#pragma unroll
+            for (int v = 0; v < T_per_access; v++) {
+                T hidden_state = activation_buffer_1[v] + bias_buffer_1[v];
+                T pre_gate = activation_buffer_2[v] + bias_buffer_2[v];
+                float pre_gate_f = conversion::to<float>(pre_gate);
+                float gate_f = (useGelu) ? old_gelu(pre_gate_f) : silu(pre_gate_f);
+                T gate = conversion::to<T>(gate_f);
+                activation_buffer_1[v] = hidden_state * gate;
+            }
+
+            mem_access::store_global<fused_geglu::granularity>(
+                output + seq_id * output_stride + channel_id, activation_buffer_1);
+        }
+    }
+}
+
+template <typename T>
+void launch_gated_activation(T* output,
+                             const T* activation,
+                             const T* bias,
+                             int rows,
+                             int output_stride,
+                             int elems_per_row,
+                             bool use_gelu,
+                             dpct::queue_ptr stream)
+{
+    /*
+    Fused bias GEGLU is a variant of the gated activation functions.
+    The input here is a matrix of [batch, seq_len, 2 * intermediate_dim]
+    where the second half of the channels act as GeLU gates for the first
+    half.
+    */
+
+    // Re-derive the above figures
+    constexpr int T_per_access = fused_geglu::granularity / sizeof(T);
+    constexpr int T_per_step = T_per_access * fused_geglu::threads;
+    constexpr int T_per_block = T_per_step * fused_geglu::steps;
+
+    const int base_channels = elems_per_row / 2;
+    const int total_elems = base_channels * rows;
+
+    sycl::range<3> block(1, 1, fused_geglu::threads);
+    sycl::range<3> grid(1, 1, (total_elems + T_per_block - 1) / T_per_block);
+
+    if (use_gelu) {
+        /*
+        DPCT1049:9: The work-group size passed to the SYCL kernel may exceed the limit. To get the
+        device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
+        */
+        dpct::has_capability_or_fail(stream->get_device(),
+                                     {sycl::aspect::fp64, sycl::aspect::fp16});
+        stream->parallel_for(
+            sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1) {
+                fused_gate_activation<T, true>(
+                    output, activation, bias, base_channels, output_stride, total_elems);
+            });
+    } else {
+        /*
+        DPCT1049:10: The work-group size passed to the SYCL kernel may exceed the limit. To get the
+        device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
+        */
+        dpct::has_capability_or_fail(stream->get_device(),
+                                     {sycl::aspect::fp64, sycl::aspect::fp16});
+        stream->parallel_for(
+            sycl::nd_range<3>(grid * block, block), [=](sycl::nd_item<3> item_ct1) {
+                fused_gate_activation<T, false>(
+                    output, activation, bias, base_channels, output_stride, total_elems);
+            });
+    }
+}
+
+#define INSTANTIATE_LAUNCH_GATED_ACTIVATION(T) \
+    template void launch_gated_activation(     \
+        T*, const T*, const T*, int, int, int, bool, dpct::queue_ptr);
+
+INSTANTIATE_LAUNCH_GATED_ACTIVATION(sycl::half);
+#ifdef BF16_AVAILABLE
+INSTANTIATE_LAUNCH_GATED_ACTIVATION(sycl::ext::oneapi::bfloat16);
+#endif
+INSTANTIATE_LAUNCH_GATED_ACTIVATION(float);
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/inference_onednn_wrappers.cpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/inference_onednn_wrappers.cpp
deleted file mode 100644
index f51e647..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/inference_onednn_wrappers.cpp
+++ /dev/null
@@ -1,254 +0,0 @@
-#include <ATen/ATen.h>
-#include "inference_onednn_wrappers.hpp"
-#include "inference_sycl_layers.hpp"
-
-struct hash_pair {
-  static size_t hash_combine( size_t lhs, size_t rhs ) {
-    lhs ^= rhs + 0x9e3779b9 + (lhs << 6) + (lhs >> 2);
-    return lhs;
-  }
-
-  template <class T1, class T2>
-  std::size_t operator() (const std::pair<T1, T2> &pair) const {
-    return hash_combine(std::hash<T1>()(pair.first), std::hash<T2>()(pair.second));
-  }
-};
-
-// DNNL engine and stream should be permnantly existing and binding to sycl queue
-static std::pair<dnnl::engine, dnnl::stream> get_dnnl_engine_stream(sycl::queue queue) {
-  static std::unordered_map<sycl::queue, dnnl::stream> dnnl_streams;
-  auto it_stream = dnnl_streams.find(queue);
-
-  static std::unordered_map<std::pair<sycl::device, sycl::context>, dnnl::engine, hash_pair> dnnl_engines;
-  auto context = std::make_pair(queue.get_device(), queue.get_context());
-  // if hit, we know both engine and queue are preserved
-  if (it_stream != dnnl_streams.end()) {
-    return std::make_pair(dnnl_engines[context], it_stream->second);
-  }
-
-  auto it = dnnl_engines.find(context);
-
-  dnnl::engine engine;
-  if (it != dnnl_engines.end()) {
-    engine = it->second;
-  } else {
-    engine = dnnl::sycl_interop::make_engine(context.first, context.second);
-    dnnl_engines.emplace(std::make_pair(context, engine));
-  }
-
-  dnnl::stream stream = dnnl::sycl_interop::make_stream(engine, queue);
-  dnnl_streams.emplace(std::make_pair(queue, stream));
-
-  return std::make_pair(engine, stream);
-}
-
-template <typename T, bool bmm>
-inline int onednn_matmul(sycl::queue handle,
-                         bool trans_src,
-                         bool trans_wgt,
-                         int m,
-                         int n,
-                         int k,
-                         const float alpha,
-                         const float beta,
-                         const T* src_ptr,
-                         const T* wgt_ptr,
-                         T* dst_ptr,
-                         int batch)
-{
-    /*
-     * src, [m, k], m: batch, k: in_feature
-     * wgt, [k, n], n: k: in_features, out_feature
-     * dst, [m, n], m: batch, n: out_features
-     */
-    auto engine_stream = get_dnnl_engine_stream(handle);
-    auto engine = engine_stream.first;
-    auto stream = engine_stream.second;
-
-    dnnl::memory::dims src_dims, wgt_dims, dst_dims;
-    constexpr auto dnnl_dtype_16 = std::is_same<T, fp16>::value ? dnnl::memory::data_type::f16
-                                                                : dnnl::memory::data_type::bf16;
-    if constexpr (bmm) {
-        src_dims = {batch, m, k};
-        wgt_dims = {batch, k, n};
-        dst_dims = {batch, m, n};
-    } else {
-        src_dims = {m, k};
-        wgt_dims = {k, n};
-        dst_dims = {m, n};
-    }
-
-    dnnl::memory::desc src_md, wgt_md, dst_md;
-
-    if constexpr (bmm) {
-        src_md = dnnl::memory::desc(
-            src_dims,
-            dnnl_dtype_16,
-            trans_src ? dnnl::memory::format_tag::acb : dnnl::memory::format_tag::abc);
-        wgt_md = dnnl::memory::desc(
-            wgt_dims,
-            dnnl_dtype_16,
-            trans_wgt ? dnnl::memory::format_tag::acb : dnnl::memory::format_tag::abc);
-        dst_md = dnnl::memory::desc(dst_dims, dnnl_dtype_16, dnnl::memory::format_tag::abc);
-    } else {
-        src_md = dnnl::memory::desc(
-            src_dims,
-            dnnl_dtype_16,
-            trans_src ? dnnl::memory::format_tag::ba : dnnl::memory::format_tag::ab);
-        wgt_md = dnnl::memory::desc(
-            wgt_dims,
-            dnnl_dtype_16,
-            trans_wgt ? dnnl::memory::format_tag::ba : dnnl::memory::format_tag::ab);
-        dst_md = dnnl::memory::desc(dst_dims, dnnl_dtype_16, dnnl::memory::format_tag::ab);
-    }
-
-    auto src_mem = dnnl::memory(src_md, engine, (void*)src_ptr);
-    auto wgt_mem = dnnl::memory(wgt_md, engine, (void*)wgt_ptr);
-    auto dst_mem = dnnl::memory(dst_md, engine, (void*)dst_ptr);
-
-    dnnl::primitive_attr attr;
-    attr.set_scratchpad_mode(dnnl::scratchpad_mode::user);
-    std::unordered_map<int, dnnl::memory> matmul_args;
-    if (alpha != 1.0f) {
-        float alpha_v(alpha);
-        attr.set_scales_mask(DNNL_ARG_DST, /* mask */ 0);
-        dnnl::memory alpha_mem({{1}, dnnl::memory::data_type::f32, {1}}, engine, &alpha_v);
-        matmul_args.insert({DNNL_ARG_ATTR_SCALES | DNNL_ARG_WEIGHTS, alpha_mem});
-    }
-    if (beta != 0.0f) {
-        dnnl::post_ops po;
-        po.append_sum(beta);
-        attr.set_post_ops(po);
-    }
-
-    auto matmul_pd = dnnl::matmul::primitive_desc(engine, src_md, wgt_md, dst_md, attr);
-
-    auto matmul_prim = dnnl::matmul(matmul_pd);
-    dnnl::memory::desc scratchpad_md = matmul_pd.scratchpad_desc();
-    auto options = at::TensorOptions()
-                       .dtype(at::kByte)
-                       .layout(at::kStrided)
-                       .device(at::kXPU)
-                       .requires_grad(false);
-    auto scratchpad_tensor = at::empty({(int64_t)scratchpad_md.get_size()}, options);
-    dnnl::memory scratchpad(scratchpad_md, engine, scratchpad_tensor.data_ptr());
-
-    matmul_args.insert({DNNL_ARG_SRC, src_mem});
-    matmul_args.insert({DNNL_ARG_WEIGHTS, wgt_mem});
-    matmul_args.insert({DNNL_ARG_DST, dst_mem});
-    matmul_args.insert({DNNL_ARG_SCRATCHPAD, scratchpad});
-
-    matmul_prim.execute(stream, matmul_args);
-
-    return 0;
-}
-
-template <typename T>
-int onednn_matmul_ex(sycl::queue handle,
-                     bool trans_src,
-                     bool trans_wgt,
-                     int m,
-                     int n,
-                     int k,
-                     const float alpha,
-                     const float beta,
-                     const T* src_ptr,
-                     const T* wgt_ptr,
-                     T* dst_ptr)
-{
-    return onednn_matmul<T, false>(
-        handle, trans_src, trans_wgt, m, n, k, alpha, beta, src_ptr, wgt_ptr, dst_ptr, 1);
-}
-
-template <typename T>
-int onednn_batchgemm(sycl::queue handle,
-                     int m,
-                     int n,
-                     int k,
-                     const float alpha,
-                     const float beta,
-                     const T* src_ptr,
-                     const T* wgt_ptr,
-                     T* dst_ptr,
-                     bool trans_src,
-                     bool trans_wgt,
-                     int batch)
-{
-    return onednn_matmul<T, true>(
-        handle, trans_src, trans_wgt, m, n, k, alpha, beta, src_ptr, wgt_ptr, dst_ptr, batch);
-}
-
-template int onednn_matmul_ex(sycl::queue handle,
-                              bool trans_src,
-                              bool trans_wgt,
-                              int m,
-                              int n,
-                              int k,
-                              const float alpha,
-                              const float beta,
-                              const float* src_ptr,
-                              const float* wgt_ptr,
-                              float* dst_ptr);
-
-template int onednn_matmul_ex(sycl::queue handle,
-                              bool trans_src,
-                              bool trans_wgt,
-                              int m,
-                              int n,
-                              int k,
-                              const float alpha,
-                              const float beta,
-                              const bf16* src_ptr,
-                              const bf16* wgt_ptr,
-                              bf16* dst_ptr);
-
-template int onednn_matmul_ex(sycl::queue handle,
-                              bool trans_src,
-                              bool trans_wgt,
-                              int m,
-                              int n,
-                              int k,
-                              const float alpha,
-                              const float beta,
-                              const fp16* src_ptr,
-                              const fp16* wgt_ptr,
-                              fp16* dst_ptr);
-
-template int onednn_batchgemm(sycl::queue handle,
-                              int m,
-                              int n,
-                              int k,
-                              const float alpha,
-                              const float beta,
-                              const float* src_ptr,
-                              const float* wgt_ptr,
-                              float* dst_ptr,
-                              bool trans_src,
-                              bool trans_wgt,
-                              int batch);
-
-template int onednn_batchgemm(sycl::queue handle,
-                              int m,
-                              int n,
-                              int k,
-                              const float alpha,
-                              const float beta,
-                              const bf16* src_ptr,
-                              const bf16* wgt_ptr,
-                              bf16* dst_ptr,
-                              bool trans_src,
-                              bool trans_wgt,
-                              int batch);
-
-template int onednn_batchgemm(sycl::queue handle,
-                              int m,
-                              int n,
-                              int k,
-                              const float alpha,
-                              const float beta,
-                              const fp16* src_ptr,
-                              const fp16* wgt_ptr,
-                              fp16* dst_ptr,
-                              bool trans_src,
-                              bool trans_wgt,
-                              int batch);
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/inference_onemkl_wrappers.cpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/inference_onemkl_wrappers.cpp
deleted file mode 100644
index 4a5978c..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/inference_onemkl_wrappers.cpp
+++ /dev/null
@@ -1,132 +0,0 @@
-#include <ATen/ATen.h>
-#include "inference_onemkl_wrappers.hpp"
-#include "inference_sycl_layers.hpp"
-
-
-template <typename T>
-int onemkl_matmul_ex(sycl::queue handle,
-                   oneapi::mkl::transpose transa,
-                   oneapi::mkl::transpose transb,
-                   int m,
-                   int n,
-                   int k,
-                   const float alpha,
-                   const float beta,
-                   const T* A,
-                   const T* B,
-                   T* C)
-{
-    // TODO: ldb and ldc is right? 
-    try {
-      int lda = (transa == oneapi::mkl::transpose::nontrans) ? k : m;
-      int ldb = (transb == oneapi::mkl::transpose::nontrans) ? n : k;
-      int ldc = n;
-      oneapi::mkl::blas::row_major::gemm(
-          handle, transa, transb, m, n, k, alpha, A, lda, B, ldb, beta, C, ldc);
-    } catch (sycl::exception const& exc) {
-      std::cerr << exc.what() << "Exception caught at file:" << __FILE__ << ", line:" << __LINE__
-                  << std::endl;
-      std::exit(1);
-    }
-
-    return 0;
-}
-
-// TODO: if stride_A needed
-template <typename T>
-int onemkl_strided_batched_gemm(sycl::queue handle,
-                                oneapi::mkl::transpose transa,
-                                oneapi::mkl::transpose transb,
-                                int m,
-                                int n,
-                                int k,
-                                const float alpha,
-                                const float beta,
-                                const T* A,
-                                const T* B,
-                                T* C,
-                                int batch)
-{
-    try {
-      int lda = (transa == oneapi::mkl::transpose::nontrans) ? k : m;
-      int ldb = (transb == oneapi::mkl::transpose::nontrans) ? n : k;
-      int ldc = n;
-
-      int stride_A = m * k;
-      int stride_B = k * n;
-      int stride_C = m * n;
-
-      oneapi::mkl::blas::row_major::gemm_batch(handle,
-                                    transa,
-                                    transb,
-                                    m,
-                                    n,
-                                    k,
-                                    alpha,
-                                    A,
-                                    lda,
-                                    stride_A,
-                                    B,
-                                    ldb,
-                                    stride_B,
-                                    beta,
-                                    C,
-                                    ldc,
-                                    stride_C,
-                                    batch);
-    } catch (sycl::exception const& exc) {
-      std::cerr << exc.what() << "Exception caught at file:" << __FILE__ << ", line:" << __LINE__
-                << " (batch, m, n, k)" << batch << " " << m << " " << n << " " << k << std::endl;
-      std::exit(1);
-    }
-}
-
-template int onemkl_matmul_ex(sycl::queue handle,
-                            oneapi::mkl::transpose transa,
-                            oneapi::mkl::transpose transb,
-                            int m,
-                            int n,
-                            int k,
-                            const float alpha,
-                            const float beta,
-                            const sycl::half* A,
-                            const sycl::half* B,
-                            sycl::half* C);
-
-template int onemkl_matmul_ex(sycl::queue handle,
-                            oneapi::mkl::transpose transa,
-                            oneapi::mkl::transpose transb,
-                            int m,
-                            int n,
-                            int k,
-                            const float alpha,
-                            const float beta,
-                            const float* A,
-                            const float* B,
-                            float* C);
-
-template int onemkl_strided_batched_gemm(sycl::queue handle,
-                                         oneapi::mkl::transpose op_A,
-                                         oneapi::mkl::transpose op_B,
-                                         int m,
-                                         int n,
-                                         int k,
-                                         const float alpha,
-                                         const float beta,
-                                         const sycl::half* A,
-                                         const sycl::half* B,
-                                         sycl::half* C,
-                                         int batch);
-
-template int onemkl_strided_batched_gemm(sycl::queue handle,
-                                         oneapi::mkl::transpose op_A,
-                                         oneapi::mkl::transpose op_B,
-                                         int m,
-                                         int n,
-                                         int k,
-                                         const float alpha,
-                                         const float beta,
-                                         const float* A,
-                                         const float* B,
-                                         float* C,
-                                         int batch);
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/layer_norm.cpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/layer_norm.cpp
deleted file mode 100644
index fb263b6..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/layer_norm.cpp
+++ /dev/null
@@ -1,468 +0,0 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
-
-#include "compatible.hpp"
-#include "conversion_utils.hpp"
-#include "memory_access_utils.hpp"
-#include "reduction_utils.hpp"
-#include "inference_sycl_layers.hpp"
-
-using rop = reduce::ROpType;
-
-namespace ln {
-constexpr int granularity = 16;
-} // namespace ln
-
-/*
-Primary layer norm implementation. Assumes elems_per_row % 8
-is equal to 0.
-
-Args:
-    output: buffer for output data
-    vals: buffer for input data
-    gamma: gain for normalization
-    beta: bias for normalization
-    epsilon: numeric stability
-    elems_per_row: number of elements each block will normalize
-*/
-template <typename T, int unRoll, int threadsPerGroup, int maxThreads>
-class fused_ln {
-  T *output;
-  const T *vals;
-  const T *gamma;
-  const T *beta;
-  float epsilon;
-  int elems_per_row;
-
-public:
-  fused_ln(T *output, const T *vals, const T *gamma, const T *beta,
-           float epsilon, int elems_per_row)
-      : output(output), vals(vals), gamma(gamma), beta(beta), epsilon(epsilon),
-        elems_per_row(elems_per_row){};
-
-  void operator()(sycl::nd_item<2> pos) const {
-    constexpr int T_per_load = ln::granularity / sizeof(T);
-
-    auto warp = sycl::ext::oneapi::experimental::this_sub_group();
-    auto tb = pos.get_group();
-
-    const int block_offset =
-        (tb.get_group_id(1) * (maxThreads / threadsPerGroup) * elems_per_row) +
-        (tb.get_local_id(0) * elems_per_row);
-    const int thread_offset = tb.get_local_id(1) * T_per_load;
-    const int base_offset = block_offset + thread_offset;
-    const int stride = tb.get_local_linear_range() * T_per_load;
-
-    float sum = reduce::init<rop::Add, float>();
-
-    const T *input_base = vals + base_offset;
-
-    T local_buffer[unRoll * T_per_load];
-
-#pragma unRoll
-    for (int i = 0; i < unRoll; i++) {
-      T *iteration_buffer = local_buffer + i * T_per_load;
-
-      mem_access::load_global<ln::granularity>(
-          iteration_buffer, input_base + i * stride,
-          thread_offset + i * stride < elems_per_row);
-
-#pragma unRoll
-      for (int j = 0; j < T_per_load; j++) {
-        float vals_up_cast = conversion::to<float>(iteration_buffer[j]);
-        sum = reduce::element<rop::Add>(sum, vals_up_cast);
-      }
-    }
-
-    reduce::partitioned_block<rop::Add, threadsPerGroup>(tb, warp, sum);
-    const float mean = sum / elems_per_row;
-
-    float mean_diff = reduce::init<rop::Add, float>();
-
-#pragma unRoll
-    for (int i = 0; i < unRoll; i++) {
-#pragma unRoll
-      for (int j = 0; j < T_per_load; j++) {
-        // Using a 0 value here skews the variance, have to if-guard
-        if (thread_offset + i * stride < elems_per_row) {
-          float diff =
-              (conversion::to<float>(local_buffer[i * T_per_load + j]) - mean);
-          mean_diff = reduce::element<rop::Add>(mean_diff, diff * diff);
-        }
-      }
-    }
-
-    reduce::partitioned_block<rop::Add, threadsPerGroup>(tb, warp, mean_diff);
-    const float variance = mean_diff / elems_per_row;
-    const float denom = rsqrt(variance + epsilon);
-
-    T *block_output = output + block_offset;
-
-#pragma unRoll
-    for (int i = 0; i < unRoll; i++) {
-      T *iteration_buffer = local_buffer + i * T_per_load;
-      const int iter_idx = i * stride + thread_offset;
-      const bool do_loads = iter_idx < elems_per_row;
-
-      T gamma_local[T_per_load], beta_local[T_per_load];
-
-      mem_access::load_global<ln::granularity>(gamma_local, gamma + iter_idx,
-                                               do_loads);
-      mem_access::load_global<ln::granularity>(beta_local, beta + iter_idx,
-                                               do_loads);
-
-#pragma unRoll
-      for (int j = 0; j < T_per_load; j++) {
-            float val = conversion::to<float>(iteration_buffer[j]);
-            val = (val - mean) * denom;
-            val =
-                val * conversion::to<float>(gamma_local[j]) + conversion::to<float>(beta_local[j]);
-            iteration_buffer[j] = conversion::to<T>(val);
-      }
-
-      if (do_loads) {
-        mem_access::store_global<ln::granularity>(block_output + iter_idx,
-                                                  iteration_buffer);
-      }
-    }
-  };
-};
-
-#define LAUNCH_FUSED_LN(unRollFactor, threadsPerGroup, maxThreads)             \
-  {                                                                            \
-    fused_ln<T, unRollFactor, threadsPerGroup, maxThreads> fn(                 \
-        output, vals, gamma, beta, epsilon, elems_per_row);                    \
-    stream.submit([&](sycl::handler &cmd_list) {                                \
-      cmd_list.parallel_for(sycl::nd_range<2>{grid, block}, fn);               \
-    });                                                                        \
-  }
-
-template <typename T>
-void launch_fused_ln(T *output, const T *vals, const T *gamma, const T *beta,
-                     float epsilon, int rows, int elems_per_row,
-                     sycl::queue stream) {
-  // 8 for sycl::half, 4 for float
-  constexpr int T_per_load = ln::granularity / sizeof(T);
-
-  constexpr int maxThreads = 256;
-
-  // For Float, unRoll 4, for sycl::half, unRoll 2
-  constexpr int internal_unRoll = sizeof(T) == 4 ? 4 : 2;
-
-  const bool is_subblock_schedule = (elems_per_row <= 128) ? true : false;
-  const int h_per_step =
-      is_subblock_schedule ? T_per_load : T_per_load * internal_unRoll;
-
-  // Scheduling concern: may be slightly faster for some inputs to assign
-  // multiple stages of warp-sized blocks rather than stepping up to 64/96
-  // threads
-  const int one_step_threads =
-      next_pow2((elems_per_row + h_per_step - 1) / h_per_step);
-  const int threadsPerGroup =
-      (one_step_threads < maxThreads) ? one_step_threads : maxThreads;
-
-  const int groups_per_block_max =
-      is_subblock_schedule
-          ? (maxThreads + threadsPerGroup - 1) / threadsPerGroup
-          : 1;
-  const int groups_per_block =
-      (rows < groups_per_block_max) ? rows : groups_per_block_max;
-  const int groups_launch = (groups_per_block + rows - 1) / groups_per_block;
-
-  sycl::range<2> block{(unsigned long)groups_per_block, (size_t)threadsPerGroup};
-  sycl::range<2> grid{(unsigned long)groups_per_block, (size_t)(threadsPerGroup * groups_launch)};
-
-  const int elems_per_step = threadsPerGroup * h_per_step;
-  const int external_unRoll =
-      (elems_per_row + elems_per_step - 1) / elems_per_step;
-
-  if (is_subblock_schedule) {
-    // <=128
-    if (threadsPerGroup == 1) {
-      LAUNCH_FUSED_LN(1, 1, maxThreads);
-    } else if (threadsPerGroup == 2) {
-      LAUNCH_FUSED_LN(1, 2, maxThreads);
-    } else if (threadsPerGroup == 4) {
-      LAUNCH_FUSED_LN(1, 4, maxThreads);
-    } else if (threadsPerGroup == 8) {
-      LAUNCH_FUSED_LN(1, 8, maxThreads);
-    } else if (threadsPerGroup == 16) {
-      LAUNCH_FUSED_LN(1, 16, maxThreads);
-    }
-  } else if (external_unRoll == 1) {
-    // 129 - 4096 elems
-    // (this can launch with 1-7 warps as well)
-    LAUNCH_FUSED_LN(1 * internal_unRoll, maxThreads, maxThreads);
-  } else if (external_unRoll == 2) {
-    // 4097 - 8192 elems
-    LAUNCH_FUSED_LN(2 * internal_unRoll, maxThreads, maxThreads);
-  } else if (external_unRoll == 3) {
-    // 8193 - 12288 elems
-    LAUNCH_FUSED_LN(3 * internal_unRoll, maxThreads, maxThreads);
-  } else if (external_unRoll == 4) {
-    // 12289 - 16384 elems
-    LAUNCH_FUSED_LN(4 * internal_unRoll, maxThreads, maxThreads);
-  }
-}
-
-template void launch_fused_ln(sycl::half *, const sycl::half *,
-                              const sycl::half *, const sycl::half *, float,
-                              int, int, sycl::queue);
-template void launch_fused_ln(bf16 *, const bf16 *, const bf16 *, const bf16 *,
-                              float, int, int, sycl::queue);
-template void launch_fused_ln(float *, const float *, const float *,
-                              const float *, float, int, int, sycl::queue);
-
-/*
-Fused resiual + bias + layer norm implementation. Assumes elems_per_row % 8
-is equal to 0.
-
-TODO(cmikeh2): Goal is to deprecate this implementation. The bias + residual
-need to be fused into compute-bound producer operations.
-
-Args:
-    output: buffer for output data
-    res_output: output of residual addition
-    vals: buffer for input data
-    residual: residual data
-    bias: bias of of input data
-    gamma: gain for normalization
-    beta: bias for normalization
-    epsilon: numeric stability
-    elems_per_row: number of elements each block will normalize
-Template arg:
-    StoreResidual: controls whether the residual calculation is stored
-        or not. When set to false, the input `res_output` is unused.
-*/
-template <typename T, int unRoll, int threadsPerGroup, int maxThreads,
-          bool preLnResidual>
-class fused_residual_ln {
-
-private:
-  T *output;
-  T *res_output;
-  const T *vals;
-  const T *residual;
-  const T *bias;
-  const T *gamma;
-  const T *beta;
-  float epsilon;
-  int elems_per_row;
-
-public:
-  fused_residual_ln(T *output, T *res_output, const T *vals, const T *residual,
-                    const T *bias, const T *gamma, const T *beta, float epsilon,
-                    int elems_per_row)
-      : output(output), res_output(res_output), vals(vals), residual(residual),
-        bias(bias), gamma(gamma), beta(beta), epsilon(epsilon),
-        elems_per_row(elems_per_row) {};
-
-  void operator()(sycl::nd_item<2> pos) const {
-
-    constexpr int T_per_load = ln::granularity / sizeof(T);
-
-    auto tb = pos.get_group();
-    auto warp = sycl::ext::oneapi::experimental::this_sub_group();
-
-    // X-dimension of the block
-    const int block_offset =
-        (tb.get_group_id(1) * (maxThreads / threadsPerGroup) * elems_per_row) +
-        (tb.get_local_id(0) * elems_per_row);
-    const int thread_offset = tb.get_local_id(1) * T_per_load;
-    const int base_offset = block_offset + thread_offset;
-    const int stride = tb.get_local_linear_range() * T_per_load;
-
-    float sum = reduce::init<rop::Add, float>();
-
-    const T *input_base = vals + base_offset;
-    const T *residual_base = residual + base_offset;
-    const T *bias_base = bias + thread_offset;
-
-    T local_buffer[unRoll * T_per_load];
-
-    // Unlike a vanilla layernorm, since we're fusing the two adds as well
-    // an inner unRoll seems to be less valuable. If anything, a double unRoll
-    // makes the most sense if we find we are having performance issues.
-#pragma unRoll
-    for (int i = 0; i < unRoll; i++) {
-      T *iteration_buffer = local_buffer + i * T_per_load;
-      T residual_buffer[T_per_load];
-      T bias_buffer[T_per_load];
-
-      mem_access::load_global<ln::granularity>(
-          iteration_buffer, input_base + i * stride,
-          thread_offset + i * stride < elems_per_row);
-      mem_access::load_global<ln::granularity>(
-          residual_buffer, residual_base + i * stride,
-          thread_offset + i * stride < elems_per_row);
-      mem_access::load_global<ln::granularity>(
-          bias_buffer, bias_base + i * stride,
-          thread_offset + i * stride < elems_per_row);
-
-#pragma unRoll
-      for (int j = 0; j < T_per_load; j++) {
-        float vals_up_cast = conversion::to<float>(iteration_buffer[j]);
-        float res_up_cast = conversion::to<float>(residual_buffer[j]);
-        float bias_up_cast = conversion::to<float>(bias_buffer[j]);
-        vals_up_cast = vals_up_cast + bias_up_cast + res_up_cast;
-        sum = reduce::element<rop::Add>(sum, vals_up_cast);
-        iteration_buffer[j] = conversion::to<T>(vals_up_cast);
-      }
-
-      if (preLnResidual && (thread_offset + i * stride < elems_per_row)) {
-        mem_access::store_global<ln::granularity>(
-            res_output + base_offset + i * stride, iteration_buffer);
-      }
-    }
-
-    reduce::partitioned_block<rop::Add, threadsPerGroup>(tb, warp, sum);
-    const float mean = sum / elems_per_row;
-
-    float mean_diff = reduce::init<rop::Add, float>();
-#pragma unRoll
-    for (int i = 0; i < unRoll; i++) {
-#pragma unRoll
-      for (int j = 0; j < T_per_load; j++) {
-        // Using a 0 value here skews the variance, have to if-guard
-        if (thread_offset + i * stride < elems_per_row) {
-          float diff =
-              (conversion::to<float>(local_buffer[i * T_per_load + j]) - mean);
-          mean_diff = reduce::element<rop::Add>(mean_diff, diff * diff);
-        }
-      }
-    }
-
-    reduce::partitioned_block<rop::Add, threadsPerGroup>(tb, warp, mean_diff);
-    const float variance = mean_diff / elems_per_row;
-    const float denom = rsqrt(variance + epsilon);
-
-    T *block_output = output + block_offset;
-
-#pragma unRoll
-    for (int i = 0; i < unRoll; i++) {
-      T *iteration_buffer = local_buffer + i * T_per_load;
-      const int iter_idx = i * stride + thread_offset;
-      const bool do_loads = iter_idx < elems_per_row;
-
-      T gamma_local[T_per_load], beta_local[T_per_load];
-
-      mem_access::load_global<ln::granularity>(gamma_local, gamma + iter_idx,
-                                               do_loads);
-      mem_access::load_global<ln::granularity>(beta_local, beta + iter_idx,
-                                               do_loads);
-
-#pragma unRoll
-      for (int j = 0; j < T_per_load; j++) {
-            float val = conversion::to<float>(iteration_buffer[j]);
-            val = (val - mean) * denom;
-            val =
-                val * conversion::to<float>(gamma_local[j]) + conversion::to<float>(beta_local[j]);
-            iteration_buffer[j] = conversion::to<T>(val);
-      }
-
-      if (do_loads) {
-        mem_access::store_global<ln::granularity>(block_output + iter_idx,
-                                                  iteration_buffer);
-      }
-    }
-  };
-};
-
-// TODO(cmikeh2): There's a bunch of redundancy here that needs to be
-// removed/simplified.
-#define LAUNCH_FUSED_RES_LN(unRollFactor, threadsPerGroup, maxThreads)             \
-{                                                                                  \
-  fused_residual_ln<T, unRollFactor, threadsPerGroup, maxThreads, false> fn(       \
-    output, nullptr, vals, residual, bias, gamma, beta, epsilon, elems_per_row);   \
-  stream.submit([&](sycl::handler &cmd_list) {                                      \
-      cmd_list.parallel_for(sycl::nd_range<2>{grid, block}, fn); });               \
-}
-
-template <typename T>
-void launch_fused_residual_ln(T *output, const T *vals, const T *residual,
-                              const T *bias, const T *gamma, const T *beta,
-                              float epsilon, int rows, int elems_per_row,
-                              sycl::queue stream) {
-  // 8 for sycl::half, 4 for float
-  constexpr int T_per_load = ln::granularity / sizeof(T);
-
-  constexpr int maxThreads = 256;
-
-  // For Float, unRoll 4, for sycl::half, unRoll 2
-  constexpr int internal_unRoll = sizeof(T) == 4 ? 4 : 2;
-
-  const bool is_subblock_schedule = (elems_per_row <= 128) ? true : false;
-  const int h_per_step =
-      is_subblock_schedule ? T_per_load : T_per_load * internal_unRoll;
-
-  // Scheduling concern: may be slightly faster for some inputs to assign
-  // multiple stages of warp-sized blocks rather than stepping up to 64/96
-  // threads
-  const int one_step_threads =
-      next_pow2((elems_per_row + h_per_step - 1) / h_per_step);
-  const int threadsPerGroup =
-      (one_step_threads < maxThreads) ? one_step_threads : maxThreads;
-
-  const int groups_per_block_max =
-      is_subblock_schedule
-          ? (maxThreads + threadsPerGroup - 1) / threadsPerGroup
-          : 1;
-  const int groups_per_block =
-      (rows < groups_per_block_max) ? rows : groups_per_block_max;
-  const int groups_launch = (groups_per_block + rows - 1) / groups_per_block;
-
-  sycl::range<2> block{(unsigned long)groups_per_block, (size_t)threadsPerGroup};
-  sycl::range<2> grid{(unsigned long)groups_per_block, (size_t)(threadsPerGroup * groups_launch)};
-
-  const int elems_per_step = threadsPerGroup * h_per_step;
-  const int external_unRoll =
-      (elems_per_row + elems_per_step - 1) / elems_per_step;
-
-  if (is_subblock_schedule) {
-    // <=128
-    if (threadsPerGroup == 1) {
-      LAUNCH_FUSED_RES_LN(1, 1, maxThreads);
-    } else if (threadsPerGroup == 2) {
-      LAUNCH_FUSED_RES_LN(1, 2, maxThreads);
-    } else if (threadsPerGroup == 4) {
-      LAUNCH_FUSED_RES_LN(1, 4, maxThreads);
-    } else if (threadsPerGroup == 8) {
-      LAUNCH_FUSED_RES_LN(1, 8, maxThreads);
-    } else if (threadsPerGroup == 16) {
-      LAUNCH_FUSED_RES_LN(1, 16, maxThreads);
-    }
-  } else if (external_unRoll == 1) {
-    // 129 - 4096 elems
-    // (this can launch with 1-7 warps as well)
-    LAUNCH_FUSED_RES_LN(1 * internal_unRoll, maxThreads, maxThreads);
-  } else if (external_unRoll == 2) {
-    // 4097 - 8192 elems
-    LAUNCH_FUSED_RES_LN(2 * internal_unRoll, maxThreads, maxThreads);
-  } else if (external_unRoll == 3) {
-    // 8193 - 12288 elems
-    LAUNCH_FUSED_RES_LN(3 * internal_unRoll, maxThreads, maxThreads);
-  } else if (external_unRoll == 4) {
-    // 12289 - 16384 elems
-    LAUNCH_FUSED_RES_LN(4 * internal_unRoll, maxThreads, maxThreads);
-  }
-}
-
-
-
-// No-store specializations
-template void launch_fused_residual_ln(sycl::half *, const sycl::half *,
-                                       const sycl::half *, const sycl::half *,
-                                       const sycl::half *, const sycl::half *,
-                                       float, int, int, sycl::queue);
-
-template void launch_fused_residual_ln(bf16 *, const bf16 *, const bf16 *,
-                                       const bf16 *, const bf16 *, const bf16 *,
-                                       float, int, int, sycl::queue);
-
-template void launch_fused_residual_ln(float *, const float *, const float *,
-                                       const float *, const float *,
-                                       const float *, float, int, int,
-                                       sycl::queue);
-
-
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/layer_norm.dp.cpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/layer_norm.dp.cpp
new file mode 100644
index 0000000..18a0ee5
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/layer_norm.dp.cpp
@@ -0,0 +1,617 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include "conversion_utils.h"
+#include "ds_kernel_utils.h"
+#include "inference_cuda_layers.h"
+#include "memory_access_utils.h"
+#include "reduction_utils.h"
+
+using rop = reduce::ROpType;
+
+namespace ln {
+constexpr int granularity = 16;
+}  // namespace ln
+
+/*
+Primary layer norm implementation. Assumes elems_per_row % 8
+is equal to 0.
+
+Args:
+    output: buffer for output data
+    vals: buffer for input data
+    gamma: gain for normalization
+    beta: bias for normalization
+    epsilon: numeric stability
+    elems_per_row: number of elements each block will normalize
+*/
+template <typename T, int unRoll, int threadsPerGroup, int maxThreads>
+/*
+DPCT1110:3: The total declared local variable size in device function fused_ln exceeds 128 bytes and
+may cause high register pressure. Consult with your hardware vendor to find the total register size
+available and adjust the code, or use smaller sub-group size to avoid high register pressure.
+*/
+void fused_ln(T* output,
+              const T* vals,
+              const T* gamma,
+              const T* beta,
+              float epsilon,
+              int elems_per_row)
+{
+    constexpr int T_per_load = ln::granularity / sizeof(T);
+
+    sycl::group<3> tb = sycl::ext::oneapi::experimental::this_group<3>();
+    sycl::sub_group warp = sycl::ext::oneapi::experimental::this_sub_group();
+
+    // X-dimension of the block
+    const int block_offset =
+        (tb.get_group_id()[2] * (maxThreads / threadsPerGroup) * elems_per_row) +
+        (tb.get_local_id()[1] * elems_per_row);
+    const int thread_offset = tb.get_local_id()[2] * T_per_load;
+    const int base_offset = block_offset + thread_offset;
+    const int stride =
+        sycl::ext::oneapi::experimental::this_nd_item<3>().get_local_range(2) * T_per_load;
+
+    float sum = reduce::init<rop::Add, float>();
+
+    const T* input_base = vals + base_offset;
+
+    T local_buffer[unRoll * T_per_load];
+
+#pragma unRoll
+    for (int i = 0; i < unRoll; i++) {
+        T* iteration_buffer = local_buffer + i * T_per_load;
+
+        mem_access::load_global<ln::granularity>(
+            iteration_buffer, input_base + i * stride, thread_offset + i * stride < elems_per_row);
+
+#pragma unRoll
+        for (int j = 0; j < T_per_load; j++) {
+            float vals_up_cast = conversion::to<float>(iteration_buffer[j]);
+            sum = reduce::element<rop::Add>(sum, vals_up_cast);
+        }
+    }
+
+    reduce::partitioned_block<rop::Add, threadsPerGroup>(tb, warp, sum);
+    const float mean = sum / elems_per_row;
+
+    float mean_diff = reduce::init<rop::Add, float>();
+
+#pragma unRoll
+    for (int i = 0; i < unRoll; i++) {
+#pragma unRoll
+        for (int j = 0; j < T_per_load; j++) {
+            // Using a 0 value here skews the variance, have to if-guard
+            if (thread_offset + i * stride < elems_per_row) {
+                float diff = (conversion::to<float>(local_buffer[i * T_per_load + j]) - mean);
+                mean_diff = reduce::element<rop::Add>(mean_diff, diff * diff);
+            }
+        }
+    }
+
+    reduce::partitioned_block<rop::Add, threadsPerGroup>(tb, warp, mean_diff);
+    const float variance = mean_diff / elems_per_row;
+    /*
+    DPCT1013:9: The rounding mode could not be specified and the generated code may have different
+    accuracy than the original code. Verify the correctness. SYCL math built-in function rounding
+    mode is aligned with OpenCL C 1.2 standard.
+    */
+    const float denom = sycl::rsqrt(variance + epsilon);
+
+    // const T mean_compute = conversion::to<T>(mean);
+    // const T denom_compute = conversion::to<T>(denom);
+
+    T* block_output = output + block_offset;
+
+#pragma unRoll
+    for (int i = 0; i < unRoll; i++) {
+        T* iteration_buffer = local_buffer + i * T_per_load;
+        const int iter_idx = i * stride + thread_offset;
+        const bool do_loads = iter_idx < elems_per_row;
+
+        T gamma_local[T_per_load], beta_local[T_per_load];
+
+        mem_access::load_global<ln::granularity>(gamma_local, gamma + iter_idx, do_loads);
+        mem_access::load_global<ln::granularity>(beta_local, beta + iter_idx, do_loads);
+
+#pragma unRoll
+        for (int j = 0; j < T_per_load; j++) {
+            float val = conversion::to<float>(iteration_buffer[j]);
+            val = (val - mean) * denom;
+            val =
+                val * conversion::to<float>(gamma_local[j]) + conversion::to<float>(beta_local[j]);
+            iteration_buffer[j] = conversion::to<T>(val);
+        }
+
+        if (do_loads) {
+            mem_access::store_global<ln::granularity>(block_output + iter_idx, iteration_buffer);
+        }
+    }
+}
+
+/*
+DPCT1049:4: The work-group size passed to the SYCL kernel may exceed the limit. To get the device
+limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
+*/
+#define LAUNCH_FUSED_LN(unRollFactor, threadsPerGroup, maxThreads)                                \
+  {                                                                                               \
+    dpct::has_capability_or_fail(stream->get_device(), {sycl::aspect::fp64, sycl::aspect::fp16}); \
+    stream->submit([&](sycl::handler& cgh) {                                                      \
+      T* output_ct0 = output;                                                                     \
+      const T* vals_ct1 = vals;                                                                   \
+      const T* gamma_ct2 = gamma;                                                                 \
+      const T* beta_ct3 = beta;                                                                   \
+      auto epsilon_ct4 = epsilon;                                                                 \
+      auto elems_per_row_ct5 = elems_per_row;                                                     \
+                                                                                                  \
+      cgh.parallel_for(                                                                           \
+          sycl::nd_range<3>(grid * block, block),                                                 \
+          [=](sycl::nd_item<3> item_ct1) [[intel::reqd_sub_group_size(32)]] {                     \
+            fused_ln<T, unRollFactor, threadsPerGroup, maxThreads>(                               \
+                output_ct0, vals_ct1, gamma_ct2, beta_ct3, epsilon_ct4, elems_per_row_ct5);       \
+          });                                                                                     \
+    });                                                                                           \
+  }
+
+template <typename T>
+void launch_fused_ln(T* output,
+                     const T* vals,
+                     const T* gamma,
+                     const T* beta,
+                     float epsilon,
+                     int rows,
+                     int elems_per_row,
+                     dpct::queue_ptr stream)
+{
+    // 8 for sycl::half, 4 for float
+    constexpr int T_per_load = ln::granularity / sizeof(T);
+
+    constexpr int maxThreads = 256;
+
+    // For Flaoat, unRoll 4, for sycl::half, unRoll 2
+    constexpr int internal_unRoll = sizeof(T) == 4 ? 4 : 2;
+
+    const bool is_subblock_schedule = (elems_per_row <= 128) ? true : false;
+    const int h_per_step = is_subblock_schedule ? T_per_load : T_per_load * internal_unRoll;
+
+    // Scheduling concern: may be slightly faster for some inputs to assign multiple stages of
+    // warp-sized blocks rather than stepping up to 64/96 threads
+    const int one_step_threads = next_pow2((elems_per_row + h_per_step - 1) / h_per_step);
+    const int threadsPerGroup = (one_step_threads < maxThreads) ? one_step_threads : maxThreads;
+
+    const int groups_per_block_max =
+        is_subblock_schedule ? (maxThreads + threadsPerGroup - 1) / threadsPerGroup : 1;
+    const int groups_per_block = (rows < groups_per_block_max) ? rows : groups_per_block_max;
+    const int groups_launch = (groups_per_block + rows - 1) / groups_per_block;
+
+    sycl::range<3> block(1, groups_per_block, threadsPerGroup);
+    sycl::range<3> grid(1, 1, groups_launch);
+
+    const int elems_per_step = threadsPerGroup * h_per_step;
+    const int external_unRoll = (elems_per_row + elems_per_step - 1) / elems_per_step;
+
+    if (is_subblock_schedule) {
+        // <=128
+        if (threadsPerGroup == 1) {
+            LAUNCH_FUSED_LN(1, 1, maxThreads);
+        } else if (threadsPerGroup == 2) {
+            LAUNCH_FUSED_LN(1, 2, maxThreads);
+        } else if (threadsPerGroup == 4) {
+            LAUNCH_FUSED_LN(1, 4, maxThreads);
+        } else if (threadsPerGroup == 8) {
+            LAUNCH_FUSED_LN(1, 8, maxThreads);
+        } else if (threadsPerGroup == 16) {
+            LAUNCH_FUSED_LN(1, 16, maxThreads);
+        }
+    } else if (external_unRoll == 1) {
+        // 129 - 4096 elems
+        // (this can launch with 1-7 warps as well)
+        LAUNCH_FUSED_LN(1 * internal_unRoll, maxThreads, maxThreads);
+    } else if (external_unRoll == 2) {
+        // 4097 - 8192 elems
+        LAUNCH_FUSED_LN(2 * internal_unRoll, maxThreads, maxThreads);
+    } else if (external_unRoll == 3) {
+        // 8193 - 12288 elems
+        LAUNCH_FUSED_LN(3 * internal_unRoll, maxThreads, maxThreads);
+    } else if (external_unRoll == 4) {
+        // 12289 - 16384 elems
+        LAUNCH_FUSED_LN(4 * internal_unRoll, maxThreads, maxThreads);
+    }
+}
+
+#define INSTANTIATE_FUSED_LN(T)    \
+    template void launch_fused_ln( \
+        T*, const T*, const T*, const T*, float, int, int, dpct::queue_ptr);
+
+INSTANTIATE_FUSED_LN(sycl::half);
+#ifdef BF16_AVAILABLE
+INSTANTIATE_FUSED_LN(sycl::ext::oneapi::bfloat16);
+#endif
+INSTANTIATE_FUSED_LN(float);
+
+/*
+Fused resiual + bias + layer norm implementation. Assumes elems_per_row % 8
+is equal to 0.
+
+TODO(cmikeh2): Goal is to deprecate this implementation. The bias + residual
+need to be fused into compute-bound producer operations.
+
+Args:
+    output: buffer for output data
+    res_output: output of residual addition
+    vals: buffer for input data
+    residual: residual data
+    bias: bias of of input data
+    gamma: gain for normalization
+    beta: bias for normalization
+    epsilon: numeric stability
+    elems_per_row: number of elements each block will normalize
+Template arg:
+    StoreResidual: controls whether the residual calculation is stored
+        or not. When set to false, the input `res_output` is unused.
+*/
+template <typename T, int unRoll, int threadsPerGroup, int maxThreads, bool preLnResidual>
+/*
+DPCT1110:5: The total declared local variable size in device function fused_residual_ln exceeds 128
+bytes and may cause high register pressure. Consult with your hardware vendor to find the total
+register size available and adjust the code, or use smaller sub-group size to avoid high register
+pressure.
+*/
+void fused_residual_ln(T* output,
+                       T* res_output,
+                       const T* vals,
+                       const T* residual,
+                       const T* bias,
+                       const T* gamma,
+                       const T* beta,
+                       float epsilon,
+                       int elems_per_row)
+{
+    constexpr int T_per_load = ln::granularity / sizeof(T);
+
+    sycl::group<3> tb = sycl::ext::oneapi::experimental::this_group<3>();
+    sycl::sub_group warp = sycl::ext::oneapi::experimental::this_sub_group();
+
+    // X-dimension of the block
+    const int block_offset =
+        (tb.get_group_id()[2] * (maxThreads / threadsPerGroup) * elems_per_row) +
+        (tb.get_local_id()[1] * elems_per_row);
+    const int thread_offset = tb.get_local_id()[2] * T_per_load;
+    const int base_offset = block_offset + thread_offset;
+    const int stride =
+        sycl::ext::oneapi::experimental::this_group<3>().get_local_linear_range() * T_per_load;
+
+    float sum = reduce::init<rop::Add, float>();
+
+    const T* input_base = vals + base_offset;
+    const T* residual_base = residual + base_offset;
+    const T* bias_base = bias + thread_offset;
+
+    T local_buffer[unRoll * T_per_load];
+
+    // Unlike a vanilla layernorm, since we're fusing the two adds as well
+    // an inner unRoll seems to be less valuable. If anything, a double unRoll
+    // makes the most sense if we find we are having performance issues.
+#pragma unRoll
+    for (int i = 0; i < unRoll; i++) {
+        T* iteration_buffer = local_buffer + i * T_per_load;
+        T residual_buffer[T_per_load];
+        T bias_buffer[T_per_load];
+
+        mem_access::load_global<ln::granularity>(
+            iteration_buffer, input_base + i * stride, thread_offset + i * stride < elems_per_row);
+        mem_access::load_global<ln::granularity>(residual_buffer,
+                                                 residual_base + i * stride,
+                                                 thread_offset + i * stride < elems_per_row);
+        mem_access::load_global<ln::granularity>(
+            bias_buffer, bias_base + i * stride, thread_offset + i * stride < elems_per_row);
+
+#pragma unRoll
+        for (int j = 0; j < T_per_load; j++) {
+            float vals_up_cast = conversion::to<float>(iteration_buffer[j]);
+            float res_up_cast = conversion::to<float>(residual_buffer[j]);
+            float bias_up_cast = conversion::to<float>(bias_buffer[j]);
+            vals_up_cast = vals_up_cast + bias_up_cast + res_up_cast;
+            sum = reduce::element<rop::Add>(sum, vals_up_cast);
+            iteration_buffer[j] = conversion::to<T>(vals_up_cast);
+        }
+
+        if (preLnResidual && (thread_offset + i * stride < elems_per_row)) {
+            mem_access::store_global<ln::granularity>(res_output + base_offset + i * stride,
+                                                      iteration_buffer);
+        }
+    }
+
+    reduce::partitioned_block<rop::Add, threadsPerGroup>(tb, warp, sum);
+    const float mean = sum / elems_per_row;
+
+    float mean_diff = reduce::init<rop::Add, float>();
+#pragma unRoll
+    for (int i = 0; i < unRoll; i++) {
+#pragma unRoll
+        for (int j = 0; j < T_per_load; j++) {
+            // Using a 0 value here skews the variance, have to if-guard
+            if (thread_offset + i * stride < elems_per_row) {
+                float diff = (conversion::to<float>(local_buffer[i * T_per_load + j]) - mean);
+                mean_diff = reduce::element<rop::Add>(mean_diff, diff * diff);
+            }
+        }
+    }
+
+    reduce::partitioned_block<rop::Add, threadsPerGroup>(tb, warp, mean_diff);
+    const float variance = mean_diff / elems_per_row;
+    /*
+    DPCT1013:10: The rounding mode could not be specified and the generated code may have different
+    accuracy than the original code. Verify the correctness. SYCL math built-in function rounding
+    mode is aligned with OpenCL C 1.2 standard.
+    */
+    const float denom = sycl::rsqrt(variance + epsilon);
+
+    T* block_output = output + block_offset;
+
+#pragma unRoll
+    for (int i = 0; i < unRoll; i++) {
+        T* iteration_buffer = local_buffer + i * T_per_load;
+        const int iter_idx = i * stride + thread_offset;
+        const bool do_loads = iter_idx < elems_per_row;
+
+        T gamma_local[T_per_load], beta_local[T_per_load];
+
+        mem_access::load_global<ln::granularity>(gamma_local, gamma + iter_idx, do_loads);
+        mem_access::load_global<ln::granularity>(beta_local, beta + iter_idx, do_loads);
+
+#pragma unRoll
+        for (int j = 0; j < T_per_load; j++) {
+            // iteration_buffer[j] = (iteration_buffer[j] - mean_compute) * denom_compute;
+            // iteration_buffer[j] = iteration_buffer[j] * gamma_local[j] + beta_local[j];
+            float val = conversion::to<float>(iteration_buffer[j]);
+            val = (val - mean) * denom;
+            val =
+                val * conversion::to<float>(gamma_local[j]) + conversion::to<float>(beta_local[j]);
+            iteration_buffer[j] = conversion::to<T>(val);
+        }
+
+        if (do_loads) {
+            mem_access::store_global<ln::granularity>(block_output + iter_idx, iteration_buffer);
+        }
+    }
+}
+
+// TODO(cmikeh2): There's a bunch of redundancy here that needs to be removed/simplified.
+/*
+DPCT1049:6: The work-group size passed to the SYCL kernel may exceed the limit. To get the device
+limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
+*/
+#define LAUNCH_FUSED_RES_LN(unRollFactor, threadsPerGroup, maxThreads)                            \
+  {                                                                                               \
+    dpct::has_capability_or_fail(stream->get_device(), {sycl::aspect::fp64, sycl::aspect::fp16}); \
+    stream->submit([&](sycl::handler& cgh) {                                                      \
+      T* output_ct0 = output;                                                                     \
+      auto nullptr_ct1 = nullptr;                                                                 \
+      const T* vals_ct2 = vals;                                                                   \
+      const T* residual_ct3 = residual;                                                           \
+      const T* bias_ct4 = bias;                                                                   \
+      const T* gamma_ct5 = gamma;                                                                 \
+      const T* beta_ct6 = beta;                                                                   \
+      auto epsilon_ct7 = epsilon;                                                                 \
+      auto elems_per_row_ct8 = elems_per_row;                                                     \
+                                                                                                  \
+      cgh.parallel_for(sycl::nd_range<3>(grid * block, block),                                    \
+                       [=](sycl::nd_item<3> item_ct1) [[intel::reqd_sub_group_size(32)]] {        \
+                         fused_residual_ln<T, unRollFactor, threadsPerGroup, maxThreads, false>(  \
+                             output_ct0,                                                          \
+                             nullptr_ct1,                                                         \
+                             vals_ct2,                                                            \
+                             residual_ct3,                                                        \
+                             bias_ct4,                                                            \
+                             gamma_ct5,                                                           \
+                             beta_ct6,                                                            \
+                             epsilon_ct7,                                                         \
+                             elems_per_row_ct8);                                                  \
+                       });                                                                        \
+    });                                                                                           \
+  }
+
+template <typename T>
+void launch_fused_residual_ln(T* output,
+                              const T* vals,
+                              const T* residual,
+                              const T* bias,
+                              const T* gamma,
+                              const T* beta,
+                              float epsilon,
+                              int rows,
+                              int elems_per_row,
+                              dpct::queue_ptr stream)
+{
+    // 8 for sycl::half, 4 for float
+    constexpr int T_per_load = ln::granularity / sizeof(T);
+
+    constexpr int maxThreads = 256;
+
+    // For Flaoat, unRoll 4, for sycl::half, unRoll 2
+    constexpr int internal_unRoll = sizeof(T) == 4 ? 4 : 2;
+
+    const bool is_subblock_schedule = (elems_per_row <= 128) ? true : false;
+    const int h_per_step = is_subblock_schedule ? T_per_load : T_per_load * internal_unRoll;
+
+    // Scheduling concern: may be slightly faster for some inputs to assign multiple stages of
+    // warp-sized blocks rather than stepping up to 64/96 threads
+    const int one_step_threads = next_pow2((elems_per_row + h_per_step - 1) / h_per_step);
+    const int threadsPerGroup = (one_step_threads < maxThreads) ? one_step_threads : maxThreads;
+
+    const int groups_per_block_max =
+        is_subblock_schedule ? (maxThreads + threadsPerGroup - 1) / threadsPerGroup : 1;
+    const int groups_per_block = (rows < groups_per_block_max) ? rows : groups_per_block_max;
+    const int groups_launch = (groups_per_block + rows - 1) / groups_per_block;
+
+    sycl::range<3> block(1, groups_per_block, threadsPerGroup);
+    sycl::range<3> grid(1, 1, groups_launch);
+
+    const int elems_per_step = threadsPerGroup * h_per_step;
+    const int external_unRoll = (elems_per_row + elems_per_step - 1) / elems_per_step;
+
+    if (is_subblock_schedule) {
+        // <=128
+        if (threadsPerGroup == 1) {
+            LAUNCH_FUSED_RES_LN(1, 1, maxThreads);
+        } else if (threadsPerGroup == 2) {
+            LAUNCH_FUSED_RES_LN(1, 2, maxThreads);
+        } else if (threadsPerGroup == 4) {
+            LAUNCH_FUSED_RES_LN(1, 4, maxThreads);
+        } else if (threadsPerGroup == 8) {
+            LAUNCH_FUSED_RES_LN(1, 8, maxThreads);
+        } else if (threadsPerGroup == 16) {
+            LAUNCH_FUSED_RES_LN(1, 16, maxThreads);
+        }
+    } else if (external_unRoll == 1) {
+        // 129 - 4096 elems
+        // (this can launch with 1-7 warps as well)
+        LAUNCH_FUSED_RES_LN(1 * internal_unRoll, maxThreads, maxThreads);
+    } else if (external_unRoll == 2) {
+        // 4097 - 8192 elems
+        LAUNCH_FUSED_RES_LN(2 * internal_unRoll, maxThreads, maxThreads);
+    } else if (external_unRoll == 3) {
+        // 8193 - 12288 elems
+        LAUNCH_FUSED_RES_LN(3 * internal_unRoll, maxThreads, maxThreads);
+    } else if (external_unRoll == 4) {
+        // 12289 - 16384 elems
+        LAUNCH_FUSED_RES_LN(4 * internal_unRoll, maxThreads, maxThreads);
+    }
+}
+
+/*
+DPCT1049:7: The work-group size passed to the SYCL kernel may exceed the limit. To get the device
+limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
+*/
+#define LAUNCH_FUSED_RES_LN_STORE_PRE_LN_RES(unRollFactor, threadsPerGroup, maxThreads)           \
+  {                                                                                               \
+    dpct::has_capability_or_fail(stream->get_device(), {sycl::aspect::fp64, sycl::aspect::fp16}); \
+    stream->submit([&](sycl::handler& cgh) {                                                      \
+      T* norm_output_ct0 = norm_output;                                                           \
+      T* res_output_ct1 = res_output;                                                             \
+      const T* vals_ct2 = vals;                                                                   \
+      const T* residual_ct3 = residual;                                                           \
+      const T* bias_ct4 = bias;                                                                   \
+      const T* gamma_ct5 = gamma;                                                                 \
+      const T* beta_ct6 = beta;                                                                   \
+      auto epsilon_ct7 = epsilon;                                                                 \
+      auto elems_per_row_ct8 = elems_per_row;                                                     \
+                                                                                                  \
+      cgh.parallel_for(sycl::nd_range<3>(grid * block, block),                                    \
+                       [=](sycl::nd_item<3> item_ct1) [[intel::reqd_sub_group_size(32)]] {        \
+                         fused_residual_ln<T, unRollFactor, threadsPerGroup, maxThreads, true>(   \
+                             norm_output_ct0,                                                     \
+                             res_output_ct1,                                                      \
+                             vals_ct2,                                                            \
+                             residual_ct3,                                                        \
+                             bias_ct4,                                                            \
+                             gamma_ct5,                                                           \
+                             beta_ct6,                                                            \
+                             epsilon_ct7,                                                         \
+                             elems_per_row_ct8);                                                  \
+                       });                                                                        \
+    });                                                                                           \
+  }
+
+template <typename T>
+void launch_fused_residual_ln_store_pre_ln_res(T* norm_output,
+                                               T* res_output,
+                                               const T* vals,
+                                               const T* residual,
+                                               const T* bias,
+                                               const T* gamma,
+                                               const T* beta,
+                                               float epsilon,
+                                               int rows,
+                                               int elems_per_row,
+                                               dpct::queue_ptr stream)
+{
+    // 8 for sycl::half, 4 for float
+    constexpr int T_per_load = ln::granularity / sizeof(T);
+
+    constexpr int maxThreads = 256;
+
+    // For Flaoat, unRoll 4, for sycl::half, unRoll 2
+    constexpr int internal_unRoll = sizeof(T) == 4 ? 4 : 2;
+
+    const bool is_subblock_schedule = (elems_per_row <= 128) ? true : false;
+    const int h_per_step = is_subblock_schedule ? T_per_load : T_per_load * internal_unRoll;
+
+    // Scheduling concern: may be slightly faster for some inputs to assign multiple stages of
+    // warp-sized blocks rather than stepping up to 64/96 threads
+    const int one_step_threads = next_pow2((elems_per_row + h_per_step - 1) / h_per_step);
+    const int threadsPerGroup = (one_step_threads < maxThreads) ? one_step_threads : maxThreads;
+
+    const int groups_per_block_max =
+        is_subblock_schedule ? (maxThreads + threadsPerGroup - 1) / threadsPerGroup : 1;
+    const int groups_per_block = (rows < groups_per_block_max) ? rows : groups_per_block_max;
+    const int groups_launch = (groups_per_block + rows - 1) / groups_per_block;
+
+    sycl::range<3> block(1, groups_per_block, threadsPerGroup);
+    sycl::range<3> grid(1, 1, groups_launch);
+
+    const int elems_per_step = threadsPerGroup * h_per_step;
+    const int external_unRoll = (elems_per_row + elems_per_step - 1) / elems_per_step;
+
+    if (is_subblock_schedule) {
+        // <=128
+        if (threadsPerGroup == 1) {
+            LAUNCH_FUSED_RES_LN_STORE_PRE_LN_RES(1, 1, maxThreads);
+        } else if (threadsPerGroup == 2) {
+            LAUNCH_FUSED_RES_LN_STORE_PRE_LN_RES(1, 2, maxThreads);
+        } else if (threadsPerGroup == 4) {
+            LAUNCH_FUSED_RES_LN_STORE_PRE_LN_RES(1, 4, maxThreads);
+        } else if (threadsPerGroup == 8) {
+            LAUNCH_FUSED_RES_LN_STORE_PRE_LN_RES(1, 8, maxThreads);
+        } else if (threadsPerGroup == 16) {
+            LAUNCH_FUSED_RES_LN_STORE_PRE_LN_RES(1, 16, maxThreads);
+        }
+    } else if (external_unRoll == 1) {
+        // 129 - 4096 elems
+        // (this can launch with 1-7 warps as well)
+        LAUNCH_FUSED_RES_LN_STORE_PRE_LN_RES(1 * internal_unRoll, maxThreads, maxThreads);
+    } else if (external_unRoll == 2) {
+        // 4097 - 8192 elems
+        LAUNCH_FUSED_RES_LN_STORE_PRE_LN_RES(2 * internal_unRoll, maxThreads, maxThreads);
+    } else if (external_unRoll == 3) {
+        // 8193 - 12288 elems
+        LAUNCH_FUSED_RES_LN_STORE_PRE_LN_RES(3 * internal_unRoll, maxThreads, maxThreads);
+    } else if (external_unRoll == 4) {
+        // 12289 - 16384 elems
+        LAUNCH_FUSED_RES_LN_STORE_PRE_LN_RES(4 * internal_unRoll, maxThreads, maxThreads);
+    }
+}
+
+#define INSTANTIATE_RES_LN(T)                  \
+    template void launch_fused_residual_ln<T>( \
+        T*, const T*, const T*, const T*, const T*, const T*, float, int, int, dpct::queue_ptr);
+
+#define INSTANTIATE_PRE_LN_RES(T)                                        \
+    template void launch_fused_residual_ln_store_pre_ln_res<T>(T*,       \
+                                                               T*,       \
+                                                               const T*, \
+                                                               const T*, \
+                                                               const T*, \
+                                                               const T*, \
+                                                               const T*, \
+                                                               float,    \
+                                                               int,      \
+                                                               int,      \
+                                                               dpct::queue_ptr);
+
+INSTANTIATE_RES_LN(sycl::half);
+INSTANTIATE_RES_LN(float);
+#ifdef BF16_AVAILABLE
+INSTANTIATE_RES_LN(sycl::ext::oneapi::bfloat16);
+#endif
+
+INSTANTIATE_PRE_LN_RES(sycl::half);
+INSTANTIATE_PRE_LN_RES(float);
+#ifdef BF16_AVAILABLE
+INSTANTIATE_PRE_LN_RES(sycl::ext::oneapi::bfloat16);
+#endif
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/pointwise_ops.cpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/pointwise_ops.dp.cpp
similarity index 81%
rename from intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/pointwise_ops.cpp
rename to intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/pointwise_ops.dp.cpp
index 80656cb..22bd348 100644
--- a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/pointwise_ops.cpp
+++ b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/pointwise_ops.dp.cpp
@@ -4,9 +4,10 @@
 // DeepSpeed Team
 
 #include <sycl/sycl.hpp>
-#include "conversion_utils.hpp"
-#include "compatible.hpp"
-#include "memory_access_utils.hpp"
+#include <dpct/dpct.hpp>
+#include "conversion_utils.h"
+#include "ds_kernel_utils.h"
+#include "memory_access_utils.h"
 
 namespace pwise {
 constexpr int granularity = 16;
@@ -15,9 +16,9 @@ constexpr int threads = 256;
 }  // namespace pwise
 
 template <typename T>
-void vector_add_kernel(T* out, const T* a, const T* b, float gamma, int num_elems,
-                       const sycl::nd_item<3> &item_ct1)
+void vector_add_kernel(T* out, const T* a, const T* b, float gamma, int num_elems)
 {
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
     constexpr int T_per_access = pwise::granularity / sizeof(T);
 
     const int block_offset = item_ct1.get_group(2) * pwise::threads * pwise::unroll * T_per_access;
@@ -53,7 +54,7 @@ void launch_vector_add(T* out,
                        const T* b,
                        float gamma,
                        int num_elems,
-                       sycl::queue stream)
+                       dpct::queue_ptr stream)
 {
     constexpr int T_per_access = pwise::granularity / sizeof(T);
     constexpr int T_per_block = pwise::threads * T_per_access * pwise::unroll;
@@ -62,19 +63,21 @@ void launch_vector_add(T* out,
     sycl::range<3> grid(1, 1, (num_elems + T_per_block - 1) / T_per_block);
 
     {
-        stream.parallel_for(sycl::nd_range<3>(grid * block, block),
+        dpct::has_capability_or_fail(stream->get_device(),
+                                     {sycl::aspect::fp64, sycl::aspect::fp16});
+        stream->parallel_for(sycl::nd_range<3>(grid * block, block),
                              [=](sycl::nd_item<3> item_ct1) {
-                                 vector_add_kernel(out, a, b, gamma, num_elems, item_ct1);
+                                 vector_add_kernel(out, a, b, gamma, num_elems);
                              });
     }
 }
 
 #define INSTANTIATE_VECTOR_ADD(T)       \
     template void launch_vector_add<T>( \
-        T * out, const T* a, const T* b, float gamma, int num_elems, sycl::queue stream);
+        T * out, const T* a, const T* b, float gamma, int num_elems, dpct::queue_ptr stream);
 
 INSTANTIATE_VECTOR_ADD(float)
 INSTANTIATE_VECTOR_ADD(sycl::half)
 #ifdef BF16_AVAILABLE
-INSTANTIATE_VECTOR_ADD(bf16)
+INSTANTIATE_VECTOR_ADD(sycl::ext::oneapi::bfloat16)
 #endif
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/pt_binding.cpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/pt_binding.cpp
index 21ce542..6d2ec84 100644
--- a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/pt_binding.cpp
+++ b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/pt_binding.cpp
@@ -1,16 +1,28 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+// #include <c10/cuda/CUDAStream.h>
+#include <ipex.h>
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
 #include <torch/extension.h>
 #include <stdexcept>
 #include <vector>
-#include "compatible.hpp"
-#include "inference_context.hpp"
-#include "inference_onednn_wrappers.hpp"
-#include "inference_onemkl_wrappers.hpp"
-#include "inference_sycl_layers.hpp"
+#include "inference_context.h"
+#include "inference_cublas_wrappers.h"
+#include "inference_cuda_layers.h"
+#include <cmath>
+
+std::array<int, 3> gemm_algos = std::array<int, 3>({99, 99, 99});
 
 // NOTE: This activation function type enum should be always in sync
 // with the python counterpart, otherwise the casting from python binding
 // will be incorrect.
-enum class ActivationFuncType { UNKNOWN = 0, GELU = 1, ReLU = 2 };
+enum class ActivationFuncType { UNKNOWN = 0, GELU = 1, ReLU = 2, GATED_GELU = 3, GATED_SILU = 4 };
+
+enum class NormType { UNKNOWN = 0, LayerNorm = 1, GroupNorm = 2, RMSNorm = 3 };
 
 enum class TransformerType : uint8_t { UNKNOWN = 0, GPTType = 1, BERTType = 2 };
 
@@ -47,6 +59,54 @@ inline auto get_attn_mask_stride(at::Tensor& attn_mask) -> int
     return 0;
 }
 
+template <typename T>
+at::Tensor ds_softmax(at::Tensor& attn_scores,
+                      at::Tensor& attn_mask,
+                      at::Tensor& alibi,
+                      bool triangular,
+                      bool recompute,
+                      bool local_attention,
+                      int window_size,
+                      bool async_op,
+                      float layer_scale,
+                      int head_offset,
+                      int mp_size)
+{
+    auto attn_scores_c = attn_scores.contiguous();
+    int bsz = attn_scores_c.size(0);
+
+    int seq_len = attn_scores_c.size(1);
+    int len = attn_scores_c.sizes().size();
+    if (len > 2) seq_len = attn_scores_c.size(2);
+
+    int soft_len = attn_scores_c.size(2);
+    if (len > 3) soft_len = attn_scores_c.size(3);
+
+    int heads = 1;
+    if (len > 1) heads = attn_scores_c.size(1);
+
+    auto mask_stride = get_attn_mask_stride(attn_mask);
+
+    launch_attn_softmax_v2((T*)attn_scores_c.data_ptr(),
+                           (attn_mask.sizes().size() > 1 ? (T*)attn_mask.data_ptr() : nullptr),
+                           (alibi.sizes().size() > 1 ? (T*)alibi.data_ptr() : nullptr),
+                           layer_scale,
+                           triangular,
+                           recompute,
+                           local_attention,
+                           window_size,
+                           bsz,
+                           heads,
+                           seq_len,
+                           soft_len,
+                           head_offset,
+                           mask_stride,
+                           mp_size,
+                           InferenceContext::Instance().GetCurrentStream(async_op));
+
+    return attn_scores_c;
+}
+
 template <typename T>
 void allocate_workspace(unsigned hidden_dim,
                         unsigned num_heads,
@@ -73,34 +133,212 @@ void allocate_workspace(unsigned hidden_dim,
 }
 
 template <typename T>
-at::Tensor ds_softmax(at::Tensor& attn_scores,
-                      at::Tensor& attn_mask,
-                      at::Tensor& alibi,
-                      bool triangular,
-                      bool recompute,
-                      bool local_attention,
-                      int window_size,
-                      bool async_op,
-                      float layer_scale,
-                      int head_offset,
-                      int mp_size)
+at::Tensor einsum_sec_sm_ecm(at::Tensor& Q, at::Tensor& W)
 {
-    auto attn_scores_c = attn_scores.contiguous();
-    int bsz = attn_scores_c.size(0);
+    auto options = at::TensorOptions()
+                       .dtype(Q.options().dtype())
+                       .layout(at::kStrided)
+                       .device(at::kXPU)
+                       .requires_grad(false);
+    T* workspace = (T*)InferenceContext::Instance().GetWorkSpace();
+    float alpha = 1;
+    float gemm_beta = 0.0;
 
-    int seq_len = attn_scores_c.size(1);
-    int len = attn_scores_c.sizes().size();
-    if (len > 2) seq_len = attn_scores_c.size(2);
+    /*
+    // Reallocate memory if we received a new prompt
+    if (!workspace || input.size(1) != 1) {
+        allocate_workspace<T>(W.size(1), InferenceContext::Instance().GetMaxTokenLength(),
+    Q.size(0), 1, head_size); workspace = (T*)InferenceContext::Instance().GetWorkSpace();
+    }
+    */
 
-    int soft_len = attn_scores_c.size(2);
-    if (len > 3) soft_len = attn_scores_c.size(3);
+    auto O = at::from_blob(workspace,
+                           {Q.size(1), Q.size(2), W.size(1)},
+                           c10::TensorType::contiguousStridesOf({Q.size(1), Q.size(2), W.size(1)}),
+                           nullptr,
+                           options,
+                           Q.device());
+    unsigned m = W.size(1);
+    unsigned n = Q.size(1) * Q.size(2);
+    unsigned k = Q.size(0);
+    cublas_gemm_ex(InferenceContext::Instance().GetCublasHandle(),
+                   oneapi::mkl::transpose::nontrans,
+                   oneapi::mkl::transpose::trans,
+                   m,
+                   n,
+                   k,
+                   &alpha,
+                   &gemm_beta,
+                   (T*)W.data_ptr(),
+                   (T*)Q.data_ptr(),
+                   (T*)O.data_ptr(),
+#ifdef __HIP_PLATFORM_AMD__
+                   rocblas_gemm_algo_standard);
+#else
+                   99);
+#endif
+    return O;
+}
 
-    int heads = 1;
-    if (len > 1) heads = attn_scores_c.size(1);
+template <typename T>
+void attention_unfused(at::Tensor& prev_key_cont,
+                       at::Tensor& query_cont,
+                       at::Tensor& attn_mask,
+                       at::Tensor& prev_value_cont,
+                       at::Tensor& output,
+                       int& bsz,
+                       int& seq_len,
+                       int& soft_len,
+                       int& heads,
+                       float& norm_factor,
+                       bool triangular,
+                       bool recompute,
+                       bool local_attention,
+                       int window_size)
+{
+    auto options = at::TensorOptions()
+                       .dtype(query_cont.options().dtype())
+                       .layout(at::kStrided)
+                       .device(at::kXPU)
+                       .requires_grad(false);
+    float alpha = norm_factor;
+    float gemm_beta = 0.0;
+    auto attn_score = at::empty({bsz, heads, seq_len, soft_len}, options);
+    int k = prev_value_cont.size(2) / heads;
 
     auto mask_stride = get_attn_mask_stride(attn_mask);
 
-    launch_attn_softmax_v2((T*)attn_scores_c.data_ptr(),
+    *(InferenceContext::Instance().GetCublasHandle()) =
+        *(InferenceContext::Instance().GetCurrentStream());
+    cublas_strided_batched_gemm(InferenceContext::Instance().GetCublasHandle(),
+                                soft_len,
+                                seq_len,
+                                k,
+                                &alpha,
+                                &gemm_beta,
+                                (T*)prev_key_cont.data_ptr(),
+                                (T*)query_cont.data_ptr(),
+                                (T*)attn_score.data_ptr(),
+                                oneapi::mkl::transpose::nontrans,
+                                oneapi::mkl::transpose::nontrans,
+                                soft_len * k,
+                                seq_len * k,
+                                seq_len * soft_len,
+                                bsz * heads,
+#ifdef __HIP_PLATFORM_AMD__
+                                rocblas_gemm_algo_standard);
+#else
+                                99);
+#endif
+    launch_attn_softmax_v2((T*)attn_score.data_ptr(),
+                           (T*)(attn_mask.sizes().size() > 1 ? attn_mask.data_ptr() : nullptr),
+                           (T*)nullptr,
+                           1.0,
+                           triangular,
+                           recompute,
+                           local_attention,
+                           window_size,
+                           bsz,
+                           heads,
+                           seq_len,
+                           soft_len,
+                           0,
+                           mask_stride,
+                           1,
+                           InferenceContext::Instance().GetCurrentStream(false));
+    alpha = 1.0;
+    cublas_strided_batched_gemm(InferenceContext::Instance().GetCublasHandle(),
+                                k,
+                                seq_len,
+                                soft_len,
+                                &alpha,
+                                &gemm_beta,
+                                (T*)prev_value_cont.data_ptr(),
+                                (T*)attn_score.data_ptr(),
+                                (T*)output.data_ptr(),
+                                oneapi::mkl::transpose::nontrans,
+                                oneapi::mkl::transpose::nontrans,
+                                soft_len * k,
+                                seq_len * soft_len,
+                                seq_len * k,
+                                bsz * heads,
+#ifdef __HIP_PLATFORM_AMD__
+                                rocblas_gemm_algo_standard);
+#else
+                                99);
+#endif
+}
+
+template <typename T>
+std::vector<at::Tensor> ds_softmax_context1(at::Tensor& query,
+                                            at::Tensor& prev_key,
+                                            at::Tensor& new_key,
+                                            at::Tensor& attn_mask,
+                                            at::Tensor& prev_value,
+                                            at::Tensor& new_value,
+                                            int heads,
+                                            float norm_factor,
+                                            bool merging,
+                                            bool triangular,
+                                            bool local_attention,
+                                            int window_size,
+                                            bool no_masking)
+{
+    auto query_cont = query.contiguous();
+    auto prev_key_cont = prev_key.contiguous();
+    auto prev_value_cont = prev_value.contiguous();
+
+    int new_size = (new_value.sizes().size() > 1 ? new_value.size(1) : 0);
+
+    // Attn_Score [ batch Head Sequence-length Softmax-length]
+
+    int bsz = query_cont.size(0);
+    int seq_len = query_cont.size(1);
+    int soft_len = prev_value.size(1);
+
+    auto options = at::TensorOptions()
+                       .dtype(query_cont.options().dtype())
+                       .layout(at::kStrided)
+                       .device(at::kXPU)
+                       .requires_grad(false);
+
+    auto output =
+        at::empty({prev_value.size(0), heads, seq_len, prev_value.size(2) / heads}, options);
+    attention_unfused<T>(prev_key_cont,
+                         query_cont,
+                         attn_mask,  //(no_masking ? nullptr : (T*)attn_mask.data_ptr()),
+                         prev_value_cont,
+                         output,
+                         bsz,
+                         seq_len,
+                         soft_len,
+                         heads,
+                         norm_factor,
+                         (triangular && (new_size == 0)),
+                         (new_size == 0),
+                         local_attention,
+                         window_size);
+
+    return {output, prev_key, prev_value};
+}
+
+template <typename T>
+void ds_softmax_internal(T* attn_scores,
+                         at::Tensor& attn_mask,
+                         at::Tensor& alibi,
+                         float& layer_scale,
+                         bool triangular,
+                         bool recompute,
+                         bool local_attention,
+                         int window_size,
+                         int bsz,
+                         int seq_len,
+                         int soft_len,
+                         int heads)
+{
+    auto mask_stride = get_attn_mask_stride(attn_mask);
+
+    launch_attn_softmax_v2((T*)attn_scores,
                            (attn_mask.sizes().size() > 1 ? (T*)attn_mask.data_ptr() : nullptr),
                            (alibi.sizes().size() > 1 ? (T*)alibi.data_ptr() : nullptr),
                            layer_scale,
@@ -112,109 +350,427 @@ at::Tensor ds_softmax(at::Tensor& attn_scores,
                            heads,
                            seq_len,
                            soft_len,
-                           head_offset,
+                           0,
                            mask_stride,
-                           mp_size,
-                           InferenceContext::Instance().GetCurrentStream());
+                           1,
+                           at::cuda::getCurrentCUDAStream());
+}
 
-    return attn_scores_c;
+template <typename T>
+void attention_unfused(T* prev_key_cont,
+                       T* query_cont,
+                       at::Tensor& attn_mask,
+                       T* prev_value_cont,
+                       T* output,
+                       unsigned& bsz,
+                       int& k,
+                       unsigned& seq_len,
+                       unsigned& soft_len,
+                       int& heads,
+                       float& norm_factor,
+                       bool triangular,
+                       bool recompute,
+                       bool local_attention,
+                       int window_size,
+                       at::Tensor& alibi,
+                       int layer_id)
+{
+    float layer_scale = alibi.sizes().size() > 1 ? std::max(1, layer_id) : 1.0;
+    float alpha = norm_factor * norm_factor / layer_scale;
+    float gemm_beta = 0.0;
+    T* workspace = (T*)InferenceContext::Instance().GetAttentionUnfusedWorkspace();
+
+    *(InferenceContext::Instance().GetCublasHandle()) =
+        *(InferenceContext::Instance().GetCurrentStream());
+    cublas_strided_batched_gemm(InferenceContext::Instance().GetCublasHandle(),
+                                soft_len,
+                                seq_len,
+                                k,
+                                &alpha,
+                                &gemm_beta,
+                                (T*)prev_key_cont,
+                                (T*)query_cont,
+                                workspace,
+                                oneapi::mkl::transpose::trans,
+                                oneapi::mkl::transpose::nontrans,
+                                InferenceContext::Instance().GetMaxTokenLength() * k,
+                                seq_len * k,
+                                seq_len * soft_len,
+                                bsz * heads,
+#ifdef __HIP_PLATFORM_AMD__
+                                rocblas_gemm_algo_standard);
+#else
+                                99);
+#endif
+    ds_softmax_internal<T>(workspace,
+                           attn_mask,
+                           alibi,
+                           layer_scale,
+                           triangular,
+                           recompute,
+                           local_attention,
+                           window_size,
+                           bsz,
+                           seq_len,
+                           soft_len,
+                           heads);
+    alpha = 1.0;
+    cublas_strided_batched_gemm(InferenceContext::Instance().GetCublasHandle(),
+                                k,
+                                seq_len,
+                                soft_len,
+                                &alpha,
+                                &gemm_beta,
+                                (T*)prev_value_cont,
+                                workspace,
+                                (T*)output,
+                                oneapi::mkl::transpose::nontrans,
+                                oneapi::mkl::transpose::nontrans,
+                                InferenceContext::Instance().GetMaxTokenLength() * k,
+                                seq_len * soft_len,
+                                seq_len * k,
+                                bsz * heads,
+#ifdef __HIP_PLATFORM_AMD__
+                                rocblas_gemm_algo_standard);
+#else
+                                99);
+#endif
 }
 
-at::Tensor ds_layer_norm(at::Tensor& input, at::Tensor& gamma, at::Tensor& beta, float epsilon)
+void reset_cache() { InferenceContext::Instance().reset_tokens(); }
+
+template <typename T>
+std::vector<at::Tensor> ds_softmax_context(at::Tensor& query_key_value,
+                                           at::Tensor& attn_mask,
+                                           int rotary_dim,
+                                           bool rotate_half,
+                                           bool rotate_every_two,
+                                           int heads,
+                                           int num_kv,
+                                           float norm_factor,
+                                           bool triangular,
+                                           bool local_attention,
+                                           int window_size,
+                                           bool no_masking,
+                                           unsigned layer_id,
+                                           unsigned num_layers,
+                                           at::Tensor& alibi,
+                                           float rope_theta)
 {
-    const int rows = input.size(0) * input.size(1);
-    const int elems_per_row = input.size(2);
-    auto output = at::empty_like(input);
+    unsigned bsz = query_key_value.size(0);
+    unsigned seq_len = query_key_value.size(1);
+    int k = query_key_value.size(2) / (heads + 2 * (num_kv > 0 ? num_kv : heads));
+    unsigned hidden_dim = heads * k;
 
-    if (input.options().dtype() == torch::kFloat16) {
-        launch_fused_ln((fp16*)output.data_ptr(),
-                        (const fp16*)input.data_ptr(),
-                        (const fp16*)gamma.data_ptr(),
-                        (const fp16*)beta.data_ptr(),
-                        epsilon,
-                        rows,
-                        elems_per_row,
-                        InferenceContext::Instance().GetCurrentStream());
-    } else {
-        launch_fused_ln((float*)output.data_ptr(),
-                        (const float*)input.data_ptr(),
-                        (const float*)gamma.data_ptr(),
-                        (const float*)beta.data_ptr(),
-                        epsilon,
-                        rows,
-                        elems_per_row,
-                        InferenceContext::Instance().GetCurrentStream());
+    bool is_prompt = (seq_len > 1);
+
+    if (is_prompt) InferenceContext::Instance().reset_tokens(seq_len);
+    unsigned soft_len = InferenceContext::Instance().current_tokens();
+
+    auto options = at::TensorOptions()
+                       .dtype(query_key_value.options().dtype())
+                       .layout(at::kStrided)
+                       .device(at::kXPU)
+                       .requires_grad(false);
+
+    T* workspace = (T*)InferenceContext::Instance().GetWorkSpace();
+    size_t buf_size = bsz * seq_len * hidden_dim;
+    auto output = at::from_blob(workspace + 4 * buf_size,
+                                {bsz, seq_len, hidden_dim},
+                                c10::TensorType::contiguousStridesOf({bsz, seq_len, hidden_dim}),
+                                nullptr,
+                                options,
+                                query_key_value.device());
+
+    auto query_cont = workspace + 5 * buf_size;
+    size_t offset =
+        10 * (hidden_dim * bsz * InferenceContext::Instance().GetMaxTokenLength()) +
+        layer_id * 2 * bsz * InferenceContext::Instance().GetMaxTokenLength() * hidden_dim;
+    unsigned all_tokens = soft_len;
+    auto kv_cache = workspace + offset + (hidden_dim / heads) * (is_prompt ? 0 : soft_len - 1);
+    size_t value_offset = bsz * InferenceContext::Instance().GetMaxTokenLength() * hidden_dim;
+
+    T* temp_buf = (T*)output.data_ptr() + at::numel(output);
+    launch_bias_add_transform_0213<T>((T*)query_cont,
+                                      kv_cache,
+                                      kv_cache + value_offset,
+                                      (T*)query_key_value.data_ptr(),
+                                      nullptr,
+                                      bsz,
+                                      seq_len,
+                                      (is_prompt ? 0 : soft_len - 1),
+                                      soft_len,
+                                      hidden_dim,
+                                      heads,
+                                      (num_kv > 0 ? num_kv : heads),
+                                      rotary_dim,
+                                      rotate_half,
+                                      rotate_every_two,
+                                      InferenceContext::Instance().GetCurrentStream(),
+                                      3,
+                                      InferenceContext::Instance().GetMaxTokenLength(),
+                                      rope_theta);
+    if (rotary_dim > 0 && rotate_half)
+        launch_apply_rotary_pos_emb(query_cont,
+                                    kv_cache,
+                                    k,
+                                    seq_len,
+                                    rotary_dim,
+                                    (is_prompt ? 0 : soft_len - 1),
+                                    heads,
+                                    bsz,
+                                    rope_theta,
+                                    InferenceContext::Instance().GetCurrentStream(),
+                                    InferenceContext::Instance().GetMaxTokenLength());
+
+    attention_unfused<T>(workspace + offset,
+                         (T*)query_cont,
+                         attn_mask,
+                         workspace + offset + value_offset,
+                         temp_buf,
+                         bsz,
+                         k,
+                         seq_len,
+                         all_tokens,
+                         heads,
+                         norm_factor,
+                         (triangular && is_prompt),
+                         is_prompt,
+                         local_attention,
+                         window_size,
+                         alibi,
+                         layer_id);
+    launch_transform4d_0213<T>((T*)output.data_ptr(),
+                               temp_buf,
+                               bsz,
+                               heads,
+                               seq_len,
+                               output.size(2),
+                               InferenceContext::Instance().GetCurrentStream(false),
+                               1);
+
+    if (layer_id == num_layers - 1) InferenceContext::Instance().advance_tokens();
+    auto prev_key = at::from_blob(workspace + offset,
+                                  {bsz, heads, all_tokens, k},
+                                  {hidden_dim * InferenceContext::Instance().GetMaxTokenLength(),
+                                   k * InferenceContext::Instance().GetMaxTokenLength(),
+                                   k,
+                                   1},
+                                  nullptr,
+                                  options,
+                                  query_key_value.device());
+
+    auto prev_value = at::from_blob(workspace + offset + value_offset,
+                                    {bsz, heads, all_tokens, k},
+                                    {hidden_dim * InferenceContext::Instance().GetMaxTokenLength(),
+                                     k * InferenceContext::Instance().GetMaxTokenLength(),
+                                     k,
+                                     1},
+                                    nullptr,
+                                    options,
+                                    query_key_value.device());
+
+    return {output, prev_key, prev_value};
+}
+
+template <typename T>
+at::Tensor ds_bias_gelu(at::Tensor& input, at::Tensor& bias)
+{
+    auto input_cont = input.contiguous();
+
+    int bsz = input_cont.size(0) * input_cont.size(1);
+    int intermediate_size = input_cont.size(2);
+
+    launch_bias_gelu((T*)input_cont.data_ptr(),
+                     (T*)bias.data_ptr(),
+                     intermediate_size,
+                     bsz,
+                     InferenceContext::Instance().GetCurrentStream());
+    return input_cont;
+}
+
+#define DISPATCH_GATED_ACT(T_TYPE, C_TYPE)                                         \
+    if (activation.options().dtype() == torch::T_TYPE) {                           \
+        launch_gated_activation((C_TYPE*)output.data_ptr(),                        \
+                                (const C_TYPE*)activation.data_ptr(),              \
+                                (const C_TYPE*)bias.data_ptr(),                    \
+                                rows,                                              \
+                                out_channels,                                      \
+                                channels,                                          \
+                                activation_type == ActivationFuncType::GATED_GELU, \
+                                InferenceContext::Instance().GetCurrentStream());  \
     }
 
+at::Tensor ds_gated_activation(at::Tensor& activation, at::Tensor& bias, int actFun)
+{
+    /*
+    Used in FF of Stable diffusion
+    */
+
+    const ActivationFuncType activation_type = static_cast<ActivationFuncType>(actFun);
+
+    assert(activation_type == ActivationFuncType::GATED_GELU ||
+           activation_type == ActivationFuncType::GATED_SILU);
+
+    const int batch_size = activation.size(0);
+    const int seq_len = activation.size(1);
+    const int channels = activation.size(2);
+
+    const int rows = batch_size * seq_len;
+    // Dimensionality is cut in half
+    const int out_channels = channels / 2;
+
+    auto output = at::empty({batch_size, seq_len, out_channels}, activation.options());
+
+    DISPATCH_GATED_ACT(kFloat, float);
+    DISPATCH_GATED_ACT(kHalf, sycl::half);
+#ifdef BF16_AVAILABLE
+    DISPATCH_GATED_ACT(kBFloat16, sycl::ext::oneapi::bfloat16);
+#endif
+
     return output;
 }
 
-/* Currently only used in unit testing */
-at::Tensor ds_layer_norm_residual(at::Tensor& input,
-                                  at::Tensor& bias,
-                                  at::Tensor& residual,
-                                  at::Tensor& gamma,
-                                  at::Tensor& beta,
-                                  float epsilon)
+template <typename T>
+at::Tensor ds_bias_relu(at::Tensor& input, at::Tensor& bias)
+{
+    auto input_cont = input.contiguous();
+
+    int bsz = input_cont.size(0) * input_cont.size(1);
+    int intermediate_size = input_cont.size(2);
+
+    launch_bias_relu((T*)input_cont.data_ptr(),
+                     (T*)bias.data_ptr(),
+                     intermediate_size,
+                     bsz,
+                     InferenceContext::Instance().GetCurrentStream());
+    return input_cont;
+}
+
+template <typename T>
+at::Tensor ds_bias_add(at::Tensor& input, at::Tensor& bias)
+{
+    auto input_cont = input.contiguous();
+
+    int bsz = input_cont.size(0) * input_cont.size(1);
+    int hidden_size = input_cont.size(2);
+
+    launch_bias_add((T*)input_cont.data_ptr(),
+                    (T*)bias.data_ptr(),
+                    hidden_size,
+                    bsz,
+                    InferenceContext::Instance().GetCurrentStream());
+    return input_cont;
+}
+
+template <typename T>
+at::Tensor ds_bias_residual(at::Tensor& input, at::Tensor& residual, at::Tensor& bias)
+{
+    auto input_cont = input.contiguous();
+    auto residual_cont = residual.contiguous();
+
+    int bsz = input_cont.size(0) * input_cont.size(1);
+    // launch_bias_residual((T*)input_cont.data_ptr(),
+    //                      (T*)residual_cont.data_ptr(),
+    //                      (T*)bias.data_ptr(),
+    //                      bsz,
+    //                      input_cont.size(2),
+    //                      (bias.size(0) > 1),
+    //                      InferenceContext::Instance().GetCurrentStream());
+    return input_cont;
+}
+
+#define DISPATCH_LAYER_NORM(T_TYPE, C_TYPE)                               \
+    if (input.options().dtype() == torch::T_TYPE) {                       \
+        launch_fused_ln((C_TYPE*)output.data_ptr(),                       \
+                        (const C_TYPE*)input.data_ptr(),                  \
+                        (const C_TYPE*)gamma.data_ptr(),                  \
+                        (const C_TYPE*)beta.data_ptr(),                   \
+                        epsilon,                                          \
+                        rows,                                             \
+                        elems_per_row,                                    \
+                        InferenceContext::Instance().GetCurrentStream()); \
+    }
+
+at::Tensor ds_layer_norm(at::Tensor& input, at::Tensor& gamma, at::Tensor& beta, float epsilon)
 {
     const int rows = input.size(0) * input.size(1);
     const int elems_per_row = input.size(2);
     auto output = at::empty_like(input);
 
-    if (input.options().dtype() == torch::kFloat16) {
-        launch_fused_residual_ln((fp16*)output.data_ptr(),
-                                 (const fp16*)input.data_ptr(),
-                                 (const fp16*)residual.data_ptr(),
-                                 (const fp16*)bias.data_ptr(),
-                                 (const fp16*)gamma.data_ptr(),
-                                 (const fp16*)beta.data_ptr(),
-                                 epsilon,
-                                 rows,
-                                 elems_per_row,
-                                 InferenceContext::Instance().GetCurrentStream());
-    } else {
-        launch_fused_residual_ln((float*)output.data_ptr(),
-                                 (const float*)input.data_ptr(),
-                                 (const float*)residual.data_ptr(),
-                                 (const float*)bias.data_ptr(),
-                                 (const float*)gamma.data_ptr(),
-                                 (const float*)beta.data_ptr(),
-                                 epsilon,
-                                 rows,
-                                 elems_per_row,
-                                 InferenceContext::Instance().GetCurrentStream());
+    DISPATCH_LAYER_NORM(kFloat, float);
+    DISPATCH_LAYER_NORM(kHalf, sycl::half);
+#ifdef BF16_AVAILABLE
+    DISPATCH_LAYER_NORM(kBFloat16, sycl::ext::oneapi::bfloat16);
+#endif
+
+    return output;
+}
+
+#define DISPATCH_RMS_NORM(T_TYPE, C_TYPE)                                 \
+    if (input.options().dtype() == torch::T_TYPE) {                       \
+        launch_rms_norm((C_TYPE*)output.data_ptr(),                       \
+                        (C_TYPE*)nullptr,                                 \
+                        (const C_TYPE*)input.data_ptr(),                  \
+                        (const C_TYPE*)nullptr,                           \
+                        (const C_TYPE*)gamma.data_ptr(),                  \
+                        epsilon,                                          \
+                        rows,                                             \
+                        elems_per_row,                                    \
+                        InferenceContext::Instance().GetCurrentStream()); \
     }
 
+at::Tensor ds_rms_norm(at::Tensor& input, at::Tensor& gamma, float epsilon)
+{
+    // Get number of dims of tensor
+    int num_dims = input.dim();
+    const int rows = (num_dims == 2) ? input.size(0) : input.size(0) * input.size(1);
+    const int elems_per_row = (num_dims == 2) ? input.size(1) : input.size(2);
+
+    auto output = at::empty_like(input);
+
+    DISPATCH_RMS_NORM(kFloat, float);
+    DISPATCH_RMS_NORM(kHalf, sycl::half);
+#ifdef BF16_AVAILABLE
+    DISPATCH_RMS_NORM(kBFloat16, sycl::ext::oneapi::bfloat16);
+#endif
+
     return output;
 }
 
-template <typename T>
-at::Tensor& residual_add_bias(at::Tensor& hidden_state,
-                              at::Tensor& residual,
-                              const at::Tensor& attention_output,
-                              const at::Tensor& attention_bias,
-                              const at::Tensor& final_bias,
-                              const int mp_size,
-                              const bool mlp_after_attn,
-                              const bool add_bias,
-                              const bool preln)
+#define DISPATCH_PRE_RMS_NORM(T_TYPE, C_TYPE)                             \
+    if (input.options().dtype() == torch::T_TYPE) {                       \
+        launch_rms_norm((C_TYPE*)output.data_ptr(),                       \
+                        (C_TYPE*)res_out.data_ptr(),                      \
+                        (const C_TYPE*)input.data_ptr(),                  \
+                        (const C_TYPE*)residual.data_ptr(),               \
+                        (const C_TYPE*)gamma.data_ptr(),                  \
+                        epsilon,                                          \
+                        rows,                                             \
+                        elems_per_row,                                    \
+                        InferenceContext::Instance().GetCurrentStream()); \
+    }
+
+std::vector<at::Tensor> ds_pre_rms_norm(at::Tensor& input,
+                                        at::Tensor& residual,
+                                        at::Tensor& gamma,
+                                        float epsilon)
 {
-    int bsz = residual.size(0) * residual.size(1);
-    int hidden_size = residual.size(2);
-    if (mlp_after_attn)
-        launch_bias_residual(static_cast<T*>(residual.data_ptr()),
-                             static_cast<T*>(hidden_state.data_ptr()),
-                             static_cast<T*>(attention_output.data_ptr()),
-                             static_cast<T*>(final_bias.data_ptr()),
-                             static_cast<T*>(attention_bias.data_ptr()),
-                             bsz,
-                             hidden_size,
-                             mp_size,
-                             preln,
-                             InferenceContext::Instance().GetCurrentStream());
-    else
-        throw std::runtime_error("mlp_after_attn=true is not supported!");
-    return residual;
+    // Get number of dims of tensor
+    int num_dims = input.dim();
+    const int rows = (num_dims == 2) ? input.size(0) : input.size(0) * input.size(1);
+    const int elems_per_row = (num_dims == 2) ? input.size(1) : input.size(2);
+
+    auto output = at::empty_like(input);
+    auto res_out = at::empty_like(residual);
+
+    DISPATCH_PRE_RMS_NORM(kFloat, float);
+    DISPATCH_PRE_RMS_NORM(kHalf, sycl::half);
+#ifdef BF16_AVAILABLE
+    DISPATCH_PRE_RMS_NORM(kBFloat16, sycl::ext::oneapi::bfloat16);
+#endif
+
+    return {output, res_out};
 }
 
 template <typename T>
@@ -235,87 +791,261 @@ void ds_layer_norm_internal(T* workspace,
                     InferenceContext::Instance().GetCurrentStream());
 }
 
+#define DISPATCH_LAYER_NORM_RESIDUAL(T_TYPE, C_TYPE)                               \
+    if (input.options().dtype() == torch::T_TYPE) {                                \
+        launch_fused_residual_ln((C_TYPE*)output.data_ptr(),                       \
+                                 (const C_TYPE*)input.data_ptr(),                  \
+                                 (const C_TYPE*)residual.data_ptr(),               \
+                                 (const C_TYPE*)bias.data_ptr(),                   \
+                                 (const C_TYPE*)gamma.data_ptr(),                  \
+                                 (const C_TYPE*)beta.data_ptr(),                   \
+                                 epsilon,                                          \
+                                 rows,                                             \
+                                 elems_per_row,                                    \
+                                 InferenceContext::Instance().GetCurrentStream()); \
+    }
+
+/* Currently only used in unit testing */
+at::Tensor ds_layer_norm_residual(at::Tensor& input,
+                                  at::Tensor& bias,
+                                  at::Tensor& residual,
+                                  at::Tensor& gamma,
+                                  at::Tensor& beta,
+                                  float epsilon)
+{
+    const int rows = input.size(0) * input.size(1);
+    const int elems_per_row = input.size(2);
+    auto output = at::empty_like(input);
+
+    DISPATCH_LAYER_NORM_RESIDUAL(kFloat, float);
+    DISPATCH_LAYER_NORM_RESIDUAL(kHalf, sycl::half);
+#ifdef BF16_AVAILABLE
+    DISPATCH_LAYER_NORM_RESIDUAL(kBFloat16, sycl::ext::oneapi::bfloat16);
+#endif
+
+    return output;
+}
+
+#define DISPATCH_PRE_LAYER_NORM_RESIDUAL(T_TYPE, C_TYPE)      \
+    if (input.options().dtype() == torch::T_TYPE) {           \
+        launch_fused_residual_ln_store_pre_ln_res(            \
+            (C_TYPE*)norm_output.data_ptr(),                  \
+            (C_TYPE*)res_output.data_ptr(),                   \
+            (const C_TYPE*)input.data_ptr(),                  \
+            (const C_TYPE*)residual.data_ptr(),               \
+            (const C_TYPE*)bias.data_ptr(),                   \
+            (const C_TYPE*)gamma.data_ptr(),                  \
+            (const C_TYPE*)beta.data_ptr(),                   \
+            epsilon,                                          \
+            rows,                                             \
+            elems_per_row,                                    \
+            InferenceContext::Instance().GetCurrentStream()); \
+    }
+
+/* Currently only used in unit testing */
+std::vector<at::Tensor> ds_layer_norm_residual_store_pre_ln_res(at::Tensor& input,
+                                                                at::Tensor& bias,
+                                                                at::Tensor& residual,
+                                                                at::Tensor& gamma,
+                                                                at::Tensor& beta,
+                                                                float epsilon)
+{
+    const int rows = input.size(0) * input.size(1);
+    const int elems_per_row = input.size(2);
+    auto norm_output = at::empty_like(input);
+    auto res_output = at::empty_like(input);
+
+    DISPATCH_PRE_LAYER_NORM_RESIDUAL(kFloat, float);
+    DISPATCH_PRE_LAYER_NORM_RESIDUAL(kHalf, sycl::half);
+#ifdef BF16_AVAILABLE
+    DISPATCH_PRE_LAYER_NORM_RESIDUAL(kBFloat16, sycl::ext::oneapi::bfloat16);
+#endif
+
+    return {norm_output, res_output};
+}
+
+template <typename T>
+void quantized_gemm(void* output,
+                    T* input,
+                    at::Tensor& weight,
+                    at::Tensor& qscale,
+                    int groups,
+                    int bsz,
+                    int hidden_size)
+{
+    // T* weight16 = (T*)InferenceContext::Instance().GetWorkSpace() + 12 * hidden_size * bsz;
+
+    auto options = at::TensorOptions()
+                       .dtype(at::kHalf)
+                       .layout(at::kStrided)
+                       .device(at::kXPU)
+                       .requires_grad(false);
+    auto tmp = torch::empty(weight.sizes(), options);
+    T* weight16 = (T*)tmp.data_ptr();
+    launch_dequantize(weight16,
+                      (int8_t*)weight.data_ptr(),
+                      (float*)qscale.data_ptr(),
+                      weight.size(0),
+                      weight.size(1),
+                      groups,
+                      InferenceContext::Instance().GetCurrentStream());
+
+    float alpha = (T)1.0;
+    float gemm_beta = (T)0.0;
+    cublas_gemm_ex(InferenceContext::Instance().GetCublasHandle(),
+                   oneapi::mkl::transpose::trans,
+                   oneapi::mkl::transpose::nontrans,
+                   weight.size(0),
+                   bsz,
+                   weight.size(1),
+                   &alpha,
+                   &gemm_beta,
+                   weight16,
+                   (T*)input,
+                   (T*)output,
+#ifdef __HIP_PLATFORM_AMD__
+                   rocblas_gemm_algo_standard);
+#else
+                   99);
+#endif
+}
+
 template <typename T>
-at::Tensor ds_layer_norm_test(at::Tensor& input, at::Tensor& gamma, at::Tensor& beta, float epsilon)
+at::Tensor qkv_unfused_cublas(at::Tensor& output,
+                              at::Tensor& input,
+                              at::Tensor& weight,
+                              at::Tensor& q_scale,
+                              at::Tensor& bias,
+                              at::Tensor& gamma,
+                              at::Tensor& beta,
+                              const float epsilon,
+                              bool add_bias,
+                              bool q_int8,
+                              bool transposed_mode)
 {
     int bsz = input.size(0) * input.size(1);
+    T* workspace = (T*)InferenceContext::Instance().GetWorkSpace();
+    workspace += (3 * bsz * input.size(2));
+    ds_layer_norm_internal<T>(workspace, input, gamma, beta, epsilon);
 
+    if (q_int8) {
+        quantized_gemm<T>(
+            output.data_ptr(), workspace, weight, q_scale, q_scale.size(0), bsz, input.size(2));
+    } else {
+        float alpha = (T)1.0;
+        float gemm_beta = (T)0.0;
+
+        *(InferenceContext::Instance().GetCublasHandle()) =
+            *(InferenceContext::Instance().GetCurrentStream());
+        cublas_gemm_ex(
+            InferenceContext::Instance().GetCublasHandle(),
+            (transposed_mode ? oneapi::mkl::transpose::trans : oneapi::mkl::transpose::nontrans),
+            oneapi::mkl::transpose::nontrans,
+            weight.size(transposed_mode ? 0 : 1),
+            bsz,
+            input.size(2),
+            &alpha,
+            &gemm_beta,
+            (T*)weight.data_ptr(),
+            workspace,
+            (T*)output.data_ptr(),
+#ifdef __HIP_PLATFORM_AMD__
+            rocblas_gemm_algo_standard);
+#else
+            99);
+#endif
+    }
+    if (add_bias)
+        launch_bias_add((T*)output.data_ptr(),
+                        (T*)bias.data_ptr(),
+                        (transposed_mode || q_int8) ? weight.size(0) : weight.size(1),
+                        bsz,
+                        InferenceContext::Instance().GetCurrentStream());
+    return at::from_blob(workspace,
+                         input.sizes(),
+                         c10::TensorType::contiguousStridesOf(input.sizes()),
+                         nullptr,
+                         input.options(),
+                         input.options().device());
+}
+
+template <typename T>
+std::vector<at::Tensor> ds_rms_qkv(at::Tensor& input,
+                                   at::Tensor& weight,
+                                   at::Tensor& q_scale,
+                                   at::Tensor& gamma,
+                                   const float epsilon,
+                                   bool q_int8,
+                                   bool transposed_mode)
+{
+    const int bsz = input.size(0) * input.size(1);
     T* workspace = (T*)InferenceContext::Instance().GetWorkSpace();
+    T* rms_norm_ptr = workspace + (3 * bsz * input.size(2));
+    int out_size = (transposed_mode || q_int8) ? weight.size(0) : weight.size(1);
+
+    auto options = at::TensorOptions()
+                       .dtype(input.options().dtype())
+                       .layout(at::kStrided)
+                       .device(at::kXPU)
+                       .requires_grad(false);
+    auto rms_norm = at::from_blob(rms_norm_ptr,
+                                  input.sizes(),
+                                  c10::TensorType::contiguousStridesOf(input.sizes()),
+                                  nullptr,
+                                  options,
+                                  input.device());
+    auto output = at::from_blob(
+        workspace,
+        {input.size(0), input.size(1), out_size},
+        c10::TensorType::contiguousStridesOf({input.size(0), input.size(1), out_size}),
+        nullptr,
+        options,
+        input.device());
 
-    launch_fused_ln(workspace,
+    launch_rms_norm((T*)rms_norm.data_ptr(),
+                    (T*)nullptr,
                     (const T*)input.data_ptr(),
+                    (const T*)nullptr,
                     (const T*)gamma.data_ptr(),
-                    (const T*)beta.data_ptr(),
                     epsilon,
                     bsz,
                     input.size(2),
                     InferenceContext::Instance().GetCurrentStream());
 
-    auto output_stride = c10::TensorType::contiguousStridesOf(input.sizes());
-
-    return at::from_blob(
-        workspace, input.sizes(), output_stride, nullptr, input.options(), input.device());
-}
-
-template <typename T>
-at::Tensor qkv_unfused_sycl(at::Tensor& output,
-                            at::Tensor& input,
-                            at::Tensor& weight,
-                            at::Tensor& q_scale,
-                            at::Tensor& bias,
-                            at::Tensor& gamma,
-                            at::Tensor& beta,
-                            const float epsilon,
-                            bool add_bias,
-                            bool q_int8,
-                            bool transposed_mode)
-{
-    int bsz = input.size(0) * input.size(1);
-    T* workspace = (T*)InferenceContext::Instance().GetWorkSpace();
-    workspace += (3 * bsz * input.size(2));
-    ds_layer_norm_internal<T>(workspace, input, gamma, beta, epsilon);
-
     if (q_int8) {
-        throw std::runtime_error("q_int8=true is not supported!");
+        quantized_gemm<T>((T*)output.data_ptr(),
+                          (T*)rms_norm.data_ptr(),
+                          weight,
+                          q_scale,
+                          q_scale.size(0),
+                          bsz,
+                          input.size(2));
     } else {
         float alpha = (T)1.0;
         float gemm_beta = (T)0.0;
-#ifdef USE_MKL_GEMM
-        onemkl_matmul_ex<T>(InferenceContext::Instance().GetCurrentStream(),
-                            transposed_mode ? oneapi::mkl::transpose::trans : oneapi::mkl::transpose::nontrans,
-                            oneapi::mkl::transpose::nontrans,
-                            bsz,
-                            weight.size(1),
-                            input.size(2),
-                            alpha,
-                            gemm_beta,
-                            workspace,
-                            (T*)weight.data_ptr(),
-                            (T*)output.data_ptr());
+
+        *(InferenceContext::Instance().GetCublasHandle()) =
+            *(InferenceContext::Instance().GetCurrentStream());
+        cublas_gemm_ex(
+            InferenceContext::Instance().GetCublasHandle(),
+            (transposed_mode ? oneapi::mkl::transpose::trans : oneapi::mkl::transpose::nontrans),
+            oneapi::mkl::transpose::nontrans,
+            weight.size(transposed_mode ? 0 : 1),
+            bsz,
+            input.size(2),
+            &alpha,
+            &gemm_beta,
+            (T*)weight.data_ptr(),
+            (T*)rms_norm.data_ptr(),
+            (T*)output.data_ptr(),
+#ifdef __HIP_PLATFORM_AMD__
+            rocblas_gemm_algo_standard);
 #else
-        onednn_matmul_ex<T>(InferenceContext::Instance().GetCurrentStream(),
-                            transposed_mode,
-                            false,
-                            bsz,
-                            weight.size(1),
-                            input.size(2),
-                            alpha,
-                            gemm_beta,
-                            workspace,
-                            (T*)weight.data_ptr(),
-                            (T*)output.data_ptr());
+            99);
 #endif
     }
-    if (add_bias)
-        launch_bias_add((T*)output.data_ptr(),
-                        (T*)bias.data_ptr(),
-                        q_int8 ? weight.size(0) : weight.size(1),
-                        bsz,
-                        InferenceContext::Instance().GetCurrentStream());
 
-    auto output_stride = c10::TensorType::contiguousStridesOf(input.sizes());
-    return at::from_blob(
-        workspace, input.sizes(), output_stride, nullptr, input.options(), input.device());
+    return {output, rms_norm};
 }
 
 template <typename T>
@@ -337,50 +1067,429 @@ std::vector<at::Tensor> ds_qkv_gemm(at::Tensor& input,
     auto options = at::TensorOptions()
                        .dtype(input.options().dtype())
                        .layout(at::kStrided)
-                       .device(torch::kXPU)
+                       .device(at::kXPU)
                        .requires_grad(false);
 
-    auto output_stride =
-        c10::TensorType::contiguousStridesOf({input.size(0), input.size(1), out_size});
-    auto output = at::from_blob(workspace,
-                                {input.size(0), input.size(1), out_size},
-                                output_stride,
-                                nullptr,
-                                options,
-                                input.device());
-    auto inp_norm = qkv_unfused_sycl<T>(output,
-                                        input,
-                                        weight,
-                                        q_scale,
-                                        bias,
-                                        gamma,
-                                        beta,
-                                        epsilon,
-                                        add_bias,
-                                        q_int8,
-                                        transposed_mode);
+    auto output = at::from_blob(
+        workspace,
+        {input.size(0), input.size(1), out_size},
+        c10::TensorType::contiguousStridesOf({input.size(0), input.size(1), out_size}),
+        nullptr,
+        options,
+        input.device());
+    auto inp_norm = qkv_unfused_cublas<T>(output,
+                                          input,
+                                          weight,
+                                          q_scale,
+                                          bias,
+                                          gamma,
+                                          beta,
+                                          epsilon,
+                                          add_bias,
+                                          q_int8,
+                                          transposed_mode);
 
     return {output, inp_norm};
 }
 
 template <typename T>
-at::Tensor mlp_unfused_sycl(at::Tensor& output,
-                            at::Tensor& input,
-                            at::Tensor& residual,
-                            at::Tensor& input_bias,
+void quantized_gemm(at::Tensor& output,
+                    at::Tensor& input,
+                    at::Tensor& weight,
+                    at::Tensor& qscale,
+                    int groups,
+                    int merge_count)
+{
+    int bsz = input.size(0) * input.size(1);
+    auto options = at::TensorOptions()
+                       .dtype(input.options().dtype())
+                       .layout(at::kStrided)
+                       .device(at::kXPU)
+                       .requires_grad(false);
+    auto weight16 = at::empty({weight.size(0), weight.size(1)}, options);
+
+    launch_dequantize((T*)weight16.data_ptr(),
+                      (int8_t*)weight.data_ptr(),
+                      (float*)qscale.data_ptr(),
+                      weight.size(0),
+                      weight.size(1),
+                      groups,
+                      merge_count,
+                      InferenceContext::Instance().GetCurrentStream());
+
+    float alpha = (T)1.0;
+    float gemm_beta = (T)0.0;
+    cublas_gemm_ex(InferenceContext::Instance().GetCublasHandle(),
+                   oneapi::mkl::transpose::trans,
+                   oneapi::mkl::transpose::nontrans,
+                   weight.size(0),
+                   bsz,
+                   input.size(2),
+                   &alpha,
+                   &gemm_beta,
+                   (T*)weight16.data_ptr(),
+                   (T*)input.data_ptr(),
+                   (T*)output.data_ptr(),
+#ifdef __HIP_PLATFORM_AMD__
+                   rocblas_gemm_algo_standard);
+#else
+                   99);
+#endif
+}
+
+template <typename T>
+at::Tensor ds_linear_layer(at::Tensor& input,
+                           at::Tensor& weight,
+                           at::Tensor& bias,
+                           bool add_bias,
+                           bool do_flash_attn,
+                           int num_heads,
+                           bool transposed_mode,
+                           float rope_theta)
+{
+    auto input_cont = input.contiguous();
+    auto options = at::TensorOptions()
+                       .dtype(input_cont.options().dtype())
+                       .layout(at::kStrided)
+                       .device(at::kXPU)
+                       .requires_grad(false);
+
+    int head_size = input_cont.size(2) / num_heads;
+    int bsz = input.size(0) * input.size(1);
+    int out_size = transposed_mode ? weight.size(0) : weight.size(1);
+    T* workspace = (T*)InferenceContext::Instance().GetWorkSpace();
+    auto output = at::from_blob(
+        workspace,
+        {input.size(0), input.size(1), out_size},
+        c10::TensorType::contiguousStridesOf({input.size(0), input.size(1), out_size}),
+        nullptr,
+        options,
+        input.device());
+
+    float alpha = (T)1.0;
+    float gemm_beta = (T)0.0;
+    *(InferenceContext::Instance().GetCublasHandle()) =
+        *(InferenceContext::Instance().GetCurrentStream());
+
+    cublas_gemm_ex(
+        InferenceContext::Instance().GetCublasHandle(),
+        (transposed_mode ? oneapi::mkl::transpose::trans : oneapi::mkl::transpose::nontrans),
+        oneapi::mkl::transpose::nontrans,
+        weight.size(transposed_mode ? 0 : 1),
+        bsz,
+        input_cont.size(2),
+        &alpha,
+        &gemm_beta,
+        (T*)weight.data_ptr(),
+        (T*)input_cont.data_ptr(),
+        (T*)output.data_ptr(),
+#ifdef __HIP_PLATFORM_AMD__
+        rocblas_gemm_algo_standard);
+#else
+        99);
+#endif
+    if (add_bias)
+        launch_bias_add((T*)output.data_ptr(),
+                        (T*)bias.data_ptr(),
+                        weight.size(transposed_mode ? 0 : 1),
+                        bsz,
+                        InferenceContext::Instance().GetCurrentStream());
+    bool add_padding = (head_size % 32 != 0 && head_size < 64) || (head_size % 64 != 0);
+    if (do_flash_attn) {
+        if (add_padding) {
+            int padded_head_size = head_size < 32 ? 32 : (head_size < 64 ? 64 : 128);
+            auto padded_output = workspace + output.numel();
+            auto final_output =
+                padded_output + (input.size(0) * input.size(1) * 3 * num_heads * padded_head_size);
+            pad_data(padded_output,
+                     workspace,
+                     3 * bsz * num_heads,
+                     head_size,
+                     padded_head_size,
+                     InferenceContext::Instance().GetCurrentStream());
+
+            launch_bias_add_transform_0213<T>(
+                final_output,
+                final_output + (input.size(0) * input.size(1) * num_heads * padded_head_size),
+                final_output + (input.size(0) * input.size(1) * 2 * num_heads * padded_head_size),
+                padded_output,
+                nullptr,
+                input.size(0),
+                input.size(1),
+                0,
+                input.size(1),
+                (num_heads * padded_head_size),
+                num_heads,
+                -1,
+                -1,
+                false,
+                false,
+                InferenceContext::Instance().GetCurrentStream(),
+                3,
+                input.size(1),
+                rope_theta);
+            return at::from_blob(
+                final_output,
+                {3, input.size(0), num_heads, input.size(1), padded_head_size},
+                c10::TensorType::contiguousStridesOf(
+                    {3, input.size(0), num_heads, input.size(1), padded_head_size}),
+                nullptr,
+                options,
+                input.device());
+            // return at::from_blob(padded_output, {input.size(0) * input.size(1), 3, num_heads,
+            // padded_head_size}, options);
+        } else {
+            auto final_output = workspace + output.numel();
+            launch_bias_add_transform_0213<T>(
+                final_output,
+                final_output + (input.size(0) * input.size(1) * input_cont.size(2)),
+                final_output + (input.size(0) * input.size(1) * 2 * input_cont.size(2)),
+                workspace,
+                nullptr,
+                input.size(0),
+                input.size(1),
+                0,
+                input.size(1),
+                input_cont.size(2),
+                num_heads,
+                -1,
+                -1,
+                false,
+                false,
+                InferenceContext::Instance().GetCurrentStream(),
+                3,
+                input.size(1),
+                rope_theta);
+            return at::from_blob(final_output,
+                                 {3, input.size(0), num_heads, input.size(1), head_size},
+                                 c10::TensorType::contiguousStridesOf(
+                                     {3, input.size(0), num_heads, input.size(1), head_size}),
+                                 nullptr,
+                                 options,
+                                 input.device());
+            // return at::from_blob(workspace, {input.size(0) * input.size(1), 3, num_heads,
+            // head_size}, options);
+        }
+
+    } else
+        return output;
+}
+
+template <typename T>
+std::vector<at::Tensor> add_padding(at::Tensor& query, at::Tensor& key, at::Tensor& value)
+{
+    int head_size = query.size(3);
+    int padded_head_size = head_size < 32 ? 32 : (head_size < 64 ? 64 : 128);
+    T* workspace = (T*)InferenceContext::Instance().GetWorkSpace();
+    T* key_pad_ptr = workspace + padded_head_size * query.size(0) * query.size(1) * query.size(2);
+    T* value_pad_ptr = key_pad_ptr + padded_head_size * query.size(0) * query.size(1) * 128;
+    pad_head_seq(workspace,
+                 (T*)query.data_ptr(),
+                 query.size(0) * query.size(1),
+                 query.size(2),
+                 query.size(2),
+                 head_size,
+                 padded_head_size,
+                 InferenceContext::Instance().GetCurrentStream());
+    pad_head_seq(key_pad_ptr,
+                 (T*)key.data_ptr(),
+                 query.size(0) * query.size(1),
+                 key.size(2),
+                 128,
+                 head_size,
+                 padded_head_size,
+                 InferenceContext::Instance().GetCurrentStream());
+    pad_head_seq(value_pad_ptr,
+                 (T*)value.data_ptr(),
+                 query.size(0) * query.size(1),
+                 key.size(2),
+                 128,
+                 head_size,
+                 padded_head_size,
+                 InferenceContext::Instance().GetCurrentStream());
+    return {at::from_blob(workspace,
+                          {query.size(0), query.size(1), query.size(2), padded_head_size},
+                          c10::TensorType::contiguousStridesOf(
+                              {query.size(0), query.size(1), query.size(2), padded_head_size}),
+                          nullptr,
+                          query.options(),
+                          query.options().device()),
+            at::from_blob(key_pad_ptr,
+                          {query.size(0), query.size(1), 128, padded_head_size},
+                          c10::TensorType::contiguousStridesOf(
+                              {query.size(0), query.size(1), 128, padded_head_size}),
+                          nullptr,
+                          query.options(),
+                          query.options().device()),
+            at::from_blob(value_pad_ptr,
+                          {query.size(0), query.size(1), 128, padded_head_size},
+                          c10::TensorType::contiguousStridesOf(
+                              {query.size(0), query.size(1), 128, padded_head_size}),
+                          nullptr,
+                          query.options(),
+                          query.options().device())};
+}
+
+template <typename T>
+std::vector<at::Tensor> padd_add_transform(at::Tensor& query,
+                                           at::Tensor& key,
+                                           at::Tensor& value,
+                                           int heads,
+                                           bool add_padding)
+{
+    int head_size = query.size(2) / heads;
+    int key_value_length = add_padding ? 128 : key.size(1);
+    int padded_head_size = add_padding ? (head_size < 32 ? 32 : (head_size < 64 ? 64 : 128))
+                                       : head_size;
+    T* workspace = (T*)InferenceContext::Instance().GetWorkSpace();
+    T* key_pad_ptr = workspace + padded_head_size * query.size(0) * heads * query.size(1);
+    T* value_pad_ptr = key_pad_ptr + padded_head_size * query.size(0) * heads * key_value_length;
+    launch_pad_add_transform_0213(workspace,
+                                  (T*)query.data_ptr(),
+                                  query.size(0),
+                                  query.size(2),
+                                  query.size(1),
+                                  query.size(1),
+                                  heads,
+                                  padded_head_size,
+                                  InferenceContext::Instance().GetCurrentStream());
+    launch_pad_add_transform_0213(key_pad_ptr,
+                                  (T*)key.data_ptr(),
+                                  key.size(0),
+                                  key.size(2),
+                                  key.size(1),
+                                  key_value_length,
+                                  heads,
+                                  padded_head_size,
+                                  InferenceContext::Instance().GetCurrentStream());
+    launch_pad_add_transform_0213(value_pad_ptr,
+                                  (T*)value.data_ptr(),
+                                  value.size(0),
+                                  value.size(2),
+                                  value.size(1),
+                                  key_value_length,
+                                  heads,
+                                  padded_head_size,
+                                  InferenceContext::Instance().GetCurrentStream());
+    return {at::from_blob(workspace,
+                          {query.size(0), heads, query.size(1), padded_head_size},
+                          c10::TensorType::contiguousStridesOf(
+                              {query.size(0), heads, query.size(1), padded_head_size}),
+                          nullptr,
+                          query.options(),
+                          query.options().device()),
+            at::from_blob(key_pad_ptr,
+                          {query.size(0), heads, key_value_length, padded_head_size},
+                          c10::TensorType::contiguousStridesOf(
+                              {query.size(0), heads, key_value_length, padded_head_size}),
+                          nullptr,
+                          query.options(),
+                          query.options().device()),
+            at::from_blob(value_pad_ptr,
+                          {query.size(0), heads, key_value_length, padded_head_size},
+                          c10::TensorType::contiguousStridesOf(
+                              {query.size(0), heads, key_value_length, padded_head_size}),
+                          nullptr,
+                          query.options(),
+                          query.options().device())};
+}
+
+template <typename T>
+at::Tensor ds_vector_matmul(at::Tensor& input,
                             at::Tensor& weight,
-                            at::Tensor& weight1,
-                            at::Tensor& bias,
-                            at::Tensor& gamma,
-                            at::Tensor& beta,
-                            const float epsilon,
-                            bool preLayerNorm,
-                            bool mlp_after_attn,
+                            bool async_op,
                             at::Tensor& q_scale,
-                            at::Tensor& q_scale1,
                             bool q_int8,
-                            ActivationFuncType act_func_type,
                             bool transposed_mode)
+{
+    auto options = at::TensorOptions()
+                       .dtype(input.options().dtype())
+                       .layout(at::kStrided)
+                       .device(at::kXPU)
+                       .requires_grad(false);
+    int out_size = (q_int8 || transposed_mode) ? weight.size(0) : weight.size(1);
+    int bsz = input.size(0) * input.size(1);
+
+    T* workspace = (T*)InferenceContext::Instance().GetWorkSpace();
+    auto output = at::from_blob(
+        workspace,
+        {input.size(0), input.size(1), out_size},
+        c10::TensorType::contiguousStridesOf({input.size(0), input.size(1), out_size}),
+        nullptr,
+        options,
+        input.device());
+    if (q_int8) {
+        quantized_gemm<T>(output.data_ptr(),
+                          (T*)input.data_ptr(),
+                          weight,
+                          q_scale,
+                          q_scale.size(0),
+                          bsz,
+                          input.size(2));
+    } else {
+        float alpha = (T)1.0;
+        float gemm_beta = (T)0.0;
+        *(InferenceContext::Instance().GetCublasHandle()) =
+            *(InferenceContext::Instance().GetCurrentStream(async_op));
+        cublas_gemm_ex(
+            InferenceContext::Instance().GetCublasHandle(),
+            (transposed_mode ? oneapi::mkl::transpose::trans : oneapi::mkl::transpose::nontrans),
+            oneapi::mkl::transpose::nontrans,
+            weight.size(transposed_mode ? 0 : 1),
+            bsz,
+            input.size(2),
+            &alpha,
+            &gemm_beta,
+            (T*)weight.data_ptr(),
+            (T*)input.data_ptr(),
+            (T*)output.data_ptr(),
+#ifdef __HIP_PLATFORM_AMD__
+            rocblas_gemm_algo_standard);
+#else
+            99);
+#endif
+    }
+    return output;
+}
+
+template <typename T>
+at::Tensor ds_vector_matmul_int8(at::Tensor& input,
+                                 at::Tensor& weight,
+                                 at::Tensor& q_scale,
+                                 int groups,
+                                 int merge_count)
+{
+    auto input_cont = input.contiguous();
+    auto options = at::TensorOptions()
+                       .dtype(input_cont.options().dtype())
+                       .layout(at::kStrided)
+                       .device(at::kXPU)
+                       .requires_grad(false);
+
+    auto output = at::empty({input_cont.size(0), input_cont.size(1), weight.size(1)}, options);
+
+    quantized_gemm<T>(output, input_cont, weight, q_scale, groups, merge_count);
+    return output;
+}
+
+template <typename T>
+at::Tensor mlp_unfused_cublas(at::Tensor& output,
+                              at::Tensor& input,
+                              at::Tensor& residual,
+                              at::Tensor& input_bias,
+                              at::Tensor& weight,
+                              at::Tensor& weight1,
+                              at::Tensor& bias,
+                              at::Tensor& gamma,
+                              at::Tensor& beta,
+                              const float epsilon,
+                              bool preLayerNorm,
+                              bool mlp_after_attn,
+                              at::Tensor& q_scale,
+                              at::Tensor& q_scale1,
+                              bool q_int8,
+                              ActivationFuncType act_func_type,
+                              bool transposed_mode)
 {
     int bsz = input.size(0) * input.size(1);
     T* inp_norm = (T*)InferenceContext::Instance().GetWorkSpace() + torch::numel(input) +
@@ -402,34 +1511,29 @@ at::Tensor mlp_unfused_sycl(at::Tensor& output,
         ds_layer_norm_internal(inp_norm, input, gamma, beta, epsilon);
     }
     if (q_int8) {
-        throw std::runtime_error("q_int8=true is not supported!");
+        quantized_gemm<T>(
+            intermediate, inp_norm, weight, q_scale, q_scale.size(0), bsz, input.size(2));
     } else {
         float alpha = (T)1.0;
         float gemm_beta = (T)0.0;
-#ifdef USE_MKL_GEMM
-        onemkl_matmul_ex<T>(InferenceContext::Instance().GetCurrentStream(),
-                            transposed_mode ? oneapi::mkl::transpose::trans : oneapi::mkl::transpose::nontrans,
-                            oneapi::mkl::transpose::nontrans,
-                            bsz,
-                            weight.size(1),
-                            input.size(2),
-                            alpha,
-                            gemm_beta,
-                            inp_norm,
-                            (T*)weight.data_ptr(),
-                            intermediate);
+        *(InferenceContext::Instance().GetCublasHandle()) =
+            *(InferenceContext::Instance().GetCurrentStream());
+        cublas_gemm_ex(
+            InferenceContext::Instance().GetCublasHandle(),
+            (transposed_mode ? oneapi::mkl::transpose::trans : oneapi::mkl::transpose::nontrans),
+            oneapi::mkl::transpose::nontrans,
+            weight.size(transposed_mode ? 0 : 1),
+            bsz,
+            input.size(2),
+            &alpha,
+            &gemm_beta,
+            (T*)weight.data_ptr(),
+            inp_norm,
+            intermediate,
+#ifdef __HIP_PLATFORM_AMD__
+            rocblas_gemm_algo_standard);
 #else
-        onednn_matmul_ex<T>(InferenceContext::Instance().GetCurrentStream(),
-                            transposed_mode,
-                            false,
-                            bsz,
-                            weight.size(1),
-                            input.size(2),
-                            alpha,
-                            gemm_beta,
-                            inp_norm,
-                            (T*)weight.data_ptr(),
-                            intermediate);
+            99);
 #endif
     }
     if (act_func_type == ActivationFuncType::GELU) {
@@ -439,44 +1543,51 @@ at::Tensor mlp_unfused_sycl(at::Tensor& output,
                          bsz,
                          InferenceContext::Instance().GetCurrentStream());
     } else if (act_func_type == ActivationFuncType::ReLU) {
-        throw std::runtime_error("act_func_type=relu is not supported!");
+        launch_bias_relu(intermediate,
+                         (T*)bias.data_ptr(),
+                         (transposed_mode || q_int8) ? weight.size(0) : weight.size(1),
+                         bsz,
+                         InferenceContext::Instance().GetCurrentStream());
     }
 
     if (q_int8) {
-        throw std::runtime_error("q_int8=true is not supported!");
+        quantized_gemm<T>(output.data_ptr(),
+                          intermediate,
+                          weight1,
+                          q_scale1,
+                          q_scale1.size(0),
+                          bsz,
+                          input.size(2));
     } else {
         float alpha = (T)1.0;
         float gemm_beta = (T)0.0;
-#ifdef USE_MKL_GEMM
-        onemkl_matmul_ex<T>(InferenceContext::Instance().GetCurrentStream(),
-                            transposed_mode ? oneapi::mkl::transpose::trans : oneapi::mkl::transpose::nontrans,
-                            oneapi::mkl::transpose::nontrans,
-                            bsz,
-                            weight1.size(transposed_mode ? 0 : 1),
-                            weight1.size(transposed_mode ? 1 : 0),
-                            alpha,
-                            gemm_beta,
-                            intermediate,
-                            (T*)weight1.data_ptr(),
-                            (T*)output.data_ptr());
+        *(InferenceContext::Instance().GetCublasHandle()) =
+            *(InferenceContext::Instance().GetCurrentStream());
+        cublas_gemm_ex(
+            InferenceContext::Instance().GetCublasHandle(),
+            (transposed_mode ? oneapi::mkl::transpose::trans : oneapi::mkl::transpose::nontrans),
+            oneapi::mkl::transpose::nontrans,
+            weight1.size(transposed_mode ? 0 : 1),
+            bsz,
+            weight1.size(transposed_mode ? 1 : 0),
+            &alpha,
+            &gemm_beta,
+            (T*)weight1.data_ptr(),
+            intermediate,
+            (T*)output.data_ptr(),
+#ifdef __HIP_PLATFORM_AMD__
+            rocblas_gemm_algo_standard);
 #else
-        onednn_matmul_ex<T>(InferenceContext::Instance().GetCurrentStream(),
-                            transposed_mode,
-                            false,
-                            bsz,
-                            weight1.size(transposed_mode ? 0 : 1),
-                            weight1.size(transposed_mode ? 1 : 0),
-                            alpha,
-                            gemm_beta,
-                            intermediate,
-                            (T*)weight1.data_ptr(),
-                            (T*)output.data_ptr());
+            99);
 #endif
     }
 
-    auto output_stride = c10::TensorType::contiguousStridesOf(input.sizes());
-    return at::from_blob(
-        inp_norm, input.sizes(), output_stride, nullptr, input.options(), input.device());
+    return at::from_blob(inp_norm,
+                         input.sizes(),
+                         c10::TensorType::contiguousStridesOf(input.sizes()),
+                         nullptr,
+                         input.options(),
+                         input.options().device());
 }
 
 template <typename T>
@@ -504,39 +1615,199 @@ std::vector<at::Tensor> ds_mlp_gemm(at::Tensor& input,
                        .requires_grad(false);
 
     int out_size = (q_int8 || transposed_mode) ? weight_out.size(0) : weight_out.size(1);
-    auto output_stride =
-        c10::TensorType::contiguousStridesOf({input.size(0), input.size(1), out_size});
-    auto output =
-        at::from_blob((T*)InferenceContext::Instance().GetWorkSpace() + torch::numel(input),
-                      {input.size(0), input.size(1), out_size},
-                      output_stride,
-                      nullptr,
-                      options,
-                      input.device());
+    auto output = at::from_blob(
+        (T*)InferenceContext::Instance().GetWorkSpace() + torch::numel(input),
+        {input.size(0), input.size(1), out_size},
+        c10::TensorType::contiguousStridesOf({input.size(0), input.size(1), out_size}),
+        nullptr,
+        options,
+        input.device());
     int bsz = input.size(0) * input.size(1);
 
     auto act_func_type = static_cast<ActivationFuncType>(activation_type);
-    auto res_add = mlp_unfused_sycl<T>(output,
-                                       mlp_after_attn ? input : residual,
-                                       residual,
-                                       input_bias,
-                                       weight_interm,
-                                       weight_out,
-                                       bias,
-                                       gamma,
-                                       beta,
-                                       epsilon,
-                                       preLayerNorm,
-                                       mlp_after_attn,
-                                       q_scale,
-                                       q_scale1,
-                                       q_int8,
-                                       act_func_type,
-                                       transposed_mode);
+    auto res_add = mlp_unfused_cublas<T>(output,
+                                         mlp_after_attn ? input : residual,
+                                         residual,
+                                         input_bias,
+                                         weight_interm,
+                                         weight_out,
+                                         bias,
+                                         gamma,
+                                         beta,
+                                         epsilon,
+                                         preLayerNorm,
+                                         mlp_after_attn,
+                                         q_scale,
+                                         q_scale1,
+                                         q_int8,
+                                         act_func_type,
+                                         transposed_mode);
 
     return {output, res_add};
 }
 
+template <typename T>
+std::vector<at::Tensor> ds_rms_mlp_gemm(at::Tensor& input,
+                                        at::Tensor& residual,
+                                        at::Tensor& weight_interm,
+                                        at::Tensor& weight_out,
+                                        at::Tensor& gamma,
+                                        const float epsilon,
+                                        at::Tensor& q_scale,
+                                        at::Tensor& q_scale1,
+                                        bool q_int8,
+                                        int activation_type,
+                                        bool transposed_mode)
+{
+    const int bsz = input.size(0) * input.size(1);
+    const size_t input_neurons = input.size(2);
+    const int mlp_1_out_neurons = transposed_mode ? weight_interm.size(0)
+                                                     : weight_interm.size(1);
+    const size_t mlp_2_in_neurons = transposed_mode ? weight_out.size(1) : weight_out.size(0);
+
+    auto options = at::TensorOptions()
+                       .dtype(input.options().dtype())
+                       .layout(at::kStrided)
+                       .device(at::kXPU)
+                       .requires_grad(false);
+
+    T* output_ptr = (T*)InferenceContext::Instance().GetWorkSpace() + torch::numel(input);
+    T* inp_norm_ptr = output_ptr + torch::numel(input);
+    T* intermediate_ptr = inp_norm_ptr + torch::numel(input);
+
+    auto output = at::from_blob(output_ptr,
+                                input.sizes(),
+                                c10::TensorType::contiguousStridesOf(input.sizes()),
+                                nullptr,
+                                options,
+                                input.device());
+    auto inp_norm = at::from_blob(inp_norm_ptr,
+                                  input.sizes(),
+                                  c10::TensorType::contiguousStridesOf(input.sizes()),
+                                  nullptr,
+                                  options,
+                                  input.device());
+    auto intermediate_gemm = at::from_blob(
+        intermediate_ptr,
+        {input.size(0), input.size(1), mlp_1_out_neurons},
+        c10::TensorType::contiguousStridesOf({input.size(0), input.size(1), mlp_1_out_neurons}),
+        nullptr,
+        options,
+        input.device());
+
+    auto act_func_type = static_cast<ActivationFuncType>(activation_type);
+
+    // RMS Norm, we'll update the residual in-place
+    launch_rms_norm((T*)inp_norm.data_ptr(),
+                    (T*)residual.data_ptr(),
+                    (const T*)input.data_ptr(),
+                    (const T*)residual.data_ptr(),
+                    (const T*)gamma.data_ptr(),
+                    epsilon,
+                    bsz,
+                    input_neurons,
+                    InferenceContext::Instance().GetCurrentStream());
+
+    if (q_int8) {
+        quantized_gemm<T>(intermediate_ptr,
+                          (T*)inp_norm.data_ptr(),
+                          weight_interm,
+                          q_scale,
+                          q_scale.size(0),
+                          bsz,
+                          input_neurons);
+    } else {
+        float alpha = (T)1.0;
+        float gemm_beta = (T)0.0;
+        *(InferenceContext::Instance().GetCublasHandle()) =
+            *(InferenceContext::Instance().GetCurrentStream());
+        cublas_gemm_ex(
+            InferenceContext::Instance().GetCublasHandle(),
+            (transposed_mode ? oneapi::mkl::transpose::trans : oneapi::mkl::transpose::nontrans),
+            oneapi::mkl::transpose::nontrans,
+            mlp_1_out_neurons,
+            bsz,
+            input_neurons,
+            &alpha,
+            &gemm_beta,
+            (T*)weight_interm.data_ptr(),
+            (T*)inp_norm.data_ptr(),
+            intermediate_ptr,
+#ifdef __HIP_PLATFORM_AMD__
+            rocblas_gemm_algo_standard);
+#else
+            99);
+#endif
+    }
+
+    if (act_func_type == ActivationFuncType::GELU) {
+        launch_bias_gelu(intermediate_ptr,
+                         (T*)nullptr,
+                         mlp_1_out_neurons,
+                         bsz,
+                         InferenceContext::Instance().GetCurrentStream());
+    } else if (act_func_type == ActivationFuncType::ReLU) {
+        launch_bias_relu(intermediate_ptr,
+                         (T*)nullptr,
+                         mlp_1_out_neurons,
+                         bsz,
+                         InferenceContext::Instance().GetCurrentStream());
+    } else if (act_func_type == ActivationFuncType::GATED_GELU) {
+        launch_gated_activation(intermediate_ptr,
+                                (const T*)intermediate_ptr,
+                                (const T*)nullptr,
+                                bsz,
+                                mlp_1_out_neurons,
+                                mlp_1_out_neurons,
+                                true,
+                                InferenceContext::Instance().GetCurrentStream());
+    } else if (act_func_type == ActivationFuncType::GATED_SILU) {
+        launch_gated_activation(intermediate_ptr,
+                                (const T*)intermediate_ptr,
+                                (const T*)nullptr,
+                                bsz,
+                                mlp_1_out_neurons,
+                                mlp_1_out_neurons,
+                                false,
+                                InferenceContext::Instance().GetCurrentStream());
+    }
+
+    if (q_int8) {
+        quantized_gemm<T>(output.data_ptr(),
+                          intermediate_ptr,
+                          weight_out,
+                          q_scale1,
+                          q_scale1.size(0),
+                          bsz,
+                          input.size(2));
+    } else {
+        float alpha = (T)1.0;
+        float gemm_beta = (T)0.0;
+        *(InferenceContext::Instance().GetCublasHandle()) =
+            *(InferenceContext::Instance().GetCurrentStream());
+        cublas_gemm_ex(
+            InferenceContext::Instance().GetCublasHandle(),
+            (transposed_mode ? oneapi::mkl::transpose::trans : oneapi::mkl::transpose::nontrans),
+            oneapi::mkl::transpose::nontrans,
+            input_neurons,
+            bsz,
+            mlp_2_in_neurons,
+            &alpha,
+            &gemm_beta,
+            (T*)weight_out.data_ptr(),
+            intermediate_ptr,
+            (T*)output.data_ptr(),
+#ifdef __HIP_PLATFORM_AMD__
+            rocblas_gemm_algo_standard,
+#else
+            99,
+#endif
+            mlp_1_out_neurons);
+    }
+
+    return {output, residual};
+}
+
 template <typename T>
 at::Tensor fused_gemm_gelu(at::Tensor& input,
                            at::Tensor& weight,
@@ -550,11 +1821,16 @@ at::Tensor fused_gemm_gelu(at::Tensor& input,
     auto options = at::TensorOptions()
                        .dtype(input.options().dtype())
                        .layout(at::kStrided)
-                       .device(at::kCUDA)
+                       .device(at::kXPU)
                        .requires_grad(false);
 
     int intm_dim = (transposed_mode || q_int8) ? weight.size(0) : weight.size(1);
 
+    // auto output = at::from_blob((T*)InferenceContext::Instance().GetWorkSpace() +
+    // torch::numel(input),
+    //                            {input.size(0), input.size(1), out_size},
+    //                            options);
+    // T* intermediate = (T*)input.data_ptr() + torch::numel(input);
     auto intermediate = at::empty({input.size(0), input.size(1), intm_dim}, options);
 
     int bsz = input.size(0) * input.size(1);
@@ -562,32 +1838,32 @@ at::Tensor fused_gemm_gelu(at::Tensor& input,
     float alpha = (T)1.0;
     float gemm_beta = (T)0.0;
     if (q_int8) {
-        throw std::runtime_error("q_int8=true is not supported!");
+        quantized_gemm<T>(intermediate.data_ptr(),
+                          (T*)input.data_ptr(),
+                          weight,
+                          weight_scale,
+                          weight_scale.size(0),
+                          bsz,
+                          input.size(2));
     } else {
-#ifdef USE_MKL_GEMM
-        onemkl_matmul_ex<T>(InferenceContext::Instance().GetCurrentStream(),
-                            transposed_mode ? oneapi::mkl::transpose::trans : oneapi::mkl::transpose::nontrans,
-                            oneapi::mkl::transpose::nontrans,
-                            bsz,
-                            intm_dim,
-                            input.size(2),
-                            alpha,
-                            gemm_beta,
-                            (T*)input.data_ptr(),
-                            (T*)weight.data_ptr(),
-                            (T*)intermediate.data_ptr());
+        *(InferenceContext::Instance().GetCublasHandle()) =
+            *(InferenceContext::Instance().GetCurrentStream());
+        cublas_gemm_ex(
+            InferenceContext::Instance().GetCublasHandle(),
+            (transposed_mode ? oneapi::mkl::transpose::trans : oneapi::mkl::transpose::nontrans),
+            oneapi::mkl::transpose::nontrans,
+            intm_dim,
+            bsz,
+            input.size(2),
+            &alpha,
+            &gemm_beta,
+            (T*)weight.data_ptr(),
+            (T*)input.data_ptr(),
+            (T*)intermediate.data_ptr(),
+#ifdef __HIP_PLATFORM_AMD__
+            rocblas_gemm_algo_standard);
 #else
-        onednn_matmul_ex<T>(InferenceContext::Instance().GetCurrentStream(),
-                            transposed_mode,
-                            false,
-                            bsz,
-                            intm_dim,
-                            input.size(2),
-                            alpha,
-                            gemm_beta,
-                            (T*)input.data_ptr(),
-                            (T*)weight.data_ptr(),
-                            (T*)intermediate.data_ptr());
+            99);
 #endif
     }
     launch_bias_gelu((T*)intermediate.data_ptr(),
@@ -599,94 +1875,73 @@ at::Tensor fused_gemm_gelu(at::Tensor& input,
     int out_size = (transposed_mode || q_int8) ? weight_out.size(0) : weight_out.size(1);
     auto output = at::empty({input.size(0), input.size(1), out_size}, options);
     if (q_int8) {
-        throw std::runtime_error("q_int8=true is not supported!");
+        quantized_gemm<T>(output.data_ptr(),
+                          (T*)intermediate.data_ptr(),
+                          weight_out,
+                          weight_out_scale,
+                          weight_out_scale.size(0),
+                          bsz,
+                          input.size(2));
     } else {
-#ifdef USE_MKL_GEMM
-        onemkl_matmul_ex<T>(InferenceContext::Instance().GetCurrentStream(),
-                            transposed_mode ? oneapi::mkl::transpose::trans : oneapi::mkl::transpose::nontrans,
-                            oneapi::mkl::transpose::nontrans,
-                            bsz,
-                            out_size,
-                            intm_dim,
-                            alpha,
-                            gemm_beta,
-                            (T*)intermediate.data_ptr(),
-                            (T*)weight_out.data_ptr(),
-                            (T*)output.data_ptr());
+        cublas_gemm_ex(
+            InferenceContext::Instance().GetCublasHandle(),
+            (transposed_mode ? oneapi::mkl::transpose::trans : oneapi::mkl::transpose::nontrans),
+            oneapi::mkl::transpose::nontrans,
+            out_size,
+            bsz,
+            intm_dim,
+            &alpha,
+            &gemm_beta,
+            (T*)weight_out.data_ptr(),
+            (T*)intermediate.data_ptr(),
+            (T*)output.data_ptr(),
+#ifdef __HIP_PLATFORM_AMD__
+            rocblas_gemm_algo_standard);
 #else
-        onednn_matmul_ex<T>(InferenceContext::Instance().GetCurrentStream(),
-                            transposed_mode,
-                            false,
-                            bsz,
-                            out_size,
-                            intm_dim,
-                            alpha,
-                            gemm_beta,
-                            (T*)intermediate.data_ptr(),
-                            (T*)weight_out.data_ptr(),
-                            (T*)output.data_ptr());
+            99);
 #endif
     }
+    // cudaEventRecord(InferenceContext::Instance().GetCompEvent(2),
+    //                InferenceContext::Instance().GetCurrentStream(true));
     return output;
 }
 
 template <typename T>
-at::Tensor ds_vector_matmul(at::Tensor& input,
-                            at::Tensor& weight,
-                            bool async_op,
-                            at::Tensor& q_scale,
-                            bool q_int8,
-                            bool transposed_mode)
+at::Tensor& residual_add_bias(at::Tensor& hidden_state,
+                              at::Tensor& residual,
+                              const at::Tensor& attention_output,
+                              const at::Tensor& attention_bias,
+                              const at::Tensor& final_bias,
+                              const int mp_size,
+                              const bool mlp_after_attn,
+                              const bool add_bias,
+                              const bool preln)
 {
-    auto options = at::TensorOptions()
-                       .dtype(input.options().dtype())
-                       .layout(at::kStrided)
-                       .device(at::kXPU)
-                       .requires_grad(false);
-    int out_size = q_int8 ? weight.size(0) : weight.size(1);
-    int bsz = input.size(0) * input.size(1);
-
-    T* workspace = (T*)InferenceContext::Instance().GetWorkSpace();
-    auto output_stride =
-        c10::TensorType::contiguousStridesOf({input.size(0), input.size(1), out_size});
-    auto output = at::from_blob(workspace,
-                                {input.size(0), input.size(1), out_size},
-                                output_stride,
-                                nullptr,
-                                options,
-                                input.device());
-    if (q_int8) {
-        throw std::runtime_error("q_int8=true is not supported!");
-    } else {
-        float alpha = (T)1.0;
-        float gemm_beta = (T)0.0;
-#ifdef USE_MKL_GEMM
-        onemkl_matmul_ex<T>(InferenceContext::Instance().GetCurrentStream(),
-                            transposed_mode ? oneapi::mkl::transpose::trans : oneapi::mkl::transpose::nontrans,
-                            oneapi::mkl::transpose::nontrans,
-                            bsz,
-                            weight.size(transposed_mode ? 0 : 1),
-                            input.size(2),
-                            alpha,
-                            gemm_beta,
-                            (T*)input.data_ptr(),
-                            (T*)weight.data_ptr(),
-                            (T*)output.data_ptr());
-#else
-        onednn_matmul_ex<T>(InferenceContext::Instance().GetCurrentStream(),
-                            transposed_mode,
-                            false,
-                            bsz,
-                            weight.size(transposed_mode ? 0 : 1),
-                            input.size(2),
-                            alpha,
-                            gemm_beta,
-                            (T*)input.data_ptr(),
-                            (T*)weight.data_ptr(),
-                            (T*)output.data_ptr());
-#endif
-    }
-    return output;
+    int bsz = residual.size(0) * residual.size(1);
+    int hidden_size = residual.size(2);
+    if (mlp_after_attn)
+        launch_bias_residual(static_cast<T*>(residual.data_ptr()),
+                             static_cast<T*>(hidden_state.data_ptr()),
+                             static_cast<T*>(attention_output.data_ptr()),
+                             static_cast<T*>(final_bias.data_ptr()),
+                             static_cast<T*>(attention_bias.data_ptr()),
+                             bsz,
+                             hidden_size,
+                             mp_size,
+                             preln,
+                             InferenceContext::Instance().GetCurrentStream());
+    else
+        launch_gptj_residual_add<T>(
+            static_cast<T*>(residual.data_ptr()),
+            static_cast<T*>(hidden_state.data_ptr()),
+            static_cast<T*>(attention_output.data_ptr()),
+            static_cast<T*>(final_bias.data_ptr()),
+            static_cast<T*>((add_bias ? attention_bias.data_ptr() : nullptr)),
+            hidden_size,
+            bsz,
+            mp_size,
+            InferenceContext::Instance().GetCurrentStream());
+    return residual;
 }
 
 #define DISPATCH_VECTOR_ADD(T_TYPE, C_TYPE)                                         \
@@ -706,40 +1961,179 @@ at::Tensor& _vector_add(at::Tensor& a, at::Tensor& b, float gamma)
     DISPATCH_VECTOR_ADD(Float, float)
     DISPATCH_VECTOR_ADD(Half, sycl::half)
 #ifdef BF16_AVAILABLE
-    DISPATCH_VECTOR_ADD(BFloat16, bf16)
+    DISPATCH_VECTOR_ADD(BFloat16, sycl::ext::oneapi::bfloat16)
 #endif
 
     return a;
 }
 
+std::vector<at::Tensor> apply_rotary_pos_emb(at::Tensor& mixed_query,
+                                             at::Tensor& key_layer,
+                                             unsigned rotary_dim,
+                                             unsigned offset,
+                                             unsigned num_heads,
+                                             bool rotate_half,
+                                             float rope_theta)
+{
+    auto query_cont = mixed_query.contiguous();
+    auto key_cont = key_layer.contiguous();
+
+    unsigned bsz = mixed_query.size(0);
+    unsigned head_size = mixed_query.size(2) / num_heads;
+    unsigned seq_len = mixed_query.size(1);
+
+    if (mixed_query.scalar_type() == at::kFloat)
+        launch_apply_rotary_pos_emb<float>((float*)query_cont.data_ptr(),
+                                           (float*)key_cont.data_ptr(),
+                                           head_size,
+                                           seq_len,
+                                           rotary_dim,
+                                           offset,
+                                           num_heads,
+                                           bsz,
+                                           rope_theta,
+                                           InferenceContext::Instance().GetCurrentStream(),
+                                           InferenceContext::Instance().GetMaxTokenLength());
+    else
+        launch_apply_rotary_pos_emb<sycl::half>((sycl::half*)query_cont.data_ptr(),
+                                                (sycl::half*)key_cont.data_ptr(),
+                                                head_size,
+                                                seq_len,
+                                                rotary_dim,
+                                                offset,
+                                                num_heads,
+                                                bsz,
+                                                rope_theta,
+                                                InferenceContext::Instance().GetCurrentStream(),
+                                                InferenceContext::Instance().GetMaxTokenLength());
+    return {query_cont, key_cont};
+}
+
+#define DISPATCH_MOE_RESIDUAL(T_TYPE, C_TYPE)                                           \
+    if (moe_res.scalar_type() == torch::T_TYPE) {                                       \
+        launch_moe_res_matmul<C_TYPE>((C_TYPE*)moe_res.data_ptr(),                      \
+                                      (C_TYPE*)coef.data_ptr(),                         \
+                                      (C_TYPE*)output.data_ptr(),                       \
+                                      M,                                                \
+                                      N,                                                \
+                                      InferenceContext::Instance().GetCurrentStream()); \
+    }
+
+at::Tensor moe_res_matmul(at::Tensor& moe_res, at::Tensor& coef, at::Tensor& output)
+{
+    int M = moe_res.size(0) * moe_res.size(1);
+    int N = moe_res.size(2);
+    InferenceContext::Instance().SynchComm();
+
+    DISPATCH_MOE_RESIDUAL(kFloat, float)
+    DISPATCH_MOE_RESIDUAL(kHalf, sycl::half)
+#ifdef BF16_AVAILABLE
+    DISPATCH_MOE_RESIDUAL(kBFloat16, sycl::ext::oneapi::bfloat16)
+#endif
+
+    return output;
+}
+
+void ds_release_workspace() { InferenceContext::Instance().release_workspace(); }
+
+bool ds_retake_workspace() { return InferenceContext::Instance().retake_workspace(); }
+
+template <typename T>
+at::Tensor ds_dequantize(at::Tensor& weight, at::Tensor& qscale, int groups)
+{
+    auto options = at::TensorOptions()
+                       .dtype(torch::kFloat16)
+                       .layout(at::kStrided)
+                       .device(at::kXPU)
+                       .requires_grad(false);
+    auto weight16 = at::empty({weight.size(0), weight.size(1)}, options);
+
+    launch_dequantize((T*)weight16.data_ptr(),
+                      (int8_t*)weight.data_ptr(),
+                      (float*)qscale.data_ptr(),
+                      weight.size(0),
+                      weight.size(1),
+                      groups,
+                      InferenceContext::Instance().GetCurrentStream());
+
+    return weight16;
+}
+
 PYBIND11_MODULE(TORCH_EXTENSION_NAME, m)
 {
-    m.def("layer_norm", &ds_layer_norm, "DeepSpeed layer norm (SYCL)");
-    m.def("_vector_add", &_vector_add, "DeepSpeed vector add (SYCL)");
+    m.def("softmax_context_int8",
+          &ds_softmax_context1<sycl::half>,
+          "DeepSpeed attention with int8 (CUDA)");
+
+    // The following functions handle type dispatching internally
+    m.def("gated_activation", &ds_gated_activation, "DeepSpeed Bias GEGLU (CUDA)");
+    m.def("layer_norm", &ds_layer_norm, "DeepSpeed layer norm (CUDA)");
     m.def(
-        "_layer_norm_residual", &ds_layer_norm_residual, "DeepSpeed layer norm + residual (SYCL)");
+        "_layer_norm_residual", &ds_layer_norm_residual, "DeepSpeed layer norm + residual (CUDA)");
+    m.def("layer_norm_residual_store_pre_ln_res",
+          &ds_layer_norm_residual_store_pre_ln_res,
+          "DeepSpeed layer norm + store pre Layernorm residual (CUDA)");
+    m.def("rms_norm", &ds_rms_norm, "DeepSpeed rms norm (CUDA)");
+    m.def("pre_rms_norm", &ds_pre_rms_norm, "DeepSpeed pre rms norm (CUDA)");
+    m.def("_vector_add", &_vector_add, "DeepSpeed vector add (CUDA)");
+    m.def("apply_rotary_pos_emb", &apply_rotary_pos_emb, "DeepSpeed mlp with fp16 (CUDA)");
+    m.def("moe_res_matmul", &moe_res_matmul, "DeepSpeed moe residual matmul (CUDA)");
+    m.def("reset_cache", &reset_cache, "Reset Cache for generation tasks");
+    m.def("release_workspace", &ds_release_workspace, "DeepSpeed Release Workspace");
+    m.def("retake_workspace", &ds_retake_workspace, "DeepSpeed Retake Workspace");
 
+    // The following functions are templated and need to be explicitly instantiated and bound
+    // to different python methods
 #define DEF_OPS(_name, _dtype)                                                                    \
-    m.def("softmax_" #_name, &ds_softmax<_dtype>, "DeepSpeed SoftMax with " #_name " (SYCL)");    \
-    m.def("layer_norm_" #_name, &ds_layer_norm_test<_dtype>, "DeepSpeed layer norm (SYCL)");      \
-    m.def("qkv_gemm_" #_name, &ds_qkv_gemm<_dtype>, "DeepSpeed qkv gemm with " #_name " (SYCL)"); \
-    m.def("mlp_gemm_" #_name, &ds_mlp_gemm<_dtype>, "DeepSpeed mlp with " #_name " (SYCL)");      \
+    m.def("softmax_" #_name, &ds_softmax<_dtype>, "DeepSpeed SoftMax with " #_name " (CUDA)");    \
+    m.def("softmax_context_" #_name,                                                              \
+          &ds_softmax_context<_dtype>,                                                            \
+          "DeepSpeed attention with " #_name " (CUDA)");                                          \
+    m.def("bias_gelu_" #_name, &ds_bias_gelu<_dtype>, "DeepSpeed Gelu with " #_name " (CUDA)");   \
+    m.def("bias_add_" #_name, &ds_bias_add<_dtype>, "DeepSpeed Bias Add with " #_name " (CUDA)"); \
+    m.def("bias_relu_" #_name, &ds_bias_relu<_dtype>, "DeepSpeed ReLU with " #_name " (CUDA)");   \
+    m.def("bias_residual_" #_name,                                                                \
+          &ds_bias_residual<_dtype>,                                                              \
+          "DeepSpeed residual-bias add with " #_name " (CUDA)");                                  \
+    m.def("qkv_gemm_" #_name, &ds_qkv_gemm<_dtype>, "DeepSpeed qkv gemm with " #_name " (CUDA)"); \
+    m.def("rms_qkv_gemm_" #_name,                                                                 \
+          &ds_rms_qkv<_dtype>,                                                                    \
+          "DeepSpeed rms qkv gemm with " #_name " (CUDA)");                                       \
+    m.def("mlp_gemm_" #_name, &ds_mlp_gemm<_dtype>, "DeepSpeed mlp with " #_name " (CUDA)");      \
+    m.def("rms_mlp_gemm_" #_name,                                                                 \
+          &ds_rms_mlp_gemm<_dtype>,                                                               \
+          "DeepSpeed rms mlp gemm with " #_name " (CUDA)");                                       \
     m.def("vector_matmul_" #_name,                                                                \
           &ds_vector_matmul<_dtype>,                                                              \
-          "DeepSpeed vector-MM with " #_name " (SYCL)");                                          \
-    m.def("residual_add_bias_" #_name,                                                            \
-          &residual_add_bias<_dtype>,                                                             \
-          "DeepSpeed residual add with " #_name " (SYCL)");                                       \
+          "DeepSpeed vector-MM with " #_name " (CUDA)");                                          \
+    m.def("linear_layer_" #_name,                                                                 \
+          &ds_linear_layer<_dtype>,                                                               \
+          "DeepSpeed linear_layer with " #_name " (CUDA)");                                       \
     m.def("fused_gemm_gelu_" #_name,                                                              \
           &fused_gemm_gelu<_dtype>,                                                               \
-          "DeepSpeed mlp with " #_name " (SYCL)");                                                \
+          "DeepSpeed mlp with " #_name " (CUDA)");                                                \
+    m.def("residual_add_bias_" #_name,                                                            \
+          &residual_add_bias<_dtype>,                                                             \
+          "DeepSpeed residual add with " #_name " (CUDA)");                                       \
+    m.def("einsum_sec_sm_ecm_" #_name,                                                            \
+          &einsum_sec_sm_ecm<_dtype>,                                                             \
+          "DeepSpeed vector-MM with " #_name " (CUDA)");                                          \
+    m.def("add_padding_" #_name,                                                                  \
+          &add_padding<_dtype>,                                                                   \
+          "DeepSpeed residual add with " #_name " (CUDA)");                                       \
+    m.def("pad_transform_" #_name,                                                                \
+          &padd_add_transform<_dtype>,                                                            \
+          "DeepSpeed residual add with " #_name " (CUDA)");                                       \
     m.def("allocate_workspace_" #_name,                                                           \
           &allocate_workspace<_dtype>,                                                            \
-          "DeepSpeed memory allocation for GPT inference with " #_name " (SYCL)")
+          "DeepSpeed memory allocation for GPT inference with " #_name " (CUDA)");                \
+    m.def("dequantize_" #_name,                                                                   \
+          &ds_dequantize<_dtype>,                                                                 \
+          "DeepSpeed dequantize with " #_name " (CUDA)")
 
     DEF_OPS(fp32, float);
-    DEF_OPS(fp16, fp16);
-#ifndef USE_MKL_GEMM
-    DEF_OPS(bf16, bf16);
+    DEF_OPS(fp16, sycl::half);
+#ifdef BF16_AVAILABLE
+    DEF_OPS(bf16, sycl::ext::oneapi::bfloat16);
 #endif
 }
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/relu.dp.cpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/relu.dp.cpp
new file mode 100644
index 0000000..77d4807
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/relu.dp.cpp
@@ -0,0 +1,85 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include "conversion_utils.h"
+#include "inference_cuda_layers.h"
+#include "memory_access_utils.h"
+
+#define MAX_CAP 4
+#define MAX_SEQ 2048
+
+inline float relu(const float x) { return x < 0 ? 0 : x; }
+
+/*
+In-place relu(biasAdd(x)) for channels last
+*/
+template <typename T>
+void fused_bias_relu(T* input, const T* bias, int total_count, int intermediate_size)
+{
+    // Input restriction: intermediate_size % vals_per_access == 0
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    constexpr int granularity = 16;
+    constexpr int values_per_access = granularity / sizeof(T);
+    const int offset =
+        (item_ct1.get_group(2) * item_ct1.get_local_range(2) + item_ct1.get_local_id(2)) *
+        values_per_access;
+
+    if (offset < total_count) {
+        T data[values_per_access];
+        T data_bias[values_per_access];
+        mem_access::load_global<granularity>(data, input + offset);
+        mem_access::load_global<granularity>(
+            data_bias, bias + (offset % intermediate_size), bias != nullptr);
+
+#pragma unroll
+        for (int i = 0; i < values_per_access; i++) {
+            float data_f = conversion::to<float>(data[i]);
+            float bias_f = conversion::to<float>(data_bias[i]);
+            data[i] = conversion::to<T>(relu(data_f + bias_f));
+        }
+
+        mem_access::store_global<granularity>(input + offset, data);
+    }
+}
+
+template <typename T>
+void launch_bias_relu(T* input,
+                      const T* bias,
+                      int intermediate_size,
+                      int batch_size,
+                      dpct::queue_ptr stream)
+{
+    constexpr int threads = 1024;
+    constexpr int granularity = 16;
+
+    const int total_count = batch_size * intermediate_size;
+    const int elems_per_block = threads * (granularity / sizeof(T));
+    sycl::range<3> block_dims(1, 1, threads);
+    sycl::range<3> grid_dims(1, 1, (total_count + elems_per_block - 1) / elems_per_block);
+
+    /*
+    DPCT1049:0: The work-group size passed to the SYCL kernel may exceed the limit. To get the
+    device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
+    */
+    {
+        dpct::has_capability_or_fail(stream->get_device(),
+                                     {sycl::aspect::fp64, sycl::aspect::fp16});
+        stream->parallel_for(sycl::nd_range<3>(grid_dims * block_dims, block_dims),
+                             [=](sycl::nd_item<3> item_ct1) {
+                                 fused_bias_relu(input, bias, total_count, intermediate_size);
+                             });
+    }
+}
+
+#define INSTANTIATE_LAUNCH_BIAS_RELU(T) \
+    template void launch_bias_relu<T>(T*, const T*, int, int, dpct::queue_ptr);
+
+INSTANTIATE_LAUNCH_BIAS_RELU(float)
+#ifdef BF16_AVAILABLE
+INSTANTIATE_LAUNCH_BIAS_RELU(sycl::ext::oneapi::bfloat16)
+#endif
+INSTANTIATE_LAUNCH_BIAS_RELU(sycl::half)
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/rms_norm.dp.cpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/rms_norm.dp.cpp
new file mode 100644
index 0000000..ce3d8cd
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/rms_norm.dp.cpp
@@ -0,0 +1,327 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include "conversion_utils.h"
+#include "ds_kernel_utils.h"
+#include "inference_cuda_layers.h"
+#include "memory_access_utils.h"
+#include "reduction_utils.h"
+
+using rop = reduce::ROpType;
+
+namespace rms {
+constexpr int granularity = 16;
+}  // namespace rms
+
+template <typename T, int UNROLL, int threadsPerGroup, int maxThreads>
+/*
+DPCT1110:3: The total declared local variable size in device function rms_norm exceeds 128 bytes and
+may cause high register pressure. Consult with your hardware vendor to find the total register size
+available and adjust the code, or use smaller sub-group size to avoid high register pressure.
+*/
+void rms_norm(T* output, const T* vals, const T* gamma, float epsilon, int elems_per_row)
+{
+    constexpr int T_per_load = rms::granularity / sizeof(T);
+
+    sycl::group<3> tb = sycl::ext::oneapi::experimental::this_group<3>();
+    sycl::sub_group warp = sycl::ext::oneapi::experimental::this_sub_group();
+
+    // X-dimension of the block
+    const int block_offset =
+        (tb.get_group_id()[2] * (maxThreads / threadsPerGroup) * elems_per_row) +
+        (tb.get_local_id()[1] * elems_per_row);
+    const int thread_offset = tb.get_local_id()[2] * T_per_load;
+    const int base_offset = block_offset + thread_offset;
+    const int stride =
+        sycl::ext::oneapi::experimental::this_nd_item<3>().get_local_range(2) * T_per_load;
+
+    float var_sum = reduce::init<rop::Add, float>();
+
+    const T* input_base = vals + base_offset;
+
+    T local_buffer[UNROLL * T_per_load];
+
+#pragma unroll
+    for (int i = 0; i < UNROLL; i++) {
+        T* iteration_buffer = local_buffer + (i * T_per_load);
+
+        mem_access::load_global<rms::granularity>(iteration_buffer,
+                                                  input_base + (i * stride),
+                                                  thread_offset + (i * stride) < elems_per_row);
+
+#pragma unroll
+        for (int j = 0; j < T_per_load; j++) {
+            float up_cast = conversion::to<float>(iteration_buffer[j]);
+            float sq_val = up_cast * up_cast;
+            var_sum = reduce::element<rop::Add, float>(var_sum, sq_val);
+        }
+    }
+
+    reduce::partitioned_block<rop::Add, threadsPerGroup>(tb, warp, var_sum);
+    const float var = var_sum / elems_per_row;
+    /*
+    DPCT1013:8: The rounding mode could not be specified and the generated code may have different
+    accuracy than the original code. Verify the correctness. SYCL math built-in function rounding
+    mode is aligned with OpenCL C 1.2 standard.
+    */
+    const T denom = conversion::to<T>(sycl::rsqrt(var + epsilon));
+
+    T* block_output = output + block_offset;
+
+#pragma unroll
+    for (int i = 0; i < UNROLL; i++) {
+        T* iteration_buffer = local_buffer + (i * T_per_load);
+        const int iter_idx = i * stride + thread_offset;
+        const bool do_loads = (iter_idx < elems_per_row);
+
+        T gamma_local[T_per_load];
+
+        mem_access::load_global<rms::granularity>(gamma_local, gamma + iter_idx, do_loads);
+
+#pragma unroll
+        for (int j = 0; j < T_per_load; j++) {
+            iteration_buffer[j] *= denom;
+            iteration_buffer[j] *= gamma_local[j];
+        }
+
+        if (do_loads) {
+            mem_access::store_global<rms::granularity>(block_output + iter_idx, iteration_buffer);
+        }
+    }
+}
+
+template <typename T, int UNROLL, int threadsPerGroup, int maxThreads>
+/*
+DPCT1110:4: The total declared local variable size in device function pre_rms_norm exceeds 128 bytes
+and may cause high register pressure. Consult with your hardware vendor to find the total register
+size available and adjust the code, or use smaller sub-group size to avoid high register pressure.
+*/
+void pre_rms_norm(T* output,
+                  T* res_out,
+                  const T* vals,
+                  const T* residual,
+                  const T* gamma,
+                  float epsilon,
+                  int elems_per_row)
+{
+    constexpr int T_per_load = rms::granularity / sizeof(T);
+
+    sycl::group<3> tb = sycl::ext::oneapi::experimental::this_group<3>();
+    sycl::sub_group warp = sycl::ext::oneapi::experimental::this_sub_group();
+
+    // X-dimension of the block
+    const int block_offset =
+        (tb.get_group_id()[2] * (maxThreads / threadsPerGroup) * elems_per_row) +
+        (tb.get_local_id()[1] * elems_per_row);
+    const int thread_offset = tb.get_local_id()[2] * T_per_load;
+    const int base_offset = block_offset + thread_offset;
+    const int stride =
+        sycl::ext::oneapi::experimental::this_nd_item<3>().get_local_range(2) * T_per_load;
+
+    float var_sum = reduce::init<rop::Add, float>();
+
+    const T* input_base = vals + base_offset;
+    const T* residual_base = residual + base_offset;
+    T* res_output = res_out + base_offset;
+
+    T local_buffer[UNROLL * T_per_load];
+
+#pragma unroll
+    for (int i = 0; i < UNROLL; i++) {
+        T* iteration_buffer = local_buffer + (i * T_per_load);
+        T residual_buffer[T_per_load];
+
+        const int iter_offset = i * stride + thread_offset;
+        const bool do_loads = (iter_offset < elems_per_row);
+
+        mem_access::load_global<rms::granularity>(
+            iteration_buffer, input_base + (i * stride), do_loads);
+        mem_access::load_global<rms::granularity>(
+            residual_buffer, residual_base + (i * stride), do_loads);
+
+#pragma unroll
+        for (int j = 0; j < T_per_load; j++) {
+            iteration_buffer[j] += residual_buffer[j];
+            float vals_up_cast = conversion::to<float>(iteration_buffer[j]);
+
+            var_sum = reduce::element<rop::Add, float>(var_sum, vals_up_cast * vals_up_cast);
+        }
+
+        if (do_loads) {
+            mem_access::store_global<rms::granularity>(res_output + i * stride, iteration_buffer);
+        }
+    }
+
+    reduce::partitioned_block<rop::Add, threadsPerGroup>(tb, warp, var_sum);
+    const float var = var_sum / elems_per_row;
+    /*
+    DPCT1013:9: The rounding mode could not be specified and the generated code may have different
+    accuracy than the original code. Verify the correctness. SYCL math built-in function rounding
+    mode is aligned with OpenCL C 1.2 standard.
+    */
+    const T denom = conversion::to<T>(sycl::rsqrt(var + epsilon));
+
+    T* block_output = output + block_offset;
+
+#pragma unroll
+    for (int i = 0; i < UNROLL; i++) {
+        T* iteration_buffer = local_buffer + (i * T_per_load);
+        const int iter_idx = i * stride + thread_offset;
+        const bool do_loads = (iter_idx < elems_per_row);
+
+        T gamma_local[T_per_load];
+
+        mem_access::load_global<rms::granularity>(gamma_local, gamma + iter_idx, do_loads);
+
+#pragma unroll
+        for (int j = 0; j < T_per_load; j++) {
+            iteration_buffer[j] *= denom;
+            iteration_buffer[j] *= gamma_local[j];
+        }
+
+        if (do_loads) {
+            mem_access::store_global<rms::granularity>(block_output + iter_idx, iteration_buffer);
+        }
+    }
+}
+
+/*
+DPCT1049:6: The work-group size passed to the SYCL kernel may exceed the limit. To get the device
+limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
+*/
+#define LAUNCH_RMS_NORM(UNROLL, threadsPerGroup, maxThreads)                                      \
+  dpct::has_capability_or_fail(stream->get_device(), {sycl::aspect::fp64, sycl::aspect::fp16});   \
+  stream->submit([&](sycl::handler& cgh) {                                                        \
+    T* norm_output_ct0 = norm_output;                                                             \
+    const T* vals_ct1 = vals;                                                                     \
+    const T* gamma_ct2 = gamma;                                                                   \
+    auto epsilon_ct3 = epsilon;                                                                   \
+    auto elems_per_row_ct4 = elems_per_row;                                                       \
+                                                                                                  \
+    cgh.parallel_for(sycl::nd_range<3>(grid * block, block),                                      \
+                     [=](sycl::nd_item<3> item_ct1) [[intel::reqd_sub_group_size(32)]] {          \
+                       rms_norm<T, UNROLL, threadsPerGroup, maxThreads>(                          \
+                           norm_output_ct0, vals_ct1, gamma_ct2, epsilon_ct3, elems_per_row_ct4); \
+                     });                                                                          \
+  });
+
+/*
+DPCT1049:5: The work-group size passed to the SYCL kernel may exceed the limit. To get the device
+limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
+*/
+#define LAUNCH_PRE_RMS_NORM(UNROLL, threadsPerGroup, maxThreads)                                \
+  dpct::has_capability_or_fail(stream->get_device(), {sycl::aspect::fp64, sycl::aspect::fp16}); \
+  stream->submit([&](sycl::handler& cgh) {                                                      \
+    T* norm_output_ct0 = norm_output;                                                           \
+    T* res_output_ct1 = res_output;                                                             \
+    const T* vals_ct2 = vals;                                                                   \
+    const T* residual_ct3 = residual;                                                           \
+    const T* gamma_ct4 = gamma;                                                                 \
+    auto epsilon_ct5 = epsilon;                                                                 \
+    auto elems_per_row_ct6 = elems_per_row;                                                     \
+                                                                                                \
+    cgh.parallel_for(sycl::nd_range<3>(grid * block, block),                                    \
+                     [=](sycl::nd_item<3> item_ct1) [[intel::reqd_sub_group_size(32)]] {        \
+                       pre_rms_norm<T, UNROLL, threadsPerGroup, maxThreads>(norm_output_ct0,    \
+                                                                            res_output_ct1,     \
+                                                                            vals_ct2,           \
+                                                                            residual_ct3,       \
+                                                                            gamma_ct4,          \
+                                                                            epsilon_ct5,        \
+                                                                            elems_per_row_ct6); \
+                     });                                                                        \
+  });
+
+#define LAUNCH_ALL_RMS_NORM(UNROLL, threadsPerGroup, maxThreads) \
+    if (pre_norm) {                                              \
+        LAUNCH_PRE_RMS_NORM(UNROLL, threadsPerGroup, maxThreads) \
+    } else {                                                     \
+        LAUNCH_RMS_NORM(UNROLL, threadsPerGroup, maxThreads)     \
+    }
+
+template <typename T>
+void launch_rms_norm(T* norm_output,
+                     T* res_output,
+                     const T* vals,
+                     const T* residual,
+                     const T* gamma,
+                     float epsilon,
+                     int rows,
+                     int elems_per_row,
+                     dpct::queue_ptr stream)
+{
+    // 8 for sycl::half, 4 for float
+    constexpr int T_per_load = rms::granularity / sizeof(T);
+    constexpr int maxThreads = 256;
+    constexpr int internalUnroll = sizeof(T) == 4 ? 4 : 2;
+
+    const bool is_subblock_schedule = (elems_per_row <= 128) ? true : false;
+    const int h_per_step = is_subblock_schedule ? T_per_load : T_per_load * internalUnroll;
+
+    // Scheduling concern: may be slightly faster for some inputs to assign multiple stages of
+    // warp-sized blocks rather than stepping up to 64/96 threads
+    const int one_step_threads = next_pow2((elems_per_row + h_per_step - 1) / h_per_step);
+    const int threads_per_group = (one_step_threads < maxThreads) ? one_step_threads : maxThreads;
+
+    const int groups_per_block_max =
+        is_subblock_schedule ? (maxThreads + threads_per_group - 1) / threads_per_group : 1;
+    const int groups_per_block = (rows < groups_per_block_max) ? rows : groups_per_block_max;
+    const int groups_launch = (groups_per_block + rows - 1) / groups_per_block;
+
+    sycl::range<3> block(1, groups_per_block, threads_per_group);
+    sycl::range<3> grid(1, 1, groups_launch);
+
+    const int elems_per_step = threads_per_group * h_per_step;
+    const int external_unRoll = (elems_per_row + elems_per_step - 1) / elems_per_step;
+
+    bool pre_norm = (residual == nullptr) ? false : true;
+
+    if (is_subblock_schedule) {
+        // <=128
+        if (threads_per_group == 1) {
+            LAUNCH_ALL_RMS_NORM(1, 1, maxThreads);
+        } else if (threads_per_group == 2) {
+            LAUNCH_ALL_RMS_NORM(1, 2, maxThreads);
+        } else if (threads_per_group == 4) {
+            LAUNCH_ALL_RMS_NORM(1, 4, maxThreads);
+        } else if (threads_per_group == 8) {
+            LAUNCH_ALL_RMS_NORM(1, 8, maxThreads);
+        } else if (threads_per_group == 16) {
+            LAUNCH_ALL_RMS_NORM(1, 16, maxThreads);
+        }
+    } else if (external_unRoll == 1) {
+        // 129 - 4096 elems
+        // (this can launch with 1-7 warps as well)
+        LAUNCH_ALL_RMS_NORM(1 * internalUnroll, maxThreads, maxThreads);
+    } else if (external_unRoll == 2) {
+        // 4097 - 8192 elems
+        LAUNCH_ALL_RMS_NORM(2 * internalUnroll, maxThreads, maxThreads);
+    } else if (external_unRoll == 3) {
+        // 8193 - 12288 elems
+        LAUNCH_ALL_RMS_NORM(3 * internalUnroll, maxThreads, maxThreads);
+    } else if (external_unRoll == 4) {
+        // 12289 - 16384 elems
+        LAUNCH_ALL_RMS_NORM(4 * internalUnroll, maxThreads, maxThreads);
+    }
+}
+
+#define INSTANTIATE_LAUNCH_RMS_NORM(T)                  \
+    template void launch_rms_norm<T>(T * norm_output,   \
+                                     T * res_output,    \
+                                     const T* vals,     \
+                                     const T* residual, \
+                                     const T* gamma,    \
+                                     float epsilon,     \
+                                     int rows,          \
+                                     int elems_per_row, \
+                                     dpct::queue_ptr stream);
+
+INSTANTIATE_LAUNCH_RMS_NORM(float)
+INSTANTIATE_LAUNCH_RMS_NORM(sycl::half)
+#ifdef BF16_AVAILABLE
+INSTANTIATE_LAUNCH_RMS_NORM(sycl::ext::oneapi::bfloat16)
+#endif
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/softmax.cpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/softmax.cpp
deleted file mode 100644
index f6b635c..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/softmax.cpp
+++ /dev/null
@@ -1,575 +0,0 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
-
-#include "compatible.hpp"
-#include "conversion_utils.hpp"
-#include "inference_sycl_layers.hpp"
-
-#define MAX_REG_SIZE 8
-#define minus_infinity -10000.0
-
-template <typename T, int iterations>
-class attn_softmax_v2 {
-private:
-    T* vals;
-    T* mask;
-    T* alibi;
-    float layer_scale;
-    bool triangular;
-    bool recompute;
-    bool local_attention;
-    int window_size;
-    int total_count;
-    int heads;
-    int sequence_length;
-    int num_seq;
-    int head_offset;
-    int mask_stride;
-    int mp_size;
-    int reduceWidth;
-
-public:
-    attn_softmax_v2(T* vals,
-                    T* mask,
-                    T* alibi,
-                    float layer_scale,
-                    bool triangular,
-                    bool recompute,
-                    bool local_attention,
-                    int window_size,
-                    int total_count,
-                    int heads,
-                    int sequence_length,
-                    int num_seq,
-                    int head_offset,
-                    int mask_stride,
-                    int mp_size,
-                    int reduceWidth)
-        : vals(vals),
-          mask(mask),
-          alibi(alibi),
-          layer_scale(layer_scale),
-          triangular(triangular),
-          recompute(recompute),
-          local_attention(local_attention),
-          window_size(window_size),
-          total_count(total_count),
-          heads(heads),
-          sequence_length(sequence_length),
-          num_seq(num_seq),
-          head_offset(head_offset),
-          mask_stride(mask_stride),
-          mp_size(mp_size),
-          reduceWidth(reduceWidth){};
-
-    void operator() [[sycl::reqd_sub_group_size(WARP_SIZE)]] (sycl::nd_item<1> pos) const
-    {
-        auto b = sycl::ext::oneapi::experimental::this_group<1>();
-        auto g = sycl::ext::oneapi::experimental::this_sub_group();
-
-        float2 low_data[MAX_REG_SIZE];
-        float2 high_data[MAX_REG_SIZE];
-        const T zero_h = conversion::to<T>(0.f);
-
-        auto tid = pos.get_local_id(0);
-        int wid = tid >> 5;
-        int lane = tid & 0x1f;
-        int warp_num = pos.get_local_range(0) >> 5;
-
-        int reduce_blocks = reduceWidth >> 5;
-        int seq_lane = tid % reduceWidth;
-
-        local_ptr<float> partialSum = __group_local_memory<float[MAX_WARP_NUM]>(b);
-
-        int iter_offset = pos.get_group(0) * (warp_num / reduce_blocks) + (wid / reduce_blocks);
-        int batch_idx = iter_offset / (num_seq * heads);
-        int alibi_offset = batch_idx * heads * mp_size + head_offset;
-        int mask_offset = batch_idx * mask_stride + (iter_offset % mask_stride);
-
-        T* rvals = vals;
-
-        if (iter_offset < total_count) {
-            rvals += (iter_offset * sequence_length);
-
-            alibi_offset = (alibi_offset + ((iter_offset / num_seq) % heads)) * sequence_length;
-            mask_offset = mask_offset * sequence_length;
-            int seq_id = iter_offset % num_seq;
-
-            int real_seq_id = seq_id + (num_seq == sequence_length ? 0 : sequence_length);
-            int window_stride4 = (local_attention && (real_seq_id >> 2) > (window_size >> 2))
-                                     ? (real_seq_id >> 2) - (window_size >> 2)
-                                     : 0;
-            int window_stride =
-                (local_attention && real_seq_id >= window_size) ? real_seq_id - window_size : -1;
-
-            float max_val = minus_infinity;
-            for (int i = 0; i < iterations; i++) {
-                int data_id = i * (reduceWidth << 2) + (seq_lane);
-                bool check = (data_id >> 2) >= window_stride4;
-                bool low_x_check = check && (data_id < sequence_length) &&
-                                   (!triangular || (data_id <= seq_id)) && (data_id > window_stride);
-                bool low_y_check = check && ((data_id + reduceWidth) < sequence_length) &&
-                                   (!triangular || ((data_id + reduceWidth) <= seq_id)) &&
-                                   ((data_id + reduceWidth) > window_stride);
-                bool high_x_check = check && ((data_id + reduceWidth * 2) < sequence_length) &&
-                                    (!triangular || ((data_id + reduceWidth * 2) <= seq_id)) &&
-                                    ((data_id + reduceWidth * 2) > window_stride);
-                bool high_y_check = check && ((data_id + reduceWidth * 3) < sequence_length) &&
-                                    (!triangular || ((data_id + reduceWidth * 3) <= seq_id)) &&
-                                    ((data_id + reduceWidth * 3) > window_stride);
-
-                if (mask && alibi) {
-                    low_data[i].x() = low_x_check
-                                        ? conversion::to<float>(rvals[data_id]) * layer_scale +
-                                              (conversion::to<float>(alibi[data_id + alibi_offset])) +
-                                              (conversion::to<float>(mask[data_id + mask_offset]))
-                                        : minus_infinity;
-                    low_data[i].y() =
-                        low_y_check
-                            ? conversion::to<float>(rvals[data_id + reduceWidth]) * layer_scale +
-                                  (conversion::to<float>(alibi[data_id + alibi_offset + reduceWidth])) +
-                                  (conversion::to<float>(mask[data_id + mask_offset + reduceWidth]))
-                            : minus_infinity;
-                    high_data[i].x() =
-                        high_x_check
-                            ? conversion::to<float>(rvals[data_id + reduceWidth * 2]) * layer_scale +
-                                  (conversion::to<float>(
-                                      alibi[data_id + alibi_offset + reduceWidth * 2])) +
-                                  (conversion::to<float>(mask[data_id + mask_offset + reduceWidth * 2]))
-                            : minus_infinity;
-                    high_data[i].y() =
-                        high_y_check
-                            ? conversion::to<float>(rvals[data_id + reduceWidth * 3]) * layer_scale +
-                                  (conversion::to<float>(
-                                      alibi[data_id + alibi_offset + reduceWidth * 3])) +
-                                  (conversion::to<float>(mask[data_id + mask_offset + reduceWidth * 3]))
-                            : minus_infinity;
-                } else if (mask) {
-                    low_data[i].x() = low_x_check
-                                        ? conversion::to<float>(rvals[data_id]) * layer_scale +
-                                              (conversion::to<float>(mask[data_id + mask_offset]))
-                                        : minus_infinity;
-                    low_data[i].y() =
-                        low_y_check
-                            ? conversion::to<float>(rvals[data_id + reduceWidth]) * layer_scale +
-                                  (conversion::to<float>(mask[data_id + mask_offset + reduceWidth]))
-                            : minus_infinity;
-                    high_data[i].x() =
-                        high_x_check
-                            ? conversion::to<float>(rvals[data_id + reduceWidth * 2]) * layer_scale +
-                                  (conversion::to<float>(mask[data_id + mask_offset + reduceWidth * 2]))
-                            : minus_infinity;
-                    high_data[i].y() =
-                        high_y_check
-                            ? conversion::to<float>(rvals[data_id + reduceWidth * 3]) * layer_scale +
-                                  (conversion::to<float>(mask[data_id + mask_offset + reduceWidth * 3]))
-                            : minus_infinity;
-                } else if (alibi) {
-                    low_data[i].x() = low_x_check
-                                        ? conversion::to<float>(rvals[data_id]) * layer_scale +
-                                              (conversion::to<float>(alibi[data_id + alibi_offset]))
-                                        : minus_infinity;
-                    low_data[i].y() =
-                        low_y_check
-                            ? conversion::to<float>(rvals[data_id + reduceWidth]) * layer_scale +
-                                  (conversion::to<float>(alibi[data_id + alibi_offset + reduceWidth]))
-                            : minus_infinity;
-                    high_data[i].x() =
-                        high_x_check
-                            ? conversion::to<float>(rvals[data_id + reduceWidth * 2]) * layer_scale +
-                                  (conversion::to<float>(
-                                      alibi[data_id + alibi_offset + reduceWidth * 2]))
-                            : minus_infinity;
-                    high_data[i].y() =
-                        high_y_check
-                            ? conversion::to<float>(rvals[data_id + reduceWidth * 3]) * layer_scale +
-                                  (conversion::to<float>(
-                                      alibi[data_id + alibi_offset + reduceWidth * 3]))
-                            : minus_infinity;
-                } else {
-                    low_data[i].x() = low_x_check ? conversion::to<float>(rvals[data_id]) * layer_scale
-                                                : minus_infinity;
-                    low_data[i].y() =
-                        low_y_check ? conversion::to<float>(rvals[data_id + reduceWidth]) * layer_scale
-                                    : minus_infinity;
-                    high_data[i].x() =
-                        high_x_check
-                            ? conversion::to<float>(rvals[data_id + reduceWidth * 2]) * layer_scale
-                            : minus_infinity;
-                    high_data[i].y() =
-                        high_y_check
-                            ? conversion::to<float>(rvals[data_id + reduceWidth * 3]) * layer_scale
-                            : minus_infinity;
-                }
-
-                max_val = (low_data[i].x() > max_val ? low_data[i].x() : max_val);
-                max_val = (low_data[i].y() > max_val ? low_data[i].y() : max_val);
-                max_val = (high_data[i].x() > max_val ? high_data[i].x() : max_val);
-                max_val = (high_data[i].y() > max_val ? high_data[i].y() : max_val);
-            }
-
-            for (int i = 1; i < WARP_SIZE; i *= 2) {
-                auto temp = g.shuffle_xor(max_val, i);
-                max_val = (temp > max_val ? temp : max_val);
-            }
-
-            if (reduceWidth > WARP_SIZE) {
-                if (lane == 0) partialSum[wid] = max_val;
-                sycl::group_barrier(b, b.fence_scope);
-
-                if (lane < warp_num) max_val = partialSum[lane];
-
-                sycl::group_barrier(b, b.fence_scope);
-
-                for (int i = 1; i < reduce_blocks; i *= 2) {
-                    auto temp = g.shuffle_xor(max_val, i);
-                    max_val = (temp > max_val ? temp : max_val);
-                }
-
-                max_val = g.shuffle(max_val, tid / WARP_SIZE);
-            }
-            float sum = 0;
-            for (int i = 0; i < iterations; i++) {
-                low_data[i].x() = sycl::exp(low_data[i].x() - max_val);
-                low_data[i].y() = sycl::exp(low_data[i].y() - max_val);
-                high_data[i].x() = sycl::exp(high_data[i].x() - max_val);
-                high_data[i].y() = sycl::exp(high_data[i].y() - max_val);
-
-                sum += (low_data[i].x() + low_data[i].y() + high_data[i].x() + high_data[i].y());
-            }
-
-            for (int i = 1; i < WARP_SIZE; i *= 2) sum += g.shuffle_xor(sum, i);
-
-            if (reduceWidth > WARP_SIZE) {
-                if (lane == 0) partialSum[wid] = sum;
-                sycl::group_barrier(b, b.fence_scope);
-
-                if (lane < warp_num) sum = partialSum[lane];
-
-                sycl::group_barrier(b, b.fence_scope);
-
-                for (int i = 1; i < reduce_blocks; i *= 2) { sum += g.shuffle_xor(sum, i); }
-
-                sum = g.shuffle(sum, tid / WARP_SIZE);
-            }
-            sum += 1e-6;
-            for (int i = 0; i < iterations; i++) {
-            int data_id = i * (reduceWidth << 2) + (seq_lane);
-                if (data_id < sequence_length) {
-                rvals[data_id] = conversion::to<T>(low_data[i].x() / sum);
-                if ((data_id + reduceWidth) < sequence_length)
-                    rvals[data_id + reduceWidth] = conversion::to<T>(low_data[i].y() / sum);
-                if ((data_id + reduceWidth * 2) < sequence_length)
-                    rvals[data_id + reduceWidth * 2] = conversion::to<T>(high_data[i].x() / sum);
-                if ((data_id + reduceWidth * 3) < sequence_length)
-                    rvals[data_id + reduceWidth * 3] = conversion::to<T>(high_data[i].y() / sum);
-            }
-            }
-        }
-    };
-};
-
-template <int iterations>
-class attn_softmax_v2<float, iterations> {
-private:
-    float* vals;
-    float* attn_mask;
-    float* alibi;
-    float layer_scale;
-    bool triangular;
-    bool recompute;
-    bool local_attention;
-    int window_size;
-    int total_count;
-    int heads;
-    int sequence_length;
-    int num_seq;
-    int head_offset;
-    int mask_stride;
-    int mp_size;
-    int reduceWidth;
-
-public:
-    attn_softmax_v2(float* vals,
-                    float* attn_mask,
-                    float* alibi,
-                    float layer_scale,
-                    bool triangular,
-                    bool recompute,
-                    bool local_attention,
-                    int window_size,
-                    int total_count,
-                    int heads,
-                    int sequence_length,
-                    int num_seq,
-                    int head_offset,
-                    int mask_stride,
-                    int mp_size,
-                    int reduceWidth)
-        : vals(vals),
-          attn_mask(attn_mask),
-          alibi(alibi),
-          layer_scale(layer_scale),
-          triangular(triangular),
-          recompute(recompute),
-          local_attention(local_attention),
-          window_size(window_size),
-          total_count(total_count),
-          heads(heads),
-          sequence_length(sequence_length),
-          num_seq(num_seq),
-          head_offset(head_offset),
-          mask_stride(mask_stride),
-          mp_size(mp_size),
-          reduceWidth(reduceWidth){};
-
-    void operator() [[sycl::reqd_sub_group_size(WARP_SIZE)]] (sycl::nd_item<1> pos) const
-    {
-        auto b = sycl::ext::oneapi::experimental::this_group<1>();
-        auto g = sycl::ext::oneapi::experimental::this_sub_group();
-
-        float4 data[MAX_REG_SIZE];
-
-        auto tid = pos.get_local_id(0);
-        int wid = tid >> 5;
-        int lane = tid & 0x1f;
-        int warp_num = pos.get_local_range(0) >> 5;
-
-        int reduce_blocks = reduceWidth >> 5;
-        int seq_lane = tid % reduceWidth;
-
-        auto partialSum = local_ptr<float>(
-            local_ptr<void>(sycl::ext::oneapi::group_local_memory<float[MAX_WARP_NUM]>(b).get()));
-
-        int iter_offset = pos.get_group(0) * (warp_num / reduce_blocks) + (wid / reduce_blocks);
-
-        float* rvals = vals;
-        if (iter_offset < total_count) {
-            rvals += (iter_offset * sequence_length);
-
-            int batch_idx = iter_offset / (num_seq * heads);
-            int alibi_offset = batch_idx * heads * mp_size + head_offset;
-            int mask_offset = batch_idx * mask_stride + (iter_offset % mask_stride);
-            mask_offset = mask_offset * sequence_length;
-            int seq_id = iter_offset % num_seq;
-
-            int real_seq_id = seq_id + (num_seq == sequence_length ? 0 : sequence_length);
-            int window_stride4 = (local_attention && (real_seq_id >> 2) > (window_size >> 2))
-                                     ? (real_seq_id >> 2) - (window_size >> 2)
-                                     : 0;
-            int window_stride =
-                (local_attention && real_seq_id >= window_size) ? real_seq_id - window_size : -1;
-
-            float max_val = minus_infinity;
-
-            for (int i = 0; i < iterations; i++) {
-                int data_id = i * (reduceWidth << 2) + (seq_lane);
-                bool check = (data_id >> 2) >= window_stride4;
-                bool x_check = check && (data_id < sequence_length) &&
-                               (!triangular || (data_id <= seq_id)) && (data_id > window_stride);
-                bool y_check = check && ((data_id + reduceWidth) < sequence_length) &&
-                               (!triangular || ((data_id + reduceWidth) <= seq_id)) &&
-                               ((data_id + reduceWidth) > window_stride);
-                bool z_check = check && ((data_id + reduceWidth * 2) < sequence_length) &&
-                               (!triangular || ((data_id + reduceWidth * 2) <= seq_id)) &&
-                               ((data_id + reduceWidth * 2) > window_stride);
-                bool w_check = check && ((data_id + reduceWidth * 3) < sequence_length) &&
-                               (!triangular || ((data_id + reduceWidth * 3) <= seq_id)) &&
-                               ((data_id + reduceWidth * 3) > window_stride);
-
-                if (attn_mask) {
-                    data[i].x() = x_check ? rvals[data_id] + attn_mask[data_id + mask_offset]
-                                        : minus_infinity;
-                    data[i].y() = y_check ? rvals[data_id + reduceWidth] +
-                                              attn_mask[data_id + mask_offset + reduceWidth]
-                                        : minus_infinity;
-                    data[i].z() = z_check ? rvals[data_id + reduceWidth * 2] +
-                                              attn_mask[data_id + mask_offset + reduceWidth * 2]
-                                        : minus_infinity;
-                    data[i].w() = w_check ? rvals[data_id + reduceWidth * 3] +
-                                              attn_mask[data_id + mask_offset + reduceWidth * 3]
-                                        : minus_infinity;
-                } else {
-                    data[i].x() = x_check ? rvals[data_id] : minus_infinity;
-                    data[i].y() = y_check ? rvals[data_id + reduceWidth] : minus_infinity;
-                    data[i].z() = z_check ? rvals[data_id + reduceWidth * 2] : minus_infinity;
-                    data[i].w() = w_check ? rvals[data_id + reduceWidth * 3] : minus_infinity;
-                }
-
-                max_val = (data[i].x() > max_val ? data[i].x() : max_val);
-                max_val = (data[i].y() > max_val ? data[i].y() : max_val);
-                max_val = (data[i].z() > max_val ? data[i].z() : max_val);
-                max_val = (data[i].w() > max_val ? data[i].w() : max_val);
-            }
-
-            for (int i = 1; i < WARP_SIZE; i *= 2) {
-                auto temp = g.shuffle_xor(max_val, i);
-                max_val = (temp > max_val ? temp : max_val);
-            }
-
-            if (reduceWidth > WARP_SIZE) {
-                if (lane == 0) partialSum[wid] = max_val;
-                sycl::group_barrier(b, b.fence_scope);
-
-                if (lane < warp_num) max_val = partialSum[lane];
-
-                sycl::group_barrier(b, b.fence_scope);
-
-                for (int i = 1; i < reduce_blocks; i *= 2) {
-                    auto temp = g.shuffle_xor(max_val, i);
-                    max_val = (temp > max_val ? temp : max_val);
-                }
-
-                max_val = g.shuffle(max_val, tid / WARP_SIZE);
-            }
-
-            float sum = 0;
-            for (int i = 0; i < iterations; i++) {
-                data[i].x() = sycl::exp(data[i].x() - max_val);
-                data[i].y() = sycl::exp(data[i].y() - max_val);
-                data[i].z() = sycl::exp(data[i].z() - max_val);
-                data[i].w() = sycl::exp(data[i].w() - max_val);
-
-                sum += (data[i].x() + data[i].y() + data[i].z() + data[i].w());
-            }
-
-            for (int i = 1; i < WARP_SIZE; i *= 2) sum += g.shuffle_xor(sum, i);
-
-            if (reduceWidth > WARP_SIZE) {
-                if (lane == 0) partialSum[wid] = sum;
-                sycl::group_barrier(b, b.fence_scope);
-
-                if (lane < warp_num) sum = partialSum[lane];
-
-                sycl::group_barrier(b, b.fence_scope);
-
-                for (int i = 1; i < reduce_blocks; i *= 2) { sum += g.shuffle_xor(sum, i); }
-
-                sum = g.shuffle(sum, tid / WARP_SIZE);
-            }
-            sum += 1e-6;
-
-            for (int i = 0; i < iterations; i++) {
-            int data_id = i * (reduceWidth << 2) + (seq_lane);
-                if (data_id < sequence_length) {
-                rvals[data_id] = data[i].x() / sum;
-                if ((data_id + reduceWidth) < sequence_length)
-                    rvals[data_id + reduceWidth] = data[i].y() / sum;
-                if ((data_id + reduceWidth * 2) < sequence_length)
-                    rvals[data_id + reduceWidth * 2] = data[i].z() / sum;
-                if ((data_id + reduceWidth * 3) < sequence_length)
-                    rvals[data_id + reduceWidth * 3] = data[i].w() / sum;
-              }
-            }
-        }
-    };
-};
-
-#define LAUNCH_ATTN_SOFTMAX_V2(iterations)                         \
-    attn_softmax_v2<T, iterations> fn(vals,                        \
-                                      mask,                        \
-                                      alibi,                       \
-                                      layer_scale,                 \
-                                      triangular,                  \
-                                      recompute,                   \
-                                      local_attention,             \
-                                      window_size,                 \
-                                      total_count,                 \
-                                      heads,                       \
-                                      sequence_length,             \
-                                      num_seq,                     \
-                                      head_offset,                 \
-                                      mask_stride,                 \
-                                      mp_size,                     \
-                                      reduce_width);               \
-    stream.submit([&](sycl::handler& cmd_list) {                   \
-        cmd_list.parallel_for(sycl::nd_range<1>{grid, block}, fn); \
-    });
-
-template <typename T>
-void launch_attn_softmax_v2(T* vals,
-                            T* mask,
-                            T* alibi,
-                            float layer_scale,
-                            bool triangular,
-                            bool recompute,
-                            bool local_attention,
-                            int window_size,
-                            int batch_size,
-                            int heads,
-                            int num_seq,
-                            int sequence_length,
-                            int head_offset,
-                            int mask_stride,
-                            int mp_size,
-                            sycl::queue stream)
-{
-    const int total_count = batch_size * heads * num_seq;
-
-    // Scheduling Overview
-    // 4 element unroll with power of 2 `reduce_width` threads to a ceiling of `attn_threads`
-    // Each block should be partitioned into as many `reduce_width` blocks
-    // as can be fit.
-    constexpr int attn_threads = 256;
-    constexpr int min_reduce_width = hw_warp_size;
-    constexpr int internal_unroll = 4;
-
-    // Handle internal unroll then round to next power of 2. Bump up to minimum granularity.
-    const int thread_steps_rounded =
-        next_pow2((sequence_length + internal_unroll - 1) / internal_unroll);
-    const int thread_steps_schedule =
-        (thread_steps_rounded < min_reduce_width) ? min_reduce_width : thread_steps_rounded;
-    // Bound reduce width to the number of threads
-    const int reduce_width = (thread_steps_schedule < attn_threads) ? thread_steps_schedule
-                                                                    : attn_threads;
-    // Scale for the excess
-    const int iterations = thread_steps_schedule / reduce_width;
-    // Should be safe since reduce_width is capped to attn_threads
-    const int partitions = attn_threads / reduce_width;
-
-    // Launch params
-    sycl::range<1> block(attn_threads);
-    sycl::range<1> grid(((total_count + partitions - 1) / partitions) * attn_threads);
-
-    if (sequence_length <= 32768) {
-        if (iterations == 1) {
-            LAUNCH_ATTN_SOFTMAX_V2(1);
-        } else if (iterations == 2) {
-            LAUNCH_ATTN_SOFTMAX_V2(2);
-        } else if (iterations == 4) {
-            LAUNCH_ATTN_SOFTMAX_V2(4);
-        } else if (iterations == 8) {
-            LAUNCH_ATTN_SOFTMAX_V2(8);
-        } else if (iterations == 16) {
-            LAUNCH_ATTN_SOFTMAX_V2(16);
-        } else if (iterations == 32) {
-            LAUNCH_ATTN_SOFTMAX_V2(32);
-        } else if (iterations == 64) {
-            LAUNCH_ATTN_SOFTMAX_V2(64);
-        }
-    } else
-        throw std::runtime_error("Unsupport Seq_Length!");
-}
-
-#define INSTANTIATE_LAUNCH_ATTN_SOFTMAX_V2(T)                  \
-    template void launch_attn_softmax_v2(T* vals,              \
-                                         T* mask,              \
-                                         T* alibi,             \
-                                         float layer_scale,    \
-                                         bool triangular,      \
-                                         bool recompute,       \
-                                         bool local_attention, \
-                                         int window_size,      \
-                                         int batch_size,       \
-                                         int heads,            \
-                                         int num_seq,          \
-                                         int sequence_length,  \
-                                         int head_offset,      \
-                                         int mask_stride,      \
-                                         int mp_size,          \
-                                         sycl::queue stream);
-
-INSTANTIATE_LAUNCH_ATTN_SOFTMAX_V2(float);
-INSTANTIATE_LAUNCH_ATTN_SOFTMAX_V2(bf16);
-INSTANTIATE_LAUNCH_ATTN_SOFTMAX_V2(sycl::half);
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/softmax.dp.cpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/softmax.dp.cpp
new file mode 100644
index 0000000..261bdbf
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/softmax.dp.cpp
@@ -0,0 +1,680 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include <limits>
+#include "conversion_utils.h"
+#include "inference_cuda_layers.h"
+
+#ifndef __HIP_PLATFORM_AMD__
+#endif
+#include <cstdio>
+#include <cstdlib>
+#include <ctime>
+
+#define MAX_REG_SIZE 8
+
+#define minus_infinity -10000.0
+
+void CheckCudaErrorAux(const char* file, unsigned line)
+{
+    /*
+    DPCT1010:11: SYCL uses exceptions to report errors and does not use the error codes. The call
+    was replaced with 0. You need to rewrite this code.
+    */
+    dpct::err0 err = 0;
+    if (err == 0) return;
+    /*
+    DPCT1009:12: SYCL uses exceptions to report errors and does not use the error codes. The
+    original code was commented out and a warning string was inserted. You need to rewrite this
+    code.
+    */
+    std::cerr << "cudaGetErrorString is not supported" /*cudaGetErrorString(err)*/ << "(" << err
+              << ") at " << file << ":" << line << std::endl;
+    throw std::runtime_error("CUDA ERROR!!!\n");
+}
+
+#define CUDA_CHECK_ERROR() CheckCudaErrorAux(__FILE__, __LINE__)
+
+template <typename T, int iterations>
+/*
+DPCT1110:0: The total declared local variable size in device function attn_softmax_v2 exceeds 128
+bytes and may cause high register pressure. Consult with your hardware vendor to find the total
+register size available and adjust the code, or use smaller sub-group size to avoid high register
+pressure.
+*/
+void attn_softmax_v2(T* vals,
+                     T* mask,
+                     T* alibi,
+                     float layer_scale,
+                     bool triangular,
+                     bool recompute,
+                     bool local_attention,
+                     int window_size,
+                     int total_count,
+                     int heads,
+                     int sequence_length,
+                     int num_seq,
+                     int head_offset,
+                     int mask_stride,
+                     int mp_size,
+                     int reduceWidth)
+{
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    sycl::group<3> b = sycl::ext::oneapi::experimental::this_group<3>();
+    sycl::sub_group g = sycl::ext::oneapi::experimental::this_sub_group();
+
+    sycl::float2 low_data[MAX_REG_SIZE];
+    sycl::float2 high_data[MAX_REG_SIZE];
+    const T zero_h = conversion::to<T>(0.f);
+
+    int wid = item_ct1.get_local_id(2) >> 5;
+    int lane = item_ct1.get_local_id(2) & 0x1f;
+    int warp_num = item_ct1.get_local_range(2) >> 5;
+
+    int reduce_blocks = reduceWidth >> 5;
+    int seq_lane = item_ct1.get_local_id(2) % reduceWidth;
+
+    auto& partialSum = *sycl::ext::oneapi::group_local_memory_for_overwrite<float[MAX_WARP_NUM]>(
+        sycl::ext::oneapi::experimental::this_group<3>());
+
+    int iter_offset = item_ct1.get_group(2) * (warp_num / reduce_blocks) + (wid / reduce_blocks);
+    int batch_idx = iter_offset / (num_seq * heads);
+    int alibi_offset = batch_idx * heads * mp_size + head_offset;
+    int mask_offset = batch_idx * mask_stride + (iter_offset % mask_stride);
+
+    if (iter_offset < total_count) {
+        vals += (iter_offset * sequence_length);
+
+        alibi_offset = (alibi_offset + ((iter_offset / num_seq) % heads)) * sequence_length;
+        mask_offset = mask_offset * sequence_length;
+        int seq_id = iter_offset % num_seq;
+
+        int real_seq_id = seq_id + (num_seq == sequence_length ? 0 : sequence_length);
+        int window_stride4 = (local_attention && (real_seq_id >> 2) > (window_size >> 2))
+                                 ? (real_seq_id >> 2) - (window_size >> 2)
+                                 : 0;
+        int window_stride =
+            (local_attention && real_seq_id >= window_size) ? real_seq_id - window_size : -1;
+
+        float max_val = minus_infinity;
+        // if (lane == 0) printf("%d, %d: %d \n", wid, blockIdx.x, mask_offset);
+        for (int i = 0; i < iterations; i++) {
+            int data_id = i * (reduceWidth << 2) + (seq_lane);
+            bool check = (data_id >> 2) >= window_stride4;
+            bool low_x_check = check && (data_id < sequence_length) &&
+                               (!triangular || (data_id <= seq_id)) && (data_id > window_stride);
+            bool low_y_check = check && ((data_id + reduceWidth) < sequence_length) &&
+                               (!triangular || ((data_id + reduceWidth) <= seq_id)) &&
+                               ((data_id + reduceWidth) > window_stride);
+            bool high_x_check = check && ((data_id + reduceWidth * 2) < sequence_length) &&
+                                (!triangular || ((data_id + reduceWidth * 2) <= seq_id)) &&
+                                ((data_id + reduceWidth * 2) > window_stride);
+            bool high_y_check = check && ((data_id + reduceWidth * 3) < sequence_length) &&
+                                (!triangular || ((data_id + reduceWidth * 3) <= seq_id)) &&
+                                ((data_id + reduceWidth * 3) > window_stride);
+
+            if (mask && alibi) {
+                low_data[i].x() = low_x_check
+                                      ? conversion::to<float>(vals[data_id]) * layer_scale +
+                                            (conversion::to<float>(alibi[data_id + alibi_offset])) +
+                                            (conversion::to<float>(mask[data_id + mask_offset]))
+                                      : minus_infinity;
+                low_data[i].y() =
+                    low_y_check
+                        ? conversion::to<float>(vals[data_id + reduceWidth]) * layer_scale +
+                              (conversion::to<float>(alibi[data_id + alibi_offset + reduceWidth])) +
+                              (conversion::to<float>(mask[data_id + mask_offset + reduceWidth]))
+                        : minus_infinity;
+                high_data[i].x() =
+                    high_x_check
+                        ? conversion::to<float>(vals[data_id + reduceWidth * 2]) * layer_scale +
+                              (conversion::to<float>(
+                                  alibi[data_id + alibi_offset + reduceWidth * 2])) +
+                              (conversion::to<float>(mask[data_id + mask_offset + reduceWidth * 2]))
+                        : minus_infinity;
+                high_data[i].y() =
+                    high_y_check
+                        ? conversion::to<float>(vals[data_id + reduceWidth * 3]) * layer_scale +
+                              (conversion::to<float>(
+                                  alibi[data_id + alibi_offset + reduceWidth * 3])) +
+                              (conversion::to<float>(mask[data_id + mask_offset + reduceWidth * 3]))
+                        : minus_infinity;
+            } else if (mask) {
+                low_data[i].x() = low_x_check
+                                      ? conversion::to<float>(vals[data_id]) * layer_scale +
+                                            (conversion::to<float>(mask[data_id + mask_offset]))
+                                      : minus_infinity;
+                low_data[i].y() =
+                    low_y_check
+                        ? conversion::to<float>(vals[data_id + reduceWidth]) * layer_scale +
+                              (conversion::to<float>(mask[data_id + mask_offset + reduceWidth]))
+                        : minus_infinity;
+                high_data[i].x() =
+                    high_x_check
+                        ? conversion::to<float>(vals[data_id + reduceWidth * 2]) * layer_scale +
+                              (conversion::to<float>(mask[data_id + mask_offset + reduceWidth * 2]))
+                        : minus_infinity;
+                high_data[i].y() =
+                    high_y_check
+                        ? conversion::to<float>(vals[data_id + reduceWidth * 3]) * layer_scale +
+                              (conversion::to<float>(mask[data_id + mask_offset + reduceWidth * 3]))
+                        : minus_infinity;
+            } else if (alibi) {
+                low_data[i].x() = low_x_check
+                                      ? conversion::to<float>(vals[data_id]) * layer_scale +
+                                            (conversion::to<float>(alibi[data_id + alibi_offset]))
+                                      : minus_infinity;
+                low_data[i].y() =
+                    low_y_check
+                        ? conversion::to<float>(vals[data_id + reduceWidth]) * layer_scale +
+                              (conversion::to<float>(alibi[data_id + alibi_offset + reduceWidth]))
+                        : minus_infinity;
+                high_data[i].x() =
+                    high_x_check
+                        ? conversion::to<float>(vals[data_id + reduceWidth * 2]) * layer_scale +
+                              (conversion::to<float>(
+                                  alibi[data_id + alibi_offset + reduceWidth * 2]))
+                        : minus_infinity;
+                high_data[i].y() =
+                    high_y_check
+                        ? conversion::to<float>(vals[data_id + reduceWidth * 3]) * layer_scale +
+                              (conversion::to<float>(
+                                  alibi[data_id + alibi_offset + reduceWidth * 3]))
+                        : minus_infinity;
+            } else {
+                low_data[i].x() = low_x_check ? conversion::to<float>(vals[data_id]) * layer_scale
+                                              : minus_infinity;
+                low_data[i].y() =
+                    low_y_check ? conversion::to<float>(vals[data_id + reduceWidth]) * layer_scale
+                                : minus_infinity;
+                high_data[i].x() =
+                    high_x_check
+                        ? conversion::to<float>(vals[data_id + reduceWidth * 2]) * layer_scale
+                        : minus_infinity;
+                high_data[i].y() =
+                    high_y_check
+                        ? conversion::to<float>(vals[data_id + reduceWidth * 3]) * layer_scale
+                        : minus_infinity;
+            }
+
+            // if(lane == 0) printf("%f , %d, %d \n", low_data[i].x, data_id, seq_id);
+            max_val = (low_data[i].x() > max_val ? low_data[i].x() : max_val);
+            max_val = (low_data[i].y() > max_val ? low_data[i].y() : max_val);
+            max_val = (high_data[i].x() > max_val ? high_data[i].x() : max_val);
+            max_val = (high_data[i].y() > max_val ? high_data[i].y() : max_val);
+        }
+
+        for (int i = 1; i < WARP_SIZE; i *= 2) {
+            auto temp = sycl::permute_group_by_xor(
+                sycl::ext::oneapi::experimental::this_sub_group(), max_val, i);
+            max_val = (temp > max_val ? temp : max_val);
+        }
+
+        if (reduceWidth > WARP_SIZE) {
+            if (lane == 0) partialSum[wid] = max_val;
+            /*
+            DPCT1065:1: Consider replacing sycl::nd_item::barrier() with
+            sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if
+            there is no access to global memory.
+            */
+            item_ct1.barrier();
+
+            if (lane < warp_num) max_val = partialSum[lane];
+
+            /*
+            DPCT1065:2: Consider replacing sycl::nd_item::barrier() with
+            sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if
+            there is no access to global memory.
+            */
+            item_ct1.barrier();
+
+            for (int i = 1; i < reduce_blocks; i *= 2) {
+                auto temp = sycl::permute_group_by_xor(
+                    sycl::ext::oneapi::experimental::this_sub_group(), max_val, i);
+                max_val = (temp > max_val ? temp : max_val);
+            }
+
+            /*
+            DPCT1007:13: Migration of cooperative_groups::thread_block_tile::shfl is not supported.
+            */
+            max_val = g.shuffle(max_val, item_ct1.get_local_id(2) / WARP_SIZE);
+        }
+        float sum = 0;
+        for (int i = 0; i < iterations; i++) {
+            low_data[i].x() = sycl::native::exp(low_data[i].x() - max_val);
+            low_data[i].y() = sycl::native::exp(low_data[i].y() - max_val);
+            high_data[i].x() = sycl::native::exp(high_data[i].x() - max_val);
+            high_data[i].y() = sycl::native::exp(high_data[i].y() - max_val);
+
+            sum += (low_data[i].x() + low_data[i].y() + high_data[i].x() + high_data[i].y());
+        }
+
+        for (int i = 1; i < WARP_SIZE; i *= 2) sum +=
+            sycl::permute_group_by_xor(sycl::ext::oneapi::experimental::this_sub_group(), sum, i);
+
+        if (reduceWidth > WARP_SIZE) {
+            if (lane == 0) partialSum[wid] = sum;
+            /*
+            DPCT1065:3: Consider replacing sycl::nd_item::barrier() with
+            sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if
+            there is no access to global memory.
+            */
+            item_ct1.barrier();
+
+            if (lane < warp_num) sum = partialSum[lane];
+
+            /*
+            DPCT1065:4: Consider replacing sycl::nd_item::barrier() with
+            sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if
+            there is no access to global memory.
+            */
+            item_ct1.barrier();
+
+            for (int i = 1; i < reduce_blocks; i *= 2) {
+                sum += sycl::permute_group_by_xor(
+                    sycl::ext::oneapi::experimental::this_sub_group(), sum, i);
+            }
+
+            /*
+            DPCT1007:14: Migration of cooperative_groups::thread_block_tile::shfl is not supported.
+            */
+            sum = g.shuffle(sum, item_ct1.get_local_id(2) / WARP_SIZE);
+        }
+        sum += 1e-6;
+        for (int i = 0; i < iterations; i++) {
+            int data_id = i * (reduceWidth << 2) + (seq_lane);
+            if (data_id < sequence_length) {
+                vals[data_id] = conversion::to<T>(low_data[i].x() / sum);
+                if ((data_id + reduceWidth) < sequence_length)
+                    vals[data_id + reduceWidth] = conversion::to<T>(low_data[i].y() / sum);
+                if ((data_id + reduceWidth * 2) < sequence_length)
+                    vals[data_id + reduceWidth * 2] = conversion::to<T>(high_data[i].x() / sum);
+                if ((data_id + reduceWidth * 3) < sequence_length)
+                    vals[data_id + reduceWidth * 3] = conversion::to<T>(high_data[i].y() / sum);
+            }
+        }
+    }
+}
+
+template <int iterations>
+/*
+DPCT1110:5: The total declared local variable size in device function attn_softmax_v2 exceeds 128
+bytes and may cause high register pressure. Consult with your hardware vendor to find the total
+register size available and adjust the code, or use smaller sub-group size to avoid high register
+pressure.
+*/
+void attn_softmax_v2(float* vals,
+                     float* attn_mask,
+                     float* alibi,
+                     float layer_scale,
+                     bool triangular,
+                     bool recompute,
+                     bool local_attention,
+                     int window_size,
+                     int total_count,
+                     int heads,
+                     int sequence_length,
+                     int num_seq,
+                     int head_offset,
+                     int mask_stride,
+                     int mp_size,
+                     int reduceWidth)
+{
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    sycl::group<3> b = sycl::ext::oneapi::experimental::this_group<3>();
+    sycl::sub_group g = sycl::ext::oneapi::experimental::this_sub_group();
+
+    sycl::float4 data[MAX_REG_SIZE];
+
+    int wid = item_ct1.get_local_id(2) >> 5;
+    int lane = item_ct1.get_local_id(2) & 0x1f;
+    int warp_num = item_ct1.get_local_range(2) >> 5;
+
+    int reduce_blocks = reduceWidth >> 5;
+    int seq_lane = item_ct1.get_local_id(2) % reduceWidth;
+
+    auto& partialSum = *sycl::ext::oneapi::group_local_memory_for_overwrite<float[MAX_WARP_NUM]>(
+        sycl::ext::oneapi::experimental::this_group<3>());
+
+    int iter_offset = item_ct1.get_group(2) * (warp_num / reduce_blocks) + (wid / reduce_blocks);
+    if (iter_offset < total_count) {
+        vals += (iter_offset * sequence_length);
+
+        int batch_idx = iter_offset / (num_seq * heads);
+        int mask_offset = batch_idx * mask_stride + (iter_offset % mask_stride);
+        mask_offset = mask_offset * sequence_length;
+        int seq_id = iter_offset % num_seq;
+
+        int real_seq_id = seq_id + (num_seq == sequence_length ? 0 : sequence_length);
+        int window_stride4 = (local_attention && (real_seq_id >> 2) > (window_size >> 2))
+                                 ? (real_seq_id >> 2) - (window_size >> 2)
+                                 : 0;
+        int window_stride =
+            (local_attention && real_seq_id >= window_size) ? real_seq_id - window_size : -1;
+
+        float max_val = minus_infinity;
+
+        for (int i = 0; i < iterations; i++) {
+            int data_id = i * (reduceWidth << 2) + (seq_lane);
+            bool check = (data_id >> 2) >= window_stride4;
+            bool x_check = check && (data_id < sequence_length) &&
+                           (!triangular || (data_id <= seq_id)) && (data_id > window_stride);
+            bool y_check = check && ((data_id + reduceWidth) < sequence_length) &&
+                           (!triangular || ((data_id + reduceWidth) <= seq_id)) &&
+                           ((data_id + reduceWidth) > window_stride);
+            bool z_check = check && ((data_id + reduceWidth * 2) < sequence_length) &&
+                           (!triangular || ((data_id + reduceWidth * 2) <= seq_id)) &&
+                           ((data_id + reduceWidth * 2) > window_stride);
+            bool w_check = check && ((data_id + reduceWidth * 3) < sequence_length) &&
+                           (!triangular || ((data_id + reduceWidth * 3) <= seq_id)) &&
+                           ((data_id + reduceWidth * 3) > window_stride);
+
+            if (attn_mask) {
+                data[i].x() = x_check ? vals[data_id] + attn_mask[data_id + mask_offset]
+                                      : minus_infinity;
+                data[i].y() = y_check ? vals[data_id + reduceWidth] +
+                                            attn_mask[data_id + mask_offset + reduceWidth]
+                                      : minus_infinity;
+                data[i].z() = z_check ? vals[data_id + reduceWidth * 2] +
+                                            attn_mask[data_id + mask_offset + reduceWidth * 2]
+                                      : minus_infinity;
+                data[i].w() = w_check ? vals[data_id + reduceWidth * 3] +
+                                            attn_mask[data_id + mask_offset + reduceWidth * 3]
+                                      : minus_infinity;
+            } else {
+                data[i].x() = x_check ? vals[data_id] : minus_infinity;
+                data[i].y() = y_check ? vals[data_id + reduceWidth] : minus_infinity;
+                data[i].z() = z_check ? vals[data_id + reduceWidth * 2] : minus_infinity;
+                data[i].w() = w_check ? vals[data_id + reduceWidth * 3] : minus_infinity;
+            }
+
+            max_val = (data[i].x() > max_val ? data[i].x() : max_val);
+            max_val = (data[i].y() > max_val ? data[i].y() : max_val);
+            max_val = (data[i].z() > max_val ? data[i].z() : max_val);
+            max_val = (data[i].w() > max_val ? data[i].w() : max_val);
+        }
+
+        for (int i = 1; i < WARP_SIZE; i *= 2) {
+            auto temp = sycl::permute_group_by_xor(
+                sycl::ext::oneapi::experimental::this_sub_group(), max_val, i);
+            max_val = (temp > max_val ? temp : max_val);
+        }
+
+        if (reduceWidth > WARP_SIZE) {
+            if (lane == 0) partialSum[wid] = max_val;
+            /*
+            DPCT1065:6: Consider replacing sycl::nd_item::barrier() with
+            sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if
+            there is no access to global memory.
+            */
+            item_ct1.barrier();
+
+            if (lane < warp_num) max_val = partialSum[lane];
+
+            /*
+            DPCT1065:7: Consider replacing sycl::nd_item::barrier() with
+            sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if
+            there is no access to global memory.
+            */
+            item_ct1.barrier();
+
+            for (int i = 1; i < reduce_blocks; i *= 2) {
+                auto temp = sycl::permute_group_by_xor(
+                    sycl::ext::oneapi::experimental::this_sub_group(), max_val, i);
+                max_val = (temp > max_val ? temp : max_val);
+            }
+
+            /*
+            DPCT1007:15: Migration of cooperative_groups::thread_block_tile::shfl is not supported.
+            */
+            max_val = g.shuffle(max_val, item_ct1.get_local_id(2) / WARP_SIZE);
+        }
+
+        float sum = 0;
+        for (int i = 0; i < iterations; i++) {
+            data[i].x() = sycl::native::exp(data[i].x() - max_val);
+            data[i].y() = sycl::native::exp(data[i].y() - max_val);
+            data[i].z() = sycl::native::exp(data[i].z() - max_val);
+            data[i].w() = sycl::native::exp(data[i].w() - max_val);
+
+            sum += (data[i].x() + data[i].y() + data[i].z() + data[i].w());
+        }
+
+        for (int i = 1; i < WARP_SIZE; i *= 2) sum +=
+            sycl::permute_group_by_xor(sycl::ext::oneapi::experimental::this_sub_group(), sum, i);
+
+        if (reduceWidth > WARP_SIZE) {
+            if (lane == 0) partialSum[wid] = sum;
+            /*
+            DPCT1065:8: Consider replacing sycl::nd_item::barrier() with
+            sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if
+            there is no access to global memory.
+            */
+            item_ct1.barrier();
+
+            if (lane < warp_num) sum = partialSum[lane];
+
+            /*
+            DPCT1065:9: Consider replacing sycl::nd_item::barrier() with
+            sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if
+            there is no access to global memory.
+            */
+            item_ct1.barrier();
+
+            for (int i = 1; i < reduce_blocks; i *= 2) {
+                sum += sycl::permute_group_by_xor(
+                    sycl::ext::oneapi::experimental::this_sub_group(), sum, i);
+            }
+
+            /*
+            DPCT1007:16: Migration of cooperative_groups::thread_block_tile::shfl is not supported.
+            */
+            sum = g.shuffle(sum, item_ct1.get_local_id(2) / WARP_SIZE);
+        }
+        sum += 1e-6;
+
+        for (int i = 0; i < iterations; i++) {
+            int data_id = i * (reduceWidth << 2) + (seq_lane);
+            if (data_id < sequence_length) {
+                vals[data_id] = data[i].x() / sum;
+                if ((data_id + reduceWidth) < sequence_length)
+                    vals[data_id + reduceWidth] = data[i].y() / sum;
+                if ((data_id + reduceWidth * 2) < sequence_length)
+                    vals[data_id + reduceWidth * 2] = data[i].z() / sum;
+                if ((data_id + reduceWidth * 3) < sequence_length)
+                    vals[data_id + reduceWidth * 3] = data[i].w() / sum;
+            }
+        }
+    }
+}
+
+/*
+DPCT1049:10: The work-group size passed to the SYCL kernel may exceed the limit. To get the device
+limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
+*/
+#define LAUNCH_ATTN_SOFTMAX_V2(iterations)                                                        \
+  {                                                                                               \
+    dpct::has_capability_or_fail(stream->get_device(), {sycl::aspect::fp64, sycl::aspect::fp16}); \
+    stream->submit([&](sycl::handler& cgh) {                                                      \
+      T* vals_ct0 = vals;                                                                         \
+      T* mask_ct1 = mask;                                                                         \
+      T* alibi_ct2 = alibi;                                                                       \
+      auto layer_scale_ct3 = layer_scale;                                                         \
+      auto triangular_ct4 = triangular;                                                           \
+      auto recompute_ct5 = recompute;                                                             \
+      auto local_attention_ct6 = local_attention;                                                 \
+      auto window_size_ct7 = window_size;                                                         \
+      auto total_count_ct8 = total_count;                                                         \
+      auto heads_ct9 = heads;                                                                     \
+      auto sequence_length_ct10 = sequence_length;                                                \
+      auto num_seq_ct11 = num_seq;                                                                \
+      auto head_offset_ct12 = head_offset;                                                        \
+      auto mask_stride_ct13 = mask_stride;                                                        \
+      auto mp_size_ct14 = mp_size;                                                                \
+      auto reduce_width_ct15 = reduce_width;                                                      \
+                                                                                                  \
+      cgh.parallel_for(sycl::nd_range<3>(grid * block, block),                                    \
+                       [=](sycl::nd_item<3> item_ct1) [[intel::reqd_sub_group_size(32)]] {        \
+                         attn_softmax_v2<T, iterations>(vals_ct0,                                             \
+                                            mask_ct1,                                             \
+                                            alibi_ct2,                                            \
+                                            layer_scale_ct3,                                      \
+                                            triangular_ct4,                                       \
+                                            recompute_ct5,                                        \
+                                            local_attention_ct6,                                  \
+                                            window_size_ct7,                                      \
+                                            total_count_ct8,                                      \
+                                            heads_ct9,                                            \
+                                            sequence_length_ct10,                                 \
+                                            num_seq_ct11,                                         \
+                                            head_offset_ct12,                                     \
+                                            mask_stride_ct13,                                     \
+                                            mp_size_ct14,                                         \
+                                            reduce_width_ct15);                                   \
+                       });                                                                        \
+    });                                                                                           \
+  }
+
+template <typename T>
+void launch_attn_softmax_v2(T* vals,
+                            T* mask,
+                            T* alibi,
+                            float layer_scale,
+                            bool triangular,
+                            bool recompute,
+                            bool local_attention,
+                            int window_size,
+                            int batch_size,
+                            int heads,
+                            int num_seq,
+                            int sequence_length,
+                            int head_offset,
+                            int mask_stride,
+                            int mp_size,
+                            dpct::queue_ptr stream)
+{
+    const int total_count = batch_size * heads * num_seq;
+
+    // Scheduling Overview
+    // 4 element unroll with power of 2 `reduce_width` threads to a ceiling of `attn_threads`
+    // Each block should be partitioned into as many `reduce_width` blocks
+    // as can be fit.
+    constexpr int attn_threads = 256;
+    constexpr int min_reduce_width = hw_warp_size;
+    constexpr int internal_unroll = 4;
+
+    // Handle internal unroll then round to next power of 2. Bump up to minimum granularity.
+    const int thread_steps_rounded =
+        next_pow2((sequence_length + internal_unroll - 1) / internal_unroll);
+    const int thread_steps_schedule =
+        (thread_steps_rounded < min_reduce_width) ? min_reduce_width : thread_steps_rounded;
+    // Bound reduce width to the number of threads
+    const int reduce_width = (thread_steps_schedule < attn_threads) ? thread_steps_schedule
+                                                                    : attn_threads;
+    // Scale for the excess
+    const int iterations = thread_steps_schedule / reduce_width;
+    // Should be safe since reduce_width is capped to attn_threads
+    const int partitions = attn_threads / reduce_width;
+
+    // Launch params
+    sycl::range<3> grid(1, 1, (total_count + partitions - 1) / partitions);
+    sycl::range<3> block(1, 1, attn_threads);
+
+    if (sequence_length <= 32768) {
+        if (iterations == 1) {
+            LAUNCH_ATTN_SOFTMAX_V2(1);
+        } else if (iterations == 2) {
+            LAUNCH_ATTN_SOFTMAX_V2(2);
+        } else if (iterations == 4) {
+            LAUNCH_ATTN_SOFTMAX_V2(4);
+        } else if (iterations == 8) {
+            LAUNCH_ATTN_SOFTMAX_V2(8);
+        } else if (iterations == 16) {
+            LAUNCH_ATTN_SOFTMAX_V2(16);
+        } else if (iterations == 32) {
+            LAUNCH_ATTN_SOFTMAX_V2(32);
+        } else if (iterations == 64) {
+            LAUNCH_ATTN_SOFTMAX_V2(64);
+        }
+    } else
+        throw std::runtime_error("Unsupport Seq_Length!");
+}
+
+#define INSTANTIATE_LAUNCH_ATTN_SOFTMAX_V2(T)                  \
+    template void launch_attn_softmax_v2(T* vals,              \
+                                         T* mask,              \
+                                         T* alibi,             \
+                                         float layer_scale,    \
+                                         bool triangular,      \
+                                         bool recompute,       \
+                                         bool local_attention, \
+                                         int window_size,      \
+                                         int batch_size,       \
+                                         int heads,            \
+                                         int num_seq,          \
+                                         int sequence_length,  \
+                                         int head_offset,      \
+                                         int mask_stride,      \
+                                         int mp_size,          \
+                                         dpct::queue_ptr stream);
+
+INSTANTIATE_LAUNCH_ATTN_SOFTMAX_V2(float);
+#ifdef BF16_AVAILABLE
+INSTANTIATE_LAUNCH_ATTN_SOFTMAX_V2(sycl::ext::oneapi::bfloat16);
+#endif
+INSTANTIATE_LAUNCH_ATTN_SOFTMAX_V2(sycl::half);
+
+#define DEF_ATTN_SOFTMAX_V2_HALF(_iter)                                    \
+    template void attn_softmax_v2<sycl::half, _iter>(sycl::half * vals,    \
+                                                     sycl::half * mask,    \
+                                                     sycl::half * alibi,   \
+                                                     float layer_scale,    \
+                                                     bool triangular,      \
+                                                     bool recompute,       \
+                                                     bool local_attention, \
+                                                     int window_size,      \
+                                                     int total_count,      \
+                                                     int heads,            \
+                                                     int sequence_length,  \
+                                                     int num_seq,          \
+                                                     int head_offset,      \
+                                                     int mask_stride,      \
+                                                     int mp_size,          \
+                                                     int reduceWidth)
+
+#define DEF_ATTN_SOFTMAX_V2_BF16(_iter)                                \
+    template void attn_softmax_v2<sycl::ext::oneapi::bfloat16, _iter>( \
+        sycl::ext::oneapi::bfloat16 * vals,                            \
+        sycl::ext::oneapi::bfloat16 * mask,                            \
+        sycl::ext::oneapi::bfloat16 * alibi,                           \
+        float layer_scale,                                             \
+        bool triangular,                                               \
+        bool recompute,                                                \
+        bool local_attention,                                          \
+        int window_size,                                               \
+        int total_count,                                               \
+        int heads,                                                     \
+        int sequence_length,                                           \
+        int num_seq,                                                   \
+        int head_offset,                                               \
+        int mask_stride,                                               \
+        int mp_size,                                                   \
+        int reduceWidth)
+
+#define FOREACH_ITERATIONS(cb) \
+    cb(1);                     \
+    cb(2);                     \
+    cb(4);                     \
+    cb(8);                     \
+    cb(16);                    \
+    cb(32);                    \
+    cb(64)
+
+FOREACH_ITERATIONS(DEF_ATTN_SOFTMAX_V2_HALF);
+#ifdef BF16_AVAILABLE
+FOREACH_ITERATIONS(DEF_ATTN_SOFTMAX_V2_BF16);
+#endif
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/transform.dp.cpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/transform.dp.cpp
new file mode 100644
index 0000000..a035fca
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/csrc/transform.dp.cpp
@@ -0,0 +1,829 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+#ifndef __HIP_PLATFORM_AMD__
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#endif
+#include "conversion_utils.h"
+#include "inference_cuda_layers.h"
+
+// only used to avoid compilation error due to lack of definition.
+#ifndef BF16_AVAILABLE
+using __nv_bfloat162 = sycl::half2;
+#endif
+
+// Bias add
+
+void bias_add_transform_0213(float* output,
+                                        float* k_cache,
+                                        float* v_cache,
+                                        const float* vals,
+                                        const float* bias,
+                                        int hidden_dim,
+                                        int seq_length,
+                                        unsigned seq_offset,
+                                        int heads,
+                                        int head_stride,
+                                        int num_kv,
+                                        int rotary_dim,
+                                        bool rotate_half,
+                                        bool rotate_every_two,
+                                        int head_ext,
+                                        int max_out_tokens,
+                                        float rope_theta)
+{
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    int d0_stride = hidden_dim * seq_length;
+    int d1_stride = hidden_dim;
+    int d2_stride = hidden_dim / heads;
+
+    int d0 = item_ct1.get_group(2);              // Batch
+    int d1 = item_ct1.get_group(1);              // Sequence ID (0-127)
+    int cnt = item_ct1.get_group(0) / head_ext;  // Hidden count
+    int d2 = item_ct1.get_local_id(1) +
+             (item_ct1.get_group(0) % head_ext) * (heads / head_ext);  // Head (0-11)
+    int d3 = item_ct1.get_local_id(2);                                 // Values (groups of 4)
+
+    int d2_out_stride = d2_stride * (cnt == 0 ? seq_length : max_out_tokens);
+    int d0_out_stride = hidden_dim * (cnt == 0 ? seq_length : max_out_tokens);
+
+    const sycl::float4* vals_vec = reinterpret_cast<const sycl::float4*>(vals);
+    sycl::float4* output_vec =
+        reinterpret_cast<sycl::float4*>(cnt == 0 ? output : (cnt == 1 ? k_cache : v_cache));
+
+    vals_vec += (d0 * (d1_stride + num_kv * 2 * d2_stride) * seq_length);
+    vals_vec += d1 * (d1_stride + num_kv * 2 * d2_stride);
+    vals_vec += (cnt == 0 ? 0 : d1_stride) + (cnt == 0 ? 0 : (cnt - 1) * num_kv * d2_stride);
+    vals_vec += ((cnt == 0 ? d2 : (d2 / head_stride)) * d2_stride);
+
+    output_vec += (d1 * d2_stride);
+    output_vec += (d0 * d0_out_stride);
+    output_vec += (d2 * d2_out_stride);
+
+    unsigned seq_id = d1 + seq_offset;
+    sycl::float4 inputs = vals_vec[d3];
+    int lane = d3 & 0x1f;
+    if (cnt < 2 && rotary_dim > 0 && d3 < rotary_dim) {
+        sycl::float4 q = vals_vec[d3];
+        sycl::float2* q_f = reinterpret_cast<sycl::float2*>(&q);
+        if (rotate_every_two) {
+#pragma unroll
+            for (int o = 0; o < 2; o++) {
+                float inv_freq = (float)(((d3 << 1) + o) * 2) / (float)(rotary_dim << 2);
+                inv_freq = 1.0 / dpct::pow(rope_theta, inv_freq) * (float)seq_id;
+                q_f[o].x() =
+                    (-1.0 * q_f[o].y() * sycl::sin(inv_freq) + q_f[o].x() * sycl::cos(inv_freq));
+                q_f[o].y() = (q_f[o].x() * sycl::sin(inv_freq) + q_f[o].y() * sycl::cos(inv_freq));
+            }
+        }
+        output_vec[d3] = q;
+    } else
+        output_vec[d3] = inputs;
+}
+
+#define ATTN_H 3
+#define MAX_SEQ_LINE 10
+
+template <typename T>
+/*
+DPCT1110:0: The total declared local variable size in device function bias_add_transform_0213
+exceeds 128 bytes and may cause high register pressure. Consult with your hardware vendor to find
+the total register size available and adjust the code, or use smaller sub-group size to avoid high
+register pressure.
+*/
+void bias_add_transform_0213(T* output,  // q
+                             T* k_cache,
+                             T* v_cache,
+                             const T* vals,  // qkv
+                             const T* bias,
+                             int hidden_dim,
+                             int seq_length,
+                             unsigned seq_offset,
+                             int all_tokens,
+                             int heads,
+                             int head_stride,
+                             int num_kv,
+                             int rotary_dim,
+                             bool rotate_half,
+                             bool rotate_every_two,
+                             int head_ext,
+                             int max_out_tokens,
+                             float rope_theta)
+{
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    using T2 = typename std::conditional<std::is_same<T, sycl::half>::value,
+                                         sycl::half2,
+                                         sycl::marray<sycl::ext::oneapi::bfloat16, 2>>::type;
+    unsigned half_dim = (rotary_dim << 3) >> 1;
+    int d0_stride = hidden_dim * seq_length;
+    int d1_stride = hidden_dim;
+    int d2_stride = hidden_dim / heads;
+
+    int d0 = item_ct1.get_group(2);              // Batch
+    int d1 = item_ct1.get_group(1);              // Sequence ID (0-127)
+    int cnt = item_ct1.get_group(0) / head_ext;  // Hidden count
+    int d2 = item_ct1.get_local_id(1) +
+             (item_ct1.get_group(0) % head_ext) * (heads / head_ext);  // Head (0-11)
+    int d3 = item_ct1.get_local_id(2);                                 // Values (groups of 4)
+
+    int d2_out_stride = d2_stride * (cnt == 0 ? seq_length : max_out_tokens);
+    int d0_out_stride = hidden_dim * (cnt == 0 ? seq_length : max_out_tokens);
+
+    sycl::float4 vals_arr;
+    sycl::float4 output_arr;
+
+    T2* vals_half = reinterpret_cast<T2*>(&vals_arr);
+    T2* output_half = reinterpret_cast<T2*>(&output_arr);
+
+    const sycl::float4* vals_vec = reinterpret_cast<const sycl::float4*>(vals);
+    sycl::float4* output_vec =
+        reinterpret_cast<sycl::float4*>(cnt == 0 ? output : (cnt == 1 ? k_cache : v_cache));
+
+    vals_vec += (d0 * (d1_stride + num_kv * 2 * d2_stride) * seq_length);
+    vals_vec += (d1 * (d1_stride + num_kv * 2 * d2_stride));
+    vals_vec += (cnt == 0 ? 0 : d1_stride) + (cnt == 0 ? 0 : (cnt - 1) * num_kv * d2_stride);
+    vals_vec += ((cnt == 0 ? d2 : (d2 / head_stride)) * d2_stride);
+
+    output_vec += (d1 * d2_stride);
+    output_vec += (d0 * d0_out_stride);
+    output_vec += (d2 * d2_out_stride);
+
+    unsigned seq_id = d1 + seq_offset;
+
+    int lane = d3 & 0x1f;
+    if (cnt < 2 && rotary_dim > 0 && d3 < rotary_dim) {
+        sycl::float4 q = vals_vec[d3];
+        T2* q_h = reinterpret_cast<T2*>(&q);
+        if (rotate_every_two) {
+#pragma unroll
+            for (int o = 0; o < 4; o++) {
+                float inv_freq = (float)(((d3 << 2) + o) * 2) / (float)(rotary_dim << 3);
+                inv_freq = 1.0 / dpct::pow(rope_theta, inv_freq) * (float)seq_id;
+                float q_data[2];
+                q_data[0] = conversion::to<float>(q_h[o][0]);
+                q_data[1] = conversion::to<float>(q_h[o][1]);
+                q_h[o][0] = conversion::to<T>(-1.0 * q_data[1] * sycl::sin(inv_freq) +
+                                              q_data[0] * sycl::cos(inv_freq));
+                q_h[o][1] = conversion::to<T>(q_data[0] * sycl::sin(inv_freq) +
+                                              q_data[1] * sycl::cos(inv_freq));
+            }
+        }
+        output_vec[d3] = q;
+    } else
+        output_vec[d3] = vals_vec[d3];
+}
+
+// [B S C*H] - > C * [B A S N]
+template <>
+void launch_bias_add_transform_0213<float>(float* output,
+                                           float* k_cache,
+                                           float* v_cache,
+                                           const float* vals,
+                                           const float* bias,
+                                           int batch_size,
+                                           int seq_length,
+                                           unsigned seq_offset,
+                                           int all_tokens,
+                                           int hidden_dim,
+                                           int heads,
+                                           int num_kv,
+                                           int rotary_dim,
+                                           bool rotate_half,
+                                           bool rotate_every_two,
+                                           dpct::queue_ptr stream,
+                                           int trans_count,
+                                           int max_out_tokens,
+                                           float rope_theta)
+{
+    hidden_dim >>= 2;
+    int head_ext = (hidden_dim - 1) / MAX_THREADS + 1;
+
+    sycl::range<3> block_dim(1, (heads / head_ext), hidden_dim / heads);
+    sycl::range<3> grid_dim((trans_count * head_ext), seq_length, batch_size);
+
+    /*
+    DPCT1049:1: The work-group size passed to the SYCL kernel may exceed the limit. To get the
+    device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
+    */
+    stream->parallel_for(sycl::nd_range<3>(grid_dim * block_dim, block_dim),
+                         [=](sycl::nd_item<3> item_ct1) {
+                             bias_add_transform_0213(output,
+                                                     k_cache,
+                                                     v_cache,
+                                                     vals,
+                                                     bias,
+                                                     hidden_dim,
+                                                     seq_length,
+                                                     seq_offset,
+                                                     heads,
+                                                     num_kv > 0 ? (heads / num_kv) : 1,
+                                                     num_kv > 0 ? num_kv : heads,
+                                                     rotary_dim >> 2,
+                                                     rotate_half,
+                                                     rotate_every_two,
+                                                     head_ext,
+                                                     max_out_tokens,
+                                                     rope_theta);
+                         });
+}
+
+template <typename T>
+void launch_bias_add_transform_0213(T* output,
+                                    T* k_cache,
+                                    T* v_cache,
+                                    const T* vals,
+                                    const T* bias,
+                                    int batch_size,
+                                    int seq_length,
+                                    unsigned seq_offset,
+                                    int all_tokens,
+                                    int hidden_dim,
+                                    int heads,
+                                    int num_kv,
+                                    int rotary_dim,
+                                    bool rotate_half,
+                                    bool rotate_every_two,
+                                    dpct::queue_ptr stream,
+                                    int trans_count,
+                                    int max_out_tokens,
+                                    float rope_theta)
+{
+    hidden_dim >>= 3;
+    int head_ext = 1;  // (hidden_dim - 1) / MAX_THREADS + 1;
+    sycl::range<3> block_dim(1, (heads / head_ext), hidden_dim / heads);
+    sycl::range<3> grid_dim((trans_count * head_ext), seq_length, batch_size);
+    /*
+    DPCT1049:2: The work-group size passed to the SYCL kernel may exceed the limit. To get the
+    device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
+    */
+    {
+        dpct::has_capability_or_fail(stream->get_device(),
+                                     {sycl::aspect::fp64, sycl::aspect::fp16});
+        stream->parallel_for(sycl::nd_range<3>(grid_dim * block_dim, block_dim),
+                             [=](sycl::nd_item<3> item_ct1) {
+                                 bias_add_transform_0213(output,
+                                                         k_cache,
+                                                         v_cache,
+                                                         vals,
+                                                         bias,
+                                                         hidden_dim,
+                                                         seq_length,
+                                                         seq_offset,
+                                                         all_tokens,
+                                                         heads,
+                                                         num_kv > 0 ? (heads / num_kv) : 1,
+                                                         num_kv > 0 ? num_kv : heads,
+                                                         rotary_dim >> 3,
+                                                         rotate_half,
+                                                         rotate_every_two,
+                                                         head_ext,
+                                                         max_out_tokens,
+                                                         rope_theta);
+                             });
+    }
+}
+
+#define INSTANTIATE_LAUNCH_BIAS_ADD_TRANSFORM_0213(T)                \
+    template void launch_bias_add_transform_0213<T>(T*,              \
+                                                    T*,              \
+                                                    T*,              \
+                                                    const T*,        \
+                                                    const T*,        \
+                                                    int,             \
+                                                    int,             \
+                                                    unsigned,        \
+                                                    int,             \
+                                                    int,             \
+                                                    int,             \
+                                                    int,             \
+                                                    int,             \
+                                                    bool,            \
+                                                    bool,            \
+                                                    dpct::queue_ptr, \
+                                                    int,             \
+                                                    int,             \
+                                                    float)
+
+#ifdef BF16_AVAILABLE
+INSTANTIATE_LAUNCH_BIAS_ADD_TRANSFORM_0213(sycl::ext::oneapi::bfloat16);
+#endif
+INSTANTIATE_LAUNCH_BIAS_ADD_TRANSFORM_0213(sycl::half);
+
+// Bias add
+
+void pad_add_transform_0213(float* output,
+                                       const float* vals,
+                                       int hidden_dim,
+                                       int seq_length,
+                                       int padded_seq_len,
+                                       int heads,
+                                       int padded_head_size)
+{
+}
+
+template <typename T>
+void pad_add_transform_0213(T* output,
+                                       const T* vals,
+                                       int hidden_dim,
+                                       int seq_length,
+                                       int padded_seq_len,
+                                       int heads,
+                                       int padded_head_size)
+{
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    using T2 = typename std::conditional<std::is_same<T, sycl::half>::value,
+                                         sycl::half2,
+                                         sycl::marray<sycl::ext::oneapi::bfloat16, 2>>::type;
+    sycl::float4 ZERO;
+    const T2 zero_h = conversion::to<T2>(0.f);
+    T2* ZERO_h = reinterpret_cast<T2*>(&ZERO);
+#pragma unroll
+    for (int i = 0; i < 4; i++) ZERO_h[i] = zero_h;
+
+    int d0_stride = hidden_dim * seq_length;
+    int d1_stride = hidden_dim;
+    int d2_stride = hidden_dim / heads;
+
+    int d0 = item_ct1.get_group(2);  // Batch
+    int d1 = item_ct1.get_group(1) * item_ct1.get_local_range(0) +
+             item_ct1.get_local_id(0);  // Sequence ID (0-127)
+    int d2 = item_ct1.get_local_id(1);  // Head (0-11)
+    int d3 = item_ct1.get_local_id(2);  // Values (groups of 4)
+
+    int d2_out_stride = padded_head_size * padded_seq_len;
+    int d0_out_stride = heads * d2_out_stride;
+
+    const sycl::float4* vals_vec = reinterpret_cast<const sycl::float4*>(vals);
+    sycl::float4* output_vec = reinterpret_cast<sycl::float4*>(output);
+
+    vals_vec += (d0 * d0_stride);
+    vals_vec += (d1 * d1_stride);
+    vals_vec += (d2 * d2_stride);
+
+    output_vec += (d1 * padded_head_size);
+    output_vec += (d0 * d0_out_stride);
+    output_vec += (d2 * d2_out_stride);
+
+    if (d3 < d2_stride && d1 < seq_length)
+        output_vec[d3] = vals_vec[d3];
+    else
+        output_vec[d3] = ZERO;
+}
+
+// [B S C*H] - > C * [B A S N]
+template <>
+void launch_pad_add_transform_0213<float>(float* output,
+                                          const float* vals,
+                                          int batch_size,
+                                          int hidden_dim,
+                                          int seq_length,
+                                          int padded_seq_len,
+                                          int heads,
+                                          int padded_head_size,
+                                          dpct::queue_ptr stream)
+{
+}
+
+template <typename T>
+void launch_pad_add_transform_0213(T* output,
+                                   const T* vals,
+                                   int batch_size,
+                                   int hidden_dim,
+                                   int seq_length,
+                                   int padded_seq_len,
+                                   int heads,
+                                   int padded_head_size,
+                                   dpct::queue_ptr stream)
+{
+    hidden_dim >>= 3;
+    sycl::range<3> block_dim(2, heads, (padded_head_size >> 3));
+    sycl::range<3> grid_dim(1, padded_seq_len / 2, batch_size);
+    /*
+    DPCT1049:3: The work-group size passed to the SYCL kernel may exceed the limit. To get the
+    device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
+    */
+    {
+        dpct::has_capability_or_fail(stream->get_device(),
+                                     {sycl::aspect::fp64, sycl::aspect::fp16});
+        stream->parallel_for(sycl::nd_range<3>(grid_dim * block_dim, block_dim),
+                             [=](sycl::nd_item<3> item_ct1) {
+                                 pad_add_transform_0213(output,
+                                                        vals,
+                                                        hidden_dim,
+                                                        seq_length,
+                                                        padded_seq_len,
+                                                        heads,
+                                                        padded_head_size >> 3);
+                             });
+    }
+}
+
+#define INSTANTIATE_LAUNCH_PAD_ADD_TRANSFORM_0213_SIMPLE(T) \
+    template void launch_pad_add_transform_0213<T>(         \
+        T*, const T*, int, int, int, int, int, int, dpct::queue_ptr);
+
+INSTANTIATE_LAUNCH_PAD_ADD_TRANSFORM_0213_SIMPLE(sycl::half);
+#ifdef BF16_AVAILABLE
+INSTANTIATE_LAUNCH_PAD_ADD_TRANSFORM_0213_SIMPLE(sycl::ext::oneapi::bfloat16);
+#endif
+
+// Bias add
+template <typename T>
+void bias_add_transform_0213(T* output,
+                                        const T* vals,
+                                        const T* bias,
+                                        int hidden_dim,
+                                        int seq_length,
+                                        int heads,
+                                        int head_ext);
+
+template <>
+void bias_add_transform_0213<float>(float* output,
+                                               const float* vals,
+                                               const float* bias,
+                                               int hidden_dim,
+                                               int seq_length,
+                                               int heads,
+                                               int head_ext)
+{
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    int d0_stride = hidden_dim * seq_length;
+    int d1_stride = hidden_dim;
+    int d2_stride = hidden_dim / heads;
+
+    int d0_out_stride = d0_stride;
+    int d1_out_stride = d2_stride;
+    int d2_out_stride = d2_stride * seq_length;
+
+    int d0 = item_ct1.get_group(2);              // Batch
+    int d1 = item_ct1.get_group(1);              // Sequence ID (0-127)
+    int cnt = item_ct1.get_group(0) / head_ext;  // Hidden count
+    int d2 = item_ct1.get_local_id(1) +
+             (item_ct1.get_group(0) % head_ext) * (heads / head_ext);  // Head (0-11)
+    int d3 = item_ct1.get_local_id(2);                                 // Values (groups of 4)
+
+    const sycl::float4* vals_vec = reinterpret_cast<const sycl::float4*>(vals);
+    const sycl::float4* bias_vec = reinterpret_cast<const sycl::float4*>(bias);
+    sycl::float4* output_vec = reinterpret_cast<sycl::float4*>(output);
+
+    sycl::float4 inputs =
+        vals_vec[d0 * d0_stride * (item_ct1.get_group_range(0) / head_ext) + cnt * d1_stride +
+                 d1 * d1_stride * (item_ct1.get_group_range(0) / head_ext) + d2 * d2_stride + d3];
+    sycl::float4 biases = bias_vec[cnt * d1_stride + d2 * d2_stride + d3];
+
+    sycl::float4 outputs;
+    outputs.x() = inputs.x() + biases.x();
+    outputs.y() = inputs.y() + biases.y();
+    outputs.z() = inputs.z() + biases.z();
+    outputs.w() = inputs.w() + biases.w();
+
+    output_vec[cnt * d0_out_stride * item_ct1.get_group_range(2) + d0 * d0_out_stride +
+               d1 * d1_out_stride + d2 * d2_out_stride + d3] = outputs;
+}
+
+template <typename T>
+void bias_add_transform_0213(T* output,
+                                        const T* vals,
+                                        const T* bias,
+                                        int hidden_dim,
+                                        int seq_length,
+                                        int heads,
+                                        int head_ext)
+{
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    using T2 = typename std::conditional<std::is_same<T, sycl::half>::value,
+                                         sycl::half2,
+                                         sycl::marray<sycl::ext::oneapi::bfloat16, 2>>::type;
+    int d0_stride = hidden_dim * seq_length;
+    int d1_stride = hidden_dim;
+    int d2_stride = hidden_dim / heads;
+
+    int d2_out_stride = d2_stride * seq_length;
+
+    int d0 = item_ct1.get_group(2);              // Batch
+    int d1 = item_ct1.get_group(1);              // Sequence ID (0-127)
+    int cnt = item_ct1.get_group(0) / head_ext;  // Hidden count
+    int d2 = item_ct1.get_local_id(1) +
+             (item_ct1.get_group(0) % head_ext) * (heads / head_ext);  // Head (0-11)
+    int d3 = item_ct1.get_local_id(2);                                 // Values (groups of 4)
+
+    sycl::float4 vals_arr;
+    sycl::float4 bias_arr;
+    sycl::float4 output_arr;
+    T2* vals_half = reinterpret_cast<T2*>(&vals_arr);
+    T2* bias_half = reinterpret_cast<T2*>(&bias_arr);
+    T2* output_half = reinterpret_cast<T2*>(&output_arr);
+
+    const sycl::float4* vals_vec = reinterpret_cast<const sycl::float4*>(vals);
+    const sycl::float4* bias_vec = reinterpret_cast<const sycl::float4*>(bias);
+    sycl::float4* output_vec = reinterpret_cast<sycl::float4*>(output);
+
+    vals_vec += (d0 * d0_stride * (item_ct1.get_group_range(0) / head_ext));
+    vals_vec += (d1 * d1_stride * (item_ct1.get_group_range(0) / head_ext));
+    vals_vec += (cnt * d1_stride);
+    vals_vec += (d2 * d2_stride);
+
+    bias_vec += (cnt * d1_stride);
+    bias_vec += (d2 * d2_stride);
+
+    output_vec += (cnt * d0_stride * item_ct1.get_group_range(2));
+    output_vec += (d1 * d2_stride);
+    output_vec += (d0 * d0_stride);
+    output_vec += (d2 * d2_out_stride);
+
+    bias_arr = bias_vec[d3];
+    vals_arr = vals_vec[d3];
+
+    output_half[0] = vals_half[0] + bias_half[0];
+    output_half[1] = vals_half[1] + bias_half[1];
+    output_half[2] = vals_half[2] + bias_half[2];
+    output_half[3] = vals_half[3] + bias_half[3];
+    output_vec[d3] = output_arr;
+}
+
+template <typename T>
+/*
+DPCT1110:4: The total declared local variable size in device function bias_add_transform_0213_v2
+exceeds 128 bytes and may cause high register pressure. Consult with your hardware vendor to find
+the total register size available and adjust the code, or use smaller sub-group size to avoid high
+register pressure.
+*/
+void bias_add_transform_0213_v2(T* output,
+                                const T* vals,
+                                const T* bias,
+                                int hidden_dim,
+                                int seq_length,
+                                int heads)
+{
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    using T2 = typename std::conditional<std::is_same<T, sycl::half>::value,
+                                         sycl::half2,
+                                         sycl::marray<sycl::ext::oneapi::bfloat16, 2>>::type;
+    auto& in_data = *sycl::ext::oneapi::group_local_memory_for_overwrite<sycl::float4[3072]>(
+        sycl::ext::oneapi::experimental::this_group<3>());
+
+    int d0_stride = hidden_dim * seq_length;
+    int d1_stride = hidden_dim;
+    int d2_stride = hidden_dim / heads;
+    int iteration_stride = d1_stride * item_ct1.get_local_range(0);  // Hidden * 3 / 8
+    int batch_stride = d0_stride * item_ct1.get_local_range(0);      // Hidden * S * 3 / 8
+
+    int d0_out_stride = d0_stride;
+    int d1_out_stride = d2_stride;
+    int d2_out_stride = d2_stride * seq_length;
+
+    int d0 = item_ct1.get_group(2);      // Batch
+    int d1 = item_ct1.get_group(1);      // Sequence ID (0-127)
+    int cnt = item_ct1.get_local_id(0);  // blockIdx.z; // Hidden count
+    int d2 = item_ct1.get_local_id(1);   // Head (0-11)
+    int d3 = item_ct1.get_local_id(2);   // Values (groups of 4)
+
+    sycl::float4 vals_arr[1];
+    sycl::float4 bias_arr[1];
+    sycl::float4 output_arr[1];
+    T2* vals_half = reinterpret_cast<T2*>(vals_arr);
+    T2* bias_half = reinterpret_cast<T2*>(bias_arr);
+    T2* output_half = reinterpret_cast<T2*>(output_arr);
+
+    const sycl::float4* vals_vec = reinterpret_cast<const sycl::float4*>(vals);
+    const sycl::float4* bias_vec = reinterpret_cast<const sycl::float4*>(bias);
+    sycl::float4* output_vec = reinterpret_cast<sycl::float4*>(output);
+
+    int iter_index = cnt * d1_stride + d2 * d2_stride + d3;
+    int input_offset = d0 * batch_stride + d1 * (iteration_stride << 1);
+    bias_arr[0] = bias_vec[iter_index];
+
+#pragma unroll
+    for (int iter = 0; iter < 2; iter++) {
+        int iter_id = iter * iteration_stride + iter_index;
+        vals_arr[0] = vals_vec[input_offset + iter_id];
+
+        output_half[0] = vals_half[0] + bias_half[0];
+        output_half[1] = vals_half[1] + bias_half[1];
+        output_half[2] = vals_half[2] + bias_half[2];
+        output_half[3] = vals_half[3] + bias_half[3];
+
+        in_data[iter_id] = output_arr[0];
+    }
+    /*
+    DPCT1065:7: Consider replacing sycl::nd_item::barrier() with
+    sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there
+    is no access to global memory.
+    */
+    item_ct1.barrier();
+
+    iteration_stride = item_ct1.get_local_range(0) * (item_ct1.get_local_range(1) >> 1);
+    int matrix_stride = (d0_out_stride * item_ct1.get_group_range(2));
+    int head_count = (d2 >> 1) + cnt * (item_ct1.get_local_range(1) >> 1);
+
+    int out_index = d0 * d0_out_stride + d1 * (d1_out_stride << 1) + d3 + (d2 % 2) * d2_stride;
+
+#pragma unroll
+    for (int iter = 0; iter < 2; iter++) {
+        int iter_row = (iter * iteration_stride) + head_count;
+        int iter_offset = (iter_row % item_ct1.get_local_range(1)) * d2_out_stride +
+                          (iter_row / item_ct1.get_local_range(1)) * matrix_stride;
+        output_vec[out_index + iter_offset] =
+            in_data[iter_row * d2_stride + d3 +
+                    (d2 % 2) * (d1_stride * item_ct1.get_local_range(0))];
+    }
+}
+
+template <typename T>
+void transform4d_0213(T* out,
+                                 const T* in,
+                                 int heads,
+                                 int seq_length,
+                                 int hidden_dim,
+                                 int head_ext);
+
+template <>
+void transform4d_0213<float>(float* out,
+                                        const float* in,
+                                        int heads,
+                                        int seq_length,
+                                        int hidden_dim,
+                                        int head_ext)
+{
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    int d0_stride = hidden_dim * seq_length;
+    int d1_stride = d0_stride / heads;
+    int d2_stride = hidden_dim / heads;
+
+    int d0_out_stride = d0_stride;
+    int d1_out_stride = d2_stride;
+    int d2_out_stride = hidden_dim;
+
+    int d0 = item_ct1.get_group(2);                                                         // Batch
+    int d1 = item_ct1.get_group(1) / ((seq_length - 1) / item_ct1.get_local_range(1) + 1);  // Head
+    int d2 = (item_ct1.get_local_id(1) + item_ct1.get_local_range(1) * item_ct1.get_group(1)) %
+             seq_length;
+    int cnt = item_ct1.get_group(0);
+    int d3 = item_ct1.get_local_id(2);  // Values (groups of 8)
+
+    if (d2 < seq_length) {
+        const sycl::float4* in_vec = reinterpret_cast<const sycl::float4*>(in);
+        sycl::float4* out_vec = reinterpret_cast<sycl::float4*>(out);
+
+        sycl::float4 vals_vec = in_vec[cnt * d0_stride * item_ct1.get_group_range(2) +
+                                       d0 * d0_stride + d1 * d1_stride + d2 * d2_stride + d3];
+        out_vec[d0 * d0_out_stride * item_ct1.get_group_range(0) + cnt * d2_out_stride +
+                d1 * d1_out_stride + d2 * d2_out_stride * item_ct1.get_group_range(0) + d3] =
+            vals_vec;
+    }
+}
+
+template <typename T>
+void transform4d_0213(T* out,
+                                 const T* in,
+                                 int heads,
+                                 int seq_length,
+                                 int hidden_dim,
+                                 int head_ext)
+{
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+    int d0_stride = hidden_dim * (seq_length / head_ext);
+    int d1_stride = hidden_dim;
+    int d2_stride = hidden_dim / heads;
+
+    int d0 = item_ct1.get_group(2);  // Batch
+    int d1 =
+        item_ct1.get_local_id(1) + (item_ct1.get_group(0) % head_ext) * (heads / head_ext);  // Head
+    int d2 = item_ct1.get_group(0) / head_ext;  // Sequence
+    int cnt = item_ct1.get_group(1);            // Hidden count
+    int d3 = item_ct1.get_local_id(2);          // Values (groups of 8)
+
+    const sycl::float4* in_vec = reinterpret_cast<const sycl::float4*>(in);
+    sycl::float4* out_vec = reinterpret_cast<sycl::float4*>(out);
+
+    in_vec += (cnt * d0_stride * item_ct1.get_group_range(2));
+    in_vec += (d0 * d0_stride);
+    in_vec += (d2 * d2_stride);
+    in_vec += (d1 * d2_stride * seq_length);
+
+    out_vec += (cnt * d1_stride);
+    out_vec += (d1 * d2_stride);
+    out_vec += (d0 * d0_stride * item_ct1.get_group_range(1));
+    out_vec += (d2 * d1_stride * item_ct1.get_group_range(1));
+
+    out_vec[d3] = in_vec[d3];
+}
+
+template <typename T>
+void transform4d_0213_v2(T* out, const T* in, int heads, int seq_length, int hidden_dim)
+{
+    auto item_ct1 = sycl::ext::oneapi::experimental::this_nd_item<3>();
+auto& in_data = *sycl::ext::oneapi::group_local_memory_for_overwrite<sycl::float4[3072]>(
+    sycl::ext::oneapi::experimental::this_group<3>());
+
+    int d0_stride = hidden_dim * seq_length;
+    int d1_stride = hidden_dim;
+    int d2_stride = hidden_dim / heads;
+
+    int d0 = item_ct1.get_group(2);      // Batch
+    int d1 = item_ct1.get_local_id(1);   // Head
+    int d2 = item_ct1.get_group(1);      // Sequence
+    int cnt = item_ct1.get_local_id(0);  // Hidden count
+    int d3 = item_ct1.get_local_id(2);   // Values (groups of 8)
+
+    const sycl::float4* in_vec = reinterpret_cast<const sycl::float4*>(in);
+    sycl::float4* out_vec = reinterpret_cast<sycl::float4*>(out);
+
+    int input_offset = d0 * d0_stride + d2 * (d2_stride << 1) + d3 + (d1 % 2) * d2_stride;
+    int head_count = (d1 >> 1) + cnt * (item_ct1.get_local_range(1) >> 1);
+    int iteration_stride = item_ct1.get_local_range(0) * (item_ct1.get_local_range(1) >> 1);
+    int matrix_stride = (d0_stride * item_ct1.get_group_range(2));
+
+#pragma unroll
+    for (int iter = 0; iter < 2; iter++) {
+        int iter_row = iter * iteration_stride + head_count;
+        int iter_offset = (iter_row % item_ct1.get_local_range(1)) * d2_stride;
+
+        in_data[d3 + iter_offset +
+                (iter_row / item_ct1.get_local_range(1) + (d1 % 2) * item_ct1.get_local_range(0)) *
+                    d1_stride] = in_vec[input_offset + iter_offset * seq_length +
+                                        (iter_row / item_ct1.get_local_range(1)) * matrix_stride];
+    }
+    /*
+    DPCT1065:8: Consider replacing sycl::nd_item::barrier() with
+    sycl::nd_item::barrier(sycl::access::fence_space::local_space) for better performance if there
+    is no access to global memory.
+    */
+    item_ct1.barrier();
+
+    iteration_stride = d1_stride * item_ct1.get_local_range(0);
+    int iter_index = cnt * d1_stride + d1 * d2_stride + d3;
+    int output_offset = d0 * d0_stride * item_ct1.get_local_range(0) + d2 * (iteration_stride << 1);
+
+#pragma unroll
+    for (int iter = 0; iter < 2; iter++) {
+        int iter_id = iter * iteration_stride + iter_index;
+        out_vec[output_offset + iter_id] = in_data[iter_id];
+    }
+}
+
+// 3 * [B A S N] - > [B S C*H]
+template <>
+void launch_transform4d_0213<float>(float* out,
+                                    const float* in,
+                                    int batch_size,
+                                    int heads,
+                                    int seq_length,
+                                    int hidden_dim,
+                                    dpct::queue_ptr stream,
+                                    int trans_count)
+{
+    hidden_dim >>= 2;
+    sycl::range<3> grid_dims(trans_count, heads * ((seq_length - 1) / 8 + 1), batch_size);
+    sycl::range<3> block_dims(1, 8, hidden_dim / heads);
+    /*
+    DPCT1049:5: The work-group size passed to the SYCL kernel may exceed the limit. To get the
+    device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
+    */
+    {
+        dpct::has_capability_or_fail(stream->get_device(), {sycl::aspect::fp16});
+        stream->parallel_for(sycl::nd_range<3>(grid_dims * block_dims, block_dims),
+                             [=](sycl::nd_item<3> item_ct1) {
+                                 transform4d_0213<float>(out, in, heads, seq_length, hidden_dim, 1);
+                             });
+    }
+}
+
+template <typename T>
+void launch_transform4d_0213(T* out,
+                             const T* in,
+                             int batch_size,
+                             int heads,
+                             int seq_length,
+                             int hidden_dim,
+                             dpct::queue_ptr stream,
+                             int trans_count)
+{
+    hidden_dim >>= 3;
+    int head_ext = (hidden_dim - 1) / MAX_THREADS + 1;
+    sycl::range<3> grid_dims((seq_length * head_ext), trans_count, batch_size);
+    sycl::range<3> block_dims(1, (heads / head_ext), hidden_dim / heads);
+    /*
+    DPCT1049:6: The work-group size passed to the SYCL kernel may exceed the limit. To get the
+    device limit, query info::device::max_work_group_size. Adjust the work-group size if needed.
+    */
+    {
+        dpct::has_capability_or_fail(stream->get_device(), {sycl::aspect::fp16});
+        stream->parallel_for(sycl::nd_range<3>(grid_dims * block_dims, block_dims),
+                             [=](sycl::nd_item<3> item_ct1) {
+                                 transform4d_0213(out, in, heads, seq_length, hidden_dim, head_ext);
+                             });
+    }
+}
+
+#define INSTANTIATE_2B_LAUNCH_TRANSFORM4D(T)  \
+    template void launch_transform4d_0213<T>( \
+        T*, const T*, int, int, int, int, dpct::queue_ptr, int);
+
+INSTANTIATE_2B_LAUNCH_TRANSFORM4D(sycl::half)
+#ifdef BF16_AVAILABLE
+INSTANTIATE_2B_LAUNCH_TRANSFORM4D(sycl::ext::oneapi::bfloat16)
+#endif
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/compatible.hpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/compatible.hpp
deleted file mode 100644
index 8267e57..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/compatible.hpp
+++ /dev/null
@@ -1,67 +0,0 @@
-#pragma once
-
-#if __has_include(<sycl/sycl.hpp>)
-#include <sycl/sycl.hpp>
-using namespace sycl;
-#elif __has_include(<CL/sycl.hpp>)
-#include <CL/sycl.hpp>
-using namespace cl::sycl;
-#else
-#error "Unsupported compiler"
-#endif
-
-#define __global__
-#define __device__
-
-#define MAX_WARP_NUM 32
-#define WARP_SIZE 32
-
-constexpr int hw_warp_size = 32;
-constexpr int warpSize = 32;
-
-using bf16 = sycl::ext::oneapi::bfloat16;
-using fp16 = sycl::half;
-
-using float4 = sycl::vec<float, 4>;
-using float2 = sycl::vec<float, 2>;
-using half4 = sycl::vec<fp16, 4>;
-using half2 = sycl::vec<fp16, 2>;
-using bf164 = sycl::vec<bf16, 4>;
-using bf162 = sycl::vec<bf16, 2>;
-using uint4 = sycl::vec<uint, 4>;
-using uint2 = sycl::vec<uint, 2>;
-
-inline int next_pow2(const int val)
-{
-    int rounded_val = val - 1;
-    rounded_val |= rounded_val >> 1;
-    rounded_val |= rounded_val >> 2;
-    rounded_val |= rounded_val >> 4;
-    rounded_val |= rounded_val >> 8;
-    return rounded_val + 1;
-}
-
-template <typename T, typename Group, typename... Args>
-std::enable_if_t<std::is_trivially_destructible<T>::value && sycl::detail::is_group<Group>::value,
-                 sycl::local_ptr<typename std::remove_extent<T>::type>>
-    __SYCL_ALWAYS_INLINE __group_local_memory(Group g, Args&&... args)
-{
-    (void)g;
-#ifdef __SYCL_DEVICE_ONLY__
-    __attribute__((opencl_local))
-    std::uint8_t* AllocatedMem = __sycl_allocateLocalMemory(sizeof(T), alignof(T));
-
-    if constexpr (!std::is_trivial_v<T>) {
-        id<3> Id = __spirv::initLocalInvocationId<3, id<3>>();
-        if (Id == id<3>(0, 0, 0)) new (AllocatedMem) T(std::forward<Args>(args)...);
-        sycl::detail::workGroupBarrier();
-    }
-    return reinterpret_cast<__attribute__((opencl_local)) typename std::remove_extent<T>::type*>(
-        AllocatedMem);
-#else
-    // Silence unused variable warning
-    [&args...] {}();
-    throw sycl::exception(sycl::errc::feature_not_supported,
-        "sycl_ext_oneapi_local_memory extension is not supported on host device");
-#endif
-}
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/conversion_utils.hpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/conversion_utils.hpp
deleted file mode 100644
index cf449d9..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/conversion_utils.hpp
+++ /dev/null
@@ -1,443 +0,0 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
-
-#pragma once
-
-#include <stdint.h>
-#include <sycl/half_type.hpp>
-
-#include "compatible.hpp"
-
-namespace conversion {
-
-// Basic primitive for constructing conversions
-// sycl cannot call recursive func
-template <typename TO, typename FROM>
-inline TO to(FROM val)
-{
-    return static_cast<TO>(val);
-}
-
-#ifdef __SYCL_DEVICE_ONLY__
-template <>
-inline double to(int64_t val)
-{
-    return __imf_ll2double_rn(val);
-}
-template <>
-inline double to(int32_t val)
-{
-    return __imf_int2double_rn(val);
-}
-template <>
-inline double to(int16_t val)
-{
-    return __imf_int2double_rn(val);
-}
-template <>
-inline double to(int8_t val)
-{
-    return __imf_int2double_rn(val);
-}
-template <>
-inline double to(uint64_t val)
-{
-    return __imf_ull2double_rn(val);
-}
-template <>
-inline double to(uint32_t val)
-{
-    return __imf_uint2double_rn(val);
-}
-template <>
-inline double to(uint16_t val)
-{
-    return __imf_uint2double_rn(val);
-}
-template <>
-inline double to(uint8_t val)
-{
-    return __imf_uint2double_rn(val);
-}
-#endif
-
-#ifdef __SYCL_DEVICE_ONLY__
-template <>
-inline float to(double val)
-{
-    return __imf_double2float_rn(val);
-}
-template <>
-inline float to(int64_t val)
-{
-    return __imf_ll2float_rn(val);
-}
-template <>
-inline float to(int32_t val)
-{
-    return __imf_int2float_rn(val);
-}
-template <>
-inline float to(int16_t val)
-{
-    return __imf_int2float_rn(val);
-}
-template <>
-inline float to(int8_t val)
-{
-    return __imf_int2float_rn(val);
-}
-template <>
-inline float to(uint64_t val)
-{
-    return __imf_ull2float_rn(val);
-}
-template <>
-inline float to(uint32_t val)
-{
-    return __imf_uint2float_rn(val);
-}
-template <>
-inline float to(uint16_t val)
-{
-    return __imf_uint2float_rn(val);
-}
-template <>
-inline float to(uint8_t val)
-{
-    return __imf_uint2float_rn(val);
-}
-#endif
-
-/*********************  To Float2 Conversions *********************/
-template <>
-inline float2 to(half2 val)
-{
-    return val.convert<float>();
-}
-
-// TODO: ushort as bf16 replacement for bf16 is not compatible with sycl::vec
-template <>
-inline float2 to(sycl::ushort2 val)
-{
-    float2 tmp;
-    tmp[0] = (float)val[0];
-    tmp[1] = (float)val[1];
-    return tmp;
-}
-
-#ifdef BF16_AVAILABLE
-template <>
-inline float2 to(bf162 val)
-{
-    float2 tmp;
-    tmp[0] = (float)val[0];
-    tmp[1] = (float)val[1];
-    return tmp;
-}
-#endif
-
-/*********************  To Half Conversions *********************/
-#ifdef BF16_AVAILABLE
-// No direct conversion
-template <>
-inline half to(bf16 val)
-{
-    return to<half>(to<float>(val));
-}
-#endif
-
-/*********************  To Half2 Conversions *********************/
-template <>
-inline half2 to(float2 val)
-{
-    return val.convert<half, rounding_mode::rtn>();
-}
-template <>
-inline half2 to(float val)
-{
-    half2 tmp;
-    tmp[0] = to<half>(val);
-    tmp[1] = to<half>(val);
-    return tmp;
-}
-
-#ifdef BF16_AVAILABLE
-// No direct conversion
-template <>
-inline half2 to(bf162 val)
-{
-    return to<half2>(to<float2>(val));
-}
-#endif
-
-/*********************  To BF162 Conversions *********************/
-// TODO: use ushort as vec<bf16> replacement
-template <>
-inline sycl::ushort2 to(float2 val)
-{
-    sycl::ushort2 tmp;
-    tmp[0] = val[0];
-    tmp[1] = val[1];
-    return tmp;
-}
-
-#ifdef BF16_AVAILABLE
-template <>
-inline bf162 to(float2 val)
-{
-    bf162 tmp;
-    tmp[0] = val[0];
-    tmp[1] = val[1];
-    return tmp;
-}
-template <>
-inline bf162 to(float val)
-{
-    bf162 tmp;
-    tmp[0] = to<bf16>(val);
-    tmp[1] = to<bf16>(val);
-    return tmp;
-}
-template <>
-inline bf162 to(half2 val)
-{
-    auto tmp = to<float>(val);
-    return to<bf162>(tmp);
-}
-#endif
-
-/*********************  To INT64_T Conversions *********************/
-#ifdef __SYCL_DEVICE_ONLY__
-template <>
-inline int64_t to(double val)
-{
-    return __imf_double2ll_rn(val);
-}
-template <>
-inline int64_t to(float val)
-{
-    return __imf_float2ll_rn(val);
-}
-#endif
-template <>
-inline int64_t to(half val)
-{
-    return to<int64_t>(to<float>(val));
-}
-// No direct support for integer casts at the C++ level and I don't feel they're so important
-// to demand an PTX at this time
-
-#ifdef BF16_AVAILABLE
-template <>
-inline int64_t to(bf16 val)
-{
-    return to<int64_t>(to<float>(val));
-}
-#endif
-
-/*********************  To INT32_T Conversions *********************/
-#ifdef __SYCL_DEVICE_ONLY__
-template <>
-inline int32_t to(double val)
-{
-    return __imf_double2int_rn(val);
-}
-template <>
-inline int32_t to(float val)
-{
-    return __imf_float2int_rn(val);
-}
-#endif
-template <>
-inline int32_t to(half val)
-{
-    return to<int32_t>(to<float>(val));
-}
-// No direct support for integer casts at the C++ level and I don't feel they're so important
-// to demand an PTX at this time
-
-#ifdef BF16_AVAILABLE
-template <>
-inline int32_t to(bf16 val)
-{
-    return to<int32_t>(to<float>(val));
-}
-#endif
-
-/*********************  To INT16_T Conversions *********************/
-#ifdef __SYCL_DEVICE_ONLY__
-template <>
-inline int16_t to(double val)
-{
-    return __imf_double2int_rn(val);
-}
-template <>
-inline int16_t to(float val)
-{
-    return __imf_float2int_rn(val);
-}
-#endif
-template <>
-inline int16_t to(half val)
-{
-    return to<int16_t>(to<float>(val));
-}
-// No direct support for integer casts at the C++ level and I don't feel they're so important
-// to demand an PTX at this time
-
-#ifdef BF16_AVAILABLE
-template <>
-inline int16_t to(bf16 val)
-{
-    return to<int16_t>(to<float>(val));
-}
-#endif
-
-/*********************  To INT8_T Conversions *********************/
-#ifdef __SYCL_DEVICE_ONLY__
-template <>
-inline int8_t to(double val)
-{
-    return __imf_double2int_rn(val);
-}
-template <>
-inline int8_t to(float val)
-{
-    return __imf_float2int_rn(val);
-}
-#endif
-template <>
-inline int8_t to(half val)
-{
-    return to<int8_t>(to<float>(val));
-}
-// No direct support for integer casts at the C++ level and I don't feel they're so important
-// to demand an PTX at this time
-
-#ifdef BF16_AVAILABLE
-template <>
-inline int8_t to(bf16 val)
-{
-    return to<int8_t>(to<float>(val));
-}
-#endif
-
-/*********************  To UINT64_T Conversions *********************/
-#ifdef __SYCL_DEVICE_ONLY__
-template <>
-inline uint64_t to(double val)
-{
-    return __imf_double2ull_rn(val);
-}
-template <>
-inline uint64_t to(float val)
-{
-    return __imf_float2ull_rn(val);
-}
-#endif
-template <>
-inline uint64_t to(half val)
-{
-    return to<uint64_t>(to<float>(val));
-}
-// No direct support for integer casts at the C++ level and I don't feel they're so important
-// to demand an PTX at this time
-
-#ifdef BF16_AVAILABLE
-template <>
-inline uint64_t to(bf16 val)
-{
-    return to<uint64_t>(to<float>(val));
-}
-#endif
-
-/*********************  To UINT32_T Conversions *********************/
-#ifdef __SYCL_DEVICE_ONLY__
-template <>
-inline uint32_t to(double val)
-{
-    return __imf_double2uint_rn(val);
-}
-template <>
-inline uint32_t to(float val)
-{
-    return __imf_float2uint_rn(val);
-}
-#endif
-template <>
-inline uint32_t to(half val)
-{
-    return to<uint32_t>(to<float>(val));
-}
-// No direct support for integer casts at the C++ level and I don't feel they're so important
-// to demand an PTX at this time
-
-#ifdef BF16_AVAILABLE
-template <>
-inline uint32_t to(bf16 val)
-{
-    return to<uint32_t>(to<float>(val));
-}
-#endif
-
-/*********************  To UINT16_T Conversions *********************/
-#ifdef __SYCL_DEVICE_ONLY__
-template <>
-inline uint16_t to(double val)
-{
-    return __imf_double2uint_rn(val);
-}
-template <>
-inline uint16_t to(float val)
-{
-    return __imf_float2uint_rn(val);
-}
-#endif
-template <>
-inline uint16_t to(half val)
-{
-    return to<uint16_t>(to<float>(val));
-}
-// No direct support for integer casts at the C++ level and I don't feel they're so important
-// to demand an PTX at this time
-
-#ifdef BF16_AVAILABLE
-template <>
-inline uint16_t to(bf16 val)
-{
-    return to<uint16_t>(to<float>(val));
-}
-#endif
-
-/*********************  To UINT8_T Conversions *********************/
-#ifdef __SYCL_DEVICE_ONLY__
-template <>
-inline uint8_t to(double val)
-{
-    return __imf_double2uint_rn(val);
-}
-template <>
-inline uint8_t to(float val)
-{
-    return __imf_float2uint_rn(val);
-}
-#endif
-template <>
-inline uint8_t to(half val)
-{
-    return to<uint8_t>(to<float>(val));
-}
-// No direct support for integer casts at the C++ level and I don't feel they're so important
-// to demand an PTX at this time
-
-#ifdef BF16_AVAILABLE
-template <>
-inline uint8_t to(bf16 val)
-{
-    return to<uint8_t>(to<float>(val));
-}
-#endif
-
-}  // namespace conversion
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/dnnl_ext.hpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/dnnl_ext.hpp
deleted file mode 100644
index 7eddf07..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/dnnl_ext.hpp
+++ /dev/null
@@ -1,151 +0,0 @@
-#pragma once
-
-#include <dnnl.h>
-#include <dnnl.hpp>
-
-namespace dnnl {
-class primitive_ext : public primitive {
-public:
-  primitive_ext(const primitive base) : primitive(base) {}
-  primitive_ext(primitive&& base) : primitive(std::move(base)) {}
-
-  /// Returns a memory descriptor.
-  ///
-  /// @note
-  ///     There are also convenience methods
-  ///     #dnnl::primitive_desc_base::src_desc(),
-  ///     #dnnl::primitive_desc_base::dst_desc(), and others.
-  ///
-  /// @param what The kind of parameter to query; can be
-  ///     #dnnl::query::src_md, #dnnl::query::dst_md, etc.
-  /// @param idx Index of the parameter. For example, convolution bias can
-  ///     be queried with what = #dnnl::query::weights_md and idx = 1.
-  /// @returns The requested memory descriptor.
-  /// @returns A zero memory descriptor if the primitive does not have a
-  ///     parameter of the specified kind or index.
-  const memory::desc *query_md(query what, int idx = 0) const {
-      std::vector<query> valid_q {query::src_md, query::diff_src_md,
-              query::weights_md, query::diff_weights_md, query::dst_md,
-              query::diff_dst_md, query::workspace_md, query::scratchpad_md,
-              query::exec_arg_md};
-      if (!std::any_of(valid_q.cbegin(), valid_q.cend(),
-                  [=](query q) { return what == q; }))
-          DNNL_THROW_ERROR(dnnl_invalid_arguments,
-                  "memory descriptor query is invalid");
-
-      const dnnl_memory_desc_t *cdesc = dnnl_primitive_desc_query_md(
-              this->get_primitive_desc(), dnnl::convert_to_c(what), idx);
-      return cdesc ? reinterpret_cast<const memory::desc *>(cdesc) : nullptr;
-  }
-
-  /// Returns a source memory descriptor.
-  /// @param idx Source index.
-  /// @returns Source memory descriptor.
-  /// @returns A zero memory descriptor if the primitive does not have a
-  ///     source parameter with index @p idx.
-  const memory::desc *src_desc(int idx) const {
-      return query_md(query::src_md, idx);
-  }
-
-  /// Returns a destination memory descriptor.
-  /// @param idx Destination index.
-  /// @returns Destination memory descriptor.
-  /// @returns A zero memory descriptor if the primitive does not have a
-  ///     destination parameter with index @p idx.
-  const memory::desc *dst_desc(int idx) const {
-      return query_md(query::dst_md, idx);
-  }
-
-  /// Returns a weights memory descriptor.
-  /// @param idx Weights index.
-  /// @returns Weights memory descriptor.
-  /// @returns A zero memory descriptor if the primitive does not have a
-  ///     weights parameter with index @p idx.
-  const memory::desc *weights_desc(int idx) const {
-      return query_md(query::weights_md, idx);
-  }
-
-  /// Returns a diff source memory descriptor.
-  /// @param idx Diff source index.
-  /// @returns Diff source memory descriptor.
-  /// @returns A zero memory descriptor if the primitive does not have a
-  ///     diff source parameter with index @p idx.
-  const memory::desc *diff_src_desc(int idx) const {
-      return query_md(query::diff_src_md, idx);
-  }
-
-  /// Returns a diff destination memory descriptor.
-  /// @param idx Diff destination index.
-  /// @returns Diff destination memory descriptor.
-  /// @returns A zero memory descriptor if the primitive does not have a
-  ///     diff destination parameter with index @p idx.
-  const memory::desc *diff_dst_desc(int idx) const {
-      return query_md(query::diff_dst_md, idx);
-  }
-
-  /// Returns a diff weights memory descriptor.
-  /// @param idx Diff weights index.
-  /// @returns Diff weights memory descriptor.
-  /// @returns A zero memory descriptor if the primitive does not have a
-  ///     diff weights parameter with index @p idx.
-  const memory::desc *diff_weights_desc(int idx) const {
-      return query_md(query::diff_weights_md, idx);
-  }
-
-  // Separate versions without the index argument for documentation
-  // purposes.
-
-  /// Returns a source memory descriptor.
-  /// @returns Source memory descriptor.
-  /// @returns A zero memory descriptor if the primitive does not have a
-  ///     source parameter.
-  const memory::desc *src_desc() const { return src_desc(0); }
-
-  /// Returns a destination memory descriptor.
-  /// @returns Destination memory descriptor.
-  /// @returns A zero memory descriptor if the primitive does not have a
-  ///     destination parameter.
-  const memory::desc *dst_desc() const { return dst_desc(0); }
-
-  /// Returns a weights memory descriptor.
-  /// @returns Weights memory descriptor.
-  /// @returns A zero memory descriptor if the primitive does not have a
-  ///     weights parameter.
-  const memory::desc *weights_desc() const { return weights_desc(0); }
-
-  /// Returns a diff source memory descriptor.
-  /// @returns Diff source memory descriptor.
-  /// @returns A zero memory descriptor if the primitive does not have a
-  ///     diff source memory with.
-  const memory::desc *diff_src_desc() const { return diff_src_desc(0); }
-
-  /// Returns a diff destination memory descriptor.
-  /// @returns Diff destination memory descriptor.
-  /// @returns A zero memory descriptor if the primitive does not have a
-  ///     diff destination parameter.
-  const memory::desc *diff_dst_desc() const { return diff_dst_desc(0); }
-
-  /// Returns a diff weights memory descriptor.
-  /// @returns Diff weights memory descriptor.
-  /// @returns A zero memory descriptor if the primitive does not have a
-  ///     diff weights parameter.
-  const memory::desc *diff_weights_desc() const { return diff_weights_desc(0); }
-
-  /// Returns the workspace memory descriptor.
-  /// @returns Workspace memory descriptor.
-  /// @returns A zero memory descriptor if the primitive does not require
-  ///     workspace parameter.
-  const memory::desc *workspace_desc() const {
-      return query_md(query::workspace_md, 0);
-  }
-
-  /// Returns the scratchpad memory descriptor.
-  /// @returns scratchpad memory descriptor.
-  /// @returns A zero memory descriptor if the primitive does not require
-  ///     scratchpad parameter.
-  /// @sa @ref dev_guide_attributes_scratchpad
-  const memory::desc *scratchpad_desc() const {
-      return query_md(query::scratchpad_md, 0);
-  }
-};
-}
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/inference_context.h b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/inference_context.h
new file mode 100644
index 0000000..5e8838f
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/inference_context.h
@@ -0,0 +1,340 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+#pragma once
+
+// // #include <c10/cuda/CUDAStream.h>
+#include <ipex.h>
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include <cassert>
+#include <iostream>
+#include <vector>
+#include <dpct/blas_utils.hpp>
+
+#include <array>
+#include <unordered_map>
+#include <cmath>
+
+#include <chrono>
+
+namespace at {
+  namespace cuda {
+    dpct::queue_ptr getCurrentCUDAStream() {
+      auto device_type = c10::DeviceType::XPU;
+      c10::impl::VirtualGuardImpl impl(device_type);
+      c10::Stream c10_stream = impl.getStream(c10::Device(device_type));
+      auto& queue = xpu::get_queue_from_stream(c10_stream);
+      return &queue;
+    }
+
+    dpct::queue_ptr getStreamFromPool(bool) {
+      // not implemented
+      return nullptr;
+    }
+  }
+}
+#define MEGABYTE (1024 * 1024)
+#define GIGABYTE (1024 * 1024 * 1024)
+
+// TODO: refactor out
+#define WARP_SIZE 32
+
+#define CUDA_CHECK(callstr)                                                                    \
+    {                                                                                          \
+        cudaError_t error_code = callstr;                                                      \
+        if (error_code != cudaSuccess) {                                                       \
+            std::cerr << "CUDA error " << error_code << " at " << __FILE__ << ":" << __LINE__; \
+            assert(0);                                                                         \
+        }                                                                                      \
+    }
+
+#define CUDA_1D_KERNEL_LOOP(i, n) \
+    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < (n); i += blockDim.x * gridDim.x)
+
+#define CUDA_2D_KERNEL_LOOP(i, n, j, m)                                                          \
+    for (size_t i = blockIdx.x * blockDim.x + threadIdx.x; i < (n); i += blockDim.x * gridDim.x) \
+        for (size_t j = blockIdx.y * blockDim.y + threadIdx.y; j < (m); j += blockDim.y * gridDim.y)
+
+#define DS_CUDA_NUM_THREADS 512
+#define DS_MAXIMUM_NUM_BLOCKS 262144
+
+inline int DS_GET_BLOCKS(const int N)
+{
+    return std::max(
+        std::min((N + DS_CUDA_NUM_THREADS - 1) / DS_CUDA_NUM_THREADS, DS_MAXIMUM_NUM_BLOCKS),
+        // Use at least 1 block, since CUDA does not allow empty block
+        1);
+}
+
+class InferenceContext {
+public:
+    InferenceContext()
+         try : _workspace(nullptr), _seed(42), _curr_offset(0),
+             _stream(&dpct::get_in_order_queue()), _free_memory_size(0), _num_tokens(1),
+             _attention_unfused_workspace_offset(0), _workSpaceSize(0) {
+        _workSpaceSize = 0;
+        _workspace = 0;
+
+        int stat = DPCT_CHECK_ERROR(_cublasHandle = &dpct::get_in_order_queue());
+        if (stat != 0) {
+            // It would be nice to use cublasGetStatusName and
+            // cublasGetStatusString, but they were only added in CUDA 11.4.2.
+            auto message = std::string("Failed to create cublas handle: cublasStatus_t was ") +
+                           std::to_string(stat);
+            std::cerr << message << std::endl;
+            throw std::runtime_error(message);
+        }
+#ifndef __HIP_PLATFORM_AMD__
+        /*
+        DPCT1026:0: The call to cublasSetMathMode was removed because this call is redundant in
+        SYCL.
+        */
+#endif
+        _comp1_event = new sycl::event();
+        _comp2_event = new sycl::event();
+        _comp_event = new sycl::event();
+        _comm_event = new sycl::event();
+    }
+    catch (sycl::exception const& exc) {
+      std::cerr << exc.what() << "Exception caught at file:" << __FILE__ << ", line:" << __LINE__
+                << std::endl;
+      std::exit(1);
+    }
+
+    virtual ~InferenceContext()
+    {
+        _cublasHandle = nullptr;
+        sycl::free(_workspace, dpct::get_in_order_queue());
+        dpct::destroy_event(_comp1_event);
+        dpct::destroy_event(_comp2_event);
+        dpct::destroy_event(_comp_event);
+        dpct::destroy_event(_comm_event);
+    }
+
+    static InferenceContext& Instance()
+    {
+        static InferenceContext _ctx;
+        return _ctx;
+    }
+
+    void GenWorkSpace(const unsigned& num_layers,
+                      const unsigned& num_heads,
+                      const size_t& batch_size,
+                      const size_t& prompt_len,
+                      const size_t& hidden_dim,
+                      const unsigned& mp_size,
+                      const bool& external_cache,
+                      const size_t& elem_size,
+                      const unsigned& rank,
+                      unsigned max_out_tokens,
+                      unsigned min_out_tokens)
+    {
+    dpct::device_ext& dev_ct1 = dpct::get_current_device();
+    sycl::queue& q_ct1 = dev_ct1.in_order_queue();
+        size_t total_size;
+        /*
+        DPCT1106:1: 'cudaMemGetInfo' was migrated with the Intel extensions for device information
+        which may not be supported by all compilers or runtimes. You may need to adjust the code.
+        */
+        _free_memory_size = 21474836480;
+if (!_free_memory_size) {
+            dpct::get_current_device().get_memory_info(_free_memory_size, total_size);
+        }
+
+        // Flash attention requires padded heads and we'll conservatively allocate
+        // for that here. Flash attention is only enabled for head size <= 128 right now
+        const int head_size = hidden_dim / num_heads;
+        const int padded_head_size = head_size <= 32 ? 32 : (head_size <= 64 ? 64 : 128);
+        const int effective_head_size = (head_size > 128) ? head_size : padded_head_size;
+
+        size_t activation_size = 10 * (num_heads * effective_head_size) * batch_size;
+        // Other sequence length dimension is added when the final workSpaceSize is calculated
+        size_t temp_size = batch_size * (num_heads / mp_size) * max_out_tokens;
+        size_t cache_size =
+            num_layers * batch_size * ((num_heads * effective_head_size) / mp_size) * 2;
+        size_t minimal_requirements =
+            temp_size + (_free_memory_size > GIGABYTE ? 500 : 100) * MEGABYTE;
+        if (_free_memory_size < minimal_requirements) {
+            printf("Requested:\t%lu\nFree:\t%lu\nTotal:\t%lu\n",
+                   minimal_requirements,
+                   _free_memory_size,
+                   total_size);
+            throw std::runtime_error("Workspace can't be allocated, no enough memory.");
+        }
+
+        _max_seq_len = ((_free_memory_size - minimal_requirements) / elem_size) /
+                       (activation_size + temp_size + cache_size);
+        _max_seq_len = std::min((size_t)max_out_tokens, _max_seq_len);
+        size_t workSpaceSize = ((external_cache ? (activation_size + temp_size)
+                                                : (activation_size + temp_size + cache_size))) *
+                               _max_seq_len * elem_size;
+        temp_size *= _max_seq_len * elem_size;
+
+        if (_max_seq_len < min_out_tokens) {
+            printf(
+                "Allocatable workspace available (%ld tokens) is less than minimum requested "
+                "workspace (%d tokens)\n",
+                _max_seq_len,
+                min_out_tokens);
+            throw std::runtime_error("Workspace can't be allocated, not enough memory");
+        }
+
+        if (!_workspace) {
+            assert(_workspace == nullptr);
+            _workspace = (void*)sycl::malloc_device(workSpaceSize, q_ct1);
+        } else if (_workSpaceSize < workSpaceSize) {
+            sycl::free(_workspace, q_ct1);
+            _workspace = (void*)sycl::malloc_device(workSpaceSize, q_ct1);
+        }
+        if (rank == 0 && (!_workspace || _workSpaceSize < workSpaceSize))
+            printf(
+                "------------------------------------------------------\n"
+                "Free memory : %f (GigaBytes)  \n"
+                "Total memory: %f (GigaBytes)  \n"
+                "Requested memory: %f (GigaBytes) \n"
+                "Setting maximum total tokens (input + output) to %lu \n"
+                "WorkSpace: %p \n"
+                "------------------------------------------------------\n",
+                (float)_free_memory_size / GIGABYTE,
+                (float)total_size / GIGABYTE,
+                (float)workSpaceSize / GIGABYTE,
+                _max_seq_len,
+                _workspace);
+
+        if (!_workspace) {
+            printf("Requested:\t%lu\nFree:\t%lu\nTotal:\t%lu\n",
+                   workSpaceSize,
+                   _free_memory_size,
+                   total_size);
+            throw std::runtime_error("Workspace is null.");
+        }
+        _workSpaceSize = workSpaceSize;
+        _attention_unfused_workspace_offset = workSpaceSize - temp_size;
+    }
+    inline int GetMaxTokenLength() const { return _max_seq_len; }
+
+    dpct::event_ptr GetCompEvent(int id) { return id == 1 ? _comp1_event : _comp2_event; }
+
+    size_t get_workspace_size() const { return _workSpaceSize; }
+    void* GetWorkSpace() { return _workspace; }
+    void* GetAttentionUnfusedWorkspace()
+    {
+        return (char*)_workspace + _attention_unfused_workspace_offset;
+    }
+
+    inline unsigned new_token(unsigned layer_id)
+    {
+        if (layer_id == 0) _token_length++;
+        return _token_length;
+    }
+
+    inline void reset_tokens(unsigned initial_tokens = 1)
+    {
+        _num_tokens = initial_tokens;
+    }  //_token_length = 0; }
+
+    inline unsigned current_tokens() const { return _num_tokens; }
+
+    inline void advance_tokens() { _num_tokens++; }
+
+    dpct::queue_ptr GetCommStream(bool async_op = false)
+    {
+        if (!_comm_stream)
+            _comm_stream = async_op ? at::cuda::getStreamFromPool(true)
+                                    : at::cuda::getCurrentCUDAStream();
+        return _comm_stream;
+    }
+    dpct::queue_ptr GetCurrentStream(bool other_stream = false)
+    {
+        // get current pytorch stream.
+        if (other_stream) {
+            if (!_stream) _stream = at::cuda::getStreamFromPool(true);
+            return _stream;
+        }
+        dpct::queue_ptr stream = at::cuda::getCurrentCUDAStream();
+        return stream;
+    }
+
+    void release_workspace()
+    {
+        sycl::free(_workspace, dpct::get_in_order_queue());
+        _workspace = nullptr;
+    }
+    bool retake_workspace()
+    {
+        if (_workspace != nullptr || _workSpaceSize == 0) return true;
+        _workspace = (void*)sycl::malloc_device(_workSpaceSize, dpct::get_in_order_queue());
+        return _workspace != nullptr;
+    }
+    dpct::queue_ptr GetCublasHandle() { return _cublasHandle; }
+
+    std::pair<uint64_t, uint64_t> IncrementOffset(uint64_t offset_inc)
+    {
+        uint64_t offset = _curr_offset;
+        _curr_offset += offset_inc;
+        return std::pair<uint64_t, uint64_t>(_seed, offset);
+    }
+
+    void SetSeed(uint64_t new_seed) { _seed = new_seed; }
+
+    const std::vector<std::array<int, 3>>& GetGemmAlgos() const { return _gemm_algos; }
+
+    inline void SynchComp()
+    {
+        /*
+        DPCT1012:2: Detected kernel execution time measurement pattern and generated an initial code
+        for time measurements in SYCL. You can change the way time is measured depending on your
+        goals.
+        */
+        _comp_event_ct1 = std::chrono::steady_clock::now();
+        *_comp_event = _comp_stream->ext_oneapi_submit_barrier();
+        _comm_stream->ext_oneapi_submit_barrier({*_comp_event});
+    }
+    inline void SynchComm()
+    {
+        /*
+        DPCT1012:3: Detected kernel execution time measurement pattern and generated an initial code
+        for time measurements in SYCL. You can change the way time is measured depending on your
+        goals.
+        */
+        _comm_event_ct1 = std::chrono::steady_clock::now();
+        *_comm_event = _comm_stream->ext_oneapi_submit_barrier();
+        _comp_stream->ext_oneapi_submit_barrier({*_comm_event});
+    }
+
+private:
+    dpct::queue_ptr _cublasHandle;
+
+    dpct::event_ptr _comp_event;
+    std::chrono::time_point<std::chrono::steady_clock> _comp_event_ct1;
+    dpct::event_ptr _comm_event;
+    std::chrono::time_point<std::chrono::steady_clock> _comm_event_ct1;
+
+    void* _workspace;
+    // offset from _workspace for attention unfused memory
+    size_t _attention_unfused_workspace_offset;
+    uint64_t _seed;
+    uint64_t _curr_offset;
+
+    size_t _workSpaceSize;
+    size_t _free_memory_size;
+
+    size_t _max_seq_len;
+
+    dpct::event_ptr _comp1_event;
+    dpct::event_ptr _comp2_event;
+
+    dpct::queue_ptr _stream;
+
+    unsigned _token_length;
+    unsigned _num_tokens;
+    std::vector<std::array<int, 3>> _gemm_algos;
+
+    dpct::queue_ptr _comp_stream;
+    dpct::queue_ptr _comm_stream;
+
+    std::unordered_map<int, int> _world_sizes;
+};
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/inference_context.hpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/inference_context.hpp
deleted file mode 100644
index b60d95f..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/inference_context.hpp
+++ /dev/null
@@ -1,210 +0,0 @@
-#pragma once
-
-#include <ATen/record_function.h>
-#include <c10/core/Stream.h>
-#include <ipex.h>
-#include <torch/extension.h>
-#include <torch/library.h>
-#include <cassert>
-#include <iostream>
-#include <oneapi/mkl.hpp>
-#include <oneapi/mkl/rng/device.hpp>
-#include <vector>
-
-#include "compatible.hpp"
-
-#define MEGABYTE (1024 * 1024)
-#define GIGABYTE (1024 * 1024 * 1024)
-
-#define WARP_SIZE 32
-#define ONEMKL_OP_T oneapi::mkl::transpose::trans
-#define ONEMKL_OP_N oneapi::mkl::transpose::nontrans
-
-#define DPCPP_1D_KERNEL_LOOP(i, n) \
-    for (size_t(i) = item_ct1.get_global_id(2); (i) < (n); (i) += item_ct1.get_global_range(2))
-
-#define DPCPP_2D_KERNEL_LOOP(i, n, j, m)                                                       \
-    for (size_t i = item_ct1.get_global_id(2); (i) < (n); (i) += item_ct1.get_global_range(2)) \
-        for (size_t j = item_ct1.get_global_id(1); (j) < (m); (j) += item_ct1.get_global_range(1))
-
-#define DS_CUDA_NUM_THREADS 512
-#define DS_MAXIMUM_NUM_BLOCKS 262144
-
-inline int DS_GET_BLOCKS(const int N)
-{
-    return (std::max)(
-        (std::min)((N + DS_CUDA_NUM_THREADS - 1) / DS_CUDA_NUM_THREADS, DS_MAXIMUM_NUM_BLOCKS),
-        // Use at least 1 block, since CUDA does not allow empty block
-        1);
-}
-
-class InferenceContext {
-public:
-    InferenceContext()
-    try : _workspace(nullptr), _seed(42), _curr_offset(0), _workSpaceSize(0), _max_seq_len(0) {
-        auto type_ = c10::DeviceType::XPU;
-        c10::impl::VirtualGuardImpl impl(type_);
-        auto device_ = c10::Device(type_);
-        c10::Stream stream = impl.getStream(device_);
-        _gen = new oneapi::mkl::rng::philox4x32x10(xpu::get_queue_from_stream(stream), 123);
-        if ((_onemklQ = xpu::get_queue_from_stream(stream), 0) != 0) {
-            auto message = std::string("Fail to create onemkl queue.");
-            std::cerr << message << std::endl;
-            throw std::runtime_error(message);
-        }
-    } catch (sycl::exception const& exc) {
-        std::cerr << exc.what() << "Exception caught at file:" << __FILE__ << ", line:" << __LINE__
-                  << std::endl;
-        std::exit(1);
-    }
-
-    virtual ~InferenceContext()
-    {
-        free(_gen);
-
-        auto type_ = c10::DeviceType::XPU;
-        c10::impl::VirtualGuardImpl impl(type_);
-        auto device_ = c10::Device(type_);
-        c10::Stream stream = impl.getStream(device_);
-        sycl::free(_workspace, xpu::get_queue_from_stream(stream));
-    }
-
-    static InferenceContext& Instance()
-    {
-        static InferenceContext _ctx;
-        return _ctx;
-    }
-
-    void GenWorkSpace(const unsigned& num_layers,
-                      const unsigned& num_heads,
-                      const size_t& batch_size,
-                      const size_t& prompt_len,
-                      const size_t& hidden_dim,
-                      const unsigned& mp_size,
-                      const bool& external_cache,
-                      const size_t& elem_size,
-                      const unsigned& rank,
-                      unsigned max_out_tokens,
-                      unsigned min_out_tokens)
-    {
-        size_t total_size;
-
-        // Flash attention requires padded heads and we'll conservatively allocate
-        // for that here. Flash attention is only enabled for head size <= 128 right now
-        const int head_size = hidden_dim / num_heads;
-        const int padded_head_size = head_size <= 32 ? 32 : (head_size <= 64 ? 64 : 128);
-        const int effective_head_size = (head_size > 128) ? head_size : padded_head_size;
-
-        size_t activation_size = 16 * (num_heads * effective_head_size) * batch_size;
-        // Other sequence length dimension is added when the final workSpaceSize is calculated
-        size_t temp_size = batch_size * num_heads * max_out_tokens * 2;
-        size_t cache_size =
-            num_layers * batch_size * ((num_heads * effective_head_size) / mp_size) * 2;
-
-        _max_seq_len = (size_t)max_out_tokens;
-        size_t workSpaceSize = ((external_cache ? (activation_size + temp_size)
-                                                : (activation_size + temp_size + cache_size))) *
-                               _max_seq_len * elem_size;
-        temp_size *= _max_seq_len * elem_size;
-
-        if (_max_seq_len < min_out_tokens) {
-            printf(
-                "Allocatable workspace available (%ld tokens) is less than minimum requested "
-                "workspace (%d tokens)\n",
-                _max_seq_len,
-                min_out_tokens);
-            throw std::runtime_error("Workspace can't be allocated, not enough memory");
-        }
-
-        auto current_queue = this->GetCurrentStream();
-        if (!_workspace) {
-            assert(_workspace == nullptr);
-            _workspace = sycl::malloc_device(workSpaceSize, current_queue);
-        } else if (_workSpaceSize < workSpaceSize) {
-            sycl::free(_workspace, current_queue);
-            _workspace = sycl::malloc_device(workSpaceSize, current_queue);
-        }
-        if (rank == 0 && !_workspace)
-            printf(
-                "------------------------------------------------------\n"
-                "Requested memory: %f (GigaBytes) \n"
-                "Setting maximum total tokens (input + output) to %lu \n"
-                "------------------------------------------------------\n",
-                (float)workSpaceSize / GIGABYTE,
-                _max_seq_len);
-
-        if (!_workspace) { throw std::runtime_error("Workspace is null."); }
-        _workSpaceSize = workSpaceSize;
-    }
-
-    void SetWorkSpace(void* workspace)
-    {
-        if (!workspace) { throw std::runtime_error("Workspace is null."); }
-        _workspace = workspace;
-    }
-
-    void* GetWorkSpace() { return _workspace; }
-
-    sycl::queue GetCurrentStream()
-    {
-        auto type_ = c10::DeviceType::XPU;
-        c10::impl::VirtualGuardImpl impl(type_);
-        auto device_ = c10::Device(type_);
-        c10::Stream stream = impl.getStream(device_);
-        return xpu::get_queue_from_stream(stream);
-    }
-
-    // This could be problematic
-    sycl::queue GetNewStream()
-    {
-        auto type_ = c10::DeviceType::XPU;
-        c10::impl::VirtualGuardImpl impl(type_);
-        auto device_ = c10::Device(type_);
-        c10::Stream stream = impl.getStream(device_);
-
-        return xpu::get_queue_from_stream(stream);
-    }
-
-    sycl::queue GetOneMKLQ() { return _onemklQ; }
-
-    std::pair<uint64_t, uint64_t> IncrementOffset(uint64_t offset_inc)
-    {
-        uint64_t offset = _curr_offset;
-        _curr_offset += offset_inc;
-        // set _GPT_DEBUG_ and fix seed to avoid randomness
-#ifdef _GPT_DEBUG_
-        return std::pair<uint64_t, uint64_t>(_seed, 0);
-#else
-        return std::pair<uint64_t, uint64_t>(_seed, offset);
-#endif
-    }
-
-    void SetSeed(uint64_t new_seed) { _seed = new_seed; }
-
-    void TestGemmFP16(bool test_gemm, int batch_size, int seq_len, int head_num, int size_per_head)
-    {
-        // avoid rerun.
-        if (_gemm_algos.size() > 0) return;
-
-        // Use default algo.
-        _gemm_algos.push_back(std::array<int, 3>({99, 99, 99}));
-        _gemm_algos.push_back(std::array<int, 3>({99, 99, 99}));
-        _gemm_algos.push_back(std::array<int, 3>({99, 99, 99}));
-        _gemm_algos.push_back(std::array<int, 3>({99, 99, 99}));
-        _gemm_algos.push_back(std::array<int, 3>({99, 99, 99}));
-    }
-
-    const std::vector<std::array<int, 3>>& GetGemmAlgos() const { return _gemm_algos; }
-
-private:
-    oneapi::mkl::rng::philox4x32x10* _gen;
-    sycl::queue _onemklQ;
-    void* _workspace;
-    uint64_t _seed;
-    uint64_t _curr_offset;
-
-    size_t _workSpaceSize;
-    size_t _max_seq_len;
-
-    std::vector<std::array<int, 3>> _gemm_algos;
-};
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/inference_cublas_wrappers.h b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/inference_cublas_wrappers.h
new file mode 100644
index 0000000..3683ad1
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/inference_cublas_wrappers.h
@@ -0,0 +1,458 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+#pragma once
+
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include <assert.h>
+#include <dpct/blas_utils.hpp>
+
+#ifdef BF16_AVAILABLE
+#endif
+#ifndef __HIP_PLATFORM_AMD__
+#endif
+#include <stdio.h>
+#include <dpct/lib_common_utils.hpp>
+
+#ifdef __HIP_PLATFORM_AMD__
+int cublas_gemm_ex(rocblas_handle handle,
+                   rocblas_operation transa,
+                   rocblas_operation transb,
+                   int m,
+                   int n,
+                   int k,
+                   const float* alpha,
+                   const float* beta,
+                   const float* A,
+                   const float* B,
+                   float* C,
+                   rocblas_gemm_algo algo,
+                   int b_stride = -1)
+#else
+int cublas_gemm_ex(dpct::queue_ptr handle,
+                   oneapi::mkl::transpose transa,
+                   oneapi::mkl::transpose transb,
+                   int m,
+                   int n,
+                   int k,
+                   const float* alpha,
+                   const float* beta,
+                   const float* A,
+                   const float* B,
+                   float* C,
+                   int algo,
+                   int b_stride = -1)
+#endif
+ try {
+    const int ldb = (b_stride == -1) ? ((transb == oneapi::mkl::transpose::nontrans) ? k : n)
+                                     : b_stride;
+#ifdef __HIP_PLATFORM_AMD__
+    rocblas_status status = rocblas_gemm_ex(handle,
+                                            transa,
+                                            transb,
+                                            m,
+                                            n,
+                                            k,
+                                            (const void*)alpha,
+                                            (const void*)A,
+                                            rocblas_datatype_f32_r,
+                                            (transa == rocblas_operation_none) ? m : k,
+                                            (const void*)B,
+                                            rocblas_datatype_f32_r,
+                                            ldb,
+                                            (const void*)beta,
+                                            C,
+                                            rocblas_datatype_f32_r,
+                                            m,
+                                            C,
+                                            rocblas_datatype_f32_r,
+                                            m,
+                                            rocblas_datatype_f32_r,
+                                            algo,
+                                            0,
+                                            0);
+#else
+    int status = DPCT_CHECK_ERROR(dpct::gemm(*handle,
+                                             transa,
+                                             transb,
+                                             m,
+                                             n,
+                                             k,
+                                             (const void*)alpha,
+                                             (const void*)A,
+                                             dpct::library_data_t::real_float,
+                                             (transa == oneapi::mkl::transpose::nontrans) ? m : k,
+                                             (const void*)B,
+                                             dpct::library_data_t::real_float,
+                                             ldb,
+                                             (const void*)beta,
+                                             C,
+                                             dpct::library_data_t::real_float,
+                                             m,
+                                             dpct::library_data_t::real_float));
+#endif
+
+#ifdef __HIP_PLATFORM_AMD__
+    if (status != rocblas_status_success) {
+#else
+    if (status != 0) {
+#endif
+        fprintf(stderr,
+                "!!!! kernel execution error. (m: %d, n: %d, k: %d, error: %d) \n",
+                m,
+                n,
+                k,
+                (int)status);
+        return EXIT_FAILURE;
+    }
+    return 0;
+}
+catch (sycl::exception const& exc) {
+  std::cerr << exc.what() << "Exception caught at file:" << __FILE__ << ", line:" << __LINE__
+            << std::endl;
+  std::exit(1);
+}
+
+template <typename T>
+#ifdef __HIP_PLATFORM_AMD__
+int cublas_gemm_ex(rocblas_handle handle,
+                   rocblas_operation transa,
+                   rocblas_operation transb,
+                   int m,
+                   int n,
+                   int k,
+                   const float* alpha,
+                   const float* beta,
+                   const T* A,
+                   const T* B,
+                   T* C,
+                   rocblas_gemm_algo algo,
+                   int b_stride = -1)
+#else
+int cublas_gemm_ex(dpct::queue_ptr handle,
+                   oneapi::mkl::transpose transa,
+                   oneapi::mkl::transpose transb,
+                   int m,
+                   int n,
+                   int k,
+                   const float* alpha,
+                   const float* beta,
+                   const T* A,
+                   const T* B,
+                   T* C,
+                   int algo,
+                   int b_stride = -1)
+#endif
+ try {
+    const int ldb = (b_stride == -1) ? ((transb == oneapi::mkl::transpose::nontrans) ? k : n)
+                                     : b_stride;
+#ifdef __HIP_PLATFORM_AMD__
+    constexpr auto rocblas_dtype_16 = std::is_same<T, sycl::half>::value ? rocblas_datatype_f16_r
+                                                                     : rocblas_datatype_bf16_r;
+    rocblas_status status = rocblas_gemm_ex(handle,
+                                            transa,
+                                            transb,
+                                            m,
+                                            n,
+                                            k,
+                                            (const void*)alpha,
+                                            (const void*)A,
+                                            rocblas_dtype_16,
+                                            (transa == rocblas_operation_none) ? m : k,
+                                            (const void*)B,
+                                            rocblas_dtype_16,
+                                            ldb,
+                                            (const void*)beta,
+                                            (void*)C,
+                                            rocblas_dtype_16,
+                                            m,
+                                            (void*)C,
+                                            rocblas_dtype_16,
+                                            m,
+                                            rocblas_datatype_f32_r,
+                                            algo,
+                                            0,
+                                            0);
+#else
+    constexpr auto cublas_dtype_16 = std::is_same<T, sycl::half>::value
+                                         ? dpct::library_data_t::real_half
+                                         : dpct::library_data_t::real_bfloat16;
+    int status = DPCT_CHECK_ERROR(dpct::gemm(*handle,
+                                             transa,
+                                             transb,
+                                             m,
+                                             n,
+                                             k,
+                                             (const void*)alpha,
+                                             (const void*)A,
+                                             cublas_dtype_16,
+                                             (transa == oneapi::mkl::transpose::nontrans) ? m : k,
+                                             (const void*)B,
+                                             cublas_dtype_16,
+                                             ldb,
+                                             (const void*)beta,
+                                             (void*)C,
+                                             cublas_dtype_16,
+                                             m,
+                                             dpct::library_data_t::real_float));
+#endif
+
+#ifdef __HIP_PLATFORM_AMD__
+    if (status != rocblas_status_success) {
+#else
+    if (status != 0) {
+#endif
+        fprintf(stderr,
+                "!!!! kernel execution error. (m: %d, n: %d, k: %d, error: %d) \n",
+                m,
+                n,
+                k,
+                (int)status);
+        return EXIT_FAILURE;
+    }
+    return 0;
+}
+catch (sycl::exception const& exc) {
+  std::cerr << exc.what() << "Exception caught at file:" << __FILE__ << ", line:" << __LINE__
+            << std::endl;
+  std::exit(1);
+}
+
+#ifdef __HIP_PLATFORM_AMD__
+int cublas_strided_batched_gemm(rocblas_handle handle,
+                                int m,
+                                int n,
+                                int k,
+                                const float* alpha,
+                                const float* beta,
+                                const float* A,
+                                const float* B,
+                                float* C,
+                                rocblas_operation op_A,
+                                rocblas_operation op_B,
+                                int stride_A,
+                                int stride_B,
+                                int stride_C,
+                                int batch,
+                                rocblas_gemm_algo algo)
+#else
+int cublas_strided_batched_gemm(dpct::queue_ptr handle,
+                                int m,
+                                int n,
+                                int k,
+                                const float* alpha,
+                                const float* beta,
+                                const float* A,
+                                const float* B,
+                                float* C,
+                                oneapi::mkl::transpose op_A,
+                                oneapi::mkl::transpose op_B,
+                                int stride_A,
+                                int stride_B,
+                                int stride_C,
+                                int batch,
+                                int algo)
+#endif
+ try {
+#ifdef __HIP_PLATFORM_AMD__
+    rocblas_status status =
+        rocblas_gemm_strided_batched_ex(handle,
+                                        op_A,
+                                        op_B,
+                                        m,
+                                        n,
+                                        k,
+                                        alpha,
+                                        A,
+                                        rocblas_datatype_f32_r,
+                                        (op_A == rocblas_operation_none) ? m : k,
+                                        stride_A,
+                                        B,
+                                        rocblas_datatype_f32_r,
+                                        (op_B == rocblas_operation_none) ? k : n,
+                                        stride_B,
+                                        beta,
+                                        C,
+                                        rocblas_datatype_f32_r,
+                                        m,
+                                        stride_C,
+                                        C,
+                                        rocblas_datatype_f32_r,
+                                        m,
+                                        stride_C,
+                                        batch,
+                                        rocblas_datatype_f32_r,
+                                        algo,
+                                        0,
+                                        0);
+#else
+    int status =
+        DPCT_CHECK_ERROR(dpct::gemm_batch(*handle,
+                                          op_A,
+                                          op_B,
+                                          m,
+                                          n,
+                                          k,
+                                          alpha,
+                                          A,
+                                          dpct::library_data_t::real_float,
+                                          (op_A == oneapi::mkl::transpose::nontrans) ? m : k,
+                                          stride_A,
+                                          B,
+                                          dpct::library_data_t::real_float,
+                                          (op_B == oneapi::mkl::transpose::nontrans) ? k : n,
+                                          stride_B,
+                                          beta,
+                                          C,
+                                          dpct::library_data_t::real_float,
+                                          m,
+                                          stride_C,
+                                          batch,
+                                          dpct::library_data_t::real_float));
+#endif
+
+#ifdef __HIP_PLATFORM_AMD__
+    if (status != rocblas_status_success) {
+#else
+    if (status != 0) {
+#endif
+        fprintf(stderr,
+                "!!!! kernel execution error. (batch: %d, m: %d, n: %d, k: %d, error: %d) \n",
+                batch,
+                m,
+                n,
+                k,
+                (int)status);
+        return EXIT_FAILURE;
+    }
+    return 0;
+}
+catch (sycl::exception const& exc) {
+  std::cerr << exc.what() << "Exception caught at file:" << __FILE__ << ", line:" << __LINE__
+            << std::endl;
+  std::exit(1);
+}
+
+template <typename T>
+#ifdef __HIP_PLATFORM_AMD__
+int cublas_strided_batched_gemm(rocblas_handle handle,
+                                int m,
+                                int n,
+                                int k,
+                                const float* alpha,
+                                const float* beta,
+                                const T* A,
+                                const T* B,
+                                T* C,
+                                rocblas_operation op_A,
+                                rocblas_operation op_B,
+                                int stride_A,
+                                int stride_B,
+                                int stride_C,
+                                int batch,
+                                rocblas_gemm_algo algo)
+#else
+int cublas_strided_batched_gemm(dpct::queue_ptr handle,
+                                int m,
+                                int n,
+                                int k,
+                                const float* alpha,
+                                const float* beta,
+                                const T* A,
+                                const T* B,
+                                T* C,
+                                oneapi::mkl::transpose op_A,
+                                oneapi::mkl::transpose op_B,
+                                int stride_A,
+                                int stride_B,
+                                int stride_C,
+                                int batch,
+                                int algo)
+#endif
+ try {
+#ifdef __HIP_PLATFORM_AMD__
+    constexpr auto rocblas_dtype_16 = std::is_same<T, sycl::half>::value ? rocblas_datatype_f16_r
+                                                                     : rocblas_datatype_bf16_r;
+    rocblas_status status =
+        rocblas_gemm_strided_batched_ex(handle,
+                                        op_A,
+                                        op_B,
+                                        m,
+                                        n,
+                                        k,
+                                        alpha,
+                                        A,
+                                        rocblas_dtype_16,
+                                        (op_A == rocblas_operation_none) ? m : k,
+                                        stride_A,
+                                        B,
+                                        rocblas_dtype_16,
+                                        (op_B == rocblas_operation_none) ? k : n,
+                                        stride_B,
+                                        beta,
+                                        C,
+                                        rocblas_dtype_16,
+                                        m,
+                                        stride_C,
+                                        C,
+                                        rocblas_dtype_16,
+                                        m,
+                                        stride_C,
+                                        batch,
+                                        rocblas_datatype_f32_r,
+                                        algo,
+                                        0,
+                                        0);
+#else
+    constexpr auto cublas_dtype_16 = std::is_same<T, sycl::half>::value
+                                         ? dpct::library_data_t::real_half
+                                         : dpct::library_data_t::real_bfloat16;
+    int status =
+        DPCT_CHECK_ERROR(dpct::gemm_batch(*handle,
+                                          op_A,
+                                          op_B,
+                                          m,
+                                          n,
+                                          k,
+                                          alpha,
+                                          A,
+                                          cublas_dtype_16,
+                                          (op_A == oneapi::mkl::transpose::nontrans) ? m : k,
+                                          stride_A,
+                                          B,
+                                          cublas_dtype_16,
+                                          (op_B == oneapi::mkl::transpose::nontrans) ? k : n,
+                                          stride_B,
+                                          beta,
+                                          C,
+                                          cublas_dtype_16,
+                                          m,
+                                          stride_C,
+                                          batch,
+                                          dpct::library_data_t::real_float));
+#endif
+
+#ifdef __HIP_PLATFORM_AMD__
+    if (status != rocblas_status_success) {
+#else
+    if (status != 0) {
+#endif
+        fprintf(stderr,
+                "!!!! kernel execution error. (m: %d, n: %d, k: %d, error: %d) \n",
+                m,
+                n,
+                k,
+                (int)status);
+        return EXIT_FAILURE;
+    }
+
+    return 0;
+}
+catch (sycl::exception const& exc) {
+  std::cerr << exc.what() << "Exception caught at file:" << __FILE__ << ", line:" << __LINE__
+            << std::endl;
+  std::exit(1);
+}
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/inference_cuda_layers.h b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/inference_cuda_layers.h
new file mode 100644
index 0000000..f761b8b
--- /dev/null
+++ b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/inference_cuda_layers.h
@@ -0,0 +1,251 @@
+// Copyright (c) Microsoft Corporation.
+// SPDX-License-Identifier: Apache-2.0
+
+// DeepSpeed Team
+
+#pragma once
+
+#include <sycl/sycl.hpp>
+#include <dpct/dpct.hpp>
+#include "ds_kernel_utils.h"
+
+#ifdef BF16_AVAILABLE
+#endif
+#include <stdio.h>
+#include <stdlib.h>
+#include <cassert>
+#include <iostream>
+
+#define MAX_WARP_NUM 32
+#define WARP_SIZE 32
+
+#define MAX_THREADS 1024
+#define SMs 80
+
+#define MAX_REGISTERS 256
+
+template <typename T>
+void launch_attn_softmax_v2(T* vals,
+                            T* mask,
+                            T* alibi,
+                            float layer_scale,
+                            bool triangular,
+                            bool recompute,
+                            bool local_attention,
+                            int window_size,
+                            int batch_size,
+                            int heads,
+                            int num_seq,
+                            int sequence_length,
+                            int offset,
+                            int mask_stride,
+                            int mp_size,
+                            dpct::queue_ptr stream);
+
+// Fused bias add with gelu activation
+template <typename T>
+void launch_bias_gelu(T* input,
+                      const T* bias,
+                      int intermediate_size,
+                      int batch_size,
+                      dpct::queue_ptr stream);
+
+template <typename T>
+void launch_gated_activation(T* output,
+                             const T* activation,
+                             const T* bias,
+                             int rows,
+                             int output_stride,
+                             int elems_per_row,
+                             bool use_gelu,
+                             dpct::queue_ptr stream);
+
+// Fused bias add with relu activation
+template <typename T>
+void launch_bias_relu(T* input,
+                      const T* bias,
+                      int intermediate_size,
+                      int batch_size,
+                      dpct::queue_ptr stream);
+
+template <typename T>
+void launch_bias_add(T* input,
+                     const T* bias,
+                     int hidden_size,
+                     int batch_size,
+                     dpct::queue_ptr stream);
+
+template <typename T>
+void launch_bias_residual(T* input,
+                          T* output,
+                          T* attn,
+                          T* bias,
+                          T* attn_bias,
+                          int batch,
+                          int hidden_dim,
+                          int mp_size,
+                          bool preln,
+                          dpct::queue_ptr stream);
+
+template <typename T>
+void launch_fused_ln(T* output,
+                     const T* vals,
+                     const T* gamma,
+                     const T* beta,
+                     float epsilon,
+                     int rows,
+                     int elems_per_row,
+                     dpct::queue_ptr stream);
+
+template <typename T>
+void launch_fused_residual_ln(T* output,
+                              const T* vals,
+                              const T* residual,
+                              const T* bias,
+                              const T* gamma,
+                              const T* beta,
+                              float epsilon,
+                              int rows,
+                              int elems_per_row,
+                              dpct::queue_ptr stream);
+
+template <typename T>
+void launch_fused_residual_ln_store_pre_ln_res(T* norm_output,
+                                               T* res_output,
+                                               const T* vals,
+                                               const T* residual,
+                                               const T* bias,
+                                               const T* gamma,
+                                               const T* beta,
+                                               float epsilon,
+                                               int rows,
+                                               int elems_per_row,
+                                               dpct::queue_ptr stream);
+
+template <typename T>
+void launch_rms_norm(T* norm_output,
+                     T* res_output,
+                     const T* vals,
+                     const T* residual,
+                     const T* gamma,
+                     float epsilon,
+                     int rows,
+                     int elems_per_row,
+                     dpct::queue_ptr stream);
+
+template <typename T>
+void launch_dequantize(T* output,
+                       const int8_t* input,
+                       const float* qscale,
+                       unsigned output_size,
+                       unsigned hidden_dim,
+                       unsigned groups,
+                       unsigned merge_count,
+                       dpct::queue_ptr stream);
+
+template <typename T>
+void launch_dequantize(T* output,
+                       const int8_t* input,
+                       const float* qscale,
+                       unsigned output_size,
+                       unsigned hidden_dim,
+                       unsigned groups,
+                       dpct::queue_ptr stream);
+template <typename T>
+void launch_gptj_residual_add(T* input,
+                              T* output,
+                              T* attn,
+                              T* bias,
+                              T* attn_bias,
+                              int batch,
+                              int head_size,
+                              int mp_size,
+                              dpct::queue_ptr stream);
+
+template <typename T>
+void launch_apply_rotary_pos_emb(T* mixed_query,
+                                 T* key_layer,
+                                 unsigned head_size,
+                                 unsigned seq_len,
+                                 unsigned rotary_dim,
+                                 unsigned offset,
+                                 unsigned num_heads,
+                                 unsigned batch,
+                                 float rope_theta,
+                                 dpct::queue_ptr stream,
+                                 int max_out_tokens);
+
+template <typename T>
+void launch_moe_res_matmul(T* residual,
+                           T* coef,
+                           T* mlp_out,
+                           int seq_len,
+                           int hidden_dim,
+                           dpct::queue_ptr stream);
+
+// 4D transform [0, 1, 2, 3] -> [0, 2, 1, 3]
+template <typename T>
+void launch_transform4d_0213(T* out,
+                             const T* in,
+                             int batch_size,
+                             int heads,
+                             int seq_length,
+                             int hidden_dim,
+                             dpct::queue_ptr stream,
+                             int trans_count);
+template <typename T>
+void launch_bias_add_transform_0213(T* outputs,
+                                    T* vals,
+                                    T* vals1,
+                                    const T* vals2,
+                                    const T* bias,
+                                    int batch_size,
+                                    int seq_length,
+                                    unsigned seq_offset,
+                                    int seq_length1,
+                                    int hidden_dim,
+                                    int heads,
+                                    int num_kv,
+                                    int rotary_dim,
+                                    bool rotate_half,
+                                    bool rotate_every_two,
+                                    dpct::queue_ptr stream,
+                                    int trans_count,
+                                    int max_out_tokens,
+                                    float rope_theta);
+template <typename T>
+void pad_data(T* padded_output,
+              T* output,
+              int bsz,
+              int head_size,
+              int padded_head_size,
+              dpct::queue_ptr stream);
+
+template <typename T>
+void pad_head_seq(T* padded_output,
+                  T* output,
+                  int bsz,
+                  int seq_len,
+                  int padded_seq_len,
+                  int head_size,
+                  int padded_head_size,
+                  dpct::queue_ptr stream);
+
+template <typename T>
+void launch_pad_add_transform_0213(T* output,
+                                   const T* vals,
+                                   int batch_size,
+                                   int hidden_dim,
+                                   int seq_length,
+                                   int padded_seq_len,
+                                   int heads,
+                                   int padded_head_size,
+                                   dpct::queue_ptr stream);
+
+template <typename T>
+void launch_vector_add(T* out,
+                       const T* a,
+                       const T* b,
+                       float gamma,
+                       int num_elems,
+                       dpct::queue_ptr stream);
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/inference_onednn_wrappers.hpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/inference_onednn_wrappers.hpp
deleted file mode 100644
index bace39f..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/inference_onednn_wrappers.hpp
+++ /dev/null
@@ -1,30 +0,0 @@
-#pragma once
-#include <oneapi/dnnl/dnnl_sycl.hpp>
-#include <sycl/sycl.hpp>
-
-template <typename T>
-int onednn_matmul_ex(sycl::queue handle,
-                     bool trans_src,
-                     bool trans_wgt,
-                     int m,
-                     int n,
-                     int k,
-                     const float alpha,
-                     const float beta,
-                     const T* src_ptr,
-                     const T* wgt_ptr,
-                     T* dst_ptr);
-
-template <typename T>
-int onednn_batchgemm(sycl::queue handle,
-                     int m,
-                     int n,
-                     int k,
-                     const float alpha,
-                     const float beta,
-                     const T* src_ptr,
-                     const T* wgt_ptr,
-                     T* dst_ptr,
-                     bool trans_src,
-                     bool trans_wgt,
-                     int batch);
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/inference_onemkl_wrappers.hpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/inference_onemkl_wrappers.hpp
deleted file mode 100644
index 6bf7973..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/inference_onemkl_wrappers.hpp
+++ /dev/null
@@ -1,41 +0,0 @@
-#pragma once
-
-#include <assert.h>
-#if __has_include(<sycl/sycl.hpp>)
-#include <sycl/sycl.hpp>
-#elif __has_include(<CL/sycl.hpp>)
-#include <CL/sycl.hpp>
-#else
-#error "Unsupported compiler"
-#endif
-#include <oneapi/mkl.hpp>
-#include <oneapi/dnnl/dnnl_sycl.hpp>
-
-#include <stdio.h>
-
-template <typename T>
-int onemkl_matmul_ex(sycl::queue handle,
-                     oneapi::mkl::transpose transa,
-                     oneapi::mkl::transpose transb,
-                     int m,
-                     int n,
-                     int k,
-                     const float alpha,
-                     const float beta,
-                     const T* A,
-                     const T* B,
-                     T* C);
-
-template <typename T>
-int onemkl_strided_batched_gemm(sycl::queue handle,
-                                oneapi::mkl::transpose transa,
-                                oneapi::mkl::transpose transb,
-                                int m,
-                                int n,
-                                int k,
-                                const float alpha,
-                                const float beta,
-                                const T* A,
-                                const T* B,
-                                T* C,
-                                int batch);
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/inference_sycl_layers.hpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/inference_sycl_layers.hpp
deleted file mode 100644
index 2fbd2f1..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/inference_sycl_layers.hpp
+++ /dev/null
@@ -1,77 +0,0 @@
-#pragma once
-#include "compatible.hpp"
-
-template <typename T>
-void launch_attn_softmax_v2(T* vals,
-                            T* mask,
-                            T* alibi,
-                            float layer_scale,
-                            bool triangular,
-                            bool recompute,
-                            bool local_attention,
-                            int window_size,
-                            int batch_size,
-                            int heads,
-                            int num_seq,
-                            int sequence_length,
-                            int head_offset,
-                            int mask_stride,
-                            int mp_size,
-                            sycl::queue stream);
-
-// Fused bias add with gelu activation
-template <typename T>
-void launch_bias_gelu(T* input,
-                      const T* bias,
-                      int intermediate_size,
-                      int batch_size,
-                      sycl::queue stream);
-
-template <typename T>
-void launch_bias_add(T* input,
-                     const T* bias,
-                     int intermediate_size,
-                     int batch_size,
-                     sycl::queue stream);
-
-template <typename T>
-void launch_bias_residual(T* residual,
-                          T* hidden_state,
-                          T* attn,
-                          T* bias,
-                          T* attn_bias,
-                          int batch,
-                          int hidden_dim,
-                          int mp_size,
-                          bool preln,
-                          sycl::queue stream);
-
-template <typename T>
-void launch_fused_ln(T* output,
-                     const T* vals,
-                     const T* gamma,
-                     const T* beta,
-                     float epsilon,
-                     int rows,
-                     int elems_per_row,
-                     sycl::queue stream);
-
-template <typename T>
-void launch_fused_residual_ln(T* output,
-                              const T* vals,
-                              const T* residual,
-                              const T* bias,
-                              const T* gamma,
-                              const T* beta,
-                              float epsilon,
-                              int rows,
-                              int elems_per_row,
-                              sycl::queue stream);
-
-template <typename T>
-void launch_vector_add(T* out,
-                       const T* a,
-                       const T* b,
-                       float gamma,
-                       int num_elems,
-                       sycl::queue stream);
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/lru_cache.hpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/lru_cache.hpp
deleted file mode 100644
index 59b3ee7..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/lru_cache.hpp
+++ /dev/null
@@ -1,397 +0,0 @@
-#pragma once
-
-#include <string>
-#include <unordered_map>
-#include <list>
-#ifdef WIN32
-#include <intrin.h>
-#endif
-
-namespace onednnContext {
-
-template <class key_t, class value_t,
-         template <typename ...> class map = std::unordered_map>
-class lru_cache {
-public:
-  class node_t;
-
-  typedef typename std::pair<key_t, value_t> value_type;
-
-  // Only need opaque node_t pointer, it'll compile
-  typedef typename std::list<node_t>::iterator iterator;
-  typedef typename std::list<node_t>::const_iterator const_iterator;
-
-  typedef typename map<key_t, iterator>::iterator map_it;
-  typedef typename map<key_t, iterator>::const_iterator
-    const_map_it;
-
-  // Only class possible, we can't use typedef or using. Or can we?
-  class node_t : public std::pair<map_it, value_t> {
-  public:
-    node_t (const std::pair<map_it, value_t> &l) :
-      std::pair<map_it, value_t>(l) {}
-    node_t (std::pair<map_it, value_t> &&l) :
-      std::pair<map_it, value_t>(std::move(l)) {}
-  };
-
-  typedef typename std::list<node_t>::size_type size_type;
-
-  lru_cache(size_type capacity) : capacity_(capacity) {}
-
-  size_type size() const { map_.size(); }
-  size_type max_size() const { return capacity_; }
-  void resize(size_type new_capacity) {
-    capacity_ = new_capacity;
-
-    // Trim cache
-    while (map_.size() > capacity_) {
-      auto last = vlist_.end();
-      last --;
-      map_.erase(last->first);
-      vlist_.pop_back();
-    }
-  }
-
-  iterator begin() noexcept {
-    auto it = map_.begin();
-    if (it == map_.end()) {
-      return vlist_.end();
-    }
-    return it->second;
-  }
-  const_iterator begin() const noexcept {
-    const auto it = map_.begin();
-    if (it == map_.end()) {
-      return vlist_.end();
-    }
-    return it->second;
-  }
-  iterator end() noexcept {
-    return vlist_.end();
-  }
-  const_iterator end() const noexcept {
-    return vlist_.end();
-  }
-
-  iterator find(const key_t &key) {
-    auto it = map_.find(key);
-    if (it == map_.end()) {
-      return end();
-    } else {
-      vlist_.splice(vlist_.begin(), vlist_, it->second);
-      return it->second;
-    }
-  }
-
-  // Is this feasible?
-  const_iterator find(const key_t &key) const {
-    const auto it = map_.find(key);
-    if (it == map_.end()) {
-      return end();
-    } else {
-      vlist_.splice(vlist_.begin(), vlist_, it->second);
-      return it->second;
-    }
-  }
-
-  bool empty() const noexcept {
-    return vlist_.empty();
-  }
-
-  void clear() noexcept {
-    vlist_.clear();
-    map_.clear();
-  }
-
-  // Can we?
-  // template <class... Args>
-  // std::pair<iterator, bool> emplace(Args&&... args) {
-  // }
-
-  std::pair<iterator, bool> insert(const value_type& value) {
-    auto map_it = map_.find(value.first);
-
-    if (map_it == map_.end()) {
-      vlist_.push_front(std::make_pair(map_it, value.second));
-      auto list_it = vlist_.begin();
-      auto updated = map_.insert(map_it, std::make_pair(value.first, list_it));
-      // Update node to pointer to new map position
-      list_it->first = updated;
-    } else
-      return std::make_pair(map_it->second, false);
-
-    // Trim cache
-    while (map_.size() > capacity_) {
-      auto last = vlist_.end();
-      last --;
-      map_.erase(last->first);
-      vlist_.pop_back();
-    }
-
-    return std::make_pair(vlist_.begin(), true);
-  }
-
-  iterator erase(iterator pos) {
-    auto map_pos = pos->first;
-    map_.erase(map_pos);
-    return vlist_.erase(pos);
-  }
-
-  // Warning: carefully check iterator validity
-  void swap(lru_cache & other) {
-    std::swap(vlist_, other.vlist_);
-    std::swap(map_, other.map_);
-    std::swap(capacity_, other.capacity_);
-  }
-
-private:
-  std::list<node_t> vlist_;
-  map<key_t, iterator> map_;
-  size_type capacity_;
-};
-
-template <class key_t, class value_t>
-class lru_multicache {
-public:
-  class node_t;
-
-  typedef typename std::pair<key_t, value_t> value_type;
-
-  // Only need opaque node_t pointer, it'll compile
-  typedef typename std::list<node_t>::iterator iterator;
-  typedef typename std::list<node_t>::const_iterator const_iterator;
-
-  typedef typename std::unordered_multimap<key_t, iterator>::iterator map_it;
-  typedef typename std::unordered_multimap<key_t, iterator>::const_iterator
-    const_map_it;
-
-  // Only class possible, we can't use typedef or using. Or can we?
-  class node_t : public std::pair<map_it, value_t> {
-  public:
-    node_t (const std::pair<map_it, value_t> &l) :
-      std::pair<map_it, value_t>(l) {}
-    node_t (std::pair<map_it, value_t> &&l) :
-      std::pair<map_it, value_t>(std::move(l)) {}
-  };
-
-  typedef typename std::list<node_t>::size_type size_type;
-
-  lru_multicache(size_type capacity) : capacity_(capacity) {}
-
-  size_type size() const { map_.size(); }
-  size_type max_size() const { return capacity_; }
-  void resize(size_type new_capacity) {
-    capacity_ = new_capacity;
-
-    // Trim cache
-    while (map_.size() > capacity_) {
-      auto last = vlist_.end();
-      last --;
-      map_.erase(last->first);
-      vlist_.pop_back();
-    }
-  }
-
-  iterator begin() noexcept {
-    auto it = map_.begin();
-    if (it == map_.end()) {
-      return vlist_.end();
-    }
-    return it->second;
-  }
-  const_iterator begin() const noexcept {
-    const auto it = map_.begin();
-    if (it == map_.end()) {
-      return vlist_.end();
-    }
-    return it->second;
-  }
-  iterator end() noexcept {
-    return vlist_.end();
-  }
-  const_iterator end() const noexcept {
-    return vlist_.end();
-  }
-
-  iterator find(const key_t &key) {
-    auto it = map_.find(key);
-    if (it == map_.end()) {
-      return end();
-    } else {
-      vlist_.splice(vlist_.begin(), vlist_, it->second);
-      return it->second;
-    }
-  }
-
-  // Is this feasible?
-  const_iterator find(const key_t &key) const {
-    const auto it = map_.find(key);
-    if (it == map_.end()) {
-      return end();
-    } else {
-      vlist_.splice(vlist_.begin(), vlist_, it->second);
-      return it->second;
-    }
-  }
-
-  bool empty() const noexcept {
-    return vlist_.empty();
-  }
-
-  void clear() noexcept {
-    vlist_.clear();
-    map_.clear();
-  }
-
-  // Can we?
-  // template <class... Args>
-  // std::pair<iterator, bool> emplace(Args&&... args) {
-  // }
-
-  std::pair<iterator, bool> insert(const value_type& value) {
-    auto map_it = map_.find(value.first);
-
-    if (map_it == map_.end()) {
-      vlist_.push_front(std::make_pair(map_it, value.second));
-      auto list_it = vlist_.begin();
-      auto updated = map_.insert(std::make_pair(value.first, list_it));
-      // Update node to pointer to new map position
-      list_it->first = updated;
-    } else
-      return std::make_pair(map_it->second, false);
-
-    // Trim cache
-    while (map_.size() > capacity_) {
-      auto last = vlist_.end();
-      last --;
-      map_.erase(last->first);
-      vlist_.pop_back();
-    }
-
-    return std::make_pair(vlist_.begin(), true);
-  }
-
-  iterator erase(iterator pos) {
-    auto map_pos = pos->first;
-    map_.erase(map_pos);
-    return vlist_.erase(pos);
-  }
-
-  // Warning: carefully check iterator validity
-  void swap(lru_multicache & other) {
-    std::swap(vlist_, other.vlist_);
-    std::swap(map_, other.map_);
-    std::swap(capacity_, other.capacity_);
-  }
-
-private:
-  std::list<node_t> vlist_;
-  std::unordered_multimap<key_t, iterator> map_;
-  size_type capacity_;
-};
-
-template <class value_t, size_t capacity = 128, class key_t = std::string>
-class computation_cache {
-public:
-  using iterator = typename lru_cache<key_t, value_t>::iterator;
-
-  template <typename ...Ts>
-  static inline iterator create(const key_t& key, Ts&&... args) {
-    auto it = t_store().insert(
-        std::make_pair(key,value_t(std::forward<Ts>(args)...)));
-    return it.first;
-  }
-
-  static inline value_t& fetch(iterator it) {
-    return it->second;
-  }
-
-  static inline void update(value_t &val, iterator it) {
-    it->second = val;
-  }
-
-  static inline iterator find(const key_t& key) {
-    return t_store().find(key);
-  }
-
-  static inline iterator end() {
-    return t_store().end();
-  }
-
-  template <typename ...Ts>
-  static inline value_t& fetch_or_create(const key_t& key, Ts&&... args) {
-    return fetch(create(key, std::forward<Ts>(args)...));
-  }
-
-  static inline void release(
-      const key_t& key, const value_t& computation) {
-    // Empty
-  }
-
-  static inline void release(
-      const key_t& key, value_t&& computation) {
-    // Empty
-  }
-
-  static inline lru_cache<key_t, value_t> &t_store() {
-    static thread_local lru_cache<key_t, value_t> t_store_(capacity);
-    return t_store_;
-  }
-};
-
-template <class value_t, size_t capacity = 128, class key_t = std::string>
-class computation_gcache {
-public:
-  using iterator = typename lru_cache<key_t, value_t>::iterator;
-
-protected:
-  template <typename ...Ts>
-  static inline value_t create(Ts&&... args) {
-    return value_t(std::forward<Ts>(args)...);
-  }
-
-  static inline value_t& fetch(iterator it) {
-    auto comp = std::move(it->second);
-    g_store().erase(it);
-    return comp;
-  }
-
-  static inline iterator find(const key_t& key) {
-    return g_store().find(key);
-  }
-
-  static inline iterator end() {
-    return g_store().end();
-  }
-
-public:
-  template <typename ...Ts>
-  static inline value_t& fetch_or_create(const key_t& key, Ts&&... args) {
-    const auto it = g_store().find(key);
-
-    if (it != g_store().end()) {
-      value_t comp = std::move(it->second);
-      g_store().erase(it);
-      return comp;
-    }
-
-    return value_t(std::forward<Ts>(args)...);
-  }
-
-  static inline void release(
-      const key_t& key, const value_t& computation) {
-    g_store().insert(std::make_pair(key, computation));
-  }
-
-  static inline void release(
-      const key_t& key, value_t&& computation) {
-    g_store().insert(std::make_pair(key, std::move(computation)));
-  }
-
-  static lru_cache<key_t, value_t> &g_store() {
-    static lru_cache<key_t, value_t> g_store_(capacity);
-    return g_store_;
-  }
-};
-
-}
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/memory_access_utils.hpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/memory_access_utils.hpp
deleted file mode 100644
index a656807..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/memory_access_utils.hpp
+++ /dev/null
@@ -1,511 +0,0 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
-
-#pragma once
-
-#include "compatible.hpp"
-
-/////////////////////////////// Memory Access Utils ///////////////////////////////
-namespace mem_access {
-
-enum class LoadPolicy {
-    CacheAll,       // Cache at all levels
-    CacheGlobal,    // Cache at L2 only
-    CacheStreaming  // Cache with evict first policy
-};
-
-enum class StorePolicy {
-    Writeback,      // Cache in L1, write-back on eviction
-    CacheGlobal,    // Bypass L1, write-back on eviction
-    CacheStreaming  // Allocate cache line with evict first policy
-};
-
-template <int AccessSize, LoadPolicy policy = LoadPolicy::CacheAll>
-inline void load_global(void* dst, const void* src);
-
-template <int AccessSize, LoadPolicy policy = LoadPolicy::CacheAll>
-inline void load_global(void* dst, const void* src, bool do_access);
-
-// Shared accesses have no cache policy
-template <int AccessSize>
-inline void load_shared(void* dst, const void* src);
-
-template <int AccessSize>
-inline void load_shared(void* dst, const void* src, bool do_access);
-
-template <int AccessSize, StorePolicy policy = StorePolicy::Writeback>
-inline void store_global(void* dst, const void* src);
-
-// Shared accesses have no cache policy
-template <int AccessSize>
-inline void store_shared(void* dst, const void* src);
-
-
-// Util for tracking pipeline buffers
-// TODO: Evaluate whether this should also be guarded by ASYNC_COPY_AVAILABLE
-template <int max>
-class BufferTracker {
-public:
-    int current_state;
-
-    inline BufferTracker() : current_state(0) {}
-
-    inline int get()
-    {
-        int return_val = current_state++;
-        current_state = (current_state == max ? 0 : current_state);
-        return return_val;
-    }
-};
-
-inline uint32_t lane_id()
-{
-    auto pos = sycl::ext::oneapi::experimental::this_nd_item<1>();
-    return pos.get_local_id(0) & (warpSize - 1);  // Portable
-}
-
-/////////// Load Global ///////////
-template <>
-inline void load_global<16>(void* dst, const void* src)
-{
-    uint4* data = reinterpret_cast<uint4*>(dst);
-    const uint4* src_cast = reinterpret_cast<const uint4*>(src);
-    data[0] = src_cast[0];
-}
-
-template <>
-inline void load_global<16>(void* dst, const void* src, bool do_access)
-{
-    uint4* data = reinterpret_cast<uint4*>(dst);
-    const uint4* src_cast = reinterpret_cast<const uint4*>(src);
-    if (do_access) {
-        data[0] = src_cast[0];
-    } else {
-        data[0].x() = 0;
-        data[0].y() = 0;
-        data[0].z() = 0;
-        data[0].w() = 0;
-    }
-}
-
-template <>
-inline void load_global<16, LoadPolicy::CacheGlobal>(void* dst, const void* src)
-{
-    uint4* data = reinterpret_cast<uint4*>(dst);
-    const uint4* src_cast = reinterpret_cast<const uint4*>(src);
-    data[0] = src_cast[0];
-}
-
-template <>
-inline void load_global<16, LoadPolicy::CacheGlobal>(void* dst,
-                                                                         const void* src,
-                                                                         bool do_access)
-{
-    uint4* data = reinterpret_cast<uint4*>(dst);
-    const uint4* src_cast = reinterpret_cast<const uint4*>(src);
-    if (do_access) {
-        data[0] = src_cast[0];
-    } else {
-        data[0].x() = 0;
-        data[0].y() = 0;
-        data[0].z() = 0;
-        data[0].w() = 0;
-    }
-}
-
-template <>
-inline void load_global<16, LoadPolicy::CacheStreaming>(void* dst,
-                                                                            const void* src)
-{
-    uint4* data = reinterpret_cast<uint4*>(dst);
-    const uint4* src_cast = reinterpret_cast<const uint4*>(src);
-    data[0] = src_cast[0];
-}
-
-template <>
-inline void load_global<16, LoadPolicy::CacheStreaming>(void* dst,
-                                                                            const void* src,
-                                                                            bool do_access)
-{
-    uint4* data = reinterpret_cast<uint4*>(dst);
-    const uint4* src_cast = reinterpret_cast<const uint4*>(src);
-    if (do_access) {
-        data[0] = src_cast[0];
-    } else {
-        data[0].x() = 0;
-        data[0].y() = 0;
-        data[0].z() = 0;
-        data[0].w() = 0;
-    }
-}
-
-template <>
-inline void load_global<8>(void* dst, const void* src)
-{
-    uint2* data = reinterpret_cast<uint2*>(dst);
-    const uint2* src_cast = reinterpret_cast<const uint2*>(src);
-    data[0] = src_cast[0];
-}
-
-template <>
-inline void load_global<8>(void* dst, const void* src, bool do_access)
-{
-    uint2* data = reinterpret_cast<uint2*>(dst);
-    const uint2* src_cast = reinterpret_cast<const uint2*>(src);
-    if (do_access) {
-        data[0] = src_cast[0];
-    } else {
-        data[0].x() = 0;
-        data[0].y() = 0;
-    }
-}
-
-template <>
-inline void load_global<8, LoadPolicy::CacheGlobal>(void* dst, const void* src)
-{
-    uint2* data = reinterpret_cast<uint2*>(dst);
-    const uint2* src_cast = reinterpret_cast<const uint2*>(src);
-    data[0] = src_cast[0];
-}
-
-template <>
-inline void load_global<8, LoadPolicy::CacheGlobal>(void* dst,
-                                                                        const void* src,
-                                                                        bool do_access)
-{
-    uint2* data = reinterpret_cast<uint2*>(dst);
-    const uint2* src_cast = reinterpret_cast<const uint2*>(src);
-    if (do_access) {
-        data[0] = src_cast[0];
-    } else {
-        data[0].x() = 0;
-        data[0].y() = 0;
-    }
-}
-
-template <>
-inline void load_global<8, LoadPolicy::CacheStreaming>(void* dst,
-                                                                           const void* src)
-{
-    uint2* data = reinterpret_cast<uint2*>(dst);
-    const uint2* src_cast = reinterpret_cast<const uint2*>(src);
-    data[0] = src_cast[0];
-}
-
-template <>
-inline void load_global<8, LoadPolicy::CacheStreaming>(void* dst,
-                                                                           const void* src,
-                                                                           bool do_access)
-{
-    uint2* data = reinterpret_cast<uint2*>(dst);
-    const uint2* src_cast = reinterpret_cast<const uint2*>(src);
-    if (do_access) {
-        data[0] = src_cast[0];
-    } else {
-        data[0].x() = 0;
-        data[0].y() = 0;
-    }
-}
-
-template <>
-inline void load_global<4>(void* dst, const void* src)
-{
-    int32_t* data = reinterpret_cast<int32_t*>(dst);
-    const int32_t* src_cast = reinterpret_cast<const int32_t*>(src);
-    data[0] = src_cast[0];
-}
-
-template <>
-inline void load_global<4>(void* dst, const void* src, bool do_access)
-{
-    int32_t* data = reinterpret_cast<int32_t*>(dst);
-    const int32_t* src_cast = reinterpret_cast<const int32_t*>(src);
-    if (do_access) {
-        data[0] = src_cast[0];
-    } else {
-        data[0] = 0;
-    }
-}
-
-template <>
-inline void load_global<4, LoadPolicy::CacheGlobal>(void* dst, const void* src)
-{
-    int32_t* data = reinterpret_cast<int32_t*>(dst);
-    const int32_t* src_cast = reinterpret_cast<const int32_t*>(src);
-    data[0] = src_cast[0];
-}
-
-template <>
-inline void load_global<4, LoadPolicy::CacheGlobal>(void* dst,
-                                                                        const void* src,
-                                                                        bool do_access)
-{
-    int32_t* data = reinterpret_cast<int32_t*>(dst);
-    const int32_t* src_cast = reinterpret_cast<const int32_t*>(src);
-    if (do_access) {
-        data[0] = src_cast[0];
-    } else {
-        data[0] = 0;
-    }
-}
-
-template <>
-inline void load_global<4, LoadPolicy::CacheStreaming>(void* dst,
-                                                                           const void* src)
-{
-    int32_t* data = reinterpret_cast<int32_t*>(dst);
-    const int32_t* src_cast = reinterpret_cast<const int32_t*>(src);
-    data[0] = src_cast[0];
-}
-
-template <>
-inline void load_global<4, LoadPolicy::CacheStreaming>(void* dst,
-                                                                           const void* src,
-                                                                           bool do_access)
-{
-    int32_t* data = reinterpret_cast<int32_t*>(dst);
-    const int32_t* src_cast = reinterpret_cast<const int32_t*>(src);
-    if (do_access) {
-        data[0] = src_cast[0];
-    } else {
-        data[0] = 0;
-    }
-}
-
-template <>
-inline void load_global<2>(void* dst, const void* src)
-{
-    int16_t* data = reinterpret_cast<int16_t*>(dst);
-    const int16_t* src_cast = reinterpret_cast<const int16_t*>(src);
-    data[0] = src_cast[0];
-}
-
-template <>
-inline void load_global<2>(void* dst, const void* src, bool do_access)
-{
-    int16_t* data = reinterpret_cast<int16_t*>(dst);
-    const int16_t* src_cast = reinterpret_cast<const int16_t*>(src);
-    if (do_access) {
-        data[0] = src_cast[0];
-    } else {
-        data[0] = 0;
-    }
-}
-
-template <>
-inline void load_global<2, LoadPolicy::CacheGlobal>(void* dst, const void* src)
-{
-    int16_t* data = reinterpret_cast<int16_t*>(dst);
-    const int16_t* src_cast = reinterpret_cast<const int16_t*>(src);
-    data[0] = src_cast[0];
-}
-
-template <>
-inline void load_global<2, LoadPolicy::CacheGlobal>(void* dst,
-                                                                        const void* src,
-                                                                        bool do_access)
-{
-    int16_t* data = reinterpret_cast<int16_t*>(dst);
-    const int16_t* src_cast = reinterpret_cast<const int16_t*>(src);
-    if (do_access) {
-        data[0] = src_cast[0];
-    } else {
-        data[0] = 0;
-    }
-}
-
-template <>
-inline void load_global<2, LoadPolicy::CacheStreaming>(void* dst,
-                                                                           const void* src)
-{
-    int16_t* data = reinterpret_cast<int16_t*>(dst);
-    const int16_t* src_cast = reinterpret_cast<const int16_t*>(src);
-    data[0] = src_cast[0];
-}
-
-template <>
-inline void load_global<2, LoadPolicy::CacheStreaming>(void* dst,
-                                                                           const void* src,
-                                                                           bool do_access)
-{
-    int16_t* data = reinterpret_cast<int16_t*>(dst);
-    const int16_t* src_cast = reinterpret_cast<const int16_t*>(src);
-    if (do_access) {
-        data[0] = src_cast[0];
-    } else {
-        data[0] = 0;
-    }
-}
-
-template <>
-inline void load_shared<16>(void* dst, const void* src)
-{
-    uint4* data = reinterpret_cast<uint4*>(dst);
-    const uint4* src_cast = reinterpret_cast<const uint4*>(src);
-    data[0] = src_cast[0];
-}
-
-template <>
-inline void load_shared<16>(void* dst, const void* src, bool do_access)
-{
-    uint4* data = reinterpret_cast<uint4*>(dst);
-    const uint4* src_cast = reinterpret_cast<const uint4*>(src);
-    if (do_access) {
-        data[0] = src_cast[0];
-    } else {
-        data[0].x() = 0;
-        data[0].y() = 0;
-        data[0].z() = 0;
-        data[0].w() = 0;
-    }
-}
-
-template <>
-inline void load_shared<8>(void* dst, const void* src)
-{
-    uint2* data = reinterpret_cast<uint2*>(dst);
-    const uint2* src_cast = reinterpret_cast<const uint2*>(src);
-    data[0] = src_cast[0];
-}
-
-template <>
-inline void load_shared<8>(void* dst, const void* src, bool do_access)
-{
-    uint2* data = reinterpret_cast<uint2*>(dst);
-    const uint2* src_cast = reinterpret_cast<const uint2*>(src);
-    if (do_access) {
-        data[0] = src_cast[0];
-    } else {
-        data[0].x() = 0;
-        data[0].y() = 0;
-    }
-}
-
-template <>
-inline void load_shared<4>(void* dst, const void* src)
-{
-    int32_t* data = reinterpret_cast<int32_t*>(dst);
-    const int32_t* src_cast = reinterpret_cast<const int32_t*>(src);
-    data[0] = src_cast[0];
-}
-
-template <>
-inline void load_shared<4>(void* dst, const void* src, bool do_access)
-{
-    int32_t* data = reinterpret_cast<int32_t*>(dst);
-    const int32_t* src_cast = reinterpret_cast<const int32_t*>(src);
-    if (do_access) {
-        data[0] = src_cast[0];
-    } else {
-        data[0] = 0;
-    }
-}
-
-/////////// Store Global ///////////
-
-template <>
-inline void store_global<16>(void* dst, const void* src)
-{
-    const uint4* data = reinterpret_cast<const uint4*>(src);
-    uint4* dst_cast = reinterpret_cast<uint4*>(dst);
-    dst_cast[0] = data[0];
-}
-
-template <>
-inline void store_global<16, StorePolicy::CacheGlobal>(void* dst,
-                                                                           const void* src)
-{
-    const uint4* data = reinterpret_cast<const uint4*>(src);
-    uint4* dst_cast = reinterpret_cast<uint4*>(dst);
-    dst_cast[0] = data[0];
-}
-
-template <>
-inline void store_global<16, StorePolicy::CacheStreaming>(void* dst,
-                                                                              const void* src)
-{
-    const uint4* data = reinterpret_cast<const uint4*>(src);
-    uint4* dst_cast = reinterpret_cast<uint4*>(dst);
-    dst_cast[0] = data[0];
-}
-
-template <>
-inline void store_global<8>(void* dst, const void* src)
-{
-    const uint2* data = reinterpret_cast<const uint2*>(src);
-    uint2* dst_cast = reinterpret_cast<uint2*>(dst);
-    dst_cast[0] = data[0];
-}
-
-template <>
-inline void store_global<8, StorePolicy::CacheGlobal>(void* dst,
-                                                                          const void* src)
-{
-    const uint2* data = reinterpret_cast<const uint2*>(src);
-    uint2* dst_cast = reinterpret_cast<uint2*>(dst);
-    dst_cast[0] = data[0];
-}
-
-template <>
-inline void store_global<8, StorePolicy::CacheStreaming>(void* dst,
-                                                                             const void* src)
-{
-    const uint2* data = reinterpret_cast<const uint2*>(src);
-    uint2* dst_cast = reinterpret_cast<uint2*>(dst);
-    dst_cast[0] = data[0];
-}
-
-template <>
-inline void store_global<4>(void* dst, const void* src)
-{
-    const int32_t* data = reinterpret_cast<const int32_t*>(src);
-    int32_t* dst_cast = reinterpret_cast<int32_t*>(dst);
-    dst_cast[0] = data[0];
-}
-
-template <>
-inline void store_global<4, StorePolicy::CacheGlobal>(void* dst,
-                                                                          const void* src)
-{
-    const int32_t* data = reinterpret_cast<const int32_t*>(src);
-    int32_t* dst_cast = reinterpret_cast<int32_t*>(dst);
-    dst_cast[0] = data[0];
-}
-
-template <>
-inline void store_global<4, StorePolicy::CacheStreaming>(void* dst,
-                                                                             const void* src)
-{
-    const int32_t* data = reinterpret_cast<const int32_t*>(src);
-    int32_t* dst_cast = reinterpret_cast<int32_t*>(dst);
-    dst_cast[0] = data[0];
-}
-
-/////////// Store Shared ///////////
-
-template <>
-inline void store_shared<16>(void* dst, const void* src)
-{
-    const uint4* data = reinterpret_cast<const uint4*>(src);
-    uint4* dst_cast = reinterpret_cast<uint4*>(dst);
-    dst_cast[0] = data[0];
-}
-
-template <>
-inline void store_shared<8>(void* dst, const void* src)
-{
-    const uint2* data = reinterpret_cast<const uint2*>(src);
-    uint2* dst_cast = reinterpret_cast<uint2*>(dst);
-    dst_cast[0] = data[0];
-}
-
-template <>
-inline void store_shared<4>(void* dst, const void* src)
-{
-    const int32_t* data = reinterpret_cast<const int32_t*>(src);
-    int32_t* dst_cast = reinterpret_cast<int32_t*>(dst);
-    dst_cast[0] = data[0];
-}
-
-}  // namespace mem_access
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/reduction_utils.hpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/reduction_utils.hpp
deleted file mode 100644
index d0d0008..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/transformer/inference/includes/reduction_utils.hpp
+++ /dev/null
@@ -1,548 +0,0 @@
-/*
-Copyright 2022 The Microsoft DeepSpeed Team
-*/
-
-#pragma once
-
-#include "conversion_utils.hpp"
-#include "compatible.hpp"
-#include "memory_access_utils.hpp"
-
-namespace reduce {
-
-enum class ROpType {
-    // Addition
-    Add,
-
-    // Maximum reduction
-    Max,
-
-    // Minimum reduction
-    Min,
-};
-
-constexpr int max_threads = 1024;
-constexpr int max_warps = max_threads / hw_warp_size;
-
-
-template <ROpType Op, int warp_bound = max_warps>
-void block(sycl::group<2>& tb, 
-           sycl::sub_group& warp, 
-           float& val);
-
-template <ROpType Op1, ROpType Op2, int warp_bound = max_warps>
-void block(sycl::group<2>& tb,
-           sycl::sub_group& warp,
-           float& val1,
-           float& val2);
-
-template <ROpType Op1, ROpType Op2, ROpType Op3, int warp_bound = max_warps>
-void block(sycl::group<2>& tb,
-                       sycl::sub_group& warp,
-                       float& val1,
-                       float& val2,
-                       float& val3);
-
-template <ROpType Op1, ROpType Op2, ROpType Op3, ROpType Op4, int warp_bound = max_warps>
-void block(sycl::group<2>& tb,
-           sycl::sub_group& warp,
-           float& val1,
-           float& val2,
-           float& val3,
-           float& val4);
-
-/*
-The partitioned block is a special case of the above where in the warps of a threadblock are
-partitioned into separate independent reductions. For example, I might have an 8 warp thread block
-in which each pair of warps is processing an independent piece of data. I would then reduce that
-data with the something like the following:
-``` cpp
-float max_val;
-reduce::partitioned_block<rop::Max, 2>(tb, warp, max_val);
-```
-After which, each pair of warps would have coherent data with each other. Note, this API will not
-provide correct results if the number of warps per partition is not a power of 2.
-*/
-template <ROpType Op, int num_threads>
-void partitioned_block(sycl::group<2>& tb,
-                                   sycl::sub_group& warp,
-                                   float& val);
-
-template <ROpType Op1, ROpType Op2, int num_threads>
-void partitioned_block(sycl::group<2>& tb,
-                                   sycl::sub_group& warp,
-                                   float& val1,
-                                   float& val2);
-
-template <ROpType Op1, ROpType Op2, ROpType Op3, int num_threads>
-void partitioned_block(sycl::group<2>& tb,
-                                   sycl::sub_group& warp,
-                                   float& val1,
-                                   float& val2,
-                                   float& val3);
-
-template <ROpType Op1, ROpType Op2, ROpType Op3, ROpType Op4, int num_threads>
-void partitioned_block(sycl::group<2>& tb,
-                                   sycl::sub_group& warp,
-                                   float& val1,
-                                   float& val2,
-                                   float& val3,
-                                   float& val4);
-
-/*
-Single element reduction primitives. Used inside serial collection
-loops.
-
-Example usage:
-using rop = reduce::OpType;
-float min = init<rop::Min>();
-for (int i = 0; i < 4; i++) {
-    min = reduce::element<rop::Min>(min, data[i]);
-}
-*/
-
-template <ROpType Op, typename T>
-T element(const T lhs, const T rhs);
-
-template <ROpType OType, typename T = float>
-T init();
-
-/********************** Internal reduction APIs **********************/
-
-/*
-Single element "reductions". TODO(cmikeh2): this sort of "op" concept
-should be refactored into its own implementation at some point. This interface
-may be easily expanded for new types/operations, but the typical reductions
-we need are covered with min/max/add on float.
-
-NOTE: there is no mean reduction because that relies on knowledge of how
-many values were already reduced into each scalar. Implementing this on top
-of reduce should be straightforward (can just wrap the sum reduction) and
-would be a good extension of the header.
-*/
-
-/* Float element reduce implementations */
-template <>
-float element<ROpType::Add>(const float lhs, const float rhs)
-{
-    return lhs + rhs;
-}
-
-template <>
-float element<ROpType::Max>(const float lhs, const float rhs)
-{
-    return fmaxf(lhs, rhs);
-}
-
-template <>
-float element<ROpType::Min>(const float lhs, const float rhs)
-{
-    return fminf(lhs, rhs);
-}
-
-/* sycl::half element reduce implementation */
-template <>
-sycl::half element<ROpType::Add>(const sycl::half lhs, const sycl::half rhs)
-{
-    return lhs + rhs;
-}
-
-template <>
-sycl::half element<ROpType::Max>(const sycl::half lhs, const sycl::half rhs)
-{
-    return (lhs > rhs) ? lhs : rhs;
-}
-
-template <>
-sycl::half element<ROpType::Min>(const sycl::half lhs, const sycl::half rhs)
-{
-    return (lhs < rhs) ? lhs : rhs;
-}
-
-/* sycl::half2 element reduce implementation */
-template <>
-sycl::half2 element<ROpType::Add>(const sycl::half2 lhs, const sycl::half2 rhs)
-{
-    return lhs + rhs;
-}
-
-template <>
-sycl::half2 element<ROpType::Max>(const sycl::half2 lhs, const sycl::half2 rhs)
-{
-    sycl::half2 ret_val;
-    ret_val[0] = (lhs[0] > rhs[0]) ? lhs[0] : rhs[0];
-    ret_val[1] = (lhs[1] > rhs[1]) ? lhs[1] : rhs[1];
-    return ret_val;
-}
-
-template <>
-sycl::half2 element<ROpType::Min>(const sycl::half2 lhs, const sycl::half2 rhs)
-{
-    sycl::half2 ret_val;
-    ret_val[0] = (lhs[0] < rhs[0]) ? lhs[0] : rhs[0];
-    ret_val[1] = (lhs[1] < rhs[1]) ? lhs[1] : rhs[1];
-    return ret_val;
-}
-
-/*
-Reduction initialization primitives
-*/
-template <>
-float init<ROpType::Add>()
-{
-    return 0.0f;
-}
-
-template <>
-float init<ROpType::Min>()
-{
-    // Positive infinity
-    return INFINITY;
-}
-
-template <>
-float init<ROpType::Max>()
-{
-    // Negative infinity
-    return -INFINITY;
-}
-
-template <>
-sycl::half init<ROpType::Add>()
-{
-    constexpr sycl::half zero = 0.0;
-    return sycl::half(zero);
-}
-
-template <>
-sycl::half init<ROpType::Min>()
-{
-    constexpr sycl::half inf = std::numeric_limits<sycl::half>::infinity();
-    return sycl::half(inf);
-}
-
-template <>
-sycl::half init<ROpType::Max>()
-{
-    constexpr sycl::half neg_inf = -std::numeric_limits<sycl::half>::infinity();
-    return sycl::half(neg_inf);
-}
-
-template <>
-sycl::half2 init<ROpType::Add>()
-{
-    return {0.0, 0.0};
-}
-
-template <>
-sycl::half2 init<ROpType::Min>()
-{
-    return {std::numeric_limits<sycl::half>::infinity(), std::numeric_limits<sycl::half>::infinity()};
-}
-
-template <>
-sycl::half2 init<ROpType::Max>()
-{
-    return {-std::numeric_limits<sycl::half>::infinity(), -std::numeric_limits<sycl::half>::infinity()};
-}
-
-template <ROpType Op, typename T>
-void init(T* data)
-{
-    data[0] = init<Op, T>();
-}
-
-template <ROpType Op1, ROpType Op2, typename T>
-void init(T* data)
-{
-    data[0] = init<Op1, T>();
-    data[1] = init<Op2, T>();
-}
-
-template <ROpType Op1, ROpType Op2, ROpType Op3, typename T>
-void init(T* data)
-{
-    data[0] = init<Op1, T>();
-    data[1] = init<Op2, T>();
-    data[2] = init<Op3, T>();
-}
-
-template <ROpType Op1, ROpType Op2, ROpType Op3, ROpType Op4, typename T>
-void init(T* data)
-{
-    data[0] = init<Op1, T>();
-    data[1] = init<Op2, T>();
-    data[2] = init<Op3, T>();
-    data[3] = init<Op4, T>();
-}
-
-/*
-Warp reduction primitives
-
-`reduction_width` is an unsafe template parameter, that is that
-when using `reduction_width` < hw_warp_size the warp is partitioned
-into `hw_warp_size` / `reduction_width` groups of partial sums.
-
-If someone can figure out how to use variadic templates in a reasonable way
-here (fold is C++17 only and I don't think helps and recursion feels like
-huge overkill that harms readability) that would be wonderful.
-*/
-
-template <ROpType Op, int reduce_width = hw_warp_size>
-void _warp(sycl::sub_group& warp, float* data)
-{
-#pragma unroll
-    for (int i = 1; i < reduce_width; i *= 2) {
-        data[0] = element<Op>(data[0], warp.shuffle_xor(data[0], i));
-    }
-}
-
-template <ROpType Op1, ROpType Op2, int reduce_width = hw_warp_size>
-void _warp(sycl::sub_group& warp, float* data)
-{
-#pragma unroll
-    for (int i = 1; i < reduce_width; i *= 2) {
-        data[0] = element<Op1>(data[0], warp.shuffle_xor(data[0], i));
-        data[1] = element<Op2>(data[1], warp.shuffle_xor(data[1], i));
-    }
-}
-
-template <ROpType Op1, ROpType Op2, ROpType Op3, int reduce_width = hw_warp_size>
-void _warp(sycl::sub_group& warp, float* data)
-{
-#pragma unroll
-    for (int i = 1; i < reduce_width; i *= 2) {
-        data[0] = element<Op1>(data[0], warp.shuffle_xor(data[0], i));
-        data[1] = element<Op2>(data[1], warp.shuffle_xor(data[1], i));
-        data[2] = element<Op3>(data[2], warp.shuffle_xor(data[2], i));
-    }
-}
-
-template <ROpType Op1, ROpType Op2, ROpType Op3, ROpType Op4, int reduce_width = hw_warp_size>
-void _warp(sycl::sub_group& warp, float* data)
-{
-#pragma unroll
-    for (int i = 1; i < reduce_width; i *= 2) {
-        data[0] = element<Op1>(data[0], warp.shuffle_xor(data[0], i));
-        data[1] = element<Op2>(data[1], warp.shuffle_xor(data[1], i));
-        data[2] = element<Op3>(data[2], warp.shuffle_xor(data[2], i));
-        data[3] = element<Op4>(data[3], warp.shuffle_xor(data[3], i));
-    }
-}
-
-/*
-Implementation for primary block reduction that serves both `block` and
-`partitioned_block`.
-
-`local_warp_rank` refers to the warp's location within the partition, so
-for an unpartitioned threadblock this will be equivalent to
-`warp_arg.meta_group_rank()`.
-
-Similarly, the warp offset is the `local_warp_rank` of the warp with the
-lowest rank in the partition. In the case of an 8 warp block with a
-4 warp reduction, this would map to [0, 0, 0, 0, 4, 4, 4, 4].
-
-Partition size is the number of warps per partition (equal to the thread
-block in the default case). This enables us to only perform the warp reduction
-when able to.
-*/
-template <int total_warps, ROpType... Ops>
-void _block(sycl::group<2>& tb,
-                        sycl::sub_group& warp_arg,
-                        float* data,
-                        int warp_offset)
-{
-    constexpr int elems = sizeof...(Ops);
-    // Separated for now in case this no longer is true
-    constexpr int bytes = sizeof(float);
-    // Unused when `partition_size == 1` or total_warps == 1
-    auto reduce_buffer = __group_local_memory<float[max_warps * elems]>(tb);
-
-    // Always perform warp-scope reduction
-    _warp<Ops...>(warp_arg, data);
-
-    // If max_warps == 1 let's skip the runtime check
-    if (warp_arg.get_group_range().size() > 1 && total_warps != 1) {
-        if (warp_arg.get_local_id() == 0) {
-#pragma unroll
-            for (int i = 0; i < elems; i++) {
-                mem_access::store_shared<bytes>(
-                    reduce_buffer + elems * warp_arg.get_group_id() + i, data + i);
-            }
-        }
-
-        // Synchronization inside block-uniform conditional is safe
-        sycl::group_barrier(tb, tb.fence_scope);
-
-        if (warp_arg.get_group_id() == 0) {
-            if (warp_arg.get_local_id() < warp_arg.get_group_range()) {
-#pragma unroll
-                for (int i = 0; i < elems; i++) {
-                    mem_access::load_shared<bytes>(
-                        data + i, reduce_buffer + elems * warp_arg.get_local_id() + i);
-                }
-            } else {
-                init<Ops...>(data);
-            }
-
-            _warp<Ops..., total_warps>(warp_arg, data);
-
-#pragma unroll
-            for (int i = 0; i < elems; i++) {
-                mem_access::store_shared<bytes>(reduce_buffer + elems * warp_arg.get_local_id() + i,
-                                                data + i);
-            }
-        }
-
-        // Synchronization inside block-uniform conditional is safe
-        sycl::group_barrier(tb, tb.fence_scope);
-
-#pragma unroll
-        for (int i = 0; i < elems; i++) {
-            mem_access::load_shared<bytes>(data + i,
-                                           reduce_buffer + warp_arg.get_group_id() * elems + i);
-        }
-    }
-}
-
-/*
-Main API implementations. For the most part, they just convert the individual
-variables into arrays, which makes working with them easier with a single
-implementation. In theory, we could use the `_block` implementation as another
-option, but the nature of using a pointer is a little less safe and this allows
-us to obfuscate the details of the partitioned implementation.
-*/
-template <ROpType Op, int warp_bound>
-void block(sycl::group<2>& tb, 
-           sycl::sub_group& warp, float& val)
-{
-    _block<warp_bound, Op>(tb, warp, &val, 0);
-}
-
-template <ROpType Op1, ROpType Op2, int warp_bound>
-void block(sycl::group<2>& tb,
-                       sycl::sub_group& warp,
-                       float& val1,
-                       float& val2)
-{
-    float data[2] = {val1, val2};
-    _block<warp_bound, Op1, Op2>(tb, warp, data, 0);
-    val1 = data[0];
-    val2 = data[1];
-}
-
-template <ROpType Op1, ROpType Op2, ROpType Op3, int warp_bound>
-void block(sycl::group<2>& tb,
-                       sycl::sub_group& warp,
-                       float& val1,
-                       float& val2,
-                       float& val3)
-{
-    float data[3] = {val1, val2, val3};
-    _block<warp_bound, Op1, Op2, Op3>(tb, warp, data, 0);
-    val1 = data[0];
-    val2 = data[1];
-    val3 = data[2];
-}
-
-template <ROpType Op1, ROpType Op2, ROpType Op3, ROpType Op4, int warp_bound>
-void block(sycl::group<2>& tb,
-           sycl::sub_group& warp,
-           float& val1,
-           float& val2,
-           float& val3,
-           float& val4)
-{
-    float data[4] = {val1, val2, val3, val4};
-    _block<warp_bound, Op1, Op2, Op3, Op4>(tb, warp, data, 0);
-    val1 = data[0];
-    val2 = data[1];
-    val3 = data[2];
-    val4 = data[3];
-}
-
-/*
-Note: for the partitioned blocks, the implementation does not support non-power of 2 blocks in order
-to shorten block scale reduction length.
-*/
-template <ROpType Op, int num_threads>
-void partitioned_block(sycl::group<2>& tb,
-                       sycl::sub_group& warp,
-                       float& val)
-{
-    if (num_threads <= hw_warp_size) {
-        _warp<Op, num_threads>(warp, &val);
-    } else {
-        constexpr int num_warps = num_threads / hw_warp_size;
-        const int warp_offset = warp.get_group_id() & ~(num_warps - 1);
-        _block<num_warps, Op>(tb, warp, &val, warp_offset);
-    }
-}
-
-template <ROpType Op1, ROpType Op2, int num_threads>
-void partitioned_block(sycl::group<2>& tb,
-                                   sycl::sub_group& warp,
-                                   float& val1,
-                                   float& val2)
-{
-    float data[2] = {val1, val2};
-
-    if (num_threads <= hw_warp_size) {
-        _warp<Op1, Op2, num_threads>(warp, data);
-    } else {
-        constexpr int num_warps = num_threads / hw_warp_size;
-        const int warp_offset = warp.get_group_id() & ~(num_warps - 1);
-        _block<num_warps, Op1, Op2>(tb, warp, data, warp_offset);
-    }
-
-    val1 = data[0];
-    val2 = data[1];
-}
-
-template <ROpType Op1, ROpType Op2, ROpType Op3, int num_threads>
-void partitioned_block(sycl::group<2>& tb,
-                                   sycl::sub_group& warp,
-                                   float& val1,
-                                   float& val2,
-                                   float& val3)
-{
-    float data[3] = {val1, val2, val3};
-
-    if (num_threads <= hw_warp_size) {
-        _warp<Op1, Op2, Op3, num_threads>(warp, data);
-    } else {
-        constexpr int num_warps = num_threads / hw_warp_size;
-        const int warp_offset = warp.get_group_id() & ~(num_warps - 1);
-        _block<num_warps, Op1, Op2, Op3>(tb, warp, data, warp_offset);
-    }
-
-    val1 = data[0];
-    val2 = data[1];
-    val3 = data[2];
-}
-
-template <ROpType Op1, ROpType Op2, ROpType Op3, ROpType Op4, int num_threads>
-void partitioned_block(sycl::group<2>& tb,
-                       sycl::sub_group& warp,
-                       float& val1,
-                       float& val2,
-                       float& val3,
-                       float& val4)
-{
-    float data[4] = {val1, val2, val3, val4};
-
-    if (num_threads <= hw_warp_size) {
-        _warp<Op1, Op2, Op3, Op4, num_threads>(warp, data);
-    } else {
-        constexpr int num_warps = num_threads / hw_warp_size;
-        const int warp_offset = warp.get_group_id() & ~(num_warps - 1);
-        _block<num_warps, Op1, Op2, Op3, Op4>(tb, warp, data, warp_offset);
-    }
-
-    val1 = data[0];
-    val2 = data[1];
-    val3 = data[2];
-    val4 = data[3];
-}
-
-}  // namespace reduce
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/normalize_kernels.dp.cpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/normalize_kernels.dp.cpp
deleted file mode 100644
index 008dade..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/transformer/normalize_kernels.dp.cpp
+++ /dev/null
@@ -1,2536 +0,0 @@
-#if __has_include(<sycl/sycl.hpp>)
-#include <sycl/sycl.hpp>
-using namespace sycl;
-#elif __has_include(<CL/sycl.hpp>)
-#include <CL/sycl.hpp>
-using namespace cl::sycl;
-#else
-#error "Unsupported compiler"
-#endif
-#include "custom_sycl_layers.hpp"
-/*
-  Fused bias add, residual (elementwise) add, and normalization layer.
-
-  For FP16, this kernel does not promote to FP32 in order to utilize the 2x
-  throughput for
-  __half2 instructions, and avoid the conversion overhead (1/8 of __hal2
-  arithmetic).
-
-  For specific launch constraints, see the launch functions.
-*/
-
-#define NORM_REG (128)
-#define MAX_SG_NUM (32)
-#define MAX_SG_NUM1 (MAX_SG_NUM + 1)
-#define TILE_DIM (32)
-template <bool is_mean>
-void fused_bias_residual_layer_norm(float* vals,
-                                    const float* residual,
-                                    const float* gamma,
-                                    const float* beta,
-                                    float epsilon,
-                                    bool preLayerNorm,
-                                    bool training,
-                                    float* vars,
-                                    float* means,
-                                    int row_stride,
-                                    nd_item<3> item_ct1,
-                                    float* shr)
-{
-    int iteration_stride = item_ct1.get_local_range(2);
-    int iterations = row_stride / iteration_stride;
-
-    // sycl::group<3> b = item_ct1.get_group();
-    // cg::thread_block_tile<MAX_SG_NUM> g = cg::tiled_partition<MAX_SG_NUM>(b);
-    sub_group sg = item_ct1.get_sub_group();
-
-    int row = item_ct1.get_group(2);
-    int id = item_ct1.get_local_id(2);
-    // int gid = id / MAX_SG_NUM;
-    int gid = id / MAX_SG_NUM;
-
-    float vals_arr[NORM_REG];
-
-    residual += (row * row_stride);
-    vals += (row * row_stride);
-
-    float sum = 0.f;
-    int high_index = iterations * iteration_stride + id;
-#pragma unroll
-    for (int i = 0; i < iterations; i++) {
-        vals_arr[i] = residual[i * iteration_stride + id];
-        sum += vals_arr[i];
-    }
-    if (high_index < row_stride) {
-        vals_arr[iterations] = residual[high_index];
-        sum += vals_arr[iterations];
-        iterations++;
-    }
-
-    // for (int i = 1; i < 32; i *= 2) { sum += g.shfl_down(sum, i); }
-    for (int i = 1; i < MAX_SG_NUM; i *= 2) { sum += sg.shuffle_down(sum, i); }
-
-    if (sg.get_local_id() == 0) shr[gid] = sum;
-
-    item_ct1.barrier();
-
-    if (sg.get_local_id() < (iteration_stride >> 5)) sum = shr[sg.get_local_id()];
-
-#if !defined(__STOCHASTIC_MODE__)
-    item_ct1.barrier();
-#endif
-
-    for (int i = 1; i < (iteration_stride >> 5); i *= 2) { sum += sg.shuffle_down(sum, i); }
-
-    // sum = g.shfl(sum, 0);
-    sum = sg.shuffle(sum, 0);
-    float mean = sum / row_stride;
-    // if (training)
-    //     if (g.thread_rank() == 0) means[row] = mean;
-    if constexpr (is_mean) {
-        if (training)
-            if (sg.get_local_id() == 0) means[row] = mean;
-    }
-    float variance = 0.f;
-    for (int i = 0; i < iterations; i++) {
-        vals_arr[i] -= mean;
-        variance += vals_arr[i] * vals_arr[i];
-    }
-
-    // for (int i = 1; i < 32; i *= 2) { variance += g.shfl_down(variance, i); }
-    for (int i = 1; i < MAX_SG_NUM; i *= 2) { variance += sg.shuffle_down(variance, i); }
-
-    // if (g.thread_rank() == 0) shr[gid] = variance;
-    if (sg.get_local_id() == 0) shr[gid] = variance;
-
-    item_ct1.barrier();
-
-    if (sg.get_local_id() < (iteration_stride >> 5)) variance = shr[sg.get_local_id()];
-
-#ifndef __STOCHASTIC_MODE__
-
-    item_ct1.barrier();
-#endif
-
-    for (int i = 1; i < (iteration_stride >> 5); i *= 2) {
-        variance += sg.shuffle_down(variance, i);
-    }
-    variance = sg.shuffle(variance, 0);
-    variance /= row_stride;
-    variance += epsilon;
-
-    if (training)
-        if (sg.get_local_id() == 0) vars[row] = variance;
-
-    iterations = row_stride / iteration_stride;
-    for (int i = 0; i < iterations; i++) {
-        vals_arr[i] = vals_arr[i] * rsqrt(variance);
-        vals_arr[i] =
-            vals_arr[i] * gamma[i * iteration_stride + id] + beta[i * iteration_stride + id];
-        vals[i * iteration_stride + id] = vals_arr[i];
-    }
-    if ((high_index) < row_stride) {
-        vals_arr[iterations] = vals_arr[iterations] * rsqrt(variance);
-        vals_arr[iterations] = vals_arr[iterations] * gamma[high_index] + beta[high_index];
-        vals[high_index] = vals_arr[iterations];
-    }
-}
-
-template <bool is_mean>
-void fused_bias_residual_layer_norm(bf16* vals,
-                                    const bf16* residual,
-                                    const bf16* gamma,
-                                    const bf16* beta,
-                                    float epsilon,
-                                    bool preLayerNorm,
-                                    bool training,
-                                    bf16* vars,
-                                    bf16* means,
-                                    int row_stride,
-                                    nd_item<3> item_ct1,
-                                    float* shr)
-{
-    int iteration_stride = item_ct1.get_local_range(2);
-    int iterations = row_stride / iteration_stride;
-
-    // sycl::group<3> b = item_ct1.get_group();
-    // cg::thread_block_tile<MAX_SG_NUM> g = cg::tiled_partition<MAX_SG_NUM>(b);
-    sub_group sg = item_ct1.get_sub_group();
-
-    int row = item_ct1.get_group(2);
-    int id = item_ct1.get_local_id(2);
-    // int gid = id / MAX_SG_NUM;
-    int gid = id / MAX_SG_NUM;
-
-    float vals_arr[NORM_REG];
-
-    ushort* vals_cast = reinterpret_cast<ushort*>(vals);
-    const ushort* residual_cast = reinterpret_cast<const ushort*>(residual);
-    const ushort* gamma_cast = reinterpret_cast<const ushort*>(gamma);
-    const ushort* beta_cast = reinterpret_cast<const ushort*>(beta);
-    ushort* vars_cast = reinterpret_cast<ushort*>(vars);
-    ushort* means_cast = reinterpret_cast<ushort*>(means);
-
-    residual_cast += (row * row_stride);
-    vals_cast += (row * row_stride);
-
-    float sum = 0.f;
-    int high_index = iterations * iteration_stride + id;
-#pragma unroll
-    for (int i = 0; i < iterations; i++) {
-        vals_arr[i] = float(residual_cast[i * iteration_stride + id]);
-        sum += vals_arr[i];
-    }
-    if (high_index < row_stride) {
-        vals_arr[iterations] = float(residual_cast[high_index]);
-        sum += vals_arr[iterations];
-        iterations++;
-    }
-
-    for (int i = 1; i < MAX_SG_NUM; i *= 2) { sum += sg.shuffle_down(sum, i); }
-
-    if (sg.get_local_id() == 0) shr[gid] = sum;
-
-    item_ct1.barrier();
-
-    // if (g.thread_rank() < (iteration_stride >> 5)) sum = shr[g.thread_rank()];
-    if (sg.get_local_id() < (iteration_stride >> 5)) sum = shr[sg.get_local_id()];
-
-#if !defined(__STOCHASTIC_MODE__)
-
-    item_ct1.barrier();
-#endif
-
-    // for (int i = 1; i < (iteration_stride >> 5); i *= 2) { sum +=
-    // g.shfl_down(sum, i); }
-    for (int i = 1; i < (iteration_stride >> 5); i *= 2) { sum += sg.shuffle_down(sum, i); }
-
-    // sum = g.shfl(sum, 0);
-    sum = sg.shuffle(sum, 0);
-    float mean = sum / row_stride;
-    // if (training)
-    //     if (g.thread_rank() == 0) means[row] = mean;
-    if constexpr (is_mean) {
-        if (training)
-            if (sg.get_local_id() == 0) means_cast[row] = bf16(mean);
-    }
-    float variance = 0.f;
-    for (int i = 0; i < iterations; i++) {
-        vals_arr[i] -= mean;
-        variance += vals_arr[i] * vals_arr[i];
-    }
-
-    // for (int i = 1; i < 32; i *= 2) { variance += g.shfl_down(variance, i); }
-    for (int i = 1; i < MAX_SG_NUM; i *= 2) { variance += sg.shuffle_down(variance, i); }
-
-    // if (g.thread_rank() == 0) shr[gid] = variance;
-    if (sg.get_local_id() == 0) shr[gid] = variance;
-
-    item_ct1.barrier();
-
-    // if (g.thread_rank() < (iteration_stride >> 5)) variance =
-    // shr[g.thread_rank()];
-    if (sg.get_local_id() < (iteration_stride >> 5)) variance = shr[sg.get_local_id()];
-
-#ifndef __STOCHASTIC_MODE__
-
-    item_ct1.barrier();
-#endif
-
-    // for (int i = 1; i < (iteration_stride >> 5); i *= 2) { variance +=
-    // g.shfl_down(variance, i); }
-    for (int i = 1; i < (iteration_stride >> 5); i *= 2) {
-        variance += sg.shuffle_down(variance, i);
-    }
-    // variance = g.shfl(variance, 0);
-    variance = sg.shuffle(variance, 0);
-    variance /= row_stride;
-    variance += epsilon;
-    // if (training)
-    //     if (g.thread_rank() == 0) vars[row] = variance;
-    if (training)
-        if (sg.get_local_id() == 0) vars_cast[row] = bf16(variance);
-
-    iterations = row_stride / iteration_stride;
-    for (int i = 0; i < iterations; i++) {
-        vals_arr[i] = vals_arr[i] * rsqrt(variance);
-        vals_arr[i] = vals_arr[i] * float(gamma_cast[i * iteration_stride + id]) +
-                      float(beta_cast[i * iteration_stride + id]);
-        vals_cast[i * iteration_stride + id] = bf16(vals_arr[i]);
-    }
-    if ((high_index) < row_stride) {
-        vals_arr[iterations] = vals_arr[iterations] * rsqrt(variance);
-        vals_arr[iterations] = vals_arr[iterations] * float(gamma[high_index]) +
-                               float(beta[high_index]);
-        vals_cast[high_index] = bf16(vals_arr[iterations]);
-    }
-}
-
-template <bool is_mean>
-void fused_bias_residual_layer_norm(half* vals,
-                                    const half* residual,
-                                    const half* gamma,
-                                    const half* beta,
-                                    float epsilon,
-                                    bool preLayerNorm,
-                                    bool training,
-                                    half* vars,
-                                    half* means,
-                                    int row_stride,
-                                    nd_item<3> item_ct1,
-                                    float* shr)
-{
-    int iteration_stride = item_ct1.get_local_range(2);
-    int iterations = row_stride / iteration_stride;
-
-    // cg::thread_block b = cg::this_thread_block();
-    // cg::thread_block_tile<32> g = cg::tiled_partition<32>(b);
-    sub_group sg = item_ct1.get_sub_group();
-
-    int row = item_ct1.get_group(2);
-    int id = item_ct1.get_local_id(2);
-    // int gid = id >> 5;
-    int gid = id / MAX_SG_NUM;
-
-    float2 vals_f[NORM_REG];
-
-    half2* vals_cast = reinterpret_cast<half2*>(vals);
-    const half2* residual_cast = reinterpret_cast<const half2*>(residual);
-
-    residual_cast += (row * row_stride);
-    vals_cast += (row * row_stride);
-
-    float sum = 0.f;
-    int high_index = iterations * iteration_stride + id;
-#pragma unroll
-    // for (int i = 0; i < iterations; i++) {
-    //     vals_f[i] = __half22float2(residual_cast[i * iteration_stride + id]);
-    //     sum += vals_f[i].x;
-    //     sum += vals_f[i].y;
-    // }
-    for (int i = 0; i < iterations; i++) {
-        vals_f[i] = residual_cast[i * iteration_stride + id]
-                        .convert<float>();  // __half22float2(residual_cast[i *
-                                            // iteration_stride + id]);
-        sum += vals_f[i].x();
-        sum += vals_f[i].y();
-    }
-    // if ((high_index) < row_stride) {
-    //     vals_f[iterations] = __half22float2(residual_cast[high_index]);
-    //     sum += vals_f[iterations].x;
-    //     sum += vals_f[iterations].y;
-    //     iterations++;
-    // }
-    if ((high_index) < row_stride) {
-        vals_f[iterations] = residual_cast[high_index]
-                                 .convert<float>();  // __half22float2(residual_cast[high_index]);
-        sum += vals_f[iterations].x();
-        sum += vals_f[iterations].y();
-        iterations++;
-    }
-
-    // for (int i = 1; i < 32; i *= 2) { sum += g.shfl_down(sum, i); }
-    for (int i = 1; i < MAX_SG_NUM; i *= 2) { sum += sg.shuffle_down(sum, i); }
-
-    // if (g.thread_rank() == 0) shr[gid] = sum;
-    if (sg.get_local_id() == 0) shr[gid] = sum;
-
-    item_ct1.barrier();
-
-    // if (g.thread_rank() < (iteration_stride >> 5)) sum = shr[g.thread_rank()];
-    if (sg.get_local_id() < (iteration_stride >> 5)) sum = shr[sg.get_local_id()];
-
-#ifndef __STOCHASTIC_MODE__
-    // b.sync();
-    item_ct1.barrier();
-#endif
-
-    // for (int i = 1; i < (iteration_stride >> 5); i *= 2) { sum +=
-    // g.shfl_down(sum, i); }
-    for (int i = 1; i < (iteration_stride >> 5); i *= 2) { sum += sg.shuffle_down(sum, i); }
-    // sum = g.shfl(sum, 0);
-    sum = sg.shuffle(sum, 0);
-    float mean = sum / (row_stride * 2);
-
-    float variance = 0.f;
-    for (int i = 0; i < iterations; i++) {
-        vals_f[i].x() -= mean;
-        vals_f[i].y() -= mean;
-        variance += vals_f[i].x() * vals_f[i].x();
-        variance += vals_f[i].y() * vals_f[i].y();
-    }
-
-    // for (int i = 1; i < 32; i *= 2) { variance += g.shfl_down(variance, i); }
-    for (int i = 1; i < MAX_SG_NUM; i *= 2) { variance += sg.shuffle_down(variance, i); }
-
-    // if (g.thread_rank() == 0) shr[gid] = variance;
-    if (sg.get_local_id() == 0) shr[gid] = variance;
-
-    // b.sync();
-    item_ct1.barrier();
-
-    // if (g.thread_rank() < (iteration_stride >> 5)) variance =
-    // shr[g.thread_rank()];
-    if (sg.get_local_id() < (iteration_stride >> 5)) variance = shr[sg.get_local_id()];
-
-#ifndef __STOCHASTIC_MODE__
-    // b.sync();
-    item_ct1.barrier();
-#endif
-
-    // for (int i = 1; i < (iteration_stride >> 5); i *= 2) { variance +=
-    // g.shfl_down(variance, i); }
-    for (int i = 1; i < (iteration_stride >> 5); i *= 2) {
-        variance += sg.shuffle_down(variance, i);
-    }
-    // variance = g.shfl(variance, 0);
-    variance = sg.shuffle(variance, 0);
-    variance /= (row_stride * 2);
-    variance += epsilon;
-
-    half2 variance_h =
-        vec<float, 2>({variance, variance}).convert<half>();  // __float2half2_rn(variance);
-    const half2* gamma_cast = reinterpret_cast<const half2*>(gamma);
-    const half2* beta_cast = reinterpret_cast<const half2*>(beta);
-
-    // if (training && g.thread_rank() == 0) {
-    //     vars[row] = __float2half(variance);
-    //     means[row] = __float2half(mean);
-    // }
-    if (training && sg.get_local_id() == 0) {
-        vars[row] = vec<float, 1>(variance).convert<half>();  // __float2half(variance);
-        if constexpr (is_mean) {
-            means[row] = vec<float, 1>(mean).convert<half>();  // __float2half(mean);
-        }
-    }
-    iterations = row_stride / iteration_stride;
-    // for (int i = 0; i < iterations; i++) {
-    //     half2 vals_arr = __float22half2_rn(vals_f[i]);
-    //     vals_arr = vals_arr * h2rsqrt(variance_h);
-    //     vals_arr =
-    //         vals_arr * gamma_cast[i * iteration_stride + id] + beta_cast[i *
-    //         iteration_stride + id];
-    //     vals_cast[i * iteration_stride + id] = vals_arr;
-    // }
-    for (int i = 0; i < iterations; i++) {
-        half2 vals_arr = vals_f[i].convert<half>();  // __float22half2_rn(vals_f[i]);
-        vals_arr = vals_arr * rsqrt(variance_h);
-        vals_arr =
-            vals_arr * gamma_cast[i * iteration_stride + id] + beta_cast[i * iteration_stride + id];
-        vals_cast[i * iteration_stride + id] = vals_arr;
-    }
-    // if ((high_index) < row_stride) {
-    //     half2 vals_arr = __float22half2_rn(vals_f[iterations]);
-    //     vals_arr = vals_arr * h2rsqrt(variance_h);
-    //     vals_arr = vals_arr * gamma_cast[high_index] + beta_cast[high_index];
-    //     vals_cast[high_index] = vals_arr;
-    // }
-    if ((high_index) < row_stride) {
-        half2 vals_arr =
-            vals_f[iterations].convert<half>();  // __float22half2_rn(vals_f[iterations]);
-        vals_arr = vals_arr * rsqrt(variance_h);
-        vals_arr = vals_arr * gamma_cast[high_index] + beta_cast[high_index];
-        vals_cast[high_index] = vals_arr;
-    }
-}
-
-template <typename T>
-void launch_bias_residual_layer_norm(T* vals,
-                                     const T* residual,
-                                     const T* gamma,
-                                     const T* beta,
-                                     float epsilon,
-                                     int batch_size,
-                                     int hidden_dim,
-                                     sycl::queue* stream,
-                                     bool preLayerNorm,
-                                     bool training,
-                                     T* vars,
-                                     T* means)
-{
-    int threads = THREADS;
-
-    sycl::range<3> grid_dim(1, 1, batch_size);
-
-    if (hidden_dim > 16384 && hidden_dim <= 32768)
-        threads <<= 1;
-    else if (hidden_dim > 32768 && hidden_dim <= 65536)
-        threads <<= 2;
-    else if (hidden_dim > 65536)
-        throw std::runtime_error("Unsupport hidden_dim.");
-
-    sycl::range<3> block_dim(1, 1, threads);
-
-    stream->submit([&](sycl::handler& cgh) {
-        sycl::accessor<float, 1, sycl::access_mode::read_write, sycl::access::target::local>
-            shr_acc_ct1(sycl::range<1>(MAX_SG_NUM /*MAX_WARP_NUM*/), cgh);
-        cgh.parallel_for(sycl::nd_range<3>(grid_dim * block_dim, block_dim),
-                         [=](sycl::nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                             fused_bias_residual_layer_norm<true>(vals,
-                                                                  residual,
-                                                                  gamma,
-                                                                  beta,
-                                                                  epsilon,
-                                                                  preLayerNorm,
-                                                                  training,
-                                                                  vars,
-                                                                  means,
-                                                                  hidden_dim,
-                                                                  item_ct1,
-                                                                  shr_acc_ct1.get_pointer());
-                         });
-    });
-}
-
-template void launch_bias_residual_layer_norm<float>(float* vals,
-                                                     const float* residual,
-                                                     const float* gamma,
-                                                     const float* beta,
-                                                     float epsilon,
-                                                     int batch_size,
-                                                     int hidden_dim,
-                                                     sycl::queue* stream,
-                                                     bool preLayerNorm,
-                                                     bool training,
-                                                     float* vars,
-                                                     float* means);
-template void launch_bias_residual_layer_norm<bf16>(bf16* vals,
-                                                    const bf16* residual,
-                                                    const bf16* gamma,
-                                                    const bf16* beta,
-                                                    float epsilon,
-                                                    int batch_size,
-                                                    int hidden_dim,
-                                                    sycl::queue* stream,
-                                                    bool preLayerNorm,
-                                                    bool training,
-                                                    bf16* vars,
-                                                    bf16* means);
-template <>
-void launch_bias_residual_layer_norm<half>(half* vals,
-                                           const half* residual,
-                                           const half* gamma,
-                                           const half* beta,
-                                           float epsilon,
-                                           int batch_size,
-                                           int hidden_dim,
-                                           queue* stream,
-                                           bool preLayerNorm,
-                                           bool training,
-                                           half* vars,
-                                           half* means)
-{
-    int threads = 128;
-
-    range<3> grid_dim(1, 1, batch_size);
-
-    if (hidden_dim > 8192 && hidden_dim <= 16384)
-        threads <<= 1;
-    else if (hidden_dim > 16384 && hidden_dim <= 32768)
-        threads <<= 2;
-    else if (hidden_dim > 32768 && hidden_dim <= 65536)
-        threads <<= 3;
-    else if (hidden_dim > 65536)
-        throw std::runtime_error("Unsupport hidden_dim.");
-
-    range<3> block_dim(1, 1, threads);
-
-    stream->submit([&](handler& cgh) {
-        sycl::accessor<float, 1, sycl::access_mode::read_write, sycl::access::target::local>
-            shr_acc_ct1(sycl::range<1>(MAX_SG_NUM /*MAX_WARP_NUM*/), cgh);
-        cgh.parallel_for(nd_range<3>(grid_dim * block_dim, block_dim),
-                         [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                             fused_bias_residual_layer_norm<true>(vals,
-                                                                  residual,
-                                                                  gamma,
-                                                                  beta,
-                                                                  epsilon,
-                                                                  preLayerNorm,
-                                                                  training,
-                                                                  vars,
-                                                                  means,
-                                                                  hidden_dim / 2,
-                                                                  item_ct1,
-                                                                  shr_acc_ct1.get_pointer());
-                         });
-    });
-}
-
-/*
-  To tune this launch the following restrictions must be met:
-
-  For float:
-  row_stride == hidden_size
-  threads * iterations == row_stride
-  threads is in [32, 64, 128, 256, 512, 1024]
-
-  For half:
-  row_stride == hidden_size / 2
-  threads * iterations == row_stride
-  threads is in [32, 64, 128, 256, 512, 1024]
-
-*/
-
-template <typename T>
-void launch_bias_residual_layer_norm(T* vals,
-                                     const T* residual,
-                                     const T* gamma,
-                                     const T* beta,
-                                     float epsilon,
-                                     int batch_size,
-                                     int hidden_dim,
-                                     queue* stream,
-                                     bool preLayerNorm,
-                                     bool training,
-                                     T* vars)
-{
-    int threads = THREADS;
-
-    range<3> grid_dim(1, 1, batch_size);
-
-    // There are some limitations to call below functions, now just enumerate the
-    // situations.
-
-    if (hidden_dim > 16384 && hidden_dim <= 32768)
-        threads <<= 1;
-    else if (hidden_dim > 32768 && hidden_dim <= 65536)
-        threads <<= 2;
-    else if (hidden_dim > 65536)
-        throw std::runtime_error("Unsupport hidden_dim.");
-
-    range<3> block_dim(1, 1, threads);
-
-    stream->submit([&](handler& cgh) {
-        accessor<float, 1, access_mode::read_write, access::target::local> shr_acc_ct1(
-            range<1>(MAX_SG_NUM /*MAX_WARP_NUM*/), cgh);
-
-        cgh.parallel_for(nd_range<3>(grid_dim * block_dim, block_dim),
-                         [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                             fused_bias_residual_layer_norm<false>(vals,
-                                                                   residual,
-                                                                   gamma,
-                                                                   beta,
-                                                                   epsilon,
-                                                                   preLayerNorm,
-                                                                   training,
-                                                                   vars,
-                                                                   nullptr,
-                                                                   hidden_dim,
-                                                                   item_ct1,
-                                                                   shr_acc_ct1.get_pointer());
-                         });
-    });
-}
-
-template void launch_bias_residual_layer_norm<float>(float* vals,
-                                                     const float* residual,
-                                                     const float* gamma,
-                                                     const float* beta,
-                                                     float epsilon,
-                                                     int batch_size,
-                                                     int hidden_dim,
-                                                     queue* stream,
-                                                     bool preLayerNorm,
-                                                     bool training,
-                                                     float* vars);
-template void launch_bias_residual_layer_norm<bf16>(bf16* vals,
-                                                    const bf16* residual,
-                                                    const bf16* gamma,
-                                                    const bf16* beta,
-                                                    float epsilon,
-                                                    int batch_size,
-                                                    int hidden_dim,
-                                                    queue* stream,
-                                                    bool preLayerNorm,
-                                                    bool training,
-                                                    bf16* vars);
-template <>
-void launch_bias_residual_layer_norm<half>(half* vals,
-                                           const half* residual,
-                                           const half* gamma,
-                                           const half* beta,
-                                           float epsilon,
-                                           int batch_size,
-                                           int hidden_dim,
-                                           queue* stream,
-                                           bool preLayerNorm,
-                                           bool training,
-                                           half* vars)
-{
-    int threads = 128;
-
-    range<3> grid_dim(1, 1, batch_size);
-
-    // There are some limitations to call below functions, now just enumerate the
-    // situations.
-
-    if (hidden_dim > 8192 && hidden_dim <= 16384)
-        threads <<= 1;
-    else if (hidden_dim > 16384 && hidden_dim <= 32768)
-        threads <<= 2;
-    else if (hidden_dim > 32768 && hidden_dim <= 65536)
-        threads <<= 3;
-    else if (hidden_dim > 65536)
-        throw std::runtime_error("Unsupport hidden_dim.");
-
-    range<3> block_dim(1, 1, threads);
-
-    stream->submit([&](handler& cgh) {
-        sycl::accessor<float, 1, sycl::access_mode::read_write, sycl::access::target::local>
-            shr_acc_ct1(sycl::range<1>(MAX_SG_NUM /*MAX_WARP_NUM*/), cgh);
-        cgh.parallel_for(nd_range<3>(grid_dim * block_dim, block_dim),
-                         [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                             fused_bias_residual_layer_norm<false>(vals,
-                                                                   residual,
-                                                                   gamma,
-                                                                   beta,
-                                                                   epsilon,
-                                                                   preLayerNorm,
-                                                                   training,
-                                                                   vars,
-                                                                   nullptr,
-                                                                   hidden_dim / 2,
-                                                                   item_ct1,
-                                                                   shr_acc_ct1.get_pointer());
-                         });
-    });
-}
-
-/* Normalize Gamma & Betta gradients
- * Compute gradients using either X_hat or
- * normalize input (invertible).
- * Combine transpose with gradients computation.
- */
-
-template <typename T>
-void LayerNormBackward1(const T* out_grad,
-                        const T* vals_hat,
-                        const T* gamma,
-                        const T* betta,
-                        T* gamma_grad,
-                        T* betta_grad,
-                        int rows,
-                        int width,
-                        bool invertible,
-                        nd_item<3> item_ct1,
-                        float* betta_buffer,
-                        float* gamma_buffer)
-{
-    // group<3> b = item_ct1.get_group();
-    // cg::thread_block_tile<TILE_DIM> g = cg::tiled_partition<TILE_DIM>(b);
-    sub_group sg = item_ct1.get_sub_group();
-
-    int idx = item_ct1.get_local_range(2) * item_ct1.get_group(2) + item_ct1.get_local_id(2);
-    int offset = item_ct1.get_local_id(1) * width + idx;
-    int y_stride = width * TILE_DIM;
-
-    float betta_reg = (invertible ? (float)betta[idx] : 0.0f);
-    float gamma_reg = (float)gamma[idx];
-
-    // Loop across matrix height
-    float betta_tmp = 0;
-    float gamma_tmp = 0;
-    for (int r = item_ct1.get_local_id(1); r < rows; r += TILE_DIM) {
-        float grad = (float)out_grad[offset];
-        float val = (invertible ? ((float)vals_hat[offset] - betta_reg) / gamma_reg
-                                : (float)vals_hat[offset]);
-        betta_tmp += grad;
-        gamma_tmp += (val * grad);
-
-        offset += y_stride;
-    }
-
-    // betta_buffer[item_ct1.get_local_id(2)][item_ct1.get_local_id(1)] =
-    // betta_tmp; gamma_buffer[item_ct1.get_local_id(2)][item_ct1.get_local_id(1)]
-    // = gamma_tmp;
-    betta_buffer[item_ct1.get_local_id(2) * MAX_SG_NUM1 + item_ct1.get_local_id(1)] = betta_tmp;
-    gamma_buffer[item_ct1.get_local_id(2) * MAX_SG_NUM1 + item_ct1.get_local_id(1)] = gamma_tmp;
-
-    item_ct1.barrier();
-
-    // Sum the shared buffer.
-    // float s1 =
-    // betta_buffer[item_ct1.get_local_id(1)][item_ct1.get_local_id(2)]; float s2
-    // = gamma_buffer[item_ct1.get_local_id(1)][item_ct1.get_local_id(2)];
-    float s1 = betta_buffer[item_ct1.get_local_id(1) * MAX_SG_NUM1 + item_ct1.get_local_id(2)];
-    float s2 = gamma_buffer[item_ct1.get_local_id(1) * MAX_SG_NUM1 + item_ct1.get_local_id(2)];
-
-#ifndef __STOCHASTIC_MODE__
-
-    item_ct1.barrier();
-#endif
-
-    // for (int i = 1; i < TILE_DIM; i <<= 1) {
-    //     s1 += g.shfl_down(s1, i);
-    //     s2 += g.shfl_down(s2, i);
-    // }
-    for (int i = 1; i < TILE_DIM; i <<= 1) {
-        s1 += sg.shuffle_down(s1, i);
-        s2 += sg.shuffle_down(s2, i);
-    }
-
-    if (item_ct1.get_local_id(2) == 0) {
-        int pos = item_ct1.get_group(2) * TILE_DIM + item_ct1.get_local_id(1);
-        betta_grad[pos] = s1;
-        gamma_grad[pos] = s2;
-    }
-}
-
-/* Normalize Gamma & Betta gradients
- * Compute gradients using either X_hat or
- * normalize input (invertible).
- * Combine transpose with gradients computation.
- */
-
-template <>
-void LayerNormBackward1<bf16>(const bf16* out_grad,
-                              const bf16* vals_hat,
-                              const bf16* gamma,
-                              const bf16* betta,
-                              bf16* gamma_grad,
-                              bf16* betta_grad,
-                              int rows,
-                              int width,
-                              bool invertible,
-                              nd_item<3> item_ct1,
-                              float* betta_buffer,
-                              float* gamma_buffer)
-{
-    // group<3> b = item_ct1.get_group();
-    // cg::thread_block_tile<TILE_DIM> g = cg::tiled_partition<TILE_DIM>(b);
-    sub_group sg = item_ct1.get_sub_group();
-
-    int idx = item_ct1.get_local_range(2) * item_ct1.get_group(2) + item_ct1.get_local_id(2);
-    int offset = item_ct1.get_local_id(1) * width + idx;
-    int y_stride = width * TILE_DIM;
-
-    const ushort* out_grad_cast = reinterpret_cast<const ushort*>(out_grad);
-    const ushort* vals_hat_cast = reinterpret_cast<const ushort*>(vals_hat);
-    const ushort* gamma_cast = reinterpret_cast<const ushort*>(gamma);
-    const ushort* betta_cast = reinterpret_cast<const ushort*>(betta);
-    ushort* gamma_grad_cast = reinterpret_cast<ushort*>(gamma_grad);
-    ushort* betta_grad_cast = reinterpret_cast<ushort*>(betta_grad);
-
-    float betta_reg = (invertible ? float(betta_cast[idx]) : 0.0f);
-    float gamma_reg = float(gamma_cast[idx]);
-
-    // Loop across matrix height
-    float betta_tmp = 0;
-    float gamma_tmp = 0;
-    for (int r = item_ct1.get_local_id(1); r < rows; r += TILE_DIM) {
-        float grad = float(out_grad_cast[offset]);
-        float val = (invertible ? (float(vals_hat_cast[offset]) - betta_reg) / gamma_reg
-                                : float(vals_hat_cast[offset]));
-        betta_tmp += grad;
-        gamma_tmp += (val * grad);
-
-        offset += y_stride;
-    }
-
-    // betta_buffer[item_ct1.get_local_id(2)][item_ct1.get_local_id(1)] =
-    // betta_tmp; gamma_buffer[item_ct1.get_local_id(2)][item_ct1.get_local_id(1)]
-    // = gamma_tmp;
-    betta_buffer[item_ct1.get_local_id(2) * MAX_SG_NUM1 + item_ct1.get_local_id(1)] = betta_tmp;
-    gamma_buffer[item_ct1.get_local_id(2) * MAX_SG_NUM1 + item_ct1.get_local_id(1)] = gamma_tmp;
-
-    item_ct1.barrier();
-
-    // Sum the shared buffer.
-    // float s1 =
-    // betta_buffer[item_ct1.get_local_id(1)][item_ct1.get_local_id(2)]; float s2
-    // = gamma_buffer[item_ct1.get_local_id(1)][item_ct1.get_local_id(2)];
-    float s1 = betta_buffer[item_ct1.get_local_id(1) * MAX_SG_NUM1 + item_ct1.get_local_id(2)];
-    float s2 = gamma_buffer[item_ct1.get_local_id(1) * MAX_SG_NUM1 + item_ct1.get_local_id(2)];
-
-#ifndef __STOCHASTIC_MODE__
-
-    item_ct1.barrier();
-#endif
-
-    // for (int i = 1; i < TILE_DIM; i <<= 1) {
-    //     s1 += g.shfl_down(s1, i);
-    //     s2 += g.shfl_down(s2, i);
-    // }
-    for (int i = 1; i < TILE_DIM; i <<= 1) {
-        s1 += sg.shuffle_down(s1, i);
-        s2 += sg.shuffle_down(s2, i);
-    }
-
-    if (item_ct1.get_local_id(2) == 0) {
-        int pos = item_ct1.get_group(2) * TILE_DIM + item_ct1.get_local_id(1);
-        betta_grad_cast[pos] = bf16(s1);
-        gamma_grad_cast[pos] = bf16(s2);
-    }
-}
-
-/* Normalize Gamma & Betta gradients
- * Compute gradients using the input to
- * the normalize.
- * Combine transpose with gradients computation.
- */
-
-template <typename T>
-void LayerNormBackward1(const T* out_grad,
-                        const T* X_data,
-                        const T* vars,
-                        const T* means,
-                        T* gamma_grad,
-                        T* betta_grad,
-                        int rows,
-                        int width,
-                        nd_item<3> item_ct1,
-                        float* betta_buffer,
-                        float* gamma_buffer)
-{
-    // group<3> b = item_ct1.get_group();
-    // cg::thread_block_tile<TILE_DIM> g = cg::tiled_partition<TILE_DIM>(b);
-    sub_group sg = item_ct1.get_sub_group();
-
-    int idx = item_ct1.get_local_range(2) * item_ct1.get_group(2) + item_ct1.get_local_id(2);
-    int offset = item_ct1.get_local_id(1) * width + idx;
-    int y_stride = width * TILE_DIM;
-
-    int pos = item_ct1.get_group(2) * TILE_DIM + item_ct1.get_local_id(1);
-    // Loop across matrix height
-
-    float betta_tmp = 0;
-    float gamma_tmp = 0;
-    for (int r = item_ct1.get_local_id(1); r < rows; r += TILE_DIM) {
-        float grad = (float)out_grad[offset];
-        float val = (float)X_data[offset];
-        val = (val - (float)means[r]) * rsqrt((float)vars[r]);
-        betta_tmp += grad;
-        gamma_tmp += (val * grad);
-
-        offset += y_stride;
-    }
-
-    betta_buffer[item_ct1.get_local_id(2) * MAX_SG_NUM1 + item_ct1.get_local_id(1)] = betta_tmp;
-    gamma_buffer[item_ct1.get_local_id(2) * MAX_SG_NUM1 + item_ct1.get_local_id(1)] = gamma_tmp;
-
-    item_ct1.barrier();
-
-    // Sum the shared buffer.
-    float s1 = betta_buffer[item_ct1.get_local_id(1) * MAX_SG_NUM1 + item_ct1.get_local_id(2)];
-    float s2 = gamma_buffer[item_ct1.get_local_id(1) * MAX_SG_NUM1 + item_ct1.get_local_id(2)];
-
-#ifndef __STOCHASTIC_MODE__
-
-    item_ct1.barrier();
-#endif
-
-    // for (int i = 1; i < TILE_DIM; i <<= 1) {
-    //     s1 += g.shfl_down(s1, i);
-    //     s2 += g.shfl_down(s2, i);
-    // }
-    for (int i = 1; i < TILE_DIM; i <<= 1) {
-        s1 += sg.shuffle_down(s1, i);
-        s2 += sg.shuffle_down(s2, i);
-    }
-
-    if (item_ct1.get_local_id(2) == 0) {
-        betta_grad[pos] = s1;
-        gamma_grad[pos] = s2;
-    }
-}
-
-template <>
-void LayerNormBackward1<bf16>(const bf16* out_grad,
-                              const bf16* X_data,
-                              const bf16* vars,
-                              const bf16* means,
-                              bf16* gamma_grad,
-                              bf16* betta_grad,
-                              int rows,
-                              int width,
-                              nd_item<3> item_ct1,
-                              float* betta_buffer,
-                              float* gamma_buffer)
-{
-    // group<3> b = item_ct1.get_group();
-    // cg::thread_block_tile<TILE_DIM> g = cg::tiled_partition<TILE_DIM>(b);
-    sub_group sg = item_ct1.get_sub_group();
-
-    int idx = item_ct1.get_local_range(2) * item_ct1.get_group(2) + item_ct1.get_local_id(2);
-    int offset = item_ct1.get_local_id(1) * width + idx;
-    int y_stride = width * TILE_DIM;
-
-    int pos = item_ct1.get_group(2) * TILE_DIM + item_ct1.get_local_id(1);
-    // Loop across matrix height
-
-    const ushort* out_grad_cast = reinterpret_cast<const ushort*>(out_grad);
-    const ushort* X_data_cast = reinterpret_cast<const ushort*>(X_data);
-    const ushort* vars_cast = reinterpret_cast<const ushort*>(vars);
-    const ushort* means_cast = reinterpret_cast<const ushort*>(means);
-    ushort* gamma_grad_cast = reinterpret_cast<ushort*>(gamma_grad);
-    ushort* betta_grad_cast = reinterpret_cast<ushort*>(betta_grad);
-
-    float betta_tmp = 0;
-    float gamma_tmp = 0;
-    for (int r = item_ct1.get_local_id(1); r < rows; r += TILE_DIM) {
-        float grad = float(out_grad_cast[offset]);
-        float val = float(X_data_cast[offset]);
-        val = (val - float(means_cast[r])) * rsqrt(float(vars_cast[r]));
-        betta_tmp += grad;
-        gamma_tmp += (val * grad);
-
-        offset += y_stride;
-    }
-
-    betta_buffer[item_ct1.get_local_id(2) * MAX_SG_NUM1 + item_ct1.get_local_id(1)] = betta_tmp;
-    gamma_buffer[item_ct1.get_local_id(2) * MAX_SG_NUM1 + item_ct1.get_local_id(1)] = gamma_tmp;
-
-    item_ct1.barrier();
-
-    // Sum the shared buffer.
-    float s1 = betta_buffer[item_ct1.get_local_id(1) * MAX_SG_NUM1 + item_ct1.get_local_id(2)];
-    float s2 = gamma_buffer[item_ct1.get_local_id(1) * MAX_SG_NUM1 + item_ct1.get_local_id(2)];
-
-#ifndef __STOCHASTIC_MODE__
-
-    item_ct1.barrier();
-#endif
-
-    // for (int i = 1; i < TILE_DIM; i <<= 1) {
-    //     s1 += g.shfl_down(s1, i);
-    //     s2 += g.shfl_down(s2, i);
-    // }
-    for (int i = 1; i < TILE_DIM; i <<= 1) {
-        s1 += sg.shuffle_down(s1, i);
-        s2 += sg.shuffle_down(s2, i);
-    }
-
-    if (item_ct1.get_local_id(2) == 0) {
-        betta_grad_cast[pos] = bf16(s1);
-        gamma_grad_cast[pos] = bf16(s2);
-    }
-}
-/*
-
-/* Backward Normalize (Input-Gradient)
-* Using the means and variances from the input
-* This type of backward is invertible!
-* We do the backward using the X_hat (X - u) / sqrt(variance) or the output of
-Normalization.
-*/
-template <bool is_fuseadd>
-void LayerNormBackward2(const float* out_grad,
-                        const float* out_grad_add,
-                        const float* vals_hat,
-                        const float* gamma,
-                        const float* betta,
-                        const float* vars,
-                        float* inp_grad,
-                        bool invertible,
-                        int row_stride,
-                        nd_item<3> item_ct1,
-                        float* partialSum)
-{
-    int iteration_stride = item_ct1.get_local_range(2);
-    int iterations = row_stride / iteration_stride;
-
-    // group<3> b = item_ct1.get_group();
-    // cg::thread_block_tile<MAX_SG_NUM> g = cg::tiled_partition<MAX_SG_NUM>(b);
-    sub_group sg = item_ct1.get_sub_group();
-
-    int row = item_ct1.get_group(2);
-    int id = item_ct1.get_local_id(2);
-    int wid = id / MAX_SG_NUM;
-    int warp_num = (THREADS < row_stride ? THREADS : row_stride) / MAX_SG_NUM;
-
-    out_grad += (row * row_stride);
-    if constexpr (is_fuseadd) { out_grad_add += (row * row_stride); }
-    vals_hat += (row * row_stride);
-    inp_grad += (row * row_stride);
-
-    float vals_arr[NORM_REG];
-    float vals_hat_arr[NORM_REG];
-    int high_index = iterations * iteration_stride + id;
-#pragma unroll
-    for (int i = 0; i < iterations; i++) {
-        float gamma_reg = gamma[i * iteration_stride + id];
-        vals_arr[i] = out_grad[i * iteration_stride + id];
-        vals_arr[i] *= gamma_reg;
-        vals_hat_arr[i] =
-            (invertible ? (vals_hat[i * iteration_stride + id] - betta[i * iteration_stride + id]) /
-                              gamma_reg
-                        : vals_hat[i * iteration_stride + id]);
-    }
-    if ((high_index) < row_stride) {
-        float gamma_reg = gamma[high_index];
-        vals_arr[iterations] = out_grad[high_index];
-        vals_arr[iterations] *= gamma_reg;
-        vals_hat_arr[iterations] =
-            (invertible ? (vals_hat[high_index] - betta[high_index]) / gamma_reg
-                        : vals_hat[high_index]);
-        iterations++;
-    }
-
-    float var_reg = vars[row];
-
-    float sum = 0;
-    for (int i = 0; i < iterations; i++) {
-        sum +=
-            vals_hat_arr[i] * vals_arr[i] * sqrt(var_reg);  // dval_hat = gamma * (x - u) * out_grad
-        vals_arr[i] *= rsqrt(var_reg);  // dvar_inv = gamma * out_grad / sqrt(var)
-    }
-
-    // for (int i = 1; i < MAX_SG_NUM; i *= 2) { sum += g.shfl_down(sum, i); }
-    for (int i = 1; i < MAX_SG_NUM; i *= 2) { sum += sg.shuffle_down(sum, i); }
-
-    // if (g.thread_rank() == 0) partialSum[wid] = sum;
-    if (sg.get_local_id() == 0) partialSum[wid] = sum;
-
-    item_ct1.barrier();
-
-    // if (g.thread_rank() < warp_num) sum = partialSum[g.thread_rank()];
-    if (sg.get_local_id() < warp_num) sum = partialSum[sg.get_local_id()];
-
-#ifndef __STOCHASTIC_MODE__
-
-    item_ct1.barrier();
-#endif
-
-    // for (int i = 1; i < warp_num; i *= 2) sum += g.shfl_down(sum, i);
-    for (int i = 1; i < warp_num; i *= 2) sum += sg.shuffle_down(sum, i);
-
-    // sum = g.shfl(sum, 0);
-    sum = sg.shuffle(sum, 0);
-    sum /= row_stride;
-
-    for (int i = 0; i < iterations; i++) { vals_arr[i] += ((-sum * vals_hat_arr[i]) / var_reg); }
-
-    sum = 0;
-    for (int i = 0; i < iterations; i++) { sum += vals_arr[i]; }
-
-    // for (int i = 1; i < MAX_SG_NUM; i *= 2) { sum += g.shfl_down(sum, i); }
-    for (int i = 1; i < MAX_SG_NUM; i *= 2) { sum += sg.shuffle_down(sum, i); }
-
-    // if (g.thread_rank() == 0) partialSum[wid] = sum;
-    if (sg.get_local_id() == 0) partialSum[wid] = sum;
-
-    item_ct1.barrier();
-
-    // if (g.thread_rank() < warp_num) sum = partialSum[g.thread_rank()];
-    if (sg.get_local_id() < warp_num) sum = partialSum[sg.get_local_id()];
-
-#ifndef __STOCHASTIC_MODE__
-    item_ct1.barrier();
-#endif
-
-    // for (int i = 1; i < warp_num; i *= 2) sum += g.shfl_down(sum, i);
-    for (int i = 1; i < warp_num; i *= 2) sum += sg.shuffle_down(sum, i);
-    // sum = g.shfl(sum, 0);
-    sum = sg.shuffle(sum, 0);
-    sum /= row_stride;
-
-    iterations = row_stride / iteration_stride;
-    for (int i = 0; i < iterations; i++)
-        if constexpr (is_fuseadd) {
-            inp_grad[i * iteration_stride + id] =
-                (vals_arr[i] - sum) + out_grad_add[i * iteration_stride + id];
-        } else {
-            inp_grad[i * iteration_stride + id] = (vals_arr[i] - sum);
-        }
-    if ((high_index) < row_stride)
-        if constexpr (is_fuseadd) {
-            inp_grad[high_index] = (vals_arr[iterations] - sum) + out_grad_add[high_index];
-        } else {
-            inp_grad[high_index] = (vals_arr[iterations] - sum);
-        }
-}
-
-template <bool is_fuseadd>
-void LayerNormBackward2(const bf16* out_grad,
-                        const bf16* out_grad_add,
-                        const bf16* vals_hat,
-                        const bf16* gamma,
-                        const bf16* betta,
-                        const bf16* vars,
-                        bf16* inp_grad,
-                        bool invertible,
-                        int row_stride,
-                        nd_item<3> item_ct1,
-                        float* partialSum)
-{
-    int iteration_stride = item_ct1.get_local_range(2);
-    int iterations = row_stride / iteration_stride;
-
-    // group<3> b = item_ct1.get_group();
-    // cg::thread_block_tile<MAX_SG_NUM> g = cg::tiled_partition<MAX_SG_NUM>(b);
-    sub_group sg = item_ct1.get_sub_group();
-
-    int row = item_ct1.get_group(2);
-    int id = item_ct1.get_local_id(2);
-    int wid = id / MAX_SG_NUM;
-    int warp_num = (THREADS < row_stride ? THREADS : row_stride) / MAX_SG_NUM;
-
-    const ushort* out_grad_cast = reinterpret_cast<const ushort*>(out_grad);
-    const ushort* out_grad_add_cast = reinterpret_cast<const ushort*>(out_grad_add);
-    const ushort* vals_hat_cast = reinterpret_cast<const ushort*>(vals_hat);
-    const ushort* gamma_cast = reinterpret_cast<const ushort*>(gamma);
-    const ushort* betta_cast = reinterpret_cast<const ushort*>(betta);
-    const ushort* vars_cast = reinterpret_cast<const ushort*>(vars);
-    ushort* inp_grad_cast = reinterpret_cast<ushort*>(inp_grad);
-
-    out_grad_cast += (row * row_stride);
-    if constexpr (is_fuseadd) { out_grad_add_cast += (row * row_stride); }
-    vals_hat_cast += (row * row_stride);
-    inp_grad_cast += (row * row_stride);
-
-    float vals_arr[NORM_REG];
-    float vals_hat_arr[NORM_REG];
-    int high_index = iterations * iteration_stride + id;
-#pragma unroll
-    for (int i = 0; i < iterations; i++) {
-        float gamma_reg = float(gamma_cast[i * iteration_stride + id]);
-        vals_arr[i] = float(out_grad_cast[i * iteration_stride + id]);
-        vals_arr[i] *= gamma_reg;
-        vals_hat_arr[i] = (invertible ? (float(vals_hat_cast[i * iteration_stride + id]) -
-                                         float(betta_cast[i * iteration_stride + id])) /
-                                            gamma_reg
-                                      : float(vals_hat_cast[i * iteration_stride + id]));
-    }
-    if ((high_index) < row_stride) {
-        float gamma_reg = float(gamma_cast[high_index]);
-        vals_arr[iterations] = float(out_grad_cast[high_index]);
-        vals_arr[iterations] *= gamma_reg;
-        vals_hat_arr[iterations] = (invertible ? (float(vals_hat_cast[high_index]) -
-                                                  float(betta_cast[high_index])) /
-                                                     gamma_reg
-                                               : float(vals_hat_cast[high_index]));
-        iterations++;
-    }
-
-    float var_reg = float(vars_cast[row]);
-
-    float sum = 0;
-    for (int i = 0; i < iterations; i++) {
-        sum +=
-            vals_hat_arr[i] * vals_arr[i] * sqrt(var_reg);  // dval_hat = gamma * (x - u) * out_grad
-        vals_arr[i] *= rsqrt(var_reg);  // dvar_inv = gamma * out_grad / sqrt(var)
-    }
-
-    // for (int i = 1; i < MAX_SG_NUM; i *= 2) { sum += g.shfl_down(sum, i); }
-    for (int i = 1; i < MAX_SG_NUM; i *= 2) { sum += sg.shuffle_down(sum, i); }
-
-    // if (g.thread_rank() == 0) partialSum[wid] = sum;
-    if (sg.get_local_id() == 0) partialSum[wid] = sum;
-
-    item_ct1.barrier();
-
-    // if (g.thread_rank() < warp_num) sum = partialSum[g.thread_rank()];
-    if (sg.get_local_id() < warp_num) sum = partialSum[sg.get_local_id()];
-
-#ifndef __STOCHASTIC_MODE__
-    item_ct1.barrier();
-#endif
-
-    // for (int i = 1; i < warp_num; i *= 2) sum += g.shfl_down(sum, i);
-    for (int i = 1; i < warp_num; i *= 2) sum += sg.shuffle_down(sum, i);
-
-    // sum = g.shfl(sum, 0);
-    sum = sg.shuffle(sum, 0);
-    sum /= row_stride;
-
-    for (int i = 0; i < iterations; i++) { vals_arr[i] += ((-sum * vals_hat_arr[i]) / var_reg); }
-
-    sum = 0;
-    for (int i = 0; i < iterations; i++) { sum += vals_arr[i]; }
-
-    // for (int i = 1; i < MAX_SG_NUM; i *= 2) { sum += g.shfl_down(sum, i); }
-    for (int i = 1; i < MAX_SG_NUM; i *= 2) { sum += sg.shuffle_down(sum, i); }
-
-    // if (g.thread_rank() == 0) partialSum[wid] = sum;
-    if (sg.get_local_id() == 0) partialSum[wid] = sum;
-
-    item_ct1.barrier();
-
-    // if (g.thread_rank() < warp_num) sum = partialSum[g.thread_rank()];
-    if (sg.get_local_id() < warp_num) sum = partialSum[sg.get_local_id()];
-
-#ifndef __STOCHASTIC_MODE__
-    item_ct1.barrier();
-#endif
-
-    // for (int i = 1; i < warp_num; i *= 2) sum += g.shfl_down(sum, i);
-    for (int i = 1; i < warp_num; i *= 2) sum += sg.shuffle_down(sum, i);
-    // sum = g.shfl(sum, 0);
-    sum = sg.shuffle(sum, 0);
-    sum /= row_stride;
-
-    iterations = row_stride / iteration_stride;
-    for (int i = 0; i < iterations; i++)
-        if constexpr (is_fuseadd) {
-            inp_grad_cast[i * iteration_stride + id] = bf16(
-                (vals_arr[i] - sum) + float(out_grad_add_cast[i * iteration_stride + id]));
-        } else {
-            inp_grad_cast[i * iteration_stride + id] = bf16((vals_arr[i] - sum));
-        }
-    if ((high_index) < row_stride)
-        if constexpr (is_fuseadd) {
-            inp_grad_cast[high_index] = bf16(
-                (vals_arr[iterations] - sum) + float(out_grad_add_cast[high_index]));
-        } else {
-            inp_grad_cast[high_index] = bf16((vals_arr[iterations] - sum));
-        }
-}
-
-template <bool is_fuseadd>
-void LayerNormBackward2(const half* out_grad,
-                        const half* out_grad_add,
-                        const half* vals_hat,
-                        const half* gamma,
-                        const half* betta,
-                        const half* vars,
-                        half* inp_grad,
-                        bool invertible,
-                        int row_stride,
-                        nd_item<3> item_ct1,
-                        float* partialSum)
-{
-    int iteration_stride = item_ct1.get_local_range(2);
-    int iterations = row_stride / iteration_stride;
-
-    // group<3> b = item_ct1.get_group();
-    // cg::thread_block_tile<MAX_SG_NUM> g = cg::tiled_partition<MAX_SG_NUM>(b);
-    sub_group sg = item_ct1.get_sub_group();
-
-    int row = item_ct1.get_group(2);
-    int id = item_ct1.get_local_id(2);
-    int wid = id / MAX_SG_NUM;
-    int warp_num = (iteration_stride < row_stride ? iteration_stride : row_stride) / MAX_SG_NUM;
-
-    half2 vals_arr[NORM_REG];
-    float2 vals_arr_f[NORM_REG];
-    half2 vals_hat_arr[NORM_REG];
-
-    half2* inp_grad_h = reinterpret_cast<half2*>(inp_grad);
-    const half2* out_grad_h = reinterpret_cast<const half2*>(out_grad);
-    const half2* out_grad_add_h = reinterpret_cast<const half2*>(out_grad_add);
-    const half2* vals_hat_h = reinterpret_cast<const half2*>(vals_hat);
-
-    inp_grad_h += (row * row_stride);
-    out_grad_h += (row * row_stride);
-    if constexpr (is_fuseadd) { out_grad_add_h += (row * row_stride); }
-    vals_hat_h += (row * row_stride);
-
-    const half2* gamma_h = reinterpret_cast<const half2*>(gamma);
-    const half2* betta_h = (invertible ? reinterpret_cast<const half2*>(betta) : nullptr);
-    int high_index = iterations * iteration_stride + id;
-#pragma unroll
-    for (int i = 0; i < iterations; i++) {
-        half2 gamma_reg = gamma_h[i * iteration_stride + id];
-        vals_arr[i] = out_grad_h[i * iteration_stride + id];
-        vals_arr[i] *= gamma_reg;
-        vals_hat_arr[i] =
-            (invertible
-                 ? (vals_hat_h[i * iteration_stride + id] - betta_h[i * iteration_stride + id]) /
-                       gamma_reg
-                 : vals_hat_h[i * iteration_stride + id]);
-    }
-    if ((high_index) < row_stride) {
-        half2 gamma_reg = gamma_h[high_index];
-        vals_arr[iterations] = out_grad_h[high_index];
-        vals_arr[iterations] *= gamma_reg;
-        vals_hat_arr[iterations] =
-            (invertible ? (vals_hat_h[high_index] - betta_h[high_index]) / gamma_reg
-                        : vals_hat_h[high_index]);
-        iterations++;
-    }
-    half var_h = vars[row];
-    half2 var_reg = half2{var_h, var_h};
-
-    float sum = 0.f;
-    for (int i = 0; i < iterations; i++) {
-        half2 result_h = (vals_hat_arr[i] * vals_arr[i] * sqrt(var_reg));
-        float2 result_f = result_h.convert<float, rounding_mode::automatic>();
-        sum += result_f.x();
-        sum += result_f.y();
-        vals_arr[i] *= rsqrt(var_reg);
-    }
-
-    // for (int i = 1; i < MAX_SG_NUM; i *= 2) { sum += g.shfl_down(sum, i); }
-    for (int i = 1; i < MAX_SG_NUM; i *= 2) { sum += sg.shuffle_down(sum, i); }
-
-    // if (g.thread_rank() == 0) partialSum[wid] = sum;
-    if (sg.get_local_id() == 0) partialSum[wid] = sum;
-
-    item_ct1.barrier();
-
-    // if (g.thread_rank() < warp_num) sum = partialSum[g.thread_rank()];
-    if (sg.get_local_id() < warp_num) sum = partialSum[sg.get_local_id()];
-
-#ifndef __STOCHASTIC_MODE__
-    item_ct1.barrier();
-#endif
-
-    // for (int i = 1; i < warp_num; i *= 2) sum += g.shfl_down(sum, i);
-    for (int i = 1; i < warp_num; i *= 2) sum += sg.shuffle_down(sum, i);
-
-    // sum = g.shfl(sum, 0);
-    sum = sg.shuffle(sum, 0);
-    sum /= (2 * row_stride);
-    half2 sum_h = float2{sum, sum}.convert<half>();
-
-    for (int i = 0; i < iterations; i++) {
-        half2 temp = ((-sum_h * vals_hat_arr[i]) / (var_reg));
-        vals_arr_f[i] = vals_arr[i].convert<float, rounding_mode::automatic>();
-        float2 temp_f = temp.convert<float, rounding_mode::automatic>();
-        vals_arr_f[i].x() += temp_f.x();
-        vals_arr_f[i].y() += temp_f.y();
-    }
-    sum = 0.f;
-
-    for (int i = 0; i < iterations; i++) {
-        sum += (vals_arr_f[i].x());
-        sum += (vals_arr_f[i].y());
-    }
-
-    // for (int i = 1; i < MAX_SG_NUM; i *= 2) { sum += g.shfl_down(sum, i); }
-    for (int i = 1; i < MAX_SG_NUM; i *= 2) { sum += sg.shuffle_down(sum, i); }
-
-    // if (g.thread_rank() == 0) partialSum[wid] = sum;
-    if (sg.get_local_id() == 0) partialSum[wid] = sum;
-
-    item_ct1.barrier();
-
-    // if (sg.thread_rank() < warp_num) sum = partialSum[g.thread_rank()];
-    if (sg.get_local_id() < warp_num) sum = partialSum[sg.get_local_id()];
-
-#ifndef __STOCHASTIC_MODE__
-    item_ct1.barrier();
-#endif
-
-    // for (int i = 1; i < warp_num; i *= 2) sum += g.shfl_down(sum, i);
-    for (int i = 1; i < warp_num; i *= 2) sum += sg.shuffle_down(sum, i);
-
-    // sum = sg.shfl(sum, 0);
-    sum = sg.shuffle(sum, 0);
-    sum /= (2 * row_stride);
-
-    iterations = row_stride / iteration_stride;
-    for (int i = 0; i < iterations; i++) {
-        vals_arr_f[i].x() -= sum;
-        vals_arr_f[i].y() -= sum;
-        half2 temp = vals_arr_f[i].convert<half>();
-        if constexpr (is_fuseadd) {
-            inp_grad_h[i * iteration_stride + id] =
-                temp + out_grad_add_h[i * iteration_stride + id];
-        } else {
-            inp_grad_h[i * iteration_stride + id] = temp;
-        }
-    }
-    if ((high_index) < row_stride) {
-        vals_arr_f[iterations].x() -= sum;
-        vals_arr_f[iterations].y() -= sum;
-        half2 temp = vals_arr_f[iterations].convert<half>();
-        if constexpr (is_fuseadd) {
-            inp_grad_h[high_index] = temp + out_grad_add_h[high_index];
-        } else {
-            inp_grad_h[high_index] = temp;
-        }
-    }
-}
-
-template <typename T>
-void launch_layerNorm_backward(const T* out_grad,
-                               const T* vals_hat,
-                               const T* vars,
-                               const T* gamma,
-                               T* gamma_grad,
-                               T* betta_grad,
-                               T* inp_grad,
-                               int batch,
-                               int hidden_dim,
-                               queue* stream[2],
-                               bool invertible,
-                               const T* betta)
-{
-    int threads = THREADS;
-
-    range<3> grid_dim(1, 1, hidden_dim / TILE_DIM);
-    range<3> block_dim(1, TILE_DIM, TILE_DIM);
-
-    stream[0]->submit([&](handler& cgh) {
-        accessor<float, 2, access_mode::read_write, access::target::local> betta_buffer(
-            range<2>(MAX_SG_NUM /*MAX_WARP_NUM*/, MAX_SG_NUM1), cgh);
-        accessor<float, 2, access_mode::read_write, access::target::local> gamma_buffer(
-            range<2>(MAX_SG_NUM /*MAX_WARP_NUM*/, MAX_SG_NUM1), cgh);
-
-        cgh.parallel_for(nd_range<3>(grid_dim * block_dim, block_dim),
-                         [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                             LayerNormBackward1<T>(out_grad,
-                                                   vals_hat,
-                                                   gamma,
-                                                   betta,
-                                                   gamma_grad,
-                                                   betta_grad,
-                                                   batch,
-                                                   hidden_dim,
-                                                   invertible,
-                                                   item_ct1,
-                                                   betta_buffer.get_pointer(),
-                                                   gamma_buffer.get_pointer());
-                         });
-    });
-    // LayerNormBackward1<float><<<grid_dim, block_dim, 0, stream[0]>>>(
-    //     out_grad, vals_hat, gamma, betta, gamma_grad, betta_grad, batch,
-    //     hidden_dim, invertible);
-    range<3> grid_dim2(1, 1, batch);
-
-    if (hidden_dim > 16384 && hidden_dim <= 32768)
-        threads <<= 1;
-    else if (hidden_dim > 32768 && hidden_dim <= 65536)
-        threads <<= 2;
-    else if (hidden_dim > 65536)
-        throw std::runtime_error("Unsupport hidden_dim.");
-
-    range<3> block_dim2(1, 1, threads);
-
-    stream[1]->submit([&](handler& cgh) {
-        accessor<float, 1, access_mode::read_write, access::target::local> partialSum_acc_ct1(
-            range<1>(MAX_SG_NUM /*MAX_WARP_NUM*/), cgh);
-
-        cgh.parallel_for(nd_range<3>(grid_dim2 * block_dim2, block_dim2),
-                         [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                             LayerNormBackward2<false>(out_grad,
-                                                       nullptr,
-                                                       vals_hat,
-                                                       gamma,
-                                                       betta,
-                                                       vars,
-                                                       inp_grad,
-                                                       invertible,
-                                                       hidden_dim,
-                                                       item_ct1,
-                                                       partialSum_acc_ct1.get_pointer());
-                         });
-    });
-}
-
-template void launch_layerNorm_backward<float>(const float* out_grad,
-                                               const float* vals_hat,
-                                               const float* vars,
-                                               const float* gamma,
-                                               float* gamma_grad,
-                                               float* betta_grad,
-                                               float* inp_grad,
-                                               int batch,
-                                               int hidden_dim,
-                                               queue* stream[2],
-                                               bool invertible,
-                                               const float* betta);
-
-template void launch_layerNorm_backward<bf16>(const bf16* out_grad,
-                                              const bf16* vals_hat,
-                                              const bf16* vars,
-                                              const bf16* gamma,
-                                              bf16* gamma_grad,
-                                              bf16* betta_grad,
-                                              bf16* inp_grad,
-                                              int batch,
-                                              int hidden_dim,
-                                              queue* stream[2],
-                                              bool invertible,
-                                              const bf16* betta);
-
-template <>
-void launch_layerNorm_backward<half>(const half* out_grad,
-                                     const half* vals_hat,
-                                     const half* vars,
-                                     const half* gamma,
-                                     half* gamma_grad,
-                                     half* betta_grad,
-                                     half* inp_grad,
-                                     int batch,
-                                     int hidden_dim,
-                                     queue* stream[2],
-                                     bool invertible,
-                                     const half* betta)
-{
-    int threads = THREADS;
-
-    range<3> grid_dim(1, 1, hidden_dim / TILE_DIM);
-    range<3> block_dim(1, TILE_DIM, TILE_DIM);
-
-    // LayerNormBackward1<half><<<grid_dim, block_dim, 0, stream[0]>>>(
-    //     out_grad, vals_hat, gamma, betta, gamma_grad, betta_grad, batch,
-    //     hidden_dim, invertible);
-    stream[0]->submit([&](handler& cgh) {
-        accessor<float, 2, access_mode::read_write, access::target::local> betta_buffer(
-            range<2>(MAX_SG_NUM /*MAX_WARP_NUM*/, MAX_SG_NUM1), cgh);
-        accessor<float, 2, access_mode::read_write, access::target::local> gamma_buffer(
-            range<2>(MAX_SG_NUM /*MAX_WARP_NUM*/, MAX_SG_NUM1), cgh);
-        cgh.parallel_for(nd_range<3>(grid_dim * block_dim, block_dim),
-                         [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                             LayerNormBackward1<half>(out_grad,
-                                                      vals_hat,
-                                                      gamma,
-                                                      betta,
-                                                      gamma_grad,
-                                                      betta_grad,
-                                                      batch,
-                                                      hidden_dim,
-                                                      invertible,
-                                                      item_ct1,
-                                                      betta_buffer.get_pointer(),
-                                                      gamma_buffer.get_pointer());
-                         });
-    });
-
-    range<3> grid_dim2(1, 1, batch);
-
-    if (hidden_dim > 8192 && hidden_dim <= 16384)
-        threads <<= 1;
-    else if (hidden_dim > 16384 && hidden_dim <= 32768)
-        threads <<= 2;
-    else if (hidden_dim > 32768 && hidden_dim <= 65536)
-        threads <<= 3;
-    else if (hidden_dim > 65536)
-        throw std::runtime_error("Unsupport hidden_dim.");
-
-    range<3> block_dim2(1, 1, threads / 2);
-
-    stream[1]->submit([&](handler& cgh) {
-        accessor<float, 1, access_mode::read_write, access::target::local> partialSum_acc_ct1(
-            range<1>(MAX_SG_NUM /*MAX_WARP_NUM*/), cgh);
-
-        cgh.parallel_for(nd_range<3>(grid_dim2 * block_dim2, block_dim2),
-                         [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                             LayerNormBackward2<false>(out_grad,
-                                                       nullptr,
-                                                       vals_hat,
-                                                       gamma,
-                                                       betta,
-                                                       vars,
-                                                       inp_grad,
-                                                       invertible,
-                                                       hidden_dim / 2,
-                                                       item_ct1,
-                                                       partialSum_acc_ct1.get_pointer());
-                         });
-    });
-}
-
-/* Backward Normalize (Input-Gradient)
- * Using the means and variances from the input
- * This type of backward is not invertible!
- * We do the backward using the input (X)
- */
-template <bool is_fuseadd>
-void LayerNormBackward2(const float* out_grad,
-                        const float* out_grad_add,
-                        const float* X_vals,
-                        const float* gamma,
-                        const float* vars,
-                        const float* means,
-                        float* inp_grad,
-                        int row_stride,
-                        nd_item<3> item_ct1,
-                        float* partialSum)
-{
-    int iteration_stride = item_ct1.get_local_range(2);
-    int iterations = row_stride / iteration_stride;
-
-    // group<3> b = item_ct1.get_group();
-    // cg::thread_block_tile<MAX_SG_NUM> g = cg::tiled_partition<MAX_SG_NUM>(b);
-    sub_group sg = item_ct1.get_sub_group();
-
-    int row = item_ct1.get_group(2);
-    int id = item_ct1.get_local_id(2);
-    int wid = id / MAX_SG_NUM;
-    int warp_num = (THREADS < row_stride ? THREADS : row_stride) / MAX_SG_NUM;
-
-    out_grad += (row * row_stride);
-    if constexpr (is_fuseadd) { out_grad_add += (row * row_stride); }
-    X_vals += (row * row_stride);
-    inp_grad += (row * row_stride);
-
-    float vals_arr[NORM_REG];
-    int high_index = iterations * iteration_stride + id;
-#pragma unroll
-    for (int i = 0; i < iterations; i++) {
-        float gamma_reg = gamma[i * iteration_stride + id];
-        vals_arr[i] = out_grad[i * iteration_stride + id];
-        vals_arr[i] *= gamma_reg;
-    }
-    if ((high_index) < row_stride) {
-        float gamma_reg = gamma[high_index];
-        vals_arr[iterations] = out_grad[high_index];
-        vals_arr[iterations] *= gamma_reg;
-        iterations++;
-    }
-
-    float var_reg = vars[row];
-    float mean_reg = means[row];
-
-    float sum = 0;
-    float xu[NORM_REG];
-    for (int i = 0; i < iterations; i++) {
-        xu[i] = (X_vals[i * iteration_stride + id] - mean_reg);
-        sum += vals_arr[i] * xu[i];
-        vals_arr[i] *= rsqrt(var_reg);
-    }
-
-    // for (int i = 1; i < MAX_SG_NUM; i *= 2) { sum += g.shfl_down(sum, i); }
-    for (int i = 1; i < MAX_SG_NUM; i *= 2) { sum += sg.shuffle_down(sum, i); }
-
-    // if (g.thread_rank() == 0) partialSum[wid] = sum;
-    if (sg.get_local_id() == 0) partialSum[wid] = sum;
-
-    item_ct1.barrier();
-
-    // if (g.thread_rank() < warp_num) sum = partialSum[g.thread_rank()];
-    if (sg.get_local_id() < warp_num) sum = partialSum[sg.get_local_id()];
-
-#ifndef __STOCHASTIC_MODE__
-
-    item_ct1.barrier();
-#endif
-
-    // for (int i = 1; i < warp_num; i *= 2) sum += g.shfl_down(sum, i);
-    for (int i = 1; i < warp_num; i *= 2) sum += sg.shuffle_down(sum, i);
-
-    // sum = g.shfl(sum, 0);
-    sum = sg.shuffle(sum, 0);
-    sum /= row_stride;
-
-    for (int i = 0; i < iterations; i++) {
-        vals_arr[i] += (-sum * xu[i] * rsqrt(var_reg) / (var_reg));
-    }
-
-    sum = 0;
-    for (int i = 0; i < iterations; i++) { sum += vals_arr[i]; }
-
-    // for (int i = 1; i < MAX_SG_NUM; i *= 2) { sum += g.shfl_down(sum, i); }
-    for (int i = 1; i < MAX_SG_NUM; i *= 2) { sum += sg.shuffle_down(sum, i); }
-
-    // if (g.thread_rank() == 0) partialSum[wid] = sum;
-    if (sg.get_local_id() == 0) partialSum[wid] = sum;
-
-    item_ct1.barrier();
-
-    // if (g.thread_rank() < warp_num) sum = partialSum[g.thread_rank()];
-    if (sg.get_local_id() < warp_num) sum = partialSum[sg.get_local_id()];
-
-#ifndef __STOCHASTIC_MODE__
-    item_ct1.barrier();
-#endif
-
-    for (int i = 1; i < warp_num; i *= 2) sum += sg.shuffle_down(sum, i);
-    sum = sg.shuffle(sum, 0);
-    sum /= row_stride;
-
-    iterations = row_stride / iteration_stride;
-    for (int i = 0; i < iterations; i++)
-        if constexpr (is_fuseadd) {
-            inp_grad[i * iteration_stride + id] =
-                (vals_arr[i] - sum) + out_grad_add[i * iteration_stride + id];
-        } else {
-            inp_grad[i * iteration_stride + id] = (vals_arr[i] - sum);
-        }
-    if ((high_index) < row_stride)
-        if constexpr (is_fuseadd) {
-            inp_grad[high_index] = (vals_arr[iterations] - sum) + out_grad_add[high_index];
-        } else {
-            inp_grad[high_index] = (vals_arr[iterations] - sum);
-        }
-}
-
-/* Backward Normalize (Input-Gradient)
- * Using the means and variances from the input
- * This type of backward is not invertible!
- * We do the backward using the input (X)
- */
-template <bool is_fuseadd>
-void LayerNormBackward2(const bf16* out_grad,
-                        const bf16* out_grad_add,
-                        const bf16* X_vals,
-                        const bf16* gamma,
-                        const bf16* vars,
-                        const bf16* means,
-                        bf16* inp_grad,
-                        int row_stride,
-                        nd_item<3> item_ct1,
-                        float* partialSum)
-{
-    int iteration_stride = item_ct1.get_local_range(2);
-    int iterations = row_stride / iteration_stride;
-
-    // group<3> b = item_ct1.get_group();
-    // cg::thread_block_tile<MAX_SG_NUM> g = cg::tiled_partition<MAX_SG_NUM>(b);
-    sub_group sg = item_ct1.get_sub_group();
-
-    const ushort* out_grad_cast = reinterpret_cast<const ushort*>(out_grad);
-    const ushort* out_grad_add_cast = reinterpret_cast<const ushort*>(out_grad_add);
-    const ushort* X_vals_cast = reinterpret_cast<const ushort*>(X_vals);
-    const ushort* gamma_cast = reinterpret_cast<const ushort*>(gamma);
-    const ushort* vars_cast = reinterpret_cast<const ushort*>(vars);
-    const ushort* means_cast = reinterpret_cast<const ushort*>(means);
-    ushort* inp_grad_cast = reinterpret_cast<ushort*>(inp_grad);
-
-    int row = item_ct1.get_group(2);
-    int id = item_ct1.get_local_id(2);
-    int wid = id / MAX_SG_NUM;
-    int warp_num = (THREADS < row_stride ? THREADS : row_stride) / MAX_SG_NUM;
-
-    out_grad_cast += (row * row_stride);
-    if constexpr (is_fuseadd) { out_grad_add_cast += (row * row_stride); }
-    X_vals_cast += (row * row_stride);
-    inp_grad_cast += (row * row_stride);
-
-    float vals_arr[NORM_REG];
-    int high_index = iterations * iteration_stride + id;
-#pragma unroll
-    for (int i = 0; i < iterations; i++) {
-        float gamma_reg = float(gamma_cast[i * iteration_stride + id]);
-        vals_arr[i] = float(out_grad_cast[i * iteration_stride + id]);
-        vals_arr[i] *= gamma_reg;
-    }
-    if ((high_index) < row_stride) {
-        float gamma_reg = float(gamma_cast[high_index]);
-        vals_arr[iterations] = float(out_grad_cast[high_index]);
-        vals_arr[iterations] *= gamma_reg;
-        iterations++;
-    }
-
-    float var_reg = float(vars_cast[row]);
-    float mean_reg = float(means_cast[row]);
-
-    float sum = 0;
-    float xu[NORM_REG];
-    for (int i = 0; i < iterations; i++) {
-        xu[i] = (float(X_vals_cast[i * iteration_stride + id]) - mean_reg);
-        sum += vals_arr[i] * xu[i];
-        vals_arr[i] *= rsqrt(var_reg);
-    }
-
-    // for (int i = 1; i < MAX_SG_NUM; i *= 2) { sum += g.shfl_down(sum, i); }
-    for (int i = 1; i < MAX_SG_NUM; i *= 2) { sum += sg.shuffle_down(sum, i); }
-
-    // if (g.thread_rank() == 0) partialSum[wid] = sum;
-    if (sg.get_local_id() == 0) partialSum[wid] = sum;
-
-    item_ct1.barrier();
-
-    // if (g.thread_rank() < warp_num) sum = partialSum[g.thread_rank()];
-    if (sg.get_local_id() < warp_num) sum = partialSum[sg.get_local_id()];
-
-#ifndef __STOCHASTIC_MODE__
-    item_ct1.barrier();
-#endif
-
-    // for (int i = 1; i < warp_num; i *= 2) sum += g.shfl_down(sum, i);
-    for (int i = 1; i < warp_num; i *= 2) sum += sg.shuffle_down(sum, i);
-
-    // sum = g.shfl(sum, 0);
-    sum = sg.shuffle(sum, 0);
-    sum /= row_stride;
-
-    for (int i = 0; i < iterations; i++) {
-        vals_arr[i] += (-sum * xu[i] * rsqrt(var_reg) / (var_reg));
-    }
-
-    sum = 0;
-    for (int i = 0; i < iterations; i++) { sum += vals_arr[i]; }
-
-    // for (int i = 1; i < MAX_SG_NUM; i *= 2) { sum += g.shfl_down(sum, i); }
-    for (int i = 1; i < MAX_SG_NUM; i *= 2) { sum += sg.shuffle_down(sum, i); }
-
-    // if (g.thread_rank() == 0) partialSum[wid] = sum;
-    if (sg.get_local_id() == 0) partialSum[wid] = sum;
-    item_ct1.barrier();
-
-    // if (g.thread_rank() < warp_num) sum = partialSum[g.thread_rank()];
-    if (sg.get_local_id() < warp_num) sum = partialSum[sg.get_local_id()];
-
-#ifndef __STOCHASTIC_MODE__
-    item_ct1.barrier();
-#endif
-
-    for (int i = 1; i < warp_num; i *= 2) sum += sg.shuffle_down(sum, i);
-    sum = sg.shuffle(sum, 0);
-    sum /= row_stride;
-
-    iterations = row_stride / iteration_stride;
-    for (int i = 0; i < iterations; i++)
-        if constexpr (is_fuseadd) {
-            inp_grad_cast[i * iteration_stride + id] = bf16(
-                (vals_arr[i] - sum) + float(out_grad_add_cast[i * iteration_stride + id]));
-        } else {
-            inp_grad_cast[i * iteration_stride + id] = bf16(vals_arr[i] - sum);
-        }
-    if ((high_index) < row_stride)
-        if constexpr (is_fuseadd) {
-            inp_grad_cast[high_index] = bf16(
-                (vals_arr[iterations] - sum) + float(out_grad_add_cast[high_index]));
-        } else {
-            inp_grad_cast[high_index] = bf16(vals_arr[iterations] - sum);
-        }
-}
-
-template <bool is_fuseadd>
-void LayerNormBackward2(const half* out_grad,
-                        const half* out_grad_add,
-                        const half* X_vals,
-                        const half* gamma,
-                        const half* vars,
-                        const half* means,
-                        half* inp_grad,
-                        int row_stride,
-                        nd_item<3> item_ct1,
-                        float* partialSum)
-{
-    int iteration_stride = item_ct1.get_local_range(2);
-    int iterations = row_stride / iteration_stride;
-
-    // group<3> b = item_ct1.get_group();
-    // cg::thread_block_tile<MAX_SG_NUM> g = cg::tiled_partition<MAX_SG_NUM>(b);
-    sub_group sg = item_ct1.get_sub_group();
-
-    int row = item_ct1.get_group(2);
-    int id = item_ct1.get_local_id(2);
-    int wid = id / MAX_SG_NUM;
-    int warp_num = (iteration_stride < row_stride ? iteration_stride : row_stride) / MAX_SG_NUM;
-
-    half2 vals_arr[NORM_REG];
-    float2 vals_arr_f[NORM_REG];
-
-    half2* inp_grad_h = reinterpret_cast<half2*>(inp_grad);
-    const half2* out_grad_h = reinterpret_cast<const half2*>(out_grad);
-    const half2* out_grad_add_h = reinterpret_cast<const half2*>(out_grad_add);
-    const half2* vals_hat_h = reinterpret_cast<const half2*>(X_vals);
-
-    inp_grad_h += (row * row_stride);
-    out_grad_h += (row * row_stride);
-    if constexpr (is_fuseadd) { out_grad_add_h += (row * row_stride); }
-    vals_hat_h += (row * row_stride);
-
-    const half2* gamma_h = reinterpret_cast<const half2*>(gamma);
-    int high_index = iterations * iteration_stride + id;
-#pragma unroll
-    for (int i = 0; i < iterations; i++) {
-        half2 gamma_reg = gamma_h[i * iteration_stride + id];
-        vals_arr[i] = out_grad_h[i * iteration_stride + id];
-        vals_arr[i] *= gamma_reg;  // out_grad * gamma
-    }
-    if ((high_index) < row_stride) {
-        half2 gamma_reg = gamma_h[high_index];
-        vals_arr[iterations] = out_grad_h[high_index];
-        vals_arr[iterations] *= gamma_reg;  // out_grad * gamma
-        iterations++;
-    }
-    half mean_h = means[row];
-    half var_h = vars[row];
-    half2 var_reg = half2{var_h, var_h};
-    half2 mean_reg = half2{mean_h, mean_h};
-    half2 xu[NORM_REG];
-
-    float sum = 0.f;
-    for (int i = 0; i < iterations; i++) {
-        xu[i] = (vals_hat_h[i * iteration_stride + id] - mean_reg);
-        half2 result_h = (xu[i] * vals_arr[i]);
-        float2 result_f = result_h.convert<float, rounding_mode::automatic>();
-        sum += result_f.x();
-        sum += result_f.y();
-        vals_arr[i] *= rsqrt(var_reg);
-    }
-
-    for (int i = 1; i < MAX_SG_NUM; i *= 2) { sum += sg.shuffle_down(sum, i); }
-
-    if (sg.get_local_id() == 0) partialSum[wid] = sum;
-
-    item_ct1.barrier();
-
-    if (sg.get_local_id() < warp_num) sum = partialSum[sg.get_local_id()];
-
-#ifndef __STOCHASTIC_MODE__
-    item_ct1.barrier();
-#endif
-
-    for (int i = 1; i < warp_num; i *= 2) sum += sg.shuffle_down(sum, i);
-
-    sum = sg.shuffle(sum, 0);
-    sum /= (2 * row_stride);
-    half2 sum_h = float2{sum, sum}.convert<half>();
-
-    for (int i = 0; i < iterations; i++) {
-        half2 xu_grad = ((-sum_h * xu[i] * rsqrt(var_reg)) / (var_reg));
-        vals_arr_f[i] = vals_arr[i].convert<float, rounding_mode::automatic>();
-        float2 xu_grad_f = xu_grad.convert<float, rounding_mode::automatic>();
-        vals_arr_f[i].x() += xu_grad_f.x();
-        vals_arr_f[i].y() += xu_grad_f.y();
-    }
-
-    sum = 0.f;
-    for (int i = 0; i < iterations; i++) {
-        sum += (vals_arr_f[i].x());
-        sum += (vals_arr_f[i].y());
-    }
-
-    for (int i = 1; i < MAX_SG_NUM; i *= 2) { sum += sg.shuffle_down(sum, i); }
-
-    if (sg.get_local_id() == 0) partialSum[wid] = sum;
-
-    item_ct1.barrier();
-
-    if (sg.get_local_id() < warp_num) sum = partialSum[sg.get_local_id()];
-
-#ifndef __STOCHASTIC_MODE__
-    item_ct1.barrier();
-#endif
-
-    for (int i = 1; i < warp_num; i *= 2) sum += sg.shuffle_down(sum, i);
-
-    sum = sg.shuffle(sum, 0);
-    sum /= (2 * row_stride);
-
-    iterations = row_stride / iteration_stride;
-    for (int i = 0; i < iterations; i++) {
-        vals_arr_f[i].x() -= sum;
-        vals_arr_f[i].y() -= sum;
-        half2 temp = vals_arr_f[i].convert<half>();
-        if constexpr (is_fuseadd) {
-            inp_grad_h[i * iteration_stride + id] =
-                temp + out_grad_add_h[i * iteration_stride + id];
-        } else {
-            inp_grad_h[i * iteration_stride + id] = temp;
-        }
-    }
-    if ((high_index) < row_stride) {
-        vals_arr_f[iterations].x() -= sum;
-        vals_arr_f[iterations].y() -= sum;
-        half2 temp = vals_arr_f[iterations].convert<half>();
-        if constexpr (is_fuseadd) {
-            inp_grad_h[high_index] = temp + out_grad_add_h[high_index];
-        } else {
-            inp_grad_h[high_index] = temp;
-        }
-    }
-}
-
-template <typename T>
-void launch_layerNorm_backward(const T* out_grad,
-                               const T* X_data,
-                               const T* vars,
-                               const T* means,
-                               const T* gamma,
-                               T* gamma_grad,
-                               T* betta_grad,
-                               T* inp_grad,
-                               int batch,
-                               int hidden_dim,
-                               queue* stream[2])
-{
-    int threads = THREADS;
-
-    range<3> grid_dim(1, 1, hidden_dim / TILE_DIM);
-    range<3> block_dim(1, TILE_DIM, TILE_DIM);
-
-    // LayerNormBackward1<float><<<grid_dim, block_dim, 0, stream[0]>>>(
-    //     out_grad, X_data, vars, means, gamma_grad, betta_grad, batch,
-    //     hidden_dim);
-    stream[0]->submit([&](handler& cgh) {
-        accessor<float, 2, access_mode::read_write, access::target::local> betta_buffer(
-            range<2>(MAX_SG_NUM /*MAX_WARP_NUM*/, MAX_SG_NUM1), cgh);
-        accessor<float, 2, access_mode::read_write, access::target::local> gamma_buffer(
-            range<2>(MAX_SG_NUM /*MAX_WARP_NUM*/, MAX_SG_NUM1), cgh);
-
-        cgh.parallel_for(nd_range<3>(grid_dim * block_dim, block_dim),
-                         [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                             LayerNormBackward1(out_grad,
-                                                X_data,
-                                                vars,
-                                                means,
-                                                gamma_grad,
-                                                betta_grad,
-                                                batch,
-                                                hidden_dim,
-                                                item_ct1,
-                                                betta_buffer.get_pointer(),
-                                                gamma_buffer.get_pointer());
-                         });
-    });
-
-    range<3> grid_dim2(1, 1, batch);
-
-    if (hidden_dim > 16384 && hidden_dim <= 32768)
-        threads <<= 1;
-    else if (hidden_dim > 32768 && hidden_dim <= 65536)
-        threads <<= 2;
-    else if (hidden_dim > 65536)
-        throw std::runtime_error("Unsupport hidden_dim.");
-
-    range<3> block_dim2(1, 1, threads);
-    stream[1]->submit([&](handler& cgh) {
-        accessor<float, 1, access_mode::read_write, access::target::local> partialSum_acc_ct1(
-            range<1>(MAX_SG_NUM /*MAX_WARP_NUM*/), cgh);
-
-        cgh.parallel_for(nd_range<3>(grid_dim2 * block_dim2, block_dim2),
-                         [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                             LayerNormBackward2<false>(out_grad,
-                                                       nullptr,
-                                                       X_data,
-                                                       gamma,
-                                                       vars,
-                                                       means,
-                                                       inp_grad,
-                                                       hidden_dim,
-                                                       item_ct1,
-                                                       partialSum_acc_ct1.get_pointer());
-                         });
-    });
-}
-
-template void launch_layerNorm_backward<float>(const float* out_grad,
-                                               const float* X_data,
-                                               const float* vars,
-                                               const float* means,
-                                               const float* gamma,
-                                               float* gamma_grad,
-                                               float* betta_grad,
-                                               float* inp_grad,
-                                               int batch,
-                                               int hidden_dim,
-                                               queue* stream[2]);
-template void launch_layerNorm_backward<bf16>(const bf16* out_grad,
-                                              const bf16* X_data,
-                                              const bf16* vars,
-                                              const bf16* means,
-                                              const bf16* gamma,
-                                              bf16* gamma_grad,
-                                              bf16* betta_grad,
-                                              bf16* inp_grad,
-                                              int batch,
-                                              int hidden_dim,
-                                              queue* stream[2]);
-template <>
-void launch_layerNorm_backward<half>(const half* out_grad,
-                                     const half* X_data,
-                                     const half* vars,
-                                     const half* means,
-                                     const half* gamma,
-                                     half* gamma_grad,
-                                     half* betta_grad,
-                                     half* inp_grad,
-                                     int batch,
-                                     int hidden_dim,
-                                     queue* stream[2])
-{
-    int threads = THREADS;
-
-    range<3> grid_dim(1, 1, hidden_dim / TILE_DIM);
-    range<3> block_dim(1, TILE_DIM, TILE_DIM);
-
-    // LayerNormBackward1<half><<<grid_dim, block_dim, 0, stream[0]>>>(
-    //     out_grad, X_data, vars, means, gamma_grad, betta_grad, batch,
-    //     hidden_dim);
-    stream[0]->submit([&](handler& cgh) {
-        accessor<float, 2, access_mode::read_write, access::target::local> betta_buffer(
-            range<2>(MAX_SG_NUM /*MAX_WARP_NUM*/, MAX_SG_NUM1), cgh);
-        accessor<float, 2, access_mode::read_write, access::target::local> gamma_buffer(
-            range<2>(MAX_SG_NUM /*MAX_WARP_NUM*/, MAX_SG_NUM1), cgh);
-
-        cgh.parallel_for(nd_range<3>(grid_dim * block_dim, block_dim),
-                         [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                             LayerNormBackward1<half>(out_grad,
-                                                      X_data,
-                                                      vars,
-                                                      means,
-                                                      gamma_grad,
-                                                      betta_grad,
-                                                      batch,
-                                                      hidden_dim,
-                                                      item_ct1,
-                                                      betta_buffer.get_pointer(),
-                                                      gamma_buffer.get_pointer());
-                         });
-    });
-
-    range<3> grid_dim2(1, 1, batch);
-
-    if (hidden_dim > 8192 && hidden_dim <= 16384)
-        threads <<= 1;
-    else if (hidden_dim > 16384 && hidden_dim <= 32768)
-        threads <<= 2;
-    else if (hidden_dim > 32768 && hidden_dim <= 65536)
-        threads <<= 3;
-    else if (hidden_dim > 65536)
-        throw std::runtime_error("Unsupport hidden_dim.");
-
-    range<3> block_dim2(1, 1, threads / 2);
-    stream[1]->submit([&](handler& cgh) {
-        accessor<float, 1, access_mode::read_write, access::target::local> partialSum_acc_ct1(
-            range<1>(MAX_SG_NUM /*MAX_WARP_NUM*/), cgh);
-
-        cgh.parallel_for(nd_range<3>(grid_dim2 * block_dim2, block_dim2),
-                         [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                             LayerNormBackward2<false>(out_grad,
-                                                       nullptr,
-                                                       X_data,
-                                                       gamma,
-                                                       vars,
-                                                       means,
-                                                       inp_grad,
-                                                       hidden_dim / 2,
-                                                       item_ct1,
-                                                       partialSum_acc_ct1.get_pointer());
-                         });
-    });
-}
-
-template <typename T>
-void launch_layerNorm_backward_fused_add(const T* out_grad1,
-                                         const T* out_grad2,
-                                         const T* vals_hat,
-                                         const T* vars,
-                                         const T* gamma,
-                                         T* gamma_grad,
-                                         T* betta_grad,
-                                         T* inp_grad,
-                                         int batch,
-                                         int hidden_dim,
-                                         queue* stream[2],
-                                         bool invertible,
-                                         const T* betta)
-{
-    int threads = THREADS;
-
-    range<3> grid_dim(1, 1, hidden_dim / TILE_DIM);
-    range<3> block_dim(1, TILE_DIM, TILE_DIM);
-    // LayerNormBackward1<float><<<grid_dim, block_dim, 0, stream[0]>>>(
-    //     out_grad1, vals_hat, gamma, betta, gamma_grad, betta_grad, batch,
-    //     hidden_dim, invertible);
-    stream[0]->submit([&](handler& cgh) {
-        accessor<float, 2, access_mode::read_write, access::target::local> betta_buffer(
-            range<2>(MAX_SG_NUM /*MAX_WARP_NUM*/, MAX_SG_NUM1), cgh);
-        accessor<float, 2, access_mode::read_write, access::target::local> gamma_buffer(
-            range<2>(MAX_SG_NUM /*MAX_WARP_NUM*/, MAX_SG_NUM1), cgh);
-        cgh.parallel_for(nd_range<3>(grid_dim * block_dim, block_dim),
-                         [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                             LayerNormBackward1(out_grad1,
-                                                vals_hat,
-                                                gamma,
-                                                betta,
-                                                gamma_grad,
-                                                betta_grad,
-                                                batch,
-                                                hidden_dim,
-                                                invertible,
-                                                item_ct1,
-                                                betta_buffer.get_pointer(),
-                                                gamma_buffer.get_pointer());
-                         });
-    });
-
-    range<3> grid_dim2(1, 1, batch);
-
-    if (hidden_dim > 16384 && hidden_dim <= 32768)
-        threads <<= 1;
-    else if (hidden_dim > 32768 && hidden_dim <= 65536)
-        threads <<= 2;
-    else if (hidden_dim > 65536)
-        throw std::runtime_error("Unsupport hidden_dim.");
-
-    range<3> block_dim2(1, 1, threads);
-
-    stream[1]->submit([&](handler& cgh) {
-        accessor<float, 1, access_mode::read_write, access::target::local> partialSum_acc_ct1(
-            range<1>(MAX_SG_NUM /*MAX_WARP_NUM*/), cgh);
-
-        cgh.parallel_for(nd_range<3>(grid_dim2 * block_dim2, block_dim2),
-                         [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                             LayerNormBackward2<true>(out_grad1,
-                                                      out_grad2,
-                                                      vals_hat,
-                                                      gamma,
-                                                      betta,
-                                                      vars,
-                                                      inp_grad,
-                                                      invertible,
-                                                      hidden_dim,
-                                                      item_ct1,
-                                                      partialSum_acc_ct1.get_pointer());
-                         });
-    });
-}
-
-template void launch_layerNorm_backward_fused_add<float>(const float* out_grad1,
-                                                         const float* out_grad2,
-                                                         const float* vals_hat,
-                                                         const float* vars,
-                                                         const float* gamma,
-                                                         float* gamma_grad,
-                                                         float* betta_grad,
-                                                         float* inp_grad,
-                                                         int batch,
-                                                         int hidden_dim,
-                                                         queue* stream[2],
-                                                         bool invertible,
-                                                         const float* betta);
-
-template void launch_layerNorm_backward_fused_add<bf16>(const bf16* out_grad1,
-                                                        const bf16* out_grad2,
-                                                        const bf16* vals_hat,
-                                                        const bf16* vars,
-                                                        const bf16* gamma,
-                                                        bf16* gamma_grad,
-                                                        bf16* betta_grad,
-                                                        bf16* inp_grad,
-                                                        int batch,
-                                                        int hidden_dim,
-                                                        queue* stream[2],
-                                                        bool invertible,
-                                                        const bf16* betta);
-
-template <>
-void launch_layerNorm_backward_fused_add<half>(const half* out_grad1,
-                                               const half* out_grad2,
-                                               const half* vals_hat,
-                                               const half* vars,
-                                               const half* gamma,
-                                               half* gamma_grad,
-                                               half* betta_grad,
-                                               half* inp_grad,
-                                               int batch,
-                                               int hidden_dim,
-                                               queue* stream[2],
-                                               bool invertible,
-                                               const half* betta)
-{
-    int threads = THREADS;
-
-    range<3> grid_dim(1, 1, hidden_dim / TILE_DIM);
-    range<3> block_dim(1, TILE_DIM, TILE_DIM);
-
-    // LayerNormBackward1<half><<<grid_dim, block_dim, 0, stream[0]>>>(
-    //     out_grad1, vals_hat, gamma, betta, gamma_grad, betta_grad, batch,
-    //     hidden_dim, invertible);
-    stream[0]->submit([&](handler& cgh) {
-        accessor<float, 2, access_mode::read_write, access::target::local> betta_buffer(
-            range<2>(MAX_SG_NUM /*MAX_WARP_NUM*/, MAX_SG_NUM1), cgh);
-        accessor<float, 2, access_mode::read_write, access::target::local> gamma_buffer(
-            range<2>(MAX_SG_NUM /*MAX_WARP_NUM*/, MAX_SG_NUM1), cgh);
-        cgh.parallel_for(nd_range<3>(grid_dim * block_dim, block_dim),
-                         [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                             LayerNormBackward1<half>(out_grad1,
-                                                      vals_hat,
-                                                      gamma,
-                                                      betta,
-                                                      gamma_grad,
-                                                      betta_grad,
-                                                      batch,
-                                                      hidden_dim,
-                                                      invertible,
-                                                      item_ct1,
-                                                      betta_buffer.get_pointer(),
-                                                      gamma_buffer.get_pointer());
-                         });
-    });
-
-    range<3> grid_dim2(1, 1, batch);
-
-    if (hidden_dim > 8192 && hidden_dim <= 16384)
-        threads <<= 1;
-    else if (hidden_dim > 16384 && hidden_dim <= 32768)
-        threads <<= 2;
-    else if (hidden_dim > 32768 && hidden_dim <= 65536)
-        threads <<= 3;
-    else if (hidden_dim > 65536)
-        throw std::runtime_error("Unsupport hidden_dim.");
-
-    range<3> block_dim2(1, 1, threads / 2);
-    stream[1]->submit([&](handler& cgh) {
-        accessor<float, 1, access_mode::read_write, access::target::local> partialSum_acc_ct1(
-            range<1>(MAX_SG_NUM /*MAX_WARP_NUM*/), cgh);
-
-        cgh.parallel_for(nd_range<3>(grid_dim2 * block_dim2, block_dim2),
-                         [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                             LayerNormBackward2<true>(out_grad1,
-                                                      out_grad2,
-                                                      vals_hat,
-                                                      gamma,
-                                                      betta,
-                                                      vars,
-                                                      inp_grad,
-                                                      invertible,
-                                                      hidden_dim / 2,
-                                                      item_ct1,
-                                                      partialSum_acc_ct1.get_pointer());
-                         });
-    });
-}
-
-template <typename T>
-void launch_layerNorm_backward_fused_add(const T* out_grad1,
-                                         const T* out_grad2,
-                                         const T* X_data,
-                                         const T* vars,
-                                         const T* means,
-                                         const T* gamma,
-                                         T* gamma_grad,
-                                         T* betta_grad,
-                                         T* inp_grad,
-                                         int batch,
-                                         int hidden_dim,
-                                         queue* stream[2])
-{
-    int threads = THREADS;
-
-    range<3> grid_dim(1, 1, hidden_dim / TILE_DIM);
-    range<3> block_dim(1, TILE_DIM, TILE_DIM);
-
-    // LayerNormBackward1<float><<<grid_dim, block_dim, 0, stream[0]>>>(
-    //     out_grad1, X_data, vars, means, gamma_grad, betta_grad, batch,
-    //     hidden_dim);
-    stream[0]->submit([&](handler& cgh) {
-        accessor<float, 2, access_mode::read_write, access::target::local> betta_buffer(
-            range<2>(MAX_SG_NUM /*MAX_WARP_NUM*/, MAX_SG_NUM1), cgh);
-        accessor<float, 2, access_mode::read_write, access::target::local> gamma_buffer(
-            range<2>(MAX_SG_NUM /*MAX_WARP_NUM*/, MAX_SG_NUM1), cgh);
-
-        cgh.parallel_for(nd_range<3>(grid_dim * block_dim, block_dim),
-                         [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                             LayerNormBackward1(out_grad1,
-                                                X_data,
-                                                vars,
-                                                means,
-                                                gamma_grad,
-                                                betta_grad,
-                                                batch,
-                                                hidden_dim,
-                                                item_ct1,
-                                                betta_buffer.get_pointer(),
-                                                gamma_buffer.get_pointer());
-                         });
-    });
-
-    range<3> grid_dim2(1, 1, batch);
-
-    if (hidden_dim > 16384 && hidden_dim <= 32768)
-        threads <<= 1;
-    else if (hidden_dim > 32768 && hidden_dim <= 65536)
-        threads <<= 2;
-    else if (hidden_dim > 65536)
-        throw std::runtime_error("Unsupport hidden_dim.");
-
-    range<3> block_dim2(1, 1, threads);
-    stream[1]->submit([&](handler& cgh) {
-        accessor<float, 1, access_mode::read_write, access::target::local> partialSum_acc_ct1(
-            range<1>(MAX_SG_NUM /*MAX_WARP_NUM*/), cgh);
-
-        cgh.parallel_for(nd_range<3>(grid_dim2 * block_dim2, block_dim2),
-                         [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                             LayerNormBackward2<true>(out_grad1,
-                                                      out_grad2,
-                                                      X_data,
-                                                      gamma,
-                                                      vars,
-                                                      means,
-                                                      inp_grad,
-                                                      hidden_dim,
-                                                      item_ct1,
-                                                      partialSum_acc_ct1.get_pointer());
-                         });
-    });
-}
-
-template void launch_layerNorm_backward_fused_add<float>(const float* out_grad1,
-                                                         const float* out_grad2,
-                                                         const float* X_data,
-                                                         const float* vars,
-                                                         const float* means,
-                                                         const float* gamma,
-                                                         float* gamma_grad,
-                                                         float* betta_grad,
-                                                         float* inp_grad,
-                                                         int batch,
-                                                         int hidden_dim,
-                                                         queue* stream[2]);
-template void launch_layerNorm_backward_fused_add<bf16>(const bf16* out_grad1,
-                                                        const bf16* out_grad2,
-                                                        const bf16* X_data,
-                                                        const bf16* vars,
-                                                        const bf16* means,
-                                                        const bf16* gamma,
-                                                        bf16* gamma_grad,
-                                                        bf16* betta_grad,
-                                                        bf16* inp_grad,
-                                                        int batch,
-                                                        int hidden_dim,
-                                                        queue* stream[2]);
-template <>
-void launch_layerNorm_backward_fused_add<half>(const half* out_grad1,
-                                               const half* out_grad2,
-                                               const half* X_data,
-                                               const half* vars,
-                                               const half* means,
-                                               const half* gamma,
-                                               half* gamma_grad,
-                                               half* betta_grad,
-                                               half* inp_grad,
-                                               int batch,
-                                               int hidden_dim,
-                                               queue* stream[2])
-{
-    int threads = THREADS;
-
-    range<3> grid_dim(1, 1, hidden_dim / TILE_DIM);
-    range<3> block_dim(1, TILE_DIM, TILE_DIM);
-
-    // LayerNormBackward1<half><<<grid_dim, block_dim, 0, stream[0]>>>(
-    //     out_grad1, X_data, vars, means, gamma_grad, betta_grad, batch,
-    //     hidden_dim);
-    stream[0]->submit([&](handler& cgh) {
-        accessor<float, 2, access_mode::read_write, access::target::local> betta_buffer(
-            range<2>(MAX_SG_NUM /*MAX_WARP_NUM*/, MAX_SG_NUM1), cgh);
-        accessor<float, 2, access_mode::read_write, access::target::local> gamma_buffer(
-            range<2>(MAX_SG_NUM /*MAX_WARP_NUM*/, MAX_SG_NUM1), cgh);
-        cgh.parallel_for(nd_range<3>(grid_dim * block_dim, block_dim),
-                         [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                             LayerNormBackward1<half>(out_grad1,
-                                                      X_data,
-                                                      vars,
-                                                      means,
-                                                      gamma_grad,
-                                                      betta_grad,
-                                                      batch,
-                                                      hidden_dim,
-                                                      item_ct1,
-                                                      betta_buffer.get_pointer(),
-                                                      gamma_buffer.get_pointer());
-                         });
-    });
-
-    range<3> grid_dim2(1, 1, batch);
-
-    if (hidden_dim > 8192 && hidden_dim <= 16384)
-        threads <<= 1;
-    else if (hidden_dim > 16384 && hidden_dim <= 32768)
-        threads <<= 2;
-    else if (hidden_dim > 32768 && hidden_dim <= 65536)
-        threads <<= 3;
-    else if (hidden_dim > 65536)
-        throw std::runtime_error("Unsupport hidden_dim.");
-
-    range<3> block_dim2(1, 1, threads / 2);
-    stream[1]->submit([&](handler& cgh) {
-        accessor<float, 1, access_mode::read_write, access::target::local> partialSum_acc_ct1(
-            range<1>(MAX_SG_NUM /*MAX_WARP_NUM*/), cgh);
-
-        cgh.parallel_for(nd_range<3>(grid_dim2 * block_dim2, block_dim2),
-                         [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                             LayerNormBackward2<true>(out_grad1,
-                                                      out_grad2,
-                                                      X_data,
-                                                      gamma,
-                                                      vars,
-                                                      means,
-                                                      inp_grad,
-                                                      hidden_dim / 2,
-                                                      item_ct1,
-                                                      partialSum_acc_ct1.get_pointer());
-                         });
-    });
-}
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/onednn_wrappers.dp.cpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/onednn_wrappers.dp.cpp
deleted file mode 100644
index 073d628..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/transformer/onednn_wrappers.dp.cpp
+++ /dev/null
@@ -1,127 +0,0 @@
-#include "onednn_wrappers.hpp"
-#include <oneapi/dnnl/dnnl_sycl.hpp>
-
-template <bool bmm>
-inline int onednn_matmul(sycl::queue* handle,
-                         bool trans_src,
-                         bool trans_wgt,
-                         int m,
-                         int n,
-                         int k,
-                         const float alpha,
-                         const float beta,
-                         const bf16* src_ptr,
-                         const bf16* wgt_ptr,
-                         bf16* dst_ptr,
-                         int batch)
-{
-    /*
-     * src, [m, k], m: batch, k: in_feature
-     * wgt, [k, n], n: k: in_features, out_feature
-     * dst, [m, n], m: batch, n: out_features
-     */
-    device dev = handle->get_device();
-    context ctx = handle->get_context();
-    dnnl::engine engine = dnnl::sycl_interop::make_engine(dev, ctx);
-    dnnl::stream stream = dnnl::sycl_interop::make_stream(engine, *handle);
-
-    dnnl::memory::dims src_dims, wgt_dims, dst_dims;
-
-    if constexpr (bmm) {
-        src_dims = {batch, m, k};
-        wgt_dims = {batch, k, n};
-        dst_dims = {batch, m, n};
-    } else {
-        src_dims = {m, k};
-        wgt_dims = {k, n};
-        dst_dims = {m, n};
-    }
-
-    dnnl::memory::desc src_md, wgt_md, dst_md;
-
-    if constexpr (bmm) {
-        src_md = dnnl::memory::desc(
-            src_dims,
-            dnnl::memory::data_type::bf16,
-            trans_src ? dnnl::memory::format_tag::acb : dnnl::memory::format_tag::abc);
-        wgt_md = dnnl::memory::desc(
-            wgt_dims,
-            dnnl::memory::data_type::bf16,
-            trans_wgt ? dnnl::memory::format_tag::acb : dnnl::memory::format_tag::abc);
-        dst_md = dnnl::memory::desc(
-            dst_dims, dnnl::memory::data_type::bf16, dnnl::memory::format_tag::abc);
-    } else {
-        src_md = dnnl::memory::desc(
-            src_dims,
-            dnnl::memory::data_type::bf16,
-            trans_src ? dnnl::memory::format_tag::ba : dnnl::memory::format_tag::ab);
-        wgt_md = dnnl::memory::desc(
-            wgt_dims,
-            dnnl::memory::data_type::bf16,
-            trans_wgt ? dnnl::memory::format_tag::ba : dnnl::memory::format_tag::ab);
-        dst_md = dnnl::memory::desc(
-            dst_dims, dnnl::memory::data_type::bf16, dnnl::memory::format_tag::ab);
-    }
-
-    auto src_mem = dnnl::memory(src_md, engine, (void*)src_ptr);
-    auto wgt_mem = dnnl::memory(wgt_md, engine, (void*)wgt_ptr);
-    auto dst_mem = dnnl::memory(dst_md, engine, (void*)dst_ptr);
-
-    dnnl::primitive_attr attr;
-    std::unordered_map<int, dnnl::memory> matmul_args;
-    if (alpha != 1.0f) {
-        float alpha_v(alpha);
-        attr.set_scales_mask(DNNL_ARG_DST, /* mask */ 0);
-        dnnl::memory alpha_mem({{1}, dnnl::memory::data_type::f32, {1}}, engine, &alpha_v);
-        matmul_args.insert({DNNL_ARG_ATTR_SCALES | DNNL_ARG_WEIGHTS, alpha_mem});
-    }
-    if (beta != 0.0f) {
-        dnnl::post_ops po;
-        po.append_sum(beta);
-        attr.set_post_ops(po);
-    }
-
-    auto matmul_pd = dnnl::matmul::primitive_desc(engine, src_md, wgt_md, dst_md, attr);
-
-    auto matmul_prim = dnnl::matmul(matmul_pd);
-
-    matmul_args.insert({DNNL_ARG_SRC, src_mem});
-    matmul_args.insert({DNNL_ARG_WEIGHTS, wgt_mem});
-    matmul_args.insert({DNNL_ARG_DST, dst_mem});
-
-    matmul_prim.execute(stream, matmul_args);
-    stream.wait();
-}
-
-int onednn_matmul_ex(sycl::queue* handle,
-                     bool trans_src,
-                     bool trans_wgt,
-                     int m,
-                     int n,
-                     int k,
-                     const float alpha,
-                     const float beta,
-                     const bf16* src_ptr,
-                     const bf16* wgt_ptr,
-                     bf16* dst_ptr)
-{
-    onednn_matmul<false>(
-        handle, trans_src, trans_wgt, m, n, k, alpha, beta, src_ptr, wgt_ptr, dst_ptr, 1);
-}
-
-int onednn_batchgemm(sycl::queue* handle,
-                     int m,
-                     int n,
-                     int k,
-                     const float alpha,
-                     const float beta,
-                     const bf16* src_ptr,
-                     const bf16* wgt_ptr,
-                     bf16* dst_ptr,
-                     bool trans_src,
-                     bool trans_wgt,
-                     int batch)
-{
-    onednn_matmul<true>(
-        handle, trans_src, trans_wgt, m, n, k, alpha, beta, src_ptr, wgt_ptr, dst_ptr, batch);
-}
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/onemkl_wrappers.dp.cpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/onemkl_wrappers.dp.cpp
deleted file mode 100644
index e003b99..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/transformer/onemkl_wrappers.dp.cpp
+++ /dev/null
@@ -1,150 +0,0 @@
-#include "onemkl_wrappers.hpp"
-#if __has_include(<sycl/sycl.hpp>)
-#include <sycl/sycl.hpp>
-#elif __has_include(<CL/sycl.hpp>)
-#include <CL/sycl.hpp>
-#else
-#error "Unsupported compiler"
-#endif
-
-int onemkl_gemm_ex(sycl::queue* handle,
-                   oneapi::mkl::transpose transa,
-                   oneapi::mkl::transpose transb,
-                   int m,
-                   int n,
-                   int k,
-                   const float alpha,
-                   const float beta,
-                   const float* A,
-                   const float* B,
-                   float* C)
-{
-    try {
-        int lda = (transa == oneapi::mkl::transpose::nontrans) ? m : k;
-        int ldb = (transb == oneapi::mkl::transpose::nontrans) ? k : n;
-        int ldc = m;
-        oneapi::mkl::blas::gemm(
-            *handle, transa, transb, m, n, k, alpha, A, lda, B, ldb, beta, C, ldc);
-    } catch (sycl::exception const& exc) {
-        std::cerr << exc.what() << "Exception caught at file:" << __FILE__ << ", line:" << __LINE__
-                  << std::endl;
-        std::exit(1);
-    }
-}
-
-int onemkl_gemm_ex(sycl::queue* handle,
-                   oneapi::mkl::transpose transa,
-                   oneapi::mkl::transpose transb,
-                   int m,
-                   int n,
-                   int k,
-                   const sycl::half alpha,
-                   const sycl::half beta,
-                   const sycl::half* A,
-                   const sycl::half* B,
-                   sycl::half* C)
-{
-    try {
-        int lda = (transa == oneapi::mkl::transpose::nontrans) ? m : k;
-        int ldb = (transb == oneapi::mkl::transpose::nontrans) ? k : n;
-        int ldc = m;
-        oneapi::mkl::blas::gemm(
-            *handle, transa, transb, m, n, k, alpha, A, lda, B, ldb, beta, C, ldc);
-    } catch (sycl::exception const& exc) {
-        std::cerr << exc.what() << "Exception caught at file:" << __FILE__ << ", line:" << __LINE__
-                  << std::endl;
-        std::exit(1);
-    }
-}
-
-int onemkl_strided_batched_gemm(sycl::queue* handle,
-                                int m,
-                                int n,
-                                int k,
-                                const float alpha,
-                                const float beta,
-                                const float* A,
-                                const float* B,
-                                float* C,
-                                oneapi::mkl::transpose transa,
-                                oneapi::mkl::transpose transb,
-                                int stride_A,
-                                int stride_B,
-                                int stride_C,
-                                int batch,
-                                int algo)
-{
-    try {
-        int lda = (transa == oneapi::mkl::transpose::nontrans) ? m : k;
-        int ldb = (transb == oneapi::mkl::transpose::nontrans) ? k : n;
-        int ldc = m;
-        oneapi::mkl::blas::gemm_batch(*handle,
-                                      transa,
-                                      transb,
-                                      m,
-                                      n,
-                                      k,
-                                      alpha,
-                                      A,
-                                      lda,
-                                      stride_A,
-                                      B,
-                                      ldb,
-                                      stride_B,
-                                      beta,
-                                      C,
-                                      ldc,
-                                      stride_C,
-                                      batch);
-    } catch (sycl::exception const& exc) {
-        std::cerr << exc.what() << "Exception caught at file:" << __FILE__ << ", line:" << __LINE__
-                  << " (batch, m, n, k)" << batch << " " << m << " " << n << " " << k << std::endl;
-        std::exit(1);
-    }
-}
-
-int onemkl_strided_batched_gemm(sycl::queue* handle,
-                                int m,
-                                int n,
-                                int k,
-                                const sycl::half alpha,
-                                const sycl::half beta,
-                                const sycl::half* A,
-                                const sycl::half* B,
-                                sycl::half* C,
-                                oneapi::mkl::transpose transa,
-                                oneapi::mkl::transpose transb,
-                                int stride_A,
-                                int stride_B,
-                                int stride_C,
-                                int batch,
-                                int algo)
-{
-    try {
-        int lda = (transa == oneapi::mkl::transpose::nontrans) ? m : k;
-        int ldb = (transb == oneapi::mkl::transpose::nontrans) ? k : n;
-        int ldc = m;
-        oneapi::mkl::blas::gemm_batch(*handle,
-                                      transa,
-                                      transb,
-                                      m,
-                                      n,
-                                      k,
-                                      alpha,
-                                      A,
-                                      lda,
-                                      stride_A,
-                                      B,
-                                      ldb,
-                                      stride_B,
-                                      beta,
-                                      C,
-                                      ldc,
-                                      stride_C,
-                                      batch);
-    } catch (sycl::exception const& exc) {
-        std::cerr << exc.what() << "Exception caught at file:" << __FILE__ << ", line:" << __LINE__
-                  << " (batch, m, n, k)" << batch << " " << m << " " << n << " " << k << std::endl;
-        std::exit(1);
-    }
-}
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/softmax_kernels.dp.cpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/softmax_kernels.dp.cpp
deleted file mode 100644
index 52f9755..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/transformer/softmax_kernels.dp.cpp
+++ /dev/null
@@ -1,861 +0,0 @@
-#if __has_include(<sycl/sycl.hpp>)
-#include <sycl/sycl.hpp>
-using namespace sycl;
-#elif __has_include(<CL/sycl.hpp>)
-#include <CL/sycl.hpp>
-using namespace cl::sycl;
-#else
-#error "Unsupported compiler"
-#endif
-#include "custom_sycl_layers.hpp"
-#include "general_kernels.hpp"
-
-#define MAX_SG_NUM (32)
-// Fused attention + softmax
-template <int tbSize, int blockStride, int tbSeq>
-void attn_softmax(float* vals,
-                  const float* attn_mask,
-                  int heads,
-                  int seq_length,
-                  int iterations,
-                  nd_item<3> item_ct1,
-                  float* partialSum)
-{
-    int sg_num = item_ct1.get_local_range().get(2) >> 5;
-
-    int iteration_stride = item_ct1.get_local_range().get(2);
-    int block_width = blockStride * seq_length;
-
-    // auto b = item_ct1.get_group();
-    // cg::thread_block_tile<tbSize> g = cg::tiled_partition<tbSize>(b);
-    sub_group sg = item_ct1.get_sub_group();
-
-    int batch = item_ct1.get_group(2);
-    int row = item_ct1.get_group(1);
-    int max_threads_in_sequence = std::max(seq_length, tbSeq);
-    int seq_lane = item_ct1.get_local_id(2) % max_threads_in_sequence;
-
-    int data_offset = batch * (item_ct1.get_group_range(1) * block_width) + row * block_width +
-                      (item_ct1.get_local_id(2) / max_threads_in_sequence) * seq_length;
-    int mask_offset = batch * seq_length;
-
-    int wid = item_ct1.get_local_id(2) >> 5;
-    int lane = item_ct1.get_local_id(2) & 0x1f;
-
-    float4* val_cast = reinterpret_cast<float4*>(vals);
-    const float4* attn_mask_cast = reinterpret_cast<const float4*>(attn_mask);
-
-    float4 data[MAX_THREAD_ITERATIONS];
-
-    float max_val = minus_infinity;
-
-    for (int i = 0; i < iterations; i++) {
-        int data_id = i * iteration_stride + seq_lane;
-        if (data_id < seq_length) {
-            float4 mask = attn_mask_cast[mask_offset + data_id];
-            data[i] = val_cast[data_offset + data_id];
-            data[i].x() += mask.x();
-            data[i].y() += mask.y();
-            data[i].z() += mask.z();
-            data[i].w() += mask.w();
-
-            max_val = (data[i].x() > max_val ? data[i].x() : max_val);
-            max_val = (data[i].y() > max_val ? data[i].y() : max_val);
-            max_val = (data[i].z() > max_val ? data[i].z() : max_val);
-            max_val = (data[i].w() > max_val ? data[i].w() : max_val);
-        } else {
-            data[i].x() = minus_infinity;
-            data[i].y() = minus_infinity;
-            data[i].z() = minus_infinity;
-            data[i].w() = minus_infinity;
-        }
-    }
-
-    for (int i = 1; i < tbSize; i *= 2) {
-        auto temp = sg.shuffle_xor(max_val, i);
-        max_val = (temp > max_val ? temp : max_val);
-    }
-
-    if (seq_length > tbSize) {
-        if (lane == 0) partialSum[wid] = max_val;
-        item_ct1.barrier();
-
-        if (lane < sg_num) max_val = partialSum[lane];
-
-#ifndef __STOCHASTIC_MODE__
-        item_ct1.barrier();
-#endif
-
-        int iters = sg_num;
-        if (seq_length < iteration_stride)
-            iters = sg_num / (iteration_stride / max_threads_in_sequence);
-
-        for (int i = 1; i < iters; i *= 2) {
-            auto temp = sg.shuffle_xor(max_val, i);
-            max_val = (temp > max_val ? temp : max_val);
-        }
-
-        max_val = sg.shuffle(max_val, item_ct1.get_local_id(2) / tbSize);
-    }
-
-    float sum = 0;
-    for (int i = 0; i < iterations; i++) {
-        data[i].x() = sycl::exp(data[i].x() - max_val);
-        data[i].y() = sycl::exp(data[i].y() - max_val);
-        data[i].z() = sycl::exp(data[i].z() - max_val);
-        data[i].w() = sycl::exp(data[i].w() - max_val);
-
-        sum += (data[i].x() + data[i].y() + data[i].z() + data[i].w());
-    }
-
-    for (int i = 1; i < tbSize; i *= 2) { sum += sg.shuffle_xor(sum, i); }
-
-    if (seq_length > tbSize) {
-        if (lane == 0) partialSum[wid] = sum;
-        item_ct1.barrier();
-
-        if (lane < sg_num) sum = partialSum[lane];
-
-#ifndef __STOCHASTIC_MODE__
-        item_ct1.barrier();
-#endif
-
-        int iters = sg_num;
-        if (seq_length < iteration_stride)
-            iters = sg_num / (iteration_stride / max_threads_in_sequence);
-
-        for (int i = 1; i < iters; i *= 2) { sum += sg.shuffle_xor(sum, i); }
-
-        sum = sg.shuffle(sum, item_ct1.get_local_id(2) / tbSize);
-    }
-
-    sum += 1e-6;
-
-    for (int i = 0; i < iterations; i++) {
-        data[i].x() /= sum;
-        data[i].y() /= sum;
-        data[i].z() /= sum;
-        data[i].w() /= sum;
-
-        int data_id = i * iteration_stride + seq_lane;
-        if (data_id < seq_length) val_cast[data_offset + data_id] = data[i];
-    }
-}
-
-template <int tbSize, int blockStride, int tbSeq>
-void attn_softmax(bf16* vals,
-                  const bf16* attn_mask,
-                  int heads,
-                  int seq_length,
-                  int iterations,
-                  nd_item<3> item_ct1,
-                  float* partialSum)
-{
-    int sg_num = item_ct1.get_local_range().get(2) >> 5;
-
-    int iteration_stride = item_ct1.get_local_range().get(2);
-    int block_width = blockStride * seq_length;
-
-    // auto b = item_ct1.get_group();
-    // cg::thread_block_tile<tbSize> g = cg::tiled_partition<tbSize>(b);
-    sub_group sg = item_ct1.get_sub_group();
-
-    int batch = item_ct1.get_group(2);
-    int row = item_ct1.get_group(1);
-    int max_threads_in_sequence = std::max(seq_length, tbSeq);
-    int seq_lane = item_ct1.get_local_id(2) % max_threads_in_sequence;
-
-    int data_offset = batch * (item_ct1.get_group_range(1) * block_width) + row * block_width +
-                      (item_ct1.get_local_id(2) / max_threads_in_sequence) * seq_length;
-    int mask_offset = batch * seq_length;
-
-    int wid = item_ct1.get_local_id(2) >> 5;
-    int lane = item_ct1.get_local_id(2) & 0x1f;
-
-    ushort4* val_cast = reinterpret_cast<ushort4*>(vals);
-    const ushort4* attn_mask_cast = reinterpret_cast<const ushort4*>(attn_mask);
-
-    float4 data[MAX_THREAD_ITERATIONS];
-
-    float max_val = minus_infinity;
-
-    for (int i = 0; i < iterations; i++) {
-        int data_id = i * iteration_stride + seq_lane;
-        if (data_id < seq_length) {
-            ushort4 mask_ushort = attn_mask_cast[mask_offset + data_id];
-            ushort4 val_ushort = val_cast[data_offset + data_id];
-            float4 mask = {float(mask_ushort.x()),
-                           float(mask_ushort.y()),
-                           float(mask_ushort.z()),
-                           float(mask_ushort.w())};
-            data[i] = {float(val_ushort.x()),
-                       float(val_ushort.y()),
-                       float(val_ushort.z()),
-                       float(val_ushort.w())};
-
-            data[i].x() += mask.x();
-            data[i].y() += mask.y();
-            data[i].z() += mask.z();
-            data[i].w() += mask.w();
-
-            max_val = (data[i].x() > max_val ? data[i].x() : max_val);
-            max_val = (data[i].y() > max_val ? data[i].y() : max_val);
-            max_val = (data[i].z() > max_val ? data[i].z() : max_val);
-            max_val = (data[i].w() > max_val ? data[i].w() : max_val);
-        } else {
-            data[i].x() = minus_infinity;
-            data[i].y() = minus_infinity;
-            data[i].z() = minus_infinity;
-            data[i].w() = minus_infinity;
-        }
-    }
-
-    for (int i = 1; i < tbSize; i *= 2) {
-        auto temp = sg.shuffle_xor(max_val, i);
-        max_val = (temp > max_val ? temp : max_val);
-    }
-
-    if (seq_length > tbSize) {
-        if (lane == 0) partialSum[wid] = max_val;
-        item_ct1.barrier();
-
-        if (lane < sg_num) max_val = partialSum[lane];
-
-#ifndef __STOCHASTIC_MODE__
-        item_ct1.barrier();
-#endif
-
-        int iters = sg_num;
-        if (seq_length < iteration_stride)
-            iters = sg_num / (iteration_stride / max_threads_in_sequence);
-
-        for (int i = 1; i < iters; i *= 2) {
-            auto temp = sg.shuffle_xor(max_val, i);
-            max_val = (temp > max_val ? temp : max_val);
-        }
-
-        max_val = sg.shuffle(max_val, item_ct1.get_local_id(2) / tbSize);
-    }
-
-    float sum = 0;
-    for (int i = 0; i < iterations; i++) {
-        data[i].x() = sycl::exp(data[i].x() - max_val);
-        data[i].y() = sycl::exp(data[i].y() - max_val);
-        data[i].z() = sycl::exp(data[i].z() - max_val);
-        data[i].w() = sycl::exp(data[i].w() - max_val);
-
-        sum += (data[i].x() + data[i].y() + data[i].z() + data[i].w());
-    }
-
-    for (int i = 1; i < tbSize; i *= 2) { sum += sg.shuffle_xor(sum, i); }
-
-    if (seq_length > tbSize) {
-        if (lane == 0) partialSum[wid] = sum;
-        item_ct1.barrier();
-
-        if (lane < sg_num) sum = partialSum[lane];
-
-#ifndef __STOCHASTIC_MODE__
-        item_ct1.barrier();
-#endif
-
-        int iters = sg_num;
-        if (seq_length < iteration_stride)
-            iters = sg_num / (iteration_stride / max_threads_in_sequence);
-
-        for (int i = 1; i < iters; i *= 2) { sum += sg.shuffle_xor(sum, i); }
-
-        sum = sg.shuffle(sum, item_ct1.get_local_id(2) / tbSize);
-    }
-
-    sum += 1e-6;
-
-    for (int i = 0; i < iterations; i++) {
-        data[i].x() /= sum;
-        data[i].y() /= sum;
-        data[i].z() /= sum;
-        data[i].w() /= sum;
-
-        int data_id = i * iteration_stride + seq_lane;
-        if (data_id < seq_length) {
-            ushort4 data_ushort = {bf16(data[i].x()),
-                                   bf16(data[i].y()),
-                                   bf16(data[i].z()),
-                                   bf16(data[i].w())};
-            val_cast[data_offset + data_id] = data_ushort;
-        }
-    }
-}
-
-template <int tbSize, int blockStride, int tbSeq>
-void attn_softmax(half* vals,
-                  const half* attn_mask,
-                  int heads,
-                  int seq_length,
-                  int iterations,
-                  nd_item<3> item_ct1,
-                  float* partialSum)
-{
-    int sg_num = item_ct1.get_local_range(2) >> 5;
-
-    int iteration_stride = item_ct1.get_local_range(2);
-    int block_width = blockStride * seq_length;
-
-    // cg::thread_block b = cg::this_thread_block();
-    // cg::thread_block_tile<tbSize> g = cg::tiled_partition<tbSize>(b);
-    sub_group sg = item_ct1.get_sub_group();
-
-    int batch = item_ct1.get_group(2);
-    int row = item_ct1.get_group(1);
-    int max_threads_in_sequence = std::max(seq_length, tbSeq);
-    int seq_lane = item_ct1.get_local_id(2) % max_threads_in_sequence;
-
-    int data_offset = batch * (item_ct1.get_group_range(1) * block_width) + row * block_width +
-                      (item_ct1.get_local_id(2) / max_threads_in_sequence) * seq_length;
-    int mask_offset = batch * seq_length;
-
-    int wid = item_ct1.get_local_id(2) >> 5;
-    int lane = item_ct1.get_local_id(2) & 0x1f;
-
-    float2* val_cast = reinterpret_cast<float2*>(vals);
-    const float2* attn_mask_cast = reinterpret_cast<const float2*>(attn_mask);
-
-    val_cast += data_offset;
-    attn_mask_cast += mask_offset;
-
-    float2 low_data[MAX_THREAD_ITERATIONS];
-    float2 high_data[MAX_THREAD_ITERATIONS];
-
-    float max_val = minus_infinity;
-
-    for (int i = 0; i < iterations; i++) {
-        int data_id = i * iteration_stride + seq_lane;
-        if (data_id < seq_length) {
-            float2 data = val_cast[data_id];
-            float2 mask = attn_mask_cast[data_id];
-
-            half2* data_arr = reinterpret_cast<half2*>(&data);
-            half2* mask_arr = reinterpret_cast<half2*>(&mask);
-
-            low_data[i] = data_arr[0].convert<float>();
-            high_data[i] = data_arr[1].convert<float>();
-            float2 low_mask = mask_arr[0].convert<float>();
-            float2 high_mask = mask_arr[1].convert<float>();
-
-            low_data[i].x() += low_mask.x();
-            low_data[i].y() += low_mask.y();
-            high_data[i].x() += high_mask.x();
-            high_data[i].y() += high_mask.y();
-
-            max_val = (low_data[i].x() > max_val ? low_data[i].x() : max_val);
-            max_val = (low_data[i].y() > max_val ? low_data[i].y() : max_val);
-            max_val = (high_data[i].x() > max_val ? high_data[i].x() : max_val);
-            max_val = (high_data[i].y() > max_val ? high_data[i].y() : max_val);
-        }
-    }
-
-    for (int i = 1; i < tbSize; i *= 2) {
-        auto temp = sg.shuffle_xor(max_val, i);
-        max_val = (temp > max_val ? temp : max_val);
-    }
-
-    if (seq_length > tbSize) {
-        if (lane == 0) partialSum[wid] = max_val;
-        item_ct1.barrier();
-
-        if (lane < sg_num) max_val = partialSum[lane];
-
-#ifndef __STOCHASTIC_MODE__
-        item_ct1.barrier();
-#endif
-
-        int iters = sg_num;
-        if (seq_length < iteration_stride)
-            iters = sg_num / (iteration_stride / max_threads_in_sequence);
-
-        for (int i = 1; i < iters; i *= 2) {
-            auto temp = sg.shuffle_xor(max_val, i);
-            max_val = (temp > max_val ? temp : max_val);
-        }
-
-        max_val = sg.shuffle(max_val, item_ct1.get_local_id(2) / tbSize);
-    }
-
-    float sum = 0;
-    for (int i = 0; i < iterations; i++) {
-        int data_id = i * iteration_stride + seq_lane;
-        if (data_id < seq_length) {
-            low_data[i] = sycl::exp(low_data[i] - max_val);
-            high_data[i] = sycl::exp(high_data[i] - max_val);
-            // low_data[i].x() = sycl::exp(low_data[i].x() - max_val);
-            // low_data[i].y() = sycl::exp(low_data[i].y() - max_val);
-            // high_data[i].x() = sycl::exp(high_data[i].x() - max_val);
-            // high_data[i].y() = sycl::exp(high_data[i].y() - max_val);
-
-            sum += (low_data[i].x() + low_data[i].y() + high_data[i].x() + high_data[i].y());
-        }
-    }
-
-    for (int i = 1; i < tbSize; i *= 2) { sum += sg.shuffle_xor(sum, i); }
-
-    if (seq_length > tbSize) {
-        if (lane == 0) partialSum[wid] = sum;
-        item_ct1.barrier();
-
-        if (lane < sg_num) sum = partialSum[lane];
-
-#ifndef __STOCHASTIC_MODE__
-        item_ct1.barrier();
-#endif
-
-        int iters = sg_num;
-        if (seq_length < iteration_stride)
-            iters = sg_num / (iteration_stride / max_threads_in_sequence);
-
-        for (int i = 1; i < iters; i *= 2) { sum += sg.shuffle_xor(sum, i); }
-
-        sum = sg.shuffle(sum, item_ct1.get_local_id(2) / tbSize);
-    }
-
-    sum += 1e-6;
-
-    for (int i = 0; i < iterations; i++) {
-        int data_id = i * iteration_stride + seq_lane;
-        if (data_id < seq_length) {
-            float2 result_f;
-            half2* result_h = reinterpret_cast<half2*>(&result_f);
-
-            low_data[i].x() /= sum;
-            low_data[i].y() /= sum;
-            high_data[i].x() /= sum;
-            high_data[i].y() /= sum;
-
-            result_h[0] = low_data[i].convert<half, rounding_mode::rtn>();
-            result_h[1] = high_data[i].convert<half, rounding_mode::rtn>();
-
-            val_cast[data_id] = result_f;
-        }
-    }
-}
-
-template <typename T>
-void launch_attn_softmax(T* vals,
-                         const T* attn_mask,
-                         int batch_size,
-                         int heads,
-                         int sequence_length,
-                         queue* stream)
-{
-    const int threads = 128;
-    int seq_length4 = sequence_length / 4;
-
-    int block_compute_size =
-        (seq_length4 < threads ? (int)pow(2.0, floor(log2((float)(threads / seq_length4)))) : 1);
-    range<3> grid_dim(1, heads * sequence_length / block_compute_size, batch_size);
-
-    int subblock_max_workload = MAX_THREAD_ITERATIONS * 4 * threads;
-
-    range<3> block_dim(1,
-                       1,
-                       seq_length4 > threads ? ((sequence_length + subblock_max_workload - 1) /
-                                                subblock_max_workload * threads)
-                                             : threads);
-    int iterations =
-        (sequence_length < subblock_max_workload ? (seq_length4 + threads - 1) / threads
-                                                 : MAX_THREAD_ITERATIONS);
-
-    if (sequence_length <= 8)
-        stream->submit([&](handler& cgh) {
-            accessor<float, 1, access::mode::read_write, access::target::local> data_block_acc_ct1(
-                range<1>(MAX_SG_NUM), cgh);
-            cgh.parallel_for(nd_range<3>(grid_dim * block_dim, block_dim),
-                             [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                                 attn_softmax<2, (threads / 2), 2>(
-                                     vals,
-                                     attn_mask,
-                                     heads,
-                                     seq_length4,
-                                     iterations,
-                                     item_ct1,
-                                     data_block_acc_ct1.get_pointer());
-                             });
-        });
-    else if (sequence_length <= 16)
-        stream->submit([&](handler& cgh) {
-            accessor<float, 1, access::mode::read_write, access::target::local> data_block_acc_ct1(
-                range<1>(MAX_SG_NUM), cgh);
-            cgh.parallel_for(nd_range<3>(grid_dim * block_dim, block_dim),
-                             [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                                 attn_softmax<4, (threads / 4), 4>(
-                                     vals,
-                                     attn_mask,
-                                     heads,
-                                     seq_length4,
-                                     iterations,
-                                     item_ct1,
-                                     data_block_acc_ct1.get_pointer());
-                             });
-        });
-    else if (sequence_length <= 32)
-        stream->submit([&](handler& cgh) {
-            accessor<float, 1, access::mode::read_write, access::target::local> data_block_acc_ct1(
-                range<1>(MAX_SG_NUM), cgh);
-            cgh.parallel_for(nd_range<3>(grid_dim * block_dim, block_dim),
-                             [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                                 attn_softmax<8, (threads / 8), 8>(
-                                     vals,
-                                     attn_mask,
-                                     heads,
-                                     seq_length4,
-                                     iterations,
-                                     item_ct1,
-                                     data_block_acc_ct1.get_pointer());
-                             });
-        });
-    else if (sequence_length <= 64)
-        stream->submit([&](handler& cgh) {
-            accessor<float, 1, access::mode::read_write, access::target::local> data_block_acc_ct1(
-                range<1>(MAX_SG_NUM), cgh);
-            cgh.parallel_for(nd_range<3>(grid_dim * block_dim, block_dim),
-                             [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                                 attn_softmax<16, (threads / 16), 16>(
-                                     vals,
-                                     attn_mask,
-                                     heads,
-                                     seq_length4,
-                                     iterations,
-                                     item_ct1,
-                                     data_block_acc_ct1.get_pointer());
-                             });
-        });
-    else if (sequence_length <= 128)
-        stream->submit([&](handler& cgh) {
-            accessor<float, 1, access::mode::read_write, access::target::local> data_block_acc_ct1(
-                range<1>(MAX_SG_NUM), cgh);
-            cgh.parallel_for(nd_range<3>(grid_dim * block_dim, block_dim),
-                             [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                                 attn_softmax<32, (threads / 32), 32>(
-                                     vals,
-                                     attn_mask,
-                                     heads,
-                                     seq_length4,
-                                     iterations,
-                                     item_ct1,
-                                     data_block_acc_ct1.get_pointer());
-                             });
-        });
-    else if (sequence_length <= 256)
-        stream->submit([&](handler& cgh) {
-            accessor<float, 1, access::mode::read_write, access::target::local> data_block_acc_ct1(
-                range<1>(MAX_SG_NUM), cgh);
-            cgh.parallel_for(nd_range<3>(grid_dim * block_dim, block_dim),
-                             [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                                 attn_softmax<32, (threads / 64), 64>(
-                                     vals,
-                                     attn_mask,
-                                     heads,
-                                     seq_length4,
-                                     iterations,
-                                     item_ct1,
-                                     data_block_acc_ct1.get_pointer());
-                             });
-        });
-    else {
-        const int threads = 256;
-        block_compute_size =
-            (seq_length4 < threads ? (int)pow(2.0, floor(log2((float)(threads / seq_length4))))
-                                   : 1);
-        range<3> grid_dim(1, heads * sequence_length / block_compute_size, batch_size);
-
-        int subblock_max_workload = MAX_THREAD_ITERATIONS * 4 * threads;
-
-        range<3> block_dim(1,
-                           1,
-                           seq_length4 > threads ? ((sequence_length + subblock_max_workload - 1) /
-                                                    subblock_max_workload * threads)
-                                                 : threads);
-        iterations =
-            (sequence_length < subblock_max_workload ? (seq_length4 + threads - 1) / threads
-                                                     : MAX_THREAD_ITERATIONS);
-        if (sequence_length <= 512) {
-            stream->submit([&](handler& cgh) {
-                accessor<float, 1, access::mode::read_write, access::target::local>
-                    data_block_acc_ct1(range<1>(MAX_SG_NUM), cgh);
-                cgh.parallel_for(
-                    nd_range<3>(grid_dim * block_dim, block_dim),
-                    [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                        attn_softmax<32, (threads / 128), 128>(vals,
-                                                               attn_mask,
-                                                               heads,
-                                                               seq_length4,
-                                                               iterations,
-                                                               item_ct1,
-                                                               data_block_acc_ct1.get_pointer());
-                    });
-            });
-        } else if (sequence_length < (MAX_THREADS * MAX_THREAD_ITERATIONS * 4))
-            stream->submit([&](handler& cgh) {
-                accessor<float, 1, access::mode::read_write, access::target::local>
-                    data_block_acc_ct1(range<1>(MAX_SG_NUM), cgh);
-                cgh.parallel_for(nd_range<3>(grid_dim * block_dim, block_dim),
-                                 [=](nd_item<3> item_ct1)
-                                     [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                                         attn_softmax<32, 1, 128>(vals,
-                                                                  attn_mask,
-                                                                  heads,
-                                                                  seq_length4,
-                                                                  iterations,
-                                                                  item_ct1,
-                                                                  data_block_acc_ct1.get_pointer());
-                                     });
-            });
-        else
-            throw std::runtime_error(
-                "Unsupport Seq_Length! Check the restriction of the max_threads and "
-                "max_thread_iterations!");
-    }
-}
-
-template void launch_attn_softmax<float>(float* vals,
-                                         const float* attn_mask,
-                                         int batch_size,
-                                         int heads,
-                                         int sequence_length,
-                                         queue* stream);
-
-template void launch_attn_softmax<bf16>(bf16* vals,
-                                        const bf16* attn_mask,
-                                        int batch_size,
-                                        int heads,
-                                        int sequence_length,
-                                        queue* stream);
-
-template void launch_attn_softmax<half>(half* vals,
-                                        const half* attn_mask,
-                                        int batch_size,
-                                        int heads,
-                                        int sequence_length,
-                                        queue* stream);
-
-template <typename T, int tbSize, int blockStride>
-void softmax_backward_kernel(T* out_grad,
-                             const T* soft_inp,
-                             int seq_length,
-                             nd_item<3> item_ct1,
-                             float* partialSum)
-{
-    int sg_num = item_ct1.get_local_range().get(2) >> 5;  // sg-count = num_threads / SG_SIZE (32)
-
-    int iteration_stride = item_ct1.get_local_range().get(2);
-    int block_width = blockStride * seq_length;
-
-    int iterations = (seq_length < (MAX_THREAD_ITERATIONS * iteration_stride)
-                          ? (seq_length + iteration_stride - 1) / iteration_stride
-                          : MAX_THREAD_ITERATIONS);
-
-    // auto b = item_ct1.get_group();
-    // cg::thread_block_tile<tbSize> g = cg::tiled_partition<tbSize>(b);
-    sub_group sg = item_ct1.get_sub_group();
-
-    int row = item_ct1.get_group(2);
-    int id = item_ct1.get_local_id(2);
-
-    int wid = id >> 5;
-    int lane = id & 0x1f;
-
-    T val_reg[MAX_THREAD_ITERATIONS];
-    T soft_reg[MAX_THREAD_ITERATIONS];
-    float grad_reg = 0.0f;
-
-#pragma unroll
-    for (int i = 0; i < iterations; i++) {
-        int data_id = i * iteration_stride + id;
-        if (data_id < block_width) {
-            val_reg[i] = out_grad[row * block_width + data_id];
-            soft_reg[i] = soft_inp[row * block_width + data_id];
-
-            grad_reg += ((float)val_reg[i] *
-                         (float)soft_reg[i]);  // if done in half, the multiplication, we may
-                                               // lose 2% of accuracy in computation!!
-        }
-    }
-    for (int i = 1; i < tbSize; i *= 2) grad_reg += sg.shuffle_xor(grad_reg, i);
-
-    if (seq_length > tbSize) {
-        if (lane == 0) partialSum[wid] = grad_reg;
-        item_ct1.barrier();
-
-        if (lane < sg_num) grad_reg = partialSum[lane];
-
-        int iters = sg_num;
-        if (seq_length < iteration_stride) iters = sg_num / (iteration_stride / seq_length);
-
-        for (int i = 1; i < iters; i *= 2) grad_reg += sg.shuffle_xor(grad_reg, i);
-
-        grad_reg = sg.shuffle(grad_reg, id / tbSize);
-    }
-
-    for (int i = 0; i < iterations; i++) {
-        int data_id = i * iteration_stride + id;
-        if (data_id < block_width) {
-            float temp = (float)soft_reg[i] * ((float)val_reg[i] - grad_reg);
-            out_grad[row * block_width + data_id] = (T)temp;
-        }
-    }
-}
-
-template <typename T, int ITERATIONS>
-void softmax_backward_kernel_v2(T* grad /* input & output*/,
-                                const T* output,
-                                int softmax_length,
-                                nd_item<3> item_ct1)
-{
-    int batch_idx =
-        item_ct1.get_group(2) * item_ct1.get_local_range().get(1) + item_ct1.get_local_id(1);
-    int offset = batch_idx * softmax_length + item_ct1.get_local_id(2);
-
-    grad += offset;
-    output += offset;
-
-    float sum = 0.0;
-    if constexpr (std::is_same_v<T, bf16>) {
-        float grad_reg[ITERATIONS];
-        float output_reg[ITERATIONS];
-        ushort* grad_cast = (ushort*)grad;
-        const ushort* output_cast = (const ushort*)output;
-        for (int i = 0; i < ITERATIONS; ++i) {
-            int curr_idx = item_ct1.get_local_id(2) + i * MAX_SG_NUM;
-            if (curr_idx < softmax_length) {
-                grad_reg[i] = float(grad_cast[i * MAX_SG_NUM]);
-                output_reg[i] = float(output_cast[i * MAX_SG_NUM]);
-                sum += grad_reg[i] * output_reg[i];
-            }
-        }
-
-        sub_group sg = item_ct1.get_sub_group();
-
-        for (int i = 1; i < MAX_SG_NUM; i <<= 1) sum += sg.shuffle_xor(sum, i);
-
-#pragma unroll
-        for (int i = 0; i < ITERATIONS; ++i) {
-            int curr_idx = item_ct1.get_local_id(2) + i * MAX_SG_NUM;
-            if (curr_idx < softmax_length) {
-                grad_cast[i * MAX_SG_NUM] = bf16(output_reg[i] * (grad_reg[i] - sum));
-            }
-        }
-    } else {
-        T grad_reg[ITERATIONS];
-        T output_reg[ITERATIONS];
-
-#pragma unroll
-        for (int i = 0; i < ITERATIONS; ++i) {
-            int curr_idx = item_ct1.get_local_id(2) + i * MAX_SG_NUM;
-            if (curr_idx < softmax_length) {
-                grad_reg[i] = grad[i * MAX_SG_NUM];
-                output_reg[i] = output[i * MAX_SG_NUM];
-                sum += (float)grad_reg[i] * (float)output_reg[i];
-            }
-        }
-        sub_group sg = item_ct1.get_sub_group();
-
-        for (int i = 1; i < MAX_SG_NUM; i <<= 1) sum += sg.shuffle_xor(sum, i);
-
-#pragma unroll
-        for (int i = 0; i < ITERATIONS; ++i) {
-            int curr_idx = item_ct1.get_local_id(2) + i * MAX_SG_NUM;
-            if (curr_idx < softmax_length)
-                grad[i * MAX_SG_NUM] = (float)output_reg[i] * ((float)grad_reg[i] - sum);
-        }
-    }
-}
-
-template <typename T>
-void launch_attn_softmax_backward_v2(T* out_grad,
-                                     const T* soft_inp,
-                                     int batch_size,
-                                     int heads,
-                                     int seq_length,
-                                     queue* stream)
-{
-    const int sgs_per_block = 4;
-    range<3> grid_dim(1, 1, batch_size * heads * seq_length / sgs_per_block);
-    range<3> block_dim(1, sgs_per_block, MAX_SG_NUM);
-
-    if (seq_length <= 32)
-        stream->parallel_for(nd_range<3>(grid_dim * block_dim, block_dim),
-                             [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                                 softmax_backward_kernel_v2<T, 1>(
-                                     out_grad, soft_inp, seq_length, item_ct1);
-                             });
-    else if (seq_length <= 64)
-        stream->parallel_for(nd_range<3>(grid_dim * block_dim, block_dim),
-                             [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                                 softmax_backward_kernel_v2<T, 2>(
-                                     out_grad, soft_inp, seq_length, item_ct1);
-                             });
-    else if (seq_length <= 128)
-        stream->parallel_for(nd_range<3>(grid_dim * block_dim, block_dim),
-                             [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                                 softmax_backward_kernel_v2<T, 4>(
-                                     out_grad, soft_inp, seq_length, item_ct1);
-                             });
-    else if (seq_length <= 256)
-        stream->parallel_for(nd_range<3>(grid_dim * block_dim, block_dim),
-                             [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                                 softmax_backward_kernel_v2<T, 8>(
-                                     out_grad, soft_inp, seq_length, item_ct1);
-                             });
-    else if (seq_length <= 384)
-        stream->parallel_for(nd_range<3>(grid_dim * block_dim, block_dim),
-                             [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                                 softmax_backward_kernel_v2<T, 12>(
-                                     out_grad, soft_inp, seq_length, item_ct1);
-                             });
-    else if (seq_length <= 512)
-        stream->parallel_for(nd_range<3>(grid_dim * block_dim, block_dim),
-                             [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                                 softmax_backward_kernel_v2<T, 16>(
-                                     out_grad, soft_inp, seq_length, item_ct1);
-                             });
-    else if (seq_length <= 768)
-        stream->parallel_for(nd_range<3>(grid_dim * block_dim, block_dim),
-                             [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                                 softmax_backward_kernel_v2<T, 24>(
-                                     out_grad, soft_inp, seq_length, item_ct1);
-                             });
-    else if (seq_length <= 1024)
-        stream->parallel_for(nd_range<3>(grid_dim * block_dim, block_dim),
-                             [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                                 softmax_backward_kernel_v2<T, 32>(
-                                     out_grad, soft_inp, seq_length, item_ct1);
-                             });
-    else if (seq_length <= 2048)
-        stream->parallel_for(nd_range<3>(grid_dim * block_dim, block_dim),
-                             [=](nd_item<3> item_ct1) [[intel::reqd_sub_group_size(MAX_SG_NUM)]] {
-                                 softmax_backward_kernel_v2<T, 64>(
-                                     out_grad, soft_inp, seq_length, item_ct1);
-                             });
-    else
-        throw std::runtime_error(
-            std::string("Special sequence length found in softmax backward, seq_length: ") +
-            std::to_string(seq_length));
-}
-
-template void launch_attn_softmax_backward_v2<float>(float* out_grad,
-                                                     const float* soft_inp,
-                                                     int batch_size,
-                                                     int heads,
-                                                     int seq_length,
-                                                     queue* stream);
-template void launch_attn_softmax_backward_v2<bf16>(bf16* out_grad,
-                                                    const bf16* soft_inp,
-                                                    int batch_size,
-                                                    int heads,
-                                                    int seq_length,
-                                                    queue* stream);
-template void launch_attn_softmax_backward_v2<half>(half* out_grad,
-                                                    const half* soft_inp,
-                                                    int batch_size,
-                                                    int heads,
-                                                    int seq_length,
-                                                    queue* stream);
diff --git a/intel_extension_for_deepspeed/op_builder/csrc/transformer/transform_kernels.dp.cpp b/intel_extension_for_deepspeed/op_builder/csrc/transformer/transform_kernels.dp.cpp
deleted file mode 100644
index 209fc68..0000000
--- a/intel_extension_for_deepspeed/op_builder/csrc/transformer/transform_kernels.dp.cpp
+++ /dev/null
@@ -1,897 +0,0 @@
-#if __has_include(<sycl/sycl.hpp>)
-#include <sycl/sycl.hpp>
-#elif __has_include(<CL/sycl.hpp>)
-#include <CL/sycl.hpp>
-#else
-#error "Unsupported compiler"
-#endif
-#include "custom_sycl_layers.hpp"
-
-#define rows_trans 16
-#define cols_trans 16
-
-template <typename T>
-void Transpose_Kernel(const T* inp,
-                      T* out,
-                      int row_width,
-                      int col_width,
-                      sycl::nd_item<3> item_ct1,
-                      T* data_block)
-{
-    int r = item_ct1.get_local_id(2) / cols_trans;
-    int c = item_ct1.get_local_id(2) % cols_trans;
-
-    int m = row_width / cols_trans;
-
-    int i = item_ct1.get_group(2) / m * rows_trans + r;
-    int j = item_ct1.get_group(2) % m * cols_trans + c;
-
-    int row_stride = rows_trans / ((rows_trans * cols_trans + THREADS - 1) / THREADS);
-
-    for (int k = 0; k < rows_trans; k += row_stride)
-        data_block[(k + r) * cols_trans + c] = inp[(i + k) * row_width + j];
-
-    item_ct1.barrier();
-
-    i = item_ct1.get_group(2) % m * rows_trans + r;
-    j = item_ct1.get_group(2) / m * cols_trans + c;
-
-    for (int k = 0; k < rows_trans; k += row_stride)
-        out[(i + k) * col_width + j] = data_block[c * cols_trans + r + k];
-}
-
-template <>
-void Transpose<sycl::half>(const sycl::half* inp_mat,
-                           sycl::half* out_mat,
-                           int rows,
-                           int cols,
-                           sycl::queue* stream)
-{
-    int threads = THREADS;
-
-    sycl::range<3> grid_dim(1, 1, (rows * cols + threads - 1) / threads);
-    sycl::range<3> block_dim(1, 1, threads);
-    stream->submit([&](sycl::handler& cgh) {
-        sycl::accessor<sycl::half, 1, sycl::access::mode::read_write, sycl::access::target::local>
-            data_block_acc_ct1(sycl::range<1>(rows_trans * (cols_trans + 1)), cgh);
-        cgh.parallel_for(sycl::nd_range<3>(grid_dim, block_dim), [=](sycl::nd_item<3> item_ct1) {
-            Transpose_Kernel<sycl::half>(
-                inp_mat, out_mat, cols, rows, item_ct1, data_block_acc_ct1.get_pointer());
-        });
-    });
-}
-
-template <>
-void Transpose<float>(const float* inp_mat, float* out_mat, int rows, int cols, sycl::queue* stream)
-{
-    int threads = THREADS;
-    sycl::range<3> grid_dim(1, 1, (rows * cols + threads - 1) / threads);
-    sycl::range<3> block_dim(1, 1, threads);
-
-    stream->submit([&](sycl::handler& cgh) {
-        sycl::accessor<float, 1, sycl::access::mode::read_write, sycl::access::target::local>
-            data_block_acc_ct1(sycl::range<1>(rows_trans * (cols_trans + 1)), cgh);
-        cgh.parallel_for(
-            sycl::nd_range<3>(grid_dim * block_dim, block_dim), [=](sycl::nd_item<3> item_ct1) {
-                Transpose_Kernel<float>(
-                    inp_mat, out_mat, cols, rows, item_ct1, data_block_acc_ct1.get_pointer());
-            });
-    });
-}
-
-template <typename T>
-void transform_0213(T* output,
-                    const T* vals,
-                    int hidden_dim,
-                    int seq_length,
-                    int heads,
-                    int head_ext,
-                    sycl::nd_item<3> item_ct1);
-
-template <>
-void transform_0213<float>(float* output,
-                           const float* vals,
-                           int hidden_dim,
-                           int seq_length,
-                           int heads,
-                           int head_ext,
-                           sycl::nd_item<3> item_ct1)
-{
-    int d0_stride = hidden_dim * seq_length;
-    int d1_stride = hidden_dim;
-    int d2_stride = hidden_dim / heads;
-
-    int d0_out_stride = d0_stride;
-    int d1_out_stride = d2_stride;
-    int d2_out_stride = d2_stride * seq_length;
-
-    int d0 = item_ct1.get_group(2);             // Batch
-    int d1 = item_ct1.get_group(1) / head_ext;  // Sequence ID (0-127)
-    int d2 = item_ct1.get_local_id(1) +
-             (item_ct1.get_group(1) % head_ext) * (heads / head_ext);  // Head (0-11)
-    int d3 = item_ct1.get_local_id(2);                                 // Values (groups of 4)
-
-    const sycl::float4* vals_vec = reinterpret_cast<const sycl::float4*>(vals);
-    sycl::float4* output_vec = reinterpret_cast<sycl::float4*>(output);
-
-    sycl::float4 inputs = vals_vec[d0 * d0_stride + d1 * d1_stride + d2 * d2_stride + d3];
-    output_vec[d0 * d0_out_stride + d1 * d1_out_stride + d2 * d2_out_stride + d3] = inputs;
-}
-
-template <>
-void transform_0213<bf16>(bf16* output,
-                          const bf16* vals,
-                          int hidden_dim,
-                          int seq_length,
-                          int heads,
-                          int head_ext,
-                          sycl::nd_item<3> item_ct1)
-{
-    int d0_stride = hidden_dim * seq_length;
-    int d1_stride = hidden_dim;
-    int d2_stride = hidden_dim / heads;
-
-    int d0_out_stride = d0_stride;
-    int d1_out_stride = d2_stride;
-    int d2_out_stride = d2_stride * seq_length;
-
-    int d0 = item_ct1.get_group(2);             // Batch
-    int d1 = item_ct1.get_group(1) / head_ext;  // Sequence ID (0-127)
-    int d2 = item_ct1.get_local_id(1) +
-             (item_ct1.get_group(1) % head_ext) * (heads / head_ext);  // Head (0-11)
-    int d3 = item_ct1.get_local_id(2);                                 // Values (groups of 4)
-
-    const sycl::ushort4* vals_vec = reinterpret_cast<const sycl::ushort4*>(vals);
-    sycl::ushort4* output_vec = reinterpret_cast<sycl::ushort4*>(output);
-
-    sycl::ushort4 inputs_cast = vals_vec[d0 * d0_stride + d1 * d1_stride + d2 * d2_stride + d3];
-    float4 inputs = {float(inputs_cast.x()),
-                     float(inputs_cast.y()),
-                     float(inputs_cast.z()),
-                     float(inputs_cast.w())};
-
-    sycl::float4 outputs;
-    outputs.x() = inputs.x();
-    outputs.y() = inputs.y();
-    outputs.z() = inputs.z();
-    outputs.w() = inputs.w();
-
-    ushort4 outputs_cast = {bf16(outputs.x()),
-                            bf16(outputs.y()),
-                            bf16(outputs.z()),
-                            bf16(outputs.w())};
-
-    output_vec[d0 * d0_out_stride + d1 * d1_out_stride + d2 * d2_out_stride + d3] = outputs_cast;
-}
-
-template <>
-void transform_0213<sycl::half>(sycl::half* output,
-                                const sycl::half* vals,
-                                int hidden_dim,
-                                int seq_length,
-                                int heads,
-                                int head_ext,
-                                sycl::nd_item<3> item_ct1)
-{
-    int d0_stride = hidden_dim * seq_length;
-    int d1_stride = hidden_dim;
-    int d2_stride = hidden_dim / heads;
-
-    int d0_out_stride = d0_stride;
-    int d1_out_stride = d2_stride;
-    int d2_out_stride = d2_stride * seq_length;
-
-    int d0 = item_ct1.get_group(2);             // Batch
-    int d1 = item_ct1.get_group(1) / head_ext;  // Sequence ID (0-127)
-    int d2 = item_ct1.get_local_id(1) +
-             (item_ct1.get_group(1) % head_ext) * (heads / head_ext);  // Head (0-11)
-    int d3 = item_ct1.get_local_id(2);                                 // Values (groups of 4)
-
-    sycl::float4 vals_arr[1];
-
-    const sycl::float4* vals_vec = reinterpret_cast<const sycl::float4*>(vals);
-    sycl::float4* output_vec = reinterpret_cast<sycl::float4*>(output);
-
-    vals_arr[0] = vals_vec[d0 * d0_stride + d1 * d1_stride + d2 * d2_stride + d3];
-    output_vec[d0 * d0_out_stride + d1 * d1_out_stride + d2 * d2_out_stride + d3] = vals_arr[0];
-}
-
-template <>
-void launch_transform_0213<float>(float* output,
-                                  const float* vals,
-                                  int batch_size,
-                                  int seq_length,
-                                  int hidden_dim,
-                                  int heads,
-                                  sycl::queue* stream)
-{
-    hidden_dim >>= 2;
-    int head_ext = (hidden_dim - 1) / MAX_THREADS + 1;
-    sycl::range<3> block_dim(1, (heads / head_ext), hidden_dim / heads);
-    sycl::range<3> grid_dim(1, (seq_length * head_ext), batch_size);
-
-    stream->submit([&](sycl::handler& cgh) {
-        cgh.parallel_for(sycl::nd_range<3>(grid_dim * block_dim, block_dim),
-                         [=](sycl::nd_item<3> item_ct1) {
-                             transform_0213<float>(
-                                 output, vals, hidden_dim, seq_length, heads, head_ext, item_ct1);
-                         });
-    });
-}
-
-template <>
-void launch_transform_0213<bf16>(bf16* output,
-                                 const bf16* vals,
-                                 int batch_size,
-                                 int seq_length,
-                                 int hidden_dim,
-                                 int heads,
-                                 sycl::queue* stream)
-{
-    hidden_dim >>= 2;
-    int head_ext = (hidden_dim - 1) / MAX_THREADS + 1;
-    sycl::range<3> block_dim(1, (heads / head_ext), hidden_dim / heads);
-    sycl::range<3> grid_dim(1, (seq_length * head_ext), batch_size);
-
-    stream->submit([&](sycl::handler& cgh) {
-        cgh.parallel_for(sycl::nd_range<3>(grid_dim * block_dim, block_dim),
-                         [=](sycl::nd_item<3> item_ct1) {
-                             transform_0213<bf16>(
-                                 output, vals, hidden_dim, seq_length, heads, head_ext, item_ct1);
-                         });
-    });
-}
-
-template <>
-void launch_transform_0213<sycl::half>(sycl::half* output,
-                                       const sycl::half* vals,
-                                       int batch_size,
-                                       int seq_length,
-                                       int hidden_dim,
-                                       int heads,
-                                       sycl::queue* stream)
-{
-    hidden_dim >>= 3;
-    int head_ext = (hidden_dim - 1) / MAX_THREADS + 1;
-    sycl::range<3> block_dim(1, (heads / head_ext), hidden_dim / heads);
-    sycl::range<3> grid_dim(1, (seq_length * head_ext), batch_size);
-
-    stream->submit([&](sycl::handler& cgh) {
-        cgh.parallel_for(sycl::nd_range<3>(grid_dim * block_dim, block_dim),
-                         [=](sycl::nd_item<3> item_ct1) {
-                             transform_0213<sycl::half>(
-                                 output, vals, hidden_dim, seq_length, heads, head_ext, item_ct1);
-                         });
-    });
-}
-
-// Bias add
-template <typename T>
-void bias_add_transform_0213(T* output,
-                             const T* vals,
-                             const T* bias,
-                             int hidden_dim,
-                             int seq_length,
-                             int heads,
-                             int head_ext,
-                             sycl::nd_item<3> item_ct1);
-
-template <>
-void bias_add_transform_0213<float>(float* output,
-                                    const float* vals,
-                                    const float* bias,
-                                    int hidden_dim,
-                                    int seq_length,
-                                    int heads,
-                                    int head_ext,
-                                    sycl::nd_item<3> item_ct1)
-{
-    int d0_stride = hidden_dim * seq_length;
-    int d1_stride = hidden_dim;
-    int d2_stride = hidden_dim / heads;
-
-    int d0_out_stride = d0_stride;
-    int d1_out_stride = d2_stride;
-    int d2_out_stride = d2_stride * seq_length;
-
-    int d0 = item_ct1.get_group(2);              // Batch
-    int d1 = item_ct1.get_group(1);              // Sequence ID (0-127)
-    int cnt = item_ct1.get_group(0) / head_ext;  // Hidden count
-    int d2 = item_ct1.get_local_id(1) +
-             (item_ct1.get_group(0) % head_ext) * (heads / head_ext);  // Head (0-11)
-    int d3 = item_ct1.get_local_id(2);                                 // Values (groups of 4)
-
-    const sycl::float4* vals_vec = reinterpret_cast<const sycl::float4*>(vals);
-    const sycl::float4* bias_vec = reinterpret_cast<const sycl::float4*>(bias);
-    sycl::float4* output_vec = reinterpret_cast<sycl::float4*>(output);
-
-    sycl::float4 inputs =
-        vals_vec[d0 * d0_stride * (item_ct1.get_group_range(0) / head_ext) + cnt * d1_stride +
-                 d1 * d1_stride * (item_ct1.get_group_range(0) / head_ext) + d2 * d2_stride + d3];
-    sycl::float4 biases = bias_vec[cnt * d1_stride + d2 * d2_stride + d3];
-
-    sycl::float4 outputs;
-    outputs.x() = inputs.x() + biases.x();
-    outputs.y() = inputs.y() + biases.y();
-    outputs.z() = inputs.z() + biases.z();
-    outputs.w() = inputs.w() + biases.w();
-
-    output_vec[cnt * d0_out_stride * item_ct1.get_group_range(2) + d0 * d0_out_stride +
-               d1 * d1_out_stride + d2 * d2_out_stride + d3] = outputs;
-}
-
-template <>
-void bias_add_transform_0213<bf16>(bf16* output,
-                                   const bf16* vals,
-                                   const bf16* bias,
-                                   int hidden_dim,
-                                   int seq_length,
-                                   int heads,
-                                   int head_ext,
-                                   sycl::nd_item<3> item_ct1)
-{
-    int d0_stride = hidden_dim * seq_length;
-    int d1_stride = hidden_dim;
-    int d2_stride = hidden_dim / heads;
-
-    int d0_out_stride = d0_stride;
-    int d1_out_stride = d2_stride;
-    int d2_out_stride = d2_stride * seq_length;
-
-    int d0 = item_ct1.get_group(2);              // Batch
-    int d1 = item_ct1.get_group(1);              // Sequence ID (0-127)
-    int cnt = item_ct1.get_group(0) / head_ext;  // Hidden count
-    int d2 = item_ct1.get_local_id(1) +
-             (item_ct1.get_group(0) % head_ext) * (heads / head_ext);  // Head (0-11)
-    int d3 = item_ct1.get_local_id(2);                                 // Values (groups of 4)
-
-    const sycl::ushort4* vals_vec = reinterpret_cast<const sycl::ushort4*>(vals);
-    const sycl::ushort4* bias_vec = reinterpret_cast<const sycl::ushort4*>(bias);
-    sycl::ushort4* output_vec = reinterpret_cast<sycl::ushort4*>(output);
-
-    sycl::ushort4 inputs_cast =
-        vals_vec[d0 * d0_stride * (item_ct1.get_group_range(0) / head_ext) + cnt * d1_stride +
-                 d1 * d1_stride * (item_ct1.get_group_range(0) / head_ext) + d2 * d2_stride + d3];
-    sycl::ushort4 biases_cast = bias_vec[cnt * d1_stride + d2 * d2_stride + d3];
-    float4 inputs = {float(inputs_cast.x()),
-                     float(inputs_cast.y()),
-                     float(inputs_cast.z()),
-                     float(inputs_cast.w())};
-
-    float4 biases = {float(biases_cast.x()),
-                     float(biases_cast.y()),
-                     float(biases_cast.z()),
-                     float(biases_cast.w())};
-
-    sycl::float4 outputs;
-    outputs.x() = inputs.x() + biases.x();
-    outputs.y() = inputs.y() + biases.y();
-    outputs.z() = inputs.z() + biases.z();
-    outputs.w() = inputs.w() + biases.w();
-
-    ushort4 outputs_cast = {bf16(outputs.x()),
-                            bf16(outputs.y()),
-                            bf16(outputs.z()),
-                            bf16(outputs.w())};
-    output_vec[cnt * d0_out_stride * item_ct1.get_group_range(2) + d0 * d0_out_stride +
-               d1 * d1_out_stride + d2 * d2_out_stride + d3] = outputs_cast;
-}
-
-#define ATTN_H 3
-#define MAX_SEQ_LINE 10
-
-template <>
-void bias_add_transform_0213<sycl::half>(sycl::half* output,
-                                         const sycl::half* vals,
-                                         const sycl::half* bias,
-                                         int hidden_dim,
-                                         int seq_length,
-                                         int heads,
-                                         int head_ext,
-                                         sycl::nd_item<3> item_ct1)
-{
-    int d0_stride = hidden_dim * seq_length;
-    int d1_stride = hidden_dim;
-    int d2_stride = hidden_dim / heads;
-
-    int d2_out_stride = d2_stride * seq_length;
-
-    int d0 = item_ct1.get_group(2);              // Batch
-    int d1 = item_ct1.get_group(1);              // Sequence ID (0-127)
-    int cnt = item_ct1.get_group(0) / head_ext;  // Hidden count
-    int d2 = item_ct1.get_local_id(1) +
-             (item_ct1.get_group(0) % head_ext) * (heads / head_ext);  // Head (0-11)
-    int d3 = item_ct1.get_local_id(2);                                 // Values (groups of 4)
-
-    sycl::float4 vals_arr;
-    sycl::float4 bias_arr;
-    sycl::float4 output_arr;
-    sycl::half2* vals_half = reinterpret_cast<sycl::half2*>(&vals_arr);
-    sycl::half2* bias_half = reinterpret_cast<sycl::half2*>(&bias_arr);
-    sycl::half2* output_half = reinterpret_cast<sycl::half2*>(&output_arr);
-
-    const sycl::float4* vals_vec = reinterpret_cast<const sycl::float4*>(vals);
-    const sycl::float4* bias_vec = reinterpret_cast<const sycl::float4*>(bias);
-    sycl::float4* output_vec = reinterpret_cast<sycl::float4*>(output);
-
-    vals_vec += (d0 * d0_stride * (item_ct1.get_group_range(0) / head_ext));
-    vals_vec += (d1 * d1_stride * (item_ct1.get_group_range(0) / head_ext));
-    vals_vec += (cnt * d1_stride);
-    vals_vec += (d2 * d2_stride);
-
-    bias_vec += (cnt * d1_stride);
-    bias_vec += (d2 * d2_stride);
-
-    output_vec += (cnt * d0_stride * item_ct1.get_group_range(2));
-    output_vec += (d1 * d2_stride);
-    output_vec += (d0 * d0_stride);
-    output_vec += (d2 * d2_out_stride);
-
-    bias_arr = bias_vec[d3];
-    vals_arr = vals_vec[d3];
-
-#if defined(__ACC_HALF__)
-    output_half[0] = vals_half[0] + bias_half[0];
-    output_half[1] = vals_half[1] + bias_half[1];
-    output_half[2] = vals_half[2] + bias_half[2];
-    output_half[3] = vals_half[3] + bias_half[3];
-#else
-    sycl::float2 bias_arr_f[4];
-    sycl::float2 vals_arr_f[4];
-#pragma unroll
-    for (int l = 0; l < 4; l++) {
-        bias_arr_f[l] = bias_half[l].convert<float>();
-        vals_arr_f[l] = vals_half[l].convert<float>();
-        vals_arr_f[l].x() += bias_arr_f[l].x();
-        vals_arr_f[l].y() += bias_arr_f[l].y();
-        output_half[l] = vals_arr_f[l].convert<sycl::half>();
-    }
-#endif
-    output_vec[d3] = output_arr;
-}
-
-void bias_add_transform_0213_v2(sycl::half* output,
-                                const sycl::half* vals,
-                                const sycl::half* bias,
-                                int hidden_dim,
-                                int seq_length,
-                                int heads,
-                                sycl::nd_item<3> item_ct1,
-                                sycl::float4* in_data)
-{
-    //__shared__ sycl::float4 in_data[3072];
-
-    int d0_stride = hidden_dim * seq_length;
-    int d1_stride = hidden_dim;
-    int d2_stride = hidden_dim / heads;
-    int iteration_stride = d1_stride * item_ct1.get_local_range(0);  // Hidden * 3 / 8
-    int batch_stride = d0_stride * item_ct1.get_local_range(0);      // Hidden * S * 3 / 8
-
-    int d0_out_stride = d0_stride;
-    int d1_out_stride = d2_stride;
-    int d2_out_stride = d2_stride * seq_length;
-
-    int d0 = item_ct1.get_group(2);      // Batch
-    int d1 = item_ct1.get_group(1);      // Sequence ID (0-127)
-    int cnt = item_ct1.get_local_id(0);  // item_ct1.get_group(0); Hidden count
-    int d2 = item_ct1.get_local_id(1);   // Head (0-11)
-    int d3 = item_ct1.get_local_id(2);   // Values (groups of 4)
-
-    sycl::float4 vals_arr[1];
-    sycl::float4 bias_arr[1];
-    sycl::float4 output_arr[1];
-    sycl::half2* vals_half = reinterpret_cast<sycl::half2*>(vals_arr);
-    sycl::half2* bias_half = reinterpret_cast<sycl::half2*>(bias_arr);
-    sycl::half2* output_half = reinterpret_cast<sycl::half2*>(output_arr);
-
-    const sycl::float4* vals_vec = reinterpret_cast<const sycl::float4*>(vals);
-    const sycl::float4* bias_vec = reinterpret_cast<const sycl::float4*>(bias);
-    sycl::float4* output_vec = reinterpret_cast<sycl::float4*>(output);
-
-    int iter_index = cnt * d1_stride + d2 * d2_stride + d3;
-    int input_offset = d0 * batch_stride + d1 * (iteration_stride << 1);
-    bias_arr[0] = bias_vec[iter_index];
-
-#pragma unroll
-    for (int iter = 0; iter < 2; iter++) {
-        int iter_id = iter * iteration_stride + iter_index;
-        vals_arr[0] = vals_vec[input_offset + iter_id];
-
-        output_half[0] = vals_half[0] + bias_half[0];
-        output_half[1] = vals_half[1] + bias_half[1];
-        output_half[2] = vals_half[2] + bias_half[2];
-        output_half[3] = vals_half[3] + bias_half[3];
-
-        in_data[iter_id] = output_arr[0];
-    }
-    item_ct1.barrier();
-
-    iteration_stride = item_ct1.get_local_range(0) * (item_ct1.get_local_range(1) >> 1);
-    int matrix_stride = (d0_out_stride * item_ct1.get_group_range(2));
-    int head_count = (d2 >> 1) + cnt * (item_ct1.get_local_range(1) >> 1);
-
-    int out_index = d0 * d0_out_stride + d1 * (d1_out_stride << 1) + d3 + (d2 % 2) * d2_stride;
-
-#pragma unroll
-    for (int iter = 0; iter < 2; iter++) {
-        int iter_row = (iter * iteration_stride) + head_count;
-        int iter_offset = (iter_row % item_ct1.get_local_range(1)) * d2_out_stride +
-                          (iter_row / item_ct1.get_local_range(1)) * matrix_stride;
-        output_vec[out_index + iter_offset] =
-            in_data[iter_row * d2_stride + d3 +
-                    (d2 % 2) * (d1_stride * item_ct1.get_local_range(0))];
-    }
-}
-
-// [B S C*H] - > C * [B A S N]
-template <>
-void launch_bias_add_transform_0213<float>(float* output,
-                                           const float* vals,
-                                           const float* bias,
-                                           int batch_size,
-                                           int seq_length,
-                                           int hidden_dim,
-                                           int heads,
-                                           sycl::queue* stream,
-                                           int trans_count)
-{
-    hidden_dim >>= 2;
-    int head_ext = (hidden_dim - 1) / MAX_THREADS + 1;
-
-    sycl::range<3> block_dim(1, (heads / head_ext), hidden_dim / heads);
-    sycl::range<3> grid_dim((trans_count * head_ext), seq_length, batch_size);
-
-    stream->submit([&](sycl::handler& cgh) {
-        cgh.parallel_for(
-            sycl::nd_range<3>(grid_dim * block_dim, block_dim), [=](sycl::nd_item<3> item_ct1) {
-                bias_add_transform_0213<float>(
-                    output, vals, bias, hidden_dim, seq_length, heads, head_ext, item_ct1);
-            });
-    });
-    // bias_add_transform_0213<float><<<grid_dim, block_dim, 0, stream>>>(
-    //     output, vals, bias, hidden_dim, seq_length, heads, head_ext);
-}
-
-template <>
-void launch_bias_add_transform_0213<bf16>(bf16* output,
-                                          const bf16* vals,
-                                          const bf16* bias,
-                                          int batch_size,
-                                          int seq_length,
-                                          int hidden_dim,
-                                          int heads,
-                                          sycl::queue* stream,
-                                          int trans_count)
-{
-    hidden_dim >>= 2;
-    int head_ext = (hidden_dim - 1) / MAX_THREADS + 1;
-
-    sycl::range<3> block_dim(1, (heads / head_ext), hidden_dim / heads);
-    sycl::range<3> grid_dim((trans_count * head_ext), seq_length, batch_size);
-
-    stream->submit([&](sycl::handler& cgh) {
-        cgh.parallel_for(
-            sycl::nd_range<3>(grid_dim * block_dim, block_dim), [=](sycl::nd_item<3> item_ct1) {
-                bias_add_transform_0213<bf16>(
-                    output, vals, bias, hidden_dim, seq_length, heads, head_ext, item_ct1);
-            });
-    });
-    // bias_add_transform_0213<float><<<grid_dim, block_dim, 0, stream>>>(
-    //     output, vals, bias, hidden_dim, seq_length, heads, head_ext);
-}
-
-template <>
-void launch_bias_add_transform_0213<sycl::half>(sycl::half* output,
-                                                const sycl::half* vals,
-                                                const sycl::half* bias,
-                                                int batch_size,
-                                                int seq_length,
-                                                int hidden_dim,
-                                                int heads,
-                                                sycl::queue* stream,
-                                                int trans_count)
-{
-    hidden_dim >>= 3;
-    if (hidden_dim > 128 || hidden_dim < 16) {
-        int head_ext = (hidden_dim - 1) / MAX_THREADS + 1;
-        sycl::range<3> block_dim(1, (heads / head_ext), hidden_dim / heads);
-        sycl::range<3> grid_dim((trans_count * head_ext), seq_length, batch_size);
-        stream->submit([&](sycl::handler& cgh) {
-            cgh.parallel_for(
-                sycl::nd_range<3>(grid_dim * block_dim, block_dim), [=](sycl::nd_item<3> item_ct1) {
-                    bias_add_transform_0213<sycl::half>(
-                        output, vals, bias, hidden_dim, seq_length, heads, head_ext, item_ct1);
-                });
-        });
-        // bias_add_transform_0213<sycl::half><<<grid_dim, block_dim, 0, stream>>>(
-        //     output, vals, bias, hidden_dim, seq_length, heads, head_ext);
-    } else {
-        sycl::range<3> block_dim(trans_count, heads, hidden_dim / heads);
-        sycl::range<3> grid_dim(1, seq_length / 2, batch_size);
-        stream->submit([&](sycl::handler& cgh) {
-            sycl::accessor<sycl::float4,
-                           1,
-                           sycl::access::mode::read_write,
-                           sycl::access::target::local>
-                data_block_acc_ct1(sycl::range<1>(3072), cgh);
-            cgh.parallel_for(sycl::nd_range<3>(grid_dim * block_dim, block_dim),
-                             [=](sycl::nd_item<3> item_ct1) {
-                                 bias_add_transform_0213_v2(output,
-                                                            vals,
-                                                            bias,
-                                                            hidden_dim,
-                                                            seq_length,
-                                                            heads,
-                                                            item_ct1,
-                                                            data_block_acc_ct1.get_pointer());
-                             });
-        });
-        // bias_add_transform_0213_v2<<<grid_dim, block_dim, 0, stream>>>(
-        //     output, vals, bias, hidden_dim, seq_length, heads);
-    }
-}
-
-template <typename T>
-void transform4d_0213(T* out,
-                      const T* in,
-                      int heads,
-                      int seq_length,
-                      int hidden_dim,
-                      int head_ext,
-                      sycl::nd_item<3> item_ct1);
-
-template <>
-void transform4d_0213<float>(float* out,
-                             const float* in,
-                             int heads,
-                             int seq_length,
-                             int hidden_dim,
-                             int head_ext,
-                             sycl::nd_item<3> item_ct1)
-{
-    int d0_stride = hidden_dim * seq_length;
-    int d1_stride = d0_stride / heads;
-    int d2_stride = hidden_dim / heads;
-
-    int d0_out_stride = d0_stride;
-    int d1_out_stride = d2_stride;
-    int d2_out_stride = hidden_dim;
-
-    int d0 = item_ct1.get_group(2);                                                         // Batch
-    int d1 = item_ct1.get_group(1) / ((seq_length - 1) / item_ct1.get_local_range(1) + 1);  // Head
-    int d2 = (item_ct1.get_local_id(1) + item_ct1.get_local_range(1) * item_ct1.get_group(1)) %
-             seq_length;
-    int cnt = item_ct1.get_group(0);
-    int d3 = item_ct1.get_local_id(2);  // Values (groups of 8)
-
-    if (d2 < seq_length) {
-        const sycl::float4* in_vec = reinterpret_cast<const sycl::float4*>(in);
-        sycl::float4* out_vec = reinterpret_cast<sycl::float4*>(out);
-
-        sycl::float4 vals_vec = in_vec[cnt * d0_stride * item_ct1.get_group_range(2) +
-                                       d0 * d0_stride + d1 * d1_stride + d2 * d2_stride + d3];
-        out_vec[d0 * d0_out_stride * item_ct1.get_group_range(0) + cnt * d2_out_stride +
-                d1 * d1_out_stride + d2 * d2_out_stride * item_ct1.get_group_range(0) + d3] =
-            vals_vec;
-    }
-}
-
-template <>
-void transform4d_0213<bf16>(bf16* out,
-                            const bf16* in,
-                            int heads,
-                            int seq_length,
-                            int hidden_dim,
-                            int head_ext,
-                            sycl::nd_item<3> item_ct1)
-{
-    int d0_stride = hidden_dim * seq_length;
-    int d1_stride = d0_stride / heads;
-    int d2_stride = hidden_dim / heads;
-
-    int d0_out_stride = d0_stride;
-    int d1_out_stride = d2_stride;
-    int d2_out_stride = hidden_dim;
-
-    int d0 = item_ct1.get_group(2);                                                         // Batch
-    int d1 = item_ct1.get_group(1) / ((seq_length - 1) / item_ct1.get_local_range(1) + 1);  // Head
-    int d2 = (item_ct1.get_local_id(1) + item_ct1.get_local_range(1) * item_ct1.get_group(1)) %
-             seq_length;
-    int cnt = item_ct1.get_group(0);
-    int d3 = item_ct1.get_local_id(2);  // Values (groups of 8)
-
-    if (d2 < seq_length) {
-        const sycl::ushort4* in_vec = reinterpret_cast<const sycl::ushort4*>(in);
-        sycl::ushort4* output_vec = reinterpret_cast<sycl::ushort4*>(out);
-
-        sycl::ushort4 vals_vec = in_vec[cnt * d0_stride * item_ct1.get_group_range(2) +
-                                        d0 * d0_stride + d1 * d1_stride + d2 * d2_stride + d3];
-
-        output_vec[d0 * d0_out_stride * item_ct1.get_group_range(0) + cnt * d2_out_stride +
-                   d1 * d1_out_stride + d2 * d2_out_stride * item_ct1.get_group_range(0) + d3] =
-            vals_vec;
-    }
-}
-
-template <>
-void transform4d_0213<sycl::half>(sycl::half* out,
-                                  const sycl::half* in,
-                                  int heads,
-                                  int seq_length,
-                                  int hidden_dim,
-                                  int head_ext,
-                                  sycl::nd_item<3> item_ct1)
-{
-    int d0_stride = hidden_dim * (seq_length / head_ext);
-    int d1_stride = hidden_dim;
-    int d2_stride = hidden_dim / heads;
-
-    int d0 = item_ct1.get_group(2);  // Batch
-    int d1 =
-        item_ct1.get_local_id(1) + (item_ct1.get_group(0) % head_ext) * (heads / head_ext);  // Head
-    int d2 = item_ct1.get_group(0) / head_ext;  // Sequence
-    int cnt = item_ct1.get_group(1);            // Hidden count
-    int d3 = item_ct1.get_local_id(2);          // Values (groups of 8)
-
-    const sycl::half4* in_vec = reinterpret_cast<const sycl::half4*>(in);
-    sycl::half4* out_vec = reinterpret_cast<sycl::half4*>(out);
-
-    in_vec += (cnt * d0_stride * item_ct1.get_group_range(2));
-    in_vec += (d0 * d0_stride);
-    in_vec += (d2 * d2_stride);
-    in_vec += (d1 * d2_stride * seq_length);
-
-    out_vec += (cnt * d1_stride);
-    out_vec += (d1 * d2_stride);
-    out_vec += (d0 * d0_stride * item_ct1.get_group_range(1));
-    out_vec += (d2 * d1_stride * item_ct1.get_group_range(1));
-
-    out_vec[d3] = in_vec[d3];
-}
-
-void transform4d_0213_v2(sycl::half* out,
-                         const sycl::half* in,
-                         int heads,
-                         int seq_length,
-                         int hidden_dim,
-                         sycl::nd_item<3> item_ct1,
-                         sycl::float4* in_data)
-{
-    //__shared__ sycl::float4 in_data[3072];
-
-    int d0_stride = hidden_dim * seq_length;
-    int d1_stride = hidden_dim;
-    int d2_stride = hidden_dim / heads;
-
-    int d0 = item_ct1.get_group(2);      // Batch
-    int d1 = item_ct1.get_local_id(1);   // Head
-    int d2 = item_ct1.get_group(1);      // Sequence
-    int cnt = item_ct1.get_local_id(0);  // Hidden count
-    int d3 = item_ct1.get_local_id(2);   // Values (groups of 8)
-
-    const sycl::float4* in_vec = reinterpret_cast<const sycl::float4*>(in);
-    sycl::float4* out_vec = reinterpret_cast<sycl::float4*>(out);
-
-    int input_offset = d0 * d0_stride + d2 * (d2_stride << 1) + d3 + (d1 % 2) * d2_stride;
-    int head_count = (d1 >> 1) + cnt * (item_ct1.get_local_range(1) >> 1);
-    int iteration_stride = item_ct1.get_local_range(0) * (item_ct1.get_local_range(1) >> 1);
-    int matrix_stride = (d0_stride * item_ct1.get_group_range(2));
-
-#pragma unroll
-    for (int iter = 0; iter < 2; iter++) {
-        int iter_row = iter * iteration_stride + head_count;
-        int iter_offset = (iter_row % item_ct1.get_local_range(1)) * d2_stride;
-
-        in_data[d3 + iter_offset +
-                (iter_row / item_ct1.get_local_range(1) + (d1 % 2) * item_ct1.get_local_range(0)) *
-                    d1_stride] = in_vec[input_offset + iter_offset * seq_length +
-                                        (iter_row / item_ct1.get_local_range(1)) * matrix_stride];
-    }
-    item_ct1.barrier();
-
-    iteration_stride = d1_stride * item_ct1.get_local_range(0);
-    int iter_index = cnt * d1_stride + d1 * d2_stride + d3;
-    int output_offset = d0 * d0_stride * item_ct1.get_local_range(0) + d2 * (iteration_stride << 1);
-
-#pragma unroll
-    for (int iter = 0; iter < 2; iter++) {
-        int iter_id = iter * iteration_stride + iter_index;
-        out_vec[output_offset + iter_id] = in_data[iter_id];
-    }
-}
-
-// 3 * [B A S N] - > [B S C*H]
-template <>
-void launch_transform4d_0213<float>(float* out,
-                                    const float* in,
-                                    int batch_size,
-                                    int heads,
-                                    int seq_length,
-                                    int hidden_dim,
-                                    sycl::queue* stream,
-                                    int trans_count)
-{
-    hidden_dim >>= 2;
-    sycl::range<3> grid_dims(trans_count, heads * ((seq_length - 1) / 8 + 1), batch_size);
-    sycl::range<3> block_dims(1, 8, hidden_dim / heads);
-    stream->submit([&](sycl::handler& cgh) {
-        cgh.parallel_for(
-            sycl::nd_range<3>(grid_dims * block_dims, block_dims), [=](sycl::nd_item<3> item_ct1) {
-                transform4d_0213<float>(out, in, heads, seq_length, hidden_dim, 1, item_ct1);
-            });
-    });
-    // transform4d_0213<float>
-    //     <<<grid_dims, block_dims, 0, stream>>>(out, in, heads, seq_length,
-    //     hidden_dim, 1);
-}
-
-template <>
-void launch_transform4d_0213<bf16>(bf16* out,
-                                   const bf16* in,
-                                   int batch_size,
-                                   int heads,
-                                   int seq_length,
-                                   int hidden_dim,
-                                   sycl::queue* stream,
-                                   int trans_count)
-{
-    hidden_dim >>= 2;
-    sycl::range<3> grid_dims(trans_count, heads * ((seq_length - 1) / 8 + 1), batch_size);
-    sycl::range<3> block_dims(1, 8, hidden_dim / heads);
-    stream->submit([&](sycl::handler& cgh) {
-        cgh.parallel_for(
-            sycl::nd_range<3>(grid_dims * block_dims, block_dims), [=](sycl::nd_item<3> item_ct1) {
-                transform4d_0213<bf16>(out, in, heads, seq_length, hidden_dim, 1, item_ct1);
-            });
-    });
-    // transform4d_0213<float>
-    //     <<<grid_dims, block_dims, 0, stream>>>(out, in, heads, seq_length,
-    //     hidden_dim, 1);
-}
-
-template <>
-void launch_transform4d_0213<sycl::half>(sycl::half* out,
-                                         const sycl::half* in,
-                                         int batch_size,
-                                         int heads,
-                                         int seq_length,
-                                         int hidden_dim,
-                                         sycl::queue* stream,
-                                         int trans_count)
-{
-    hidden_dim >>= 3;
-    if (hidden_dim > 128 || hidden_dim < 16) {
-        int head_ext = (hidden_dim - 1) / MAX_THREADS + 1;
-        sycl::range<3> grid_dims((seq_length * head_ext), trans_count, batch_size);
-        sycl::range<3> block_dims(1, (heads / head_ext), hidden_dim / heads);
-        stream->submit([&](sycl::handler& cgh) {
-            cgh.parallel_for(sycl::nd_range<3>(grid_dims * block_dims, block_dims),
-                             [=](sycl::nd_item<3> item_ct1) {
-                                 transform4d_0213<sycl::half>(
-                                     out, in, heads, seq_length, hidden_dim, head_ext, item_ct1);
-                             });
-        });
-    } else {
-        sycl::range<3> grid_dims(1, seq_length / 2, batch_size);
-        sycl::range<3> block_dims(trans_count, heads, hidden_dim / heads);
-        stream->submit([&](sycl::handler& cgh) {
-            sycl::accessor<sycl::float4,
-                           1,
-                           sycl::access::mode::read_write,
-                           sycl::access::target::local>
-                data_block_acc_ct1(sycl::range<1>(3072), cgh);
-            cgh.parallel_for(sycl::nd_range<3>(grid_dims * block_dims, block_dims),
-                             [=](sycl::nd_item<3> item_ct1) {
-                                 transform4d_0213_v2(out,
-                                                     in,
-                                                     heads,
-                                                     seq_length,
-                                                     hidden_dim,
-                                                     item_ct1,
-                                                     data_block_acc_ct1.get_pointer());
-                             });
-        });
-        // transform4d_0213_v2<<<grid_dims, block_dims, 0, stream>>>(
-        //     out, in, heads, seq_length, hidden_dim);
-    }
-}
diff --git a/intel_extension_for_deepspeed/op_builder/fused_adam.py b/intel_extension_for_deepspeed/op_builder/fused_adam.py
index c56e802..70d2352 100644
--- a/intel_extension_for_deepspeed/op_builder/fused_adam.py
+++ b/intel_extension_for_deepspeed/op_builder/fused_adam.py
@@ -23,7 +23,7 @@ class FusedAdamBuilder(SYCLOpBuilder):
     def include_paths(self):
         return [
             sycl_kernel_include('csrc/includes'),
-            sycl_kernel_include('csrc/adam'), 'csrc/includes'
+            sycl_kernel_include('csrc/adam'),
         ]
 
     def cxx_args(self):
diff --git a/intel_extension_for_deepspeed/op_builder/quantizer.py b/intel_extension_for_deepspeed/op_builder/quantizer.py
index b6d5ad5..155dc42 100644
--- a/intel_extension_for_deepspeed/op_builder/quantizer.py
+++ b/intel_extension_for_deepspeed/op_builder/quantizer.py
@@ -1,3 +1,8 @@
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 from .builder import SYCLOpBuilder, sycl_kernel_path, sycl_kernel_include
 
 
@@ -13,7 +18,15 @@ class QuantizerBuilder(SYCLOpBuilder):
         return f'deepspeed.ops.quantizer.{self.NAME}_op'
 
     def sources(self):
-        return []
+        return [
+            sycl_kernel_path('csrc/quantization/pt_binding.cpp'),
+            sycl_kernel_path('csrc/quantization/fake_quantizer.dp.cpp'),
+            sycl_kernel_path('csrc/quantization/quantize.dp.cpp'),
+            sycl_kernel_path('csrc/quantization/quantize_intX.dp.cpp'),
+            sycl_kernel_path('csrc/quantization/dequantize.dp.cpp'),
+            sycl_kernel_path('csrc/quantization/swizzled_quantize.dp.cpp'),
+            sycl_kernel_path('csrc/quantization/quant_reduce.dp.cpp'),
+        ]
 
     def include_paths(self):
-        return []
+        return [sycl_kernel_include('csrc/includes')]
diff --git a/intel_extension_for_deepspeed/op_builder/transformer.py b/intel_extension_for_deepspeed/op_builder/transformer.py
deleted file mode 100644
index 5793a1f..0000000
--- a/intel_extension_for_deepspeed/op_builder/transformer.py
+++ /dev/null
@@ -1,47 +0,0 @@
-"""
-Copyright 2020 The Microsoft DeepSpeed Team
-"""
-from .builder import SYCLOpBuilder, sycl_kernel_path, sycl_kernel_include
-
-
-class TransformerBuilder(SYCLOpBuilder):
-    BUILD_VAR = "DS_BUILD_TRANSFORMER"
-    NAME = "transformer"
-
-    def __init__(self, name=None):
-        name = self.NAME if name is None else name
-        super().__init__(name=name)
-
-    def absolute_name(self):
-        return f'deepspeed.ops.transformer.{self.NAME}_op'
-
-    def extra_ldflags(self):
-        return super().extra_ldflags()
-
-    def sources(self):
-        return [
-            sycl_kernel_path('csrc/transformer/onednn_wrappers.dp.cpp'),
-            sycl_kernel_path(
-                'csrc/transformer/ds_transformer_sycl.dp.cpp'),
-            sycl_kernel_path('csrc/transformer/onemkl_wrappers.dp.cpp'),
-            sycl_kernel_path('csrc/transformer/transform_kernels.dp.cpp'),
-            sycl_kernel_path('csrc/transformer/ds_gelu_sycl.dp.cpp'),
-            sycl_kernel_path('csrc/transformer/gelu_kernels.dp.cpp'),
-            sycl_kernel_path('csrc/transformer/ds_dropout_sycl.dp.cpp'),
-            sycl_kernel_path('csrc/transformer/dropout_kernels.dp.cpp'),
-            sycl_kernel_path(
-                'csrc/transformer/ds_feedforward_sycl.dp.cpp'),
-            sycl_kernel_path(
-                'csrc/transformer/ds_layer_reorder_sycl.dp.cpp'),
-            sycl_kernel_path('csrc/transformer/ds_normalize_sycl.dp.cpp'),
-            sycl_kernel_path('csrc/transformer/normalize_kernels.dp.cpp'),
-            sycl_kernel_path('csrc/transformer/ds_softmax_sycl.dp.cpp'),
-            sycl_kernel_path('csrc/transformer/softmax_kernels.dp.cpp'),
-            sycl_kernel_path(
-                'csrc/transformer/ds_stridedbatchgemm_sycl.dp.cpp'),
-            sycl_kernel_path('csrc/transformer/general_kernels.dp.cpp')
-        ]
-
-    def include_paths(self):
-        includes = [sycl_kernel_include('csrc/includes'), 'csrc/includes']
-        return includes
diff --git a/intel_extension_for_deepspeed/op_builder/transformer_inference.py b/intel_extension_for_deepspeed/op_builder/transformer_inference.py
index 889b851..6663cc2 100755
--- a/intel_extension_for_deepspeed/op_builder/transformer_inference.py
+++ b/intel_extension_for_deepspeed/op_builder/transformer_inference.py
@@ -1,3 +1,8 @@
+# Copyright (c) Microsoft Corporation.
+# SPDX-License-Identifier: Apache-2.0
+
+# DeepSpeed Team
+
 from .builder import SYCLOpBuilder, sycl_kernel_path, sycl_kernel_include
 
 
@@ -17,16 +22,22 @@ class InferenceBuilder(SYCLOpBuilder):
 
     def sources(self):
         return [
-            sycl_kernel_path('csrc/transformer/inference/csrc/softmax.cpp'),
             sycl_kernel_path('csrc/transformer/inference/csrc/pt_binding.cpp'),
-            sycl_kernel_path('csrc/transformer/inference/csrc/gelu.cpp'),
-            sycl_kernel_path('csrc/transformer/inference/csrc/inference_onednn_wrappers.cpp'),
-            sycl_kernel_path('csrc/transformer/inference/csrc/inference_onemkl_wrappers.cpp'),
-            sycl_kernel_path('csrc/transformer/inference/csrc/layer_norm.cpp'),
-            sycl_kernel_path('csrc/transformer/inference/csrc/pointwise_ops.cpp'),
+            sycl_kernel_path('csrc/transformer/inference/csrc/gelu.dp.cpp'),
+            sycl_kernel_path('csrc/transformer/inference/csrc/relu.dp.cpp'),
+            sycl_kernel_path('csrc/transformer/inference/csrc/layer_norm.dp.cpp'),
+            sycl_kernel_path('csrc/transformer/inference/csrc/rms_norm.dp.cpp'),
+            sycl_kernel_path('csrc/transformer/inference/csrc/softmax.dp.cpp'),
+            sycl_kernel_path('csrc/transformer/inference/csrc/dequantize.dp.cpp'),
+            sycl_kernel_path('csrc/transformer/inference/csrc/apply_rotary_pos_emb.dp.cpp'),
+            sycl_kernel_path('csrc/transformer/inference/csrc/transform.dp.cpp'),
+            sycl_kernel_path('csrc/transformer/inference/csrc/pointwise_ops.dp.cpp'),
         ]
 
     def include_paths(self):
-        includes = [sycl_kernel_include('csrc/transformer/inference/includes'), 'csrc/transformer/inference/includes']
+        includes = [
+            sycl_kernel_include('csrc/transformer/inference/includes'),
+            sycl_kernel_include('csrc/includes'),
+        ]
         return includes
 
diff --git a/intel_extension_for_deepspeed/xpu_accelerator.py b/intel_extension_for_deepspeed/xpu_accelerator.py
index 2848022..be9dc59 100644
--- a/intel_extension_for_deepspeed/xpu_accelerator.py
+++ b/intel_extension_for_deepspeed/xpu_accelerator.py
@@ -220,7 +220,7 @@ class XPU_Accelerator(DeepSpeedAccelerator):
         else:
             return False
 
-    # create an instance of op builder and return, name specified by class_name 
+    # create an instance of op builder and return, name specified by class_name
     def create_op_builder(self, op_name):
         builder_class = self.get_op_builder(op_name)
         if builder_class != None:
@@ -229,7 +229,7 @@ class XPU_Accelerator(DeepSpeedAccelerator):
 
     # return an op builder class, name specified by class_name
     def get_op_builder(self, class_name):
-        from intel_extension_for_deepspeed.op_builder import CPUAdagradBuilder, CPUAdamBuilder, FusedAdamBuilder, QuantizerBuilder, TransformerBuilder, UtilsBuilder, InferenceBuilder, FlashAttentionBuilder, AsyncIOBuilder
+        from intel_extension_for_deepspeed.op_builder import CPUAdagradBuilder, CPUAdamBuilder, FusedAdamBuilder, QuantizerBuilder, UtilsBuilder, InferenceBuilder, FlashAttentionBuilder, AsyncIOBuilder
         from deepspeed.ops.op_builder.sparse_attn import SparseAttnBuilder
 
         if class_name == "AsyncIOBuilder":
@@ -244,8 +244,6 @@ class XPU_Accelerator(DeepSpeedAccelerator):
             return QuantizerBuilder
         elif class_name == "SparseAttnBuilder":
             return SparseAttnBuilder
-        elif class_name == "TransformerBuilder":
-            return TransformerBuilder
         elif class_name == "UtilsBuilder":
             return UtilsBuilder
         elif class_name == "InferenceBuilder":
